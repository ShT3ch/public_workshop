__label__boundary-conditions __label__discretization PRON want to solve the problem below  beginequation   beginalign   eta udelta u  ampf   amptextin  omega  endaligned   endequation   where  omega01time  01  PRON boundary be like this  fracpartial upartial n   0  if  y0  or  1   and  u0  if  x0  or  1 use ghost node can handle the low order discretization of the neumann boundary  but the problem that PRON be face be that PRON want to include the four corner point in  the neumann boundary not in the dirichlet one  and this cause PRON problem since PRON can not use the interior equation on the corner   PRON hope PRON question be clear   mathematically speak  PRON can only pose boundary condition on part of the boundary that have nonzero  d1  dimensional measure  in other word  PRON do not make sense to say that PRON want a particular boundary condition to hold at individual point  whether PRON do or do not do anything at a corner point  PRON will nevertheless converge to the same solution  in other word  just do not bother with figure out whether or not PRON have dirichlet or neumann condition at these corner  
__label__machine-learning __label__supervised-learning __label__topic-model __label__unsupervised-learning __label__lda PRON goal be to get a smartphone name from twitter  so this be what PRON follow   1 PRON extract 100 k tweet use the keyword  smartphone    2 PRON apply lda after apply ngram tokenization and cleaning  so  PRON get noisy result such asgiveway  international  apple  iphone6  samsungs5new …   3 PRON filter the result use a smartphone list  iphone6  samsungs5iphone4s  …  extract from dbpedia in order to remove the noise   be what PRON do supervised or unsupervised machine learning   the basic idea behind supervised and unsupervised learning be   in supervised learning  one possess a training datum set that include the desire target output feature  example of popular supervised learning algorithm be linear regression  and support vector classification etc   in unsupervised learning  PRON do not possess a training datum set with the target output feature ie PRON do not know what PRON output or result look like  an example be cluster  there be really good explanation out there about this   lda be an unsupervised learning algorithm and the process PRON describe can be classify as unsupervised learning  the filtering step that PRON describe do not make the algorithm supervise because the target smartphone have not be directly correlate to the training datum and therefore be only serve as a guide to restrict the cardinality of the target variable  
__label__optimization PRON have a non  function  not in close form  that take in a few parameter  about 20  and return a real value  a few of these parameter be discrete while other be continuous  some of these parameter can only be choose from a finite space of value   since PRON do not have the function in close form  PRON can not use any gradient base method  however  the discrete nature and the boxed constraint on a few of those parameter restrict even the number of derivative free optimization technique at PRON disposal  PRON be wonder what be the option in term of optimization method that PRON can use   bayesian optimization be a principled way of sequentially find the extremum of black  box function  what be more  there a numerous software package that make PRON easy  such as bayesopt and moe  another flexible bayesian framework that PRON can use for optimization be gaussian process  global optimisation with gaussian processes 
__label__neural-network __label__word2vec __label__word-embeddings PRON have to use a neural network to classify whether some review of hotel be deceptive or truthful  PRON also have to use pre  train word embedding to fed the neural network  so PRON can use word2vec to get the word vector from a way large dataset of hotel review  however  word2vec give the possibility to use continuous bag  of  word and continuous skip  gram model for this task  which one would be generally good for this specific task   PRON think this article give general idea of pro  con between cbow and skip gram   link  accord to mikolov   skip  gram  work well with small amount of the training datum   represent well even rare word or phrase   cbow  several time faster  to train than the skip  gram  slightly good accuracy for the frequent  word   so without look at datum  assume that PRON have large amount of datum  cbow may be PRON bet  however  if PRON have label for  deceptive  and  truthful   PRON will be good to train both model  and use whichev outperform via cross  validation  
__label__orange be there a way to weight instance base upon the value of a feature   for instance for make a sample better resemble a population on certain parameter   try read up on loss cost function  sample that be close to PRON model will be assign a small cost while other a high one   httpsenwikipediaorgwikilossfunction  in orange documentation  loading PRON data  PRON say to prefix PRON column with w  to be mark as instance weight column  
__label__c++ __label__mpi the 30 version of the mpi standard formally delete the c interface  PRON be previously deprecate    while implementation may still support PRON  feature that be new in mpi3 do not have a c interface define in the mpi standard   see httpblogsciscocomperformancethempicbindingswhathappenedandwhy for more information   the motivation for remove the c interface from mpi be that PRON have no significant value over the c interface   there be very few difference other than  sg  and many feature that c user be accustom to be not employ  eg automatic type determination via template    as someone who participate in the mpi forum and work with a number of c project that have implement PRON own c interface to the mpi c function  PRON would like to know what be the desirable feature of a c interface to mpi   while PRON commit to nothing  PRON would be interested in see the implementation of a standalone mpi c interface that meet the need of many user   and yes  PRON be familiar with boostmpi  httpwwwboostorgdoclibs1540dochtmlmpihtml  but PRON only support mpi1 feature and the serialization model would be extremely difficult to support for rma   one c interface to mpi that PRON like be that of elemental  httpsgithubcompoulsonelementalblobmastersrccoreimportsmpicpp  so perhaps people can provide some pro and con wrt  that approach   in particular  PRON think mpimap solve an essential problem   to get the ball roll  here be two of PRON need   the interface should be able to eliminate redundant or unnecessary argument  eg mpiinplace   the interface should auto  detect build  in datatype ala elemental s mpimap   if  whenever possible  user  define datatype should be construct for class   let PRON first answer why PRON think c interface to mpi have generally not be overly successful  have think about the issue for a good long time when try to decide whether PRON should just use the standard c binding of mpi or building on something at high level   when PRON look at real  world mpi code  say  petsc  or in PRON case deal  ii   one find that maybe surprisingly  the number of mpi call be not actually very large  for example  in the 500k line of deal  ii  there be only 100 mpi call  a consequence of this be that the pain involve in use low  level interface such as the mpi c binding  be not too large  conversely  one would not gain all that much by use high level interface   PRON second observation be that many system have multiple mpi library instal  different mpi implementation  or different version   this pose a significant difficulty if PRON want to use  say  boostmpi that do not just consist of header file  either there need to be multiple installation of this package as well  or one need to build PRON as part of the project that use boostmpi  but that be a problem in PRON again  give that boost use PRON own build system  which be unlike anything else    so PRON think all of this have conspire against the current crop of c interface to mpi  the old mpi c binding do not offer any advantage  and external package have difficulty with the real world   this all say  here be what PRON think would be the killer feature PRON would like to have from a high  level interface   PRON should be generic  have to specify the datum type of a variable be decidedly not clike  of course  PRON also lead to error  elemental s mpimap class would already be a nice first step  though PRON can not figure out why the heck the mpimaptype variable be not static const  so that PRON can be access without create an object    PRON should have facility for stream arbitrary datum type   operations that require an mpiop argument  eg  reduction  should integrate nicely with c s stdfunction interface  so that PRON be easy to just pass a function pointer  or a lambda   rather than have to clumsily register something   boostmpi actually satisfy all of these  PRON think if PRON be a header  only library  PRON would be a lot more popular in practice  PRON would also help if PRON support post  mpi 10 function  but let PRON be honest  this cover most of what PRON need most of the time   personally  PRON do not really mind call long c  style function for the exact reason wolfgang mention  there be really few place PRON need to call PRON and even then  PRON almost always get wrap around by some high  level code   the only thing that really bother PRON with c  style mpi be custom datatype and  to a less degree  custom operation  because PRON use PRON less often   as for custom datatype  PRON would say that a good c interface should be able to support generic and efficient way of handle this  most probably through serialization  this be of course the route that boostmpi have take  which if PRON be careful  be a big time saver   as for boostmpi have extra dependency  particularly boostserialization which PRON be not header  only   PRON have recently come across a header  only c serialization library call cereal which seem promising  grant PRON require a c11 compliant compiler  PRON may worth look into and use PRON as a base for something similar to boostmpi   PRON list in no particular order of preference  the interface should   be header only  without any dependency but  ltmpihgt   and the standard library   be generic and extensible   be non  block only  if PRON want to block  then block explicitly  not by default    allow continuation  base chain of non  block operation   support extensible and efficient serialization  boost  fusion like  such that PRON work with rma    have zero abstraction penalty  ie be at least as fast as the c interface    be safe  the destructor of a non  ready future be call    stdterminate     have a strong debug mode with ton of assertion   extremely type  safe  no more int  void  for everything  heck PRON want tag to be type     PRON should work with lambda  eg all reduce  lambda    use exception consistently as error  reporting and error  handle mechanism  no more error code  no more function output argument     mpi  io should offer a non  block i  o interface in the style of boost  afio   and just follow good modern c interface design practice  define regular type  non  member non  friend function  play well with move semantic  support range operation     extras   allow PRON to choose the executor of the mpi environment  that is  which thread pool PRON use  right now PRON can have application with a mix of openmp  mpi  cuda  and tbb  all at the same time  where each run  time think PRON own the environment and thus ask the operating system for thread every time PRON feel like PRON  seriously   use the stl  and boost  name convention  why  every c programmer know PRON   PRON want to write code like this   auto buffer  sometnorank    auto future  gathercomm  rootcomm   myoffset  buffer   thenamp         when the gather be finish  this lambda will  execute at the root node  and perform an expensive operation  there asynchronously  compute datum require for load  redistribution  whose result be broadcast to the rest  of the communicator    return broadcastcomm  rootcomm   buffer      thenamp         when broadcast be finish  this lambda execute  on all process in the communicator  perform an expensive  operation asynchronously  redistribute the load   maybe use non  blocking point  to  point communication     return dosomethingwithbuffer      thenampauto result      finally perform a reduction on the result to check  everything go fine    return allreducecomm  rootcomm   result      auto acc  auto v   return acc  ampamp  v        thenampauto result      check the result at every process    if  result   return    PRON be do     else   rootonly       writesomeerrorlog        throw someexception           here nothing have happen yet        lot and lot of unrelated code that can execute concurrently  and overlap with communication       when PRON now call futureget   PRON will block  on the whole chain  which may have finish by then        futureget     think how one could chain all this operation use mpic s request  PRON would have to test at multiple  or every single  intermediate step through a whole lot of unrelated code to see if PRON can advance PRON chain without block   the github project easylambda provide a high level interface to mpi with c14   PRON think the project have similar goal and PRON will give some idea on thing that can be and be be do in this area by use modern c  guide other effort as well as easylambda PRON   the initial benchmark on performance and line of code have show promising result   follow be a short description of feature and interface PRON provide   the interface be base on datum flow programming and functional list operation that provide inherent parallelism  the parallelism be express as property of a task  the process allocation and datum distribution for the task can be request with a prll   property  there be good number of example in the webpage and code  repository that include lammps molecular dynamic post processing  explicit finite difference solution to heat equation  logistic regression etc  as an example the heat diffusion problem discuss in the article hpc be die  can be express in 20 line of code   PRON hope PRON be fine to give link rather than add more detail and example code here   disclamer  PRON be the author of the library  PRON believe PRON be not do any harm in hop to get a constructive feedback on the current interface of easylambda that may be advantageous to easylambda and any other project that pursue similar goal  
__label__machine-learning __label__neural-network __label__deep-learning __label__cnn PRON have a very fundamental question on what cnn s actually be   PRON understand fully the training process as to take a bunch of image  start with random filter  convolve  activate  calculate loss  back propagate and learn weight  fully understand   but recently PRON come across this line on slack  cnn s can act as a frequency filter as well  for example  a blur be a low  pass filter and PRON can be implement as convolution with fix weight   please explain   can not understand this at all   PRON suspect this be refer to kernel that be use in image processing for a variety of task  without see the conversation  this be PRON interpretation   the idea of a kernel in image processing be that PRON take a grid  maybe 3x3  and apply PRON to all of the location in an image  the way PRON apply PRON be by place PRON at a pixel location  then  the 9 pixel cover by the kernel be multiply by PRON weight and sum  that value be place at that pixel location in the new image  the picture below show PRON nicely  source  PRON can see how this be very much like how a convolutional filter in a cnn be apply  a set of example kernel be list on wikipedia  these kernel perform action such as blur and edge detection  so if PRON have a filter in PRON cnn with those weight  PRON would perform the same action   as PRON say  cnn convolf  in signal process the convolution be with continuous datum  here PRON be with discrete value   this article in quora explain  how the mathematic expression in neural network form the same equation than that of two function in mathematic   mathematically  apply a gaussian blur to an image be the same as convolve the image with a gaussian function  blur in wikipedia  gaussian function be high on low value so PRON kinda be a low pass filter when apply with convolution   if PRON have get the point of PRON question  PRON mean that convolutional net can learn different filter  PRON may train a net with random initialization and after training PRON can find learn  filter which be familiar to PRON or not  PRON may see that PRON model have learn sobel filter which be a high pass filter or maybe PRON may see that the weight which be learn be exactly the same as a mean filter which be a low pass filter  in all case network try to learn filter that help PRON find appropriate feature of the input  the learn filter can be anything  PRON may be know to PRON or not  so  yes  PRON can learn all kind of feature like low pass and high pass filter  also for visualization purpose to fully figure out what be go on inside PRON PRON recommend PRON take a look at here   hopefully this article image filtering in the frequency domain can help PRON understand the concept of convolution  image and frequency filtering 
__label__machine-learning __label__clustering __label__unsupervised-learning for some  8000  customer profile  in addition to a data  set  PRON have two kind of score available   type 1 score range from  0  to  1  and give the prediction probability of that profile belong to a class  the distribution of this score be severely skewed  as show  type 2 score range from  5  to  5  and be a risk score for the profile obtain use the pridit method on the actual datum  the distribution of this score be fairly normal  the objective be to bin the profile into some  n bin use the aforementioned information  the bin should represent various degree of risk associate with that profile   PRON first thought be to cluster the profile base on the two score available to PRON  however no meaningful cluster be observe   the next idea be to sort the datum on type 1 score and then cut on the point where there be a sudden change in slope  which seem like a good idea at first but here be what the plot for type 1 score look like   what PRON be think now be to transform type 1 score and type 2 score together to give some pseudo  normal type of distribution  hop that some kind of discretization may give the correct bin   PRON question be   how can PRON transform the two score into a pseudo  normal distribution   what would be a good way to approach the above problem   PRON often be appropriate to use quantile bin   ie to get four bin  choose the small 25   then up to the median  then the q75   
__label__machine-learning __label__neural-network __label__reference-request let PRON say that PRON want to generate a cryptographic key base on PRON hand  PRON be use hand just as an example   what PRON could do be PRON scan PRON hand  so that the friction ridge be apparent  and then use a key derivation algorithm  problem be that if PRON want to get the same hash  PRON have to either store the scan somewhere  or be able to produce the exact same scan  which be not very feasible   what if instead  PRON use a machine learn algorithm that take a scan of a human hand and produce some datum  subject to the following constraint   if the same human hand be scan twice  produce the same datum  if different human hand be scan  produce different datum  additionally  PRON should be hard to predict valid output of the neural net  other than brute force  so an adversary would either need to be able to generate human hand scan image  or just feed in random datum    one way PRON think of do this be to scan a bunch of hand multiple time  and then train a neural network in such a way that PRON be punish if PRON give different value for hand from the same person  or if two human get the same hand  also  PRON could have another neural network try to predict whether something be an output of the first one  and the first network be punish if the second one can do so well   PRON question be  as this problem be study before  not necessarily with hand  but produce the same output if two input satisfy some equivalence relation    PRON be describe a classification problem  where each  class  be a person  PRON be try to learn the handperson mapping   have two input map to the same output if PRON be similar enough be exactly what PRON mean to  classify  an input   the way PRON be frame PRON question match PRON perfectly with a basic task of machine learning  multi  class classification   PRON have no idea if hand recognition software actually work this way  PRON would need to learn PRON hand from a single example or a very few  so PRON expect there be some 1shot learn sorcery in the mix when use  in the wild    edit  the adversarial part be particularly interesting  there be technique for train neural network to be robust to attack  for example PRON may read about generative adversarial networks  where a second network feed input into the first to try to trick PRON  
__label__algorithms __label__efficiency PRON want to calculate the wigner quasiprobability distribution function of a particular wavefunction  the definition suggest a few straightforward way of calculate PRON  but PRON be wonder if there be something more efficient than the naive approach of either directly evaluate the integral or take the fft of the autocorrelation   
__label__r custid  totprod  totrev  lasttran  totaltran  1  1002  13  146525  11232011  4  2  1003  2  35325  112011  1  3  1004  12  24665  11252011  12  above be the retail dataset of customer  PRON need to calculate recencya new attribute when the customer be last see  from last transactionlasttran   in term of monthie  PRON purchase in last 3 month or last 6 month or may be one year in r please suggest   try this  as  posixctsys  date    format   ymd   tzuct    as  posixct11252011   format    mdytzuct    more vectorized operation in PRON situation  as  posixctsys  date    format   ymd   tzuct    as  posixctlasttran  format    mdytzuct   
__label__approximation-algorithms __label__polynomials PRON be play with the idea that a sine function be periodic  but even within one period there be symmetry  namely the second fourth of a period be the mirror image of the first fourth and the second half be minus the first half  so if PRON could approximate the sine function from  0  to  pi2   then PRON would have enough to approximate PRON everywhere for real value   for the approximation PRON want to use a polynomial of order  n  and PRON want find PRON such that the polynomial minimiz the error between PRON and sine between  0  and  pi2 therefore PRON come up with the follow problem formulation      minveca   int0fracpi2    leftsinx   sumn0nan xnright2 dx tag1      this can be solve mostly analytically by set the derivative of   1 with respect to each  an equal to zero      int0fracpi2    xm sumn0nan xndx   int0fracpi2    xm sinx  dx   quad forall  m01dot  n tag2         sumn0nfracan leftfracpi2rightnm1nm1    leftbeginarrayl  l  mleftfracpi2rightm1sumn0fracm12fracleftfrac4pi2rightnm2n1      amp  textif  m text  odd   mleft1fracm22leftfracpi2rightm1sumn0fracm22fracleftfrac4pi2rightnm2n1right    amp  textelseendarrayrighttag3      solve this problem then turn into solve a matrix equation   ax  b    but here be PRON question  if PRON increase  n PRON would expect get small and small error up to some point when the computational error of calculate the polynomial coefficient get to big  mainly probably due to the sum of the right hand side of   3 but how could these computational error be decrease  namely initially the error get small  but beyond a polynomial of order 8 the error seem to grow exponentially as can be see in the following figure   the error of the 8th order polynomial be at most  2152time 108  while an 7th8th order taylor polynomial at  x0  have maximum error of  1569times 104   PRON just wonder if PRON could do something to increase the order and decrease the error to something close to  1015  but even use the variable  precision arithmetic in matlab do not seem to affect the error at all   by use orthonormal polynomial basis function  then PRON do not need to solve a linear set of equation in order to get the polynomial coefficient  similar to the fourier series coefficient  because for large  n the matrix  of the linear set of equation  will get ill  condition   PRON be at least require that the basis function be orthogonal  but normalized function help with the ease of approximation of a function  in order to get slightly simple basis function  without transcendental coefficient induce by the upperbound of the interval of  pi2   the interval will be change into   01  such that the function PRON want to approximate become  sinleftfracpi2xright  two function be orthogonal when there inner product be equal to zero  this inner product for this interval can be write as      leftltfx   gxrightgt   int0  1  fx  gx  dt   tag1      and a function  fx be normalize when  leftltfx   fxrightgt1  in order to get similar converge of the approximation each next orthonormal polynomial basis function will contain a high power of  x the first normalized basis function of order zero will then become      p0x   1  tag2      each follow orthonormal polynomial basis function can then be find with      pkx   xk  sumi0k1leftltxk  pixrightgt  pix   tag3a         pkx   fracpkxsqrtleftltpkx   pkxrightgt tag3b      the next three basis function for example then become      beginarray     p1x   sqrt3  left1  2 xright    p2x   sqrt5  left1  6 x  6 x2right    p3x   sqrt7  left1  12 x  30 x2  20 x3right   endarray  tag4      the  nth order approximation of a function  fx on the interval   01 can then simply be find with      fnx   sumi0n  leftltfx   pixrightgt  pix tag5      in the following figure PRON can be see that the error keep decrease for high order of approximation of  sinleftfracpi2xright 
__label__gaming __label__monte-carlo-search PRON have hear about this concept in a reddit post about alpha go  PRON have try to go through the paper and the article  but could not really make sense of the algorithm   so  can someone give an easy  to  understand explanation of how the monte  carlo search algorithm work and how be PRON be use in build game  play ai bot   monte carlo method be an approach where PRON generate a large number of random value or simulation and form some sort of conlusion base on the general pattern  such as the mean and variance   as an example  PRON could use PRON for weather forecast  predict long  term weather be quite difficult  because PRON be a chaotic system where small change can lead to very different result  use monte carlo method  PRON could run a large number of simulation  each with slightly different atmospheric change  then PRON can analyze the result and for example calculate the probability of rain on a give day base on how many simulation end up with rain   as for the use of monte carlo in alpha go  PRON seem to be use the so  call monte carlo tree search  in this approach  PRON make a tree of possible move  a few turn into the future  and try to find the good sequence  however  since the number of possible move in the game of go be very large  PRON will not be able to explore very far ahead  this mean that some of the move which look good now may turn out to be bad later   so  in the monte carlo tree search  PRON pick a promising sequence of move and run one or more simulation of how the game may proceed from that point  then PRON can use the result of that simulation to get a good idea of how good that specific sequence of move really be and PRON update the tree accordingly  repeat as need until PRON find a good move   if PRON want more information or to look at some illustration  PRON find an interesting paper on the topic  c browne et al   a survey of monte carlo tree search methods  open repository  permanent link  paywall   
__label__neural-networks __label__machine-learning __label__convolutional-neural-networks __label__image-recognition __label__classification for a classification task  PRON be show a pair of exactly two image to a cnn that should answer with 0   fake pair or 1   real pair  PRON be struggle to figure out how to design the input   at the moment the network s architecture look like this   image1  image2      conv layer  conv layer                                       flatten vector    fully  connect layer    reshape to 2d image    conv layer    conv layer    conv layer    flatten vector    output  the conv layer have a 2x2 stride  thus half the image  dimension  PRON would have use the first fully  connect layer as the first layer  but then the size of PRON do not fit in PRON gpu s vram  thus  PRON have the first conv layer half the size of the image first  then combine the information with a fully  connect layer and then do the actual classification with conv layer for the combine image information   PRON very first idea be to simply add the information up  like  image1  image2   2  but this be not a good idea  since PRON heavily mix up image information   the next try be to concatenate the image to have one single image of size 400x100 instead of two 200x100 image  however  the result of this approach be quite unstable  PRON think because in the center of the big  concatenate image convolution would convolve information of both image  right border of image1  left border of image2   which again mix up image information in not really senseful way   PRON last approach be the current architecture  simply leave the combination of image1 and image2 up to one fully  connect layer  this work  kind of  the result show a nice convergence  but could be good    what be a reasonable   state  of  the  art  way to combine two image for a cnn s input   PRON clearly can not simply increase the batch size and fit the image there  since the pair be relate to each other and this relationship would get lose if PRON simply feed just one image at a time and increase the batch size   PRON can combine the image output use concatenation  please refer to this paper   httpivpleecsnorthwesternedusitesdefaultfiles07444187pdf  PRON can have a look at the figure 2  and if PRON be use caffe  there be a layer call concat layer  PRON can use PRON for PRON purpose   PRON be not fully clear about what PRON want to do  but like PRON say  if PRON want to pass the image value from the first layer to some layer  try read about skip architecture   if PRON want to use this network as real  fake finder  PRON can take the difference between two image and convert PRON to classification problem   hope PRON help   PRON be not sure what PRON mean by pair  but a common pattern for deal w pair  wise ranking be a siamese network   where a and b be a a pos  negative pair and then the feature generation block be a cnn architecture which output a feature vector for each image  cut off the softmax  and then the network try to maximise the regression loss between the two image  the two network share the same parameter and thus in the end PRON have one model which can accurately disambiguate between a positive or negative pair  
__label__petsc PRON be write a program that use petsc and slepc  and PRON be look for a convienient way to read in option from the command line  the description of petscbagsetfromoption   in the official documentation say PRON  allow set option from a bag   do this mean that if PRON create a bag with a variety of petscfoo element  PRON can read in PRON program s parameter from the command line without have to use petscoptionsgetint   and other function like PRON   PRON appear from the code  link on the very page PRON have describe  that petscbagsetfromoption literally just go through the bag s item name and check to see if PRON be in the option database  by append  in front and use petscoptionsint  petscoptionsreal  etc     PRON be silly that the link example  however  use this redundantly with the petscoptionsxxx routine  so PRON can see why this may be confusing   request like this may be send to petscusersmcsanlgov  
__label__neural-network __label__deep-learning __label__keras how do keras calculate accuracy from the classwise probability  say  for example PRON have 100 sample in the test set which can belong to one of two class  PRON also have a list of the classwise probabilit  what threshold do keras use to assign a sample to either of the two class   for binary classification  the code for accuracy metric be   kmeankequalytrue  kroundypred     which suggest that 05 be the threshold to distinguish between class  ytrue should of course be 1hot in this case   PRON be a bit different for categorical classification   kmeankequalkargmaxytrue  axis1   kargmaxypr  axis1     which mean  how often prediction have maximum in the same spot as true value   there be also an option for top  k categorical accuracy  which be similar to one above  but calculate how often target class be within the top  k prediction  
__label__algorithm __label__game-theory __label__minimax by optimal PRON mean that   if max have a win strategy then minimax will return the strategy for max with the few number of move to win   if min have a win strategy then minimax will return the strategy for max with the most number of move to lose   if neither have a win strategy then minimax will return the strategy for max with the most number of move to draw   the idea be that PRON want to win in the few number of move possible but if PRON can not win then PRON want to drag out the game for as long as possible so that the opponent have more chance of make mistake   so  how do PRON make minimax return the good strategy for max   short version below   when implement a minimax algorithm the purpose be usually to find the good possible position of a game board for the player PRON call max after some amount of move  in some game like tic  tac  toe  the game tree  a graph of all legal move  be small enough that the minimax search can be apply exhaustively to look at the whole game tree  more complex game like chess have too large of a game tree to be feasibly search exhaustively   a simple version of minimax just travel through the game tree  evaluate every legal move for the position currently be evaluate before go further and evaluate the possible answer to those move  to find an optimal win move  minimax need only to search until a win state of the game have be find  if implement use the aforementioned breadth first search minimax algorithm  PRON will have find the way to win in the least amount of move   in the case where min have a force win the truly optimal move do not exist  if min be not an optimal player  the definition of optimal can be the move that be most likely to because PRON to make an error  that enable max to force a win  that move be not necessarily the move that lead to the most move until loss   as an example  consider a position in some game where max have two move  move a and move b  move a lead to a loss in 100 move and move b to a loss in one move  naively move a be good but in this game the only legal move follow move a lead to a loss and move b lead to a position where min have hundred of legal move but only one cause PRON to win  albeit a bit extreme  this example demonstrate that optimality be hard to define in a lose position  put simply  be a very complex loss in 6 move bad than a obvious loss in 20   PRON do define a version of optimality however and implement PRON be possible  since PRON be only consider optimal move  an exhaustive search must be perform and thus  there be no reason to give a score to any position but a win  loss  and draw  the method PRON would use be to assign each state a score  much large than the maximum possible amount of move  eg a loss be 100000 a win be 100000 and a draw be 0  then PRON maintain a variable that be the depth of the search or number of move that have to be perform to reach this state  then PRON would add the number of move to the large number  so a loss in 20 move would have a score of 99980 and a draw in 15 move would have the score of 15  100000 be a bit excessive for most game but PRON just have to be large enough that a loss  win  and draw be never confuse as a draw in 100001 move would look good than a win in 1  note that this method should only be use for loss and draw since use this method for win would result in a win in 10 have a score of 100010 and a win in 20 a score of 100020 and thus look good   short version   use breadth first search   for win position  terminate the minimax when a win be find   for loss and draw  search the whole game tree and give the position a score of 0mtp for draw and lmtp for loss   l be a large number and mtp be the number of move to reach the position   minimax deal with two kind of value   estimated value determine by a heuristic function   actual value determine by a terminal state   commonly  PRON use the follow denotational semantic for value   a range of value center around 0 denote estimate value  eg 999 to 999    a value less than the small heuristic value denote a loss for max  eg 1000    a value more than the big heuristic value denote a win for max  eg 1000    the value 0 denote either an estimate draw or an actual draw   the advantage of this denotational semantic be that compare value be the same as compare number  ie PRON do not need a special comparison function   PRON can extend this denotational semantic to incorporate optimality of win and lose as follow   a range of value center around 0 denote estimate value  eg 999 to 999    a range of value less than the small heuristic value denote loss  eg 2000 to 1000    a range of value more than the big heuristic value denote win  eg 1000 to 2000    the value 0 denote either an estimate draw or an actual draw   a loss in n move be denote as m  n  where m be a sufficiently large number  eg 2000    a win in n move be denote as  m  n  where m be a sufficiently large number  eg 2000    use this denotational semantic for value require only a small change to the minimax algorithm   function minimaxnode  depth  max   if max  return negamaxnode  depth  1   else  return negamaxnode  depth  1   function negamaxnode  depth  color   if terminalnode   return 2000  if depth  0  return color  heuristicnode   value  2000  foreach child of node  v  negamaxchild  depth  1  color   if v  gt  1000  v  1  if v  gt  value  value  v  return value  incorporate optimality for draw be a lot more difficult  
__label__random-forest __label__online-learning a random forest  rf  be create by an ensemble of decision trees s  dt   by use bagging  each dt be train in a different datum subset  hence  be there any way of implement an on  line random forest by add more decision tress on new datum   for example  PRON have 10 k sample and train 10 dt s  then PRON get 1 k sample  and instead of training again the full rf  PRON add a new dt  the prediction be do now by the bayesian average of 10  1 dt s   in addition  if PRON keep all the previous datum  the new dt s can be train mainly in the new datum  where the probability of pick a sample be weight depend how many time have be already pick   there be a recent paper on this subject  on  line random forests   come from computer vision  here be an implementation  and a presentation  online random forest in 10 minute 
__label__machine-learning __label__python __label__svm __label__scikit-learn PRON be new to machine learning  PRON want to develop a face recognition system use scikit  learn  the follow be the example give in the tutorial of scikit  learn   httpscikitlearnorgstableautoexamplesapplicationsfacerecognitionhtmlexampleapplicationsfacerecognitionpy  PRON be not get how the input be be provide to the program  how should PRON load a particular image and make PRON program run to predict the label for that   please help    take a look at the code that PRON link to    download the datum  if not already on disk and load PRON as numpy array  lfwpeople  fetchlfwpeopleminfacesperperson70  resize04   fetchlfw  poeple be a routine that load the datum and be detailed here   hope this help  
__label__dataset __label__bigdata PRON use openface to extract feature vector in 128d PRON need to find a suitable database engine to store these vector for future comparison  calculate the euclidean distance between newly extract feature and those store in the database    here be an example of what PRON be try to do   PRON use openface to extract the face representation  this be a 128d feature vector   PRON then calculate the euclidean distance between this vector and those store in the database  return the vector that have the low distance  less than 09  as a match   if no vector store in the database match this criterion  PRON store the newly extract feature as a new entry in PRON dataset   what be a good database engine to achieve this   if PRON really need to do that  PRON will argue PRON be not a good idea   with postgre PRON can store an array type and write a store procedure for new item insertion   this store procedure can do whatev distance check PRON wish  such as check the distance of the new vector against all other in the database  before storage   PRON would argue against this design because PRON suspect the criterion for uniqueness  could easily change over time   PRON think PRON would be a good idea to store all of the vector except exact match   then  create another table which use the definition of uniqueness   create this table would be handle on the application side   if PRON definition of uniqueness change  no problem  just make a new table   PRON could even compare several different definition to see how PRON result be sensitive to PRON   if PRON do PRON this way  cassandra be a great database choice   PRON be specifically design for denormaliz datum storage  PRON have the same datum store in different form or variation  so that PRON application get exactly what PRON need without further computation     in PRON post PRON state that a similarity of  lt  09 would result in store a new vector   that be what PRON mean by criterion for uniqueness  
__label__gaming __label__branching-factors __label__real-time some time ago play chess be challenge for algorithm  then go game which be vastly more complex than compare to chess   how about play rts game which have enormous branch factor limit by PRON time and space  like decide what to do next   what be the successful approach to such problem   as PRON mention in the question  PRON can not solve all problem with decision tree   decision tree usually work well in a turn  base game with a good heuristic function  but in rts game take a different approach   in the case of a very complex rts game  one could implement a rule  base ai  for example  give PRON be the early game use all unit to scout for resource  if a certain criterion be meet build the base a certain way  if another criterion be meet build an army  if the army be big enough  attack  if be attack by the enemy  bring the unit back to the base to defend  each of these rule could implement various other ai technique  for example use a  star to find the optimal path between a unit s current location and destination   further optimization could be do by  group  similar unit to act like one unit  eg calculate the path for the entire group instead of each individual unit   PRON could also add fine grain rule  like if enemy be a certain distance from a unit  move closer and attack or retreat to the base  depend on health  ammo  ability  etc    the benefit of this approach be that a rule  base system execute very fast as no training or decision tree be necessary and this free up a lot of system resource for visual like physics and graphic   the disadvantage be that if the rule system be not complex enough  the player will easily recognize the pattern and the game will become predictable and boring   PRON will also notice that the more different unit PRON add to the game  the exponentially more complex the rule system become as PRON have to cater and test interaction between each type of unit in the game otherwise player may find a weakness in the game design and exploit to complete mission in way PRON be not design to be complete   one of the reason why multi  player game be so popular be that PRON do not play against set rule  but against creative people who have the ability to come up with new strategy PRON have never see before  
__label__python __label__numpy __label__scipy what do  desire error not necessarily achieve due to precision loss  mean in the context of the scipyfmin method  PRON can not seem to find an explanation anywhere   here be PRON code   import math  import numpy  import random  import scipyoptimize as opt  import matplotlibpyplot as plt  from numpy import array  from numpy import dot  from random import randint  from numpy import matrix  import sys  ns     st     lamfuntrix     timestep  1000  deltat  01  mu  07  def genst     global st  st     for i in range0  timestep    stappendrandomnormalvariate01   mathsqrtdeltat    def fval    return mathexpval   def getlamt    rate  mu  return powdeltat  1   frate   def whitenoise     global ns  for i in range0  timestep    lam  getlami   deltat  spikeatbin  numpyrandompoissonlam   nsappendspikeatbin   def genlamlogi  mu    rate  mu  return powdeltat  1   frate   def genlamfunmu    global lamfuntrix  lamfuntrix     for i in range0  timestep    lamfuntrixappendgenlamlogi  mu    def logliket    mu  t  genlamfunmu   sum  0  for i in range0timestep    val  lamfuntrixi   sum  sum    nsi   mathlogvaldeltat     valdeltat    return sum  def dermu     sum  00  for i in range0  timestep    sum   nsi   lamfuntrixi   deltat   return sum  def firstdert    mu  t  genlamfunmu   dm  dermu    return dm  genst    whitenoise    initguess  array0    val  optfmincgloglike  initguess  fprime  firstder   print val  the code be a  crumby since PRON par PRON down a bit for the question   warning  desire error not necessarily achieve due to precision loss   current function value  822835581  iterations  1  function evaluation  18  gradient evaluation  6   07943019   PRON look to PRON like the loglike function PRON be provide optimiz to infinity  that would trigger this specific error  
__label__machine-learning __label__deep-learning why use softmax as oppose to standard normalization  in the comment area of the top answer of this question  kilian batzner raise 2 question which also confuse PRON a lot  PRON seem no one give an explanation except numerical benefit   PRON get the reason for use cross  entropy loss  but how do that relate to the softmax  PRON say  the softmax function can be see as try to minimize the cross  entropy between the prediction and the truth   suppose  PRON would use standard  linear normalization  but still use the cross  entropy loss  then PRON would also try to minimize the cross  entropy  so how be the softmax link to the cross  entropy except for the numerical benefit   as for the probabilistic view  what be the motivation for look at log probability  the reasoning seem to be a bit like  PRON use ex in the softmax  because PRON interpret x as log  probabiltie   with the same reasoning PRON could say  PRON use eeex in the softmax  because PRON interpret x as log  log  log  probability  exaggerating here  of course   PRON get the numerical benefit of softmax  but what be the theoretical motivation for use PRON   PRON be more than just numerical  a quick reminder of the softmax      py  j  x   fracexjsumk1k exk       where  x be an input vector with length equal to the number of class  k the softmax function have 3 very nice property  1  PRON normalize PRON datum  output a proper probability distribution   2  be differentiable  and 3  PRON use the exp PRON mention  a few important point   the loss function be not directly relate to softmax  PRON can use standard normalization and still use cross  entropy   a  hardmax  function  ie argmax  be not differentiable  the softmax give at least a minimal amount of probability to all element in the output vector  and so be nicely differentiable  hence the term  soft  in softmax   now PRON get to PRON question  the  e in softmax be the natural exponential function  before PRON normalize  PRON transform  x as in the graph of  ex   if  x be 0 then  y1   if  x be 1  then  y27   and if  x be 2  now  y7   a huge step  this be what be call a non  linear transformation of PRON unnormalized log score  the interesting property of the exponential function combine with the normalization in the softmax be that high score in  x become much more probable than low score   an example  say  k4   and PRON log score  x be vector   2  4  2  1 the simple argmax function output       0  1  0  0      the argmax be the goal  but PRON be not differentiable and PRON can not train PRON model with PRON  a simple normalization  which be differentiable  output  the follow probability       02222  04444  02222  01111      that be really far from the argmax   whereas the softmax output       01025  07573  01025  00377      that be much close to the argmax  because PRON use the natural exponential  PRON hugely increase the probability of the big score and decrease the probability of the low score when compare with standard normalization  hence the  max  in softmax  
__label__neural-network __label__deep-learning __label__tensorflow __label__image-classification __label__hinge-loss PRON try to apply text classification in tensorflow   PRON have a network architecture use the support vector machinesvm  as classification output layer  the classification layer shape be  batchsize  56561    in loss function PRON want to use hinge loss  PRON use the bellow equation which the y  be the label  y be the prediction  classification output  and the s be the image size  the label shape be  301  which all of PRON value be  1   relate to text    when PRON use this equation PRON be give PRON an error because the shape for label and prediction be different  then the subtraction between PRON be wrong   there be some explanation for this loss function  which PRON try to apply   but PRON can not understand all of PRON  the explanation be   what PRON should to do to make the loss function be right   do the problem from label   note  the classification layer must be in this shape  batchsize  s4  s4  1    
__label__advection __label__spectral-method how can PRON determine the numerical dispersion relation of a spectral element method which lead to couple system of algebraic equation   what approach to do analysis of dispersion relation be available   the key step be to consider the advection equation   ut  aux  0   where  aomega  k be the advection speed  exact solution to this equation be of the form  ux  t   fx  at  where  fy be an arbitrary function   for example  discretize use a standard galerkin method PRON derive the weak form   intomega v ut dx  intomega a ux  0   assume that the sem solution be of the form   ux  t   sumj uj njx  and select the test function  v  nix  PRON can derive the algebraic system of equation   m fracddt   bf u   a s  bf u   0   where   mij   intomega ni nj dx  quad sij   omega ni nj  dx  assume the solution ansatz   ux  t   a exp  i  k x  omega t     PRON can insert this in the solution vector to derive a generalise eigenvalue problem of the form   i omega m  bf u   a s  bf u   0   PRON can introduce a dimensionless variable   i omega m  bf u   s  bf u   0   where  omega  fracomega ha   h be the element size and  a be the advection speed   by solve for the eigenpair   vlambda of the generalise eigenvalue problem  av  bvlambda  PRON can obtain estimate of the angular frequency  omega which can be use to characterise the numerical dispersion  the  lambdaapprox omega  in matlab this can be implement as  b  1im   a  s    v  d   eiga  b    where d contain the eigenvalue of the system and v the eigenvector  the numerical value contain in d that match the analytical the close be assume to be the approximation to the physical eigenmode   also  PRON be note that both m and s be construct to be periodic  
__label__iterative-method __label__operator-splitting __label__matrix-equations recently  PRON obtain a linear system   ax  b  where  a be a nonsingular  strictly diagonally dominant  mmatrix  then PRON also get a matrix splitting  a  s  t  where  s be also a nonsingular  strictly diagonally dominant  mmatrix  so PRON establish a stationary iteration scheme as follow     sxk1    txk    b  accord to numerical result  PRON seem that this iterative scheme be always convergent  be PRON rational from theoretical analysis  so  can PRON prove the convergence of this iterative scheme  ie   show  rhos1t   lt  1   where  rhocdot be the spectral radius   PRON can prove convergence by satisfy the spectral radius relationship PRON note  choose  s and  t such that  rhos1t   lt  1  this come about by first write two equation base on PRON operator splitting     sx  tx  b    sxk1    txk    b    where  x be exact solution and  xk be the  kth iteration s solution   now assume error can be define as   ek    x  xk base on this relationship and the two equation above  PRON end up with     sek1    tek        ek1     s1t  ek      base on the result equation  the condition that need to be meet for  ek1 to converge as  k rightarrow infty be for  rhos1t   lt  1   as PRON mention  how PRON prove this spectral radius inequality depend on the form of  s1t base on PRON choice for  s and  t 
__label__r __label__convnet __label__software-recommendation PRON do not see a package for do convolutional neural networks in r have anyone implement this kind of algorithm in r   PRON guess there be no package for cnn but PRON can write PRON own convolutional layer  mxnet or h2o will be useful for PRON   check this out   httpdmlcmlrstats20151103trainingdeepnetwithrhtml  the follow 2 package be available in r for deep neural network training   darch  package for deep architectures and restrict boltzmann machines   the darch package be build on the basis of the code from g e hinton and r r salakhutdinov  available under matlab code for deep belief net   this package be for generate neural network with many layer  deep architecture   train PRON and fine tuning with common know training algorithm like backpropagation or conjugate gradient  additionally  supervise fine  tuning can be enhance with maxout and dropout  two recently develop technique to improve fine  tuning for deep learning  cran link  httpcranumacirwebpackagesdarchindexhtml  deepnet  deep learning toolkit in r implement some deep learning architecture and neural network algorithm  include bp  rbm  dbn  deep autoencoder and so on  cran link  httpscranrprojectorgwebpackagesdeepnetindexhtml  PRON think mxnet be one of the good option if PRON code in r PRON have an r wrapper but the core be in c   PRON have several example in the web  one of PRON be the character recognition with mnist database  PRON have support for multi  gpus and also for spark   the mxnetr package be an interface of the mxnet library write in c  PRON contain feed  forward neural network and convolutional neural network  cnn   mxnetr 2016a    httpswwwisunifreiburgderesourcesroeffentlicherzugriffdeeplearninginrdeeplearninginrensetlanguageen   tensorflow for r be available   PRON provide full access to tensorflow api  the keras api  and tensorflow estimators   installation of tensorflow  excerpt below    httpstensorflowrstudiocomtensorflow  installation  to get start  install the tensorflow r package from github as  follow   devtoolsinstallgithubrstudio  tensorflow    then  use the installtensorflow   function to install tensorflow   librarytensorflow   installtensorflow    PRON can confirm that the installation succeed with   sess  tfsession   hello  lt tfconstanthello  tensorflow     sessrunhello   this will provide PRON with a default installation of  tensorflow suitable for get start with the tensorflow r package   see the article on installation to learn about more advanced option   include instal a version of tensorflow that take advantage of  nvidia gpu if PRON have the correct cuda library instal  
__label__machine-learning __label__data-analysis __label__support-vector-machines PRON have an optimization problem  PRON have to optimize some coefficient in the problem base on non  outlyingness  PRON need some measure of non  outlyingness which change with respect to current classifier  PRON idea regard this be somehow vague  may be because PRON do not have any knowledge about outlier or measure of outliyingness for any datum point  please leave any comment to make PRON clear on this   what be the optimization measure which can be optimize efficiently and of course simply   any help be highly appreciate   thank PRON very much   
__label__monte-carlo __label__random in general  and in real application   suppose PRON be use a randomized algorithm  eg use mcmc to sample from a distribution and then compute  efx for some function  f   assume PRON algorithm will face every possible input and PRON use a pseudo  random generator to provide randomness   shall PRON derandomize PRON algorithm by fix a random seed every time the algorithm start   the reason that PRON come up with this question be that by derandomiz PRON  PRON can make sure every input will have a correspond output  which make PRON feel more comfortable for practical use  however  PRON believe PRON will lose something  maybe the algorithm work very bad for bad case input  if PRON derandomiz PRON like this   PRON definitely want to derandomize PRON program during development  otherwise PRON will not be able to debug PRON since problem be not reproducible   at the same time  once PRON know the algorithm be work  PRON need to run PRON for multiple seed or with different random number generator to ensure that PRON result  such as ensemble average  standard deviation  etc  be independent of the actual sequence of random number  
__label__hyperbolic-pde __label__accuracy PRON have a numerical scheme to solve an hyperbolic system of equation of this type  this a more simple version just for clarity      fracpartial upartial tufracpartial upartial xvfracpartial upartial yfracpartial hpartial x    fracpartial vpartial tufracpartial vpartial xvfracpartial vpartial xfracpartial hpartial y    fracpartial hpartial t fracpartial upartial xfracpartial vpartial x0  PRON have  a steady state analytical solution  and PRON be check the order of accuracy of the scheme  PRON be a direction splitting scheme  and all the term should be second order accurate  PRON use 4 grid  with successive refinement  i1ldot 4   cartesian uniform grid with grid size   delta xidelta xi12  and  delta xdelta y   PRON see that the order of accuracy be somehow around 15  therefore PRON decide to replace each derivative with the correspondent analytical term  until PRON find the term responsible for the loss of accuracy  the culprit be   fracpartial hpartial x and  fracpartial hpartial y  now PRON question be   1  be PRON do right by use a fix cfl number  in the sense that every time PRON half the grid size PRON also half the time step  what if PRON use a fix dt for all the grid  note that the scheme be implicit and unconditionally stable   2  PRON be use a stagger grid  ie h be cell center and u and v be on the center of the cell edge  the term  fracpartial hpartial x and  fracpartial hpartial y be discretiz by second order central difference  when PRON print the error of  fracpartial hpartial x after a single time step start from the exact solution PRON see that PRON converge perfectly with second order accuracy  PRON be 4 time small at each refinement   however when PRON start from a second  order accurate approximation of h   fracpartial hpartial x have first order rate of convergence  PRON be try to figure this out and PRON may make sense since   fracpartial hpartial x frachm1hmdelta x   odelta x2 that be a second order accurate expression at the edge  m12  if  hm1 and  hm be exact  however  if PRON start from a second order accurate approximation of  hm1 and  hm     hm1   hexactm1 odelta x2    hm   hexactm odelta x2   as PRON would be the second order accurate solution of the system  PRON get    fracpartial hpartial x frachexactm1hexactmdelta x   fracodelta x2delta x   odelta x2 that be first order  and that s what PRON get  first order  if PRON print the numeric value of  fracpartial hpartial x for the 4 different grid  however  if PRON print  h for the first half time step PRON be second order  PRON be the second half of the splitting that because the order of accuracy   be PRON do something wrong here   PRON will answer PRON point 1  still try to find an answer for 2  since these seem to be the key of PRON question  1  this depend  if PRON analytical solution be a steady solution  then when PRON reach that solution  PRON time derivative term should be close to zero  or close to the machine error at least   thus keep a constant cfl condition be not necessary  since PRON scheme be implicit and unconditionally stable  PRON do not need to worry about the influence of the time step of the error   for some solution PRON investigate  PRON will sometimes have an analytical solution that depend on time  in this case  PRON error  if PRON have a second order time discretization  will be     e propto odelta x2   odelta t2  thus if PRON keep the cfl constant    e propto odelta x2   o  cfl delta x2  propto odelta x2     thus  PRON only need to keep the cfl constant for unsteady analytical solution  not for steady one  an example be the taylor  green vortex   2  regretfully  PRON be not sure PRON understand the question exactly at the moment  let PRON think about PRON and PRON will try to edit PRON later  
__label__combinatorics give a set of integer  l1l2ldot  ln  where each integer be associate with a set  mkinlk  lk1ldot  lk  PRON need to find all combination  m1m2ldot  mn that sum to a give integer  m  as an example  let PRON say the input be  13 and  m2 this mean that  m1in101 and  m2in32ldots3 the desire combination be then    m1m2130211  for only two integer  l1  and  l2  this be easy  but a bit more involved for a general number  PRON have a brute force algorithm implement which find all possible combination and then pick those with the right sum  but this quickly become too slow when the set grow   so PRON be wonder if there be a fast algorithm to find these  do not have to be the fast in existence if sacrifice a little speed give one easy to implement    PRON be sure this be a well know problem  but google PRON could not find anything   PRON think this may be a fun problem to solve  so PRON crank out a solution for PRON base on the comment PRON make in the problem statement  the class represent the solution  which be in c  can be find via the follow link   combinatorial solution class  the maincpp file can be write with the follow example code    include  ltstdiohgt    include  combinatorialsolnhpp   int main  int argc  char   argv     initialize combinatorial problem  combinatorialsoln soln   stdvectorltintgt  iset  1  3  2  5  6  3    solnsetintegersetiset     compute combination  combinatorialsolncombos combo  solncomputecombinations5     print result  forint i  0  i  lt  combossize    i    combosiprint        print basic result  solnshowstatistic      complete  return 1    
__label__data-visualization __label__gnuplot PRON know that gnuplot s epslatex terminal can display superscipt and subscript in title and label ax  be this also possible when use gnuplot s png terminal   if so  how do one do this   PRON be possible to use latex syntax for super and subscript in ax label and title when the terminal be set to png   to to this  set the terminal with  set term png enhance font  full  path  to  font   and then plot normally   if the font part be not include  PRON may get this error message   gdimagestringft  could not find  open font  this mean gnuplot can not find the plot on PRON system  on linux  font be generally locate in usr  share  font   PRON can not just use tex syntax unless PRON be use the tikz  epslatex  or another terminal that use latex to process the label  PRON can get some fairly crude math layout  include superscript and subscript  by use the enhanced text option to many of gnuplot s terminal  include  sometimes  png  but PRON will have to use the gnuplot enhance text syntax  which be unrelated to  latex syntax  fortunately  some of the syntax happen to coincide  include the basic use of superscript and subscript  on PRON system enhance text work with the pngcairo terminal but not plain png  if PRON do not have that  PRON can try other  such as svg  PRON have plenty of example of use both latex and enhance text in PRON new book  
__label__machine-learning __label__regression __label__logistic-regression __label__training PRON have a time series eg  23648569    PRON have train the nn classifier to predict next value use previous 4 four time series value   how can PRON re  train the model or make modification to the model if the difference between the actual value and the predict value be high   should PRON use any other learning technique for this   
__label__machine-learning __label__tensorflow import numpy as np  from alexnet import alexnet  width  80  height  60  lr  1e3  epochs  15  modelname   pygtasa  carepochsmodelformatlr   alextnetv2   epochs   model  alexnetwidth  height  lr   traindata  nploadtrainingdatav2npy    train  traindata500   test  traindata500    x  nparrayi0  for i in trainreshape1width  height1   y   i1  for i in train   testx  nparrayi0  for i in testreshape1width  height1   testy   i1  for i in test   modelfitinput   x     target   y   nepoch  epochs  validationsetinput   testx     target   testy     snapshotstep500  showmetric  true  runid  modelname    tensorboard logdir  foo  fplaygtasalog  modelsavemodelname   the file return be   need a model file in order to test the model give a tensor flow error file not find with a file extension of model  
__label__floating-point __label__precision analytically  the expression    explnxx enspace  should give 0   however  in matlab  PRON do not   x  linspace1  10  10    explogx    x   for  x in  1  10  the answer be  0  0  444089209850063e16  0  0  0  0  177635683940025e15  177635683940025e15  177635683940025e15  what be happen and how can PRON fix this   the numeric precision be not perfect  PRON get round error during PRON computation  when work with float  do not check if PRON be  0  but check if PRON absolute distance to 0 be small than some epsilon    edit  an alternate view  64bit float number represent a discrete set  s for a function  f to be exactly invertible   PRON should be a bijection from  s to  s  suppose PRON be interested in a fast grow function like  exp at one point   exp x  gt  x  if  m be the maximum element from  s  then  fmge m the strict version  fmgt  m be not  allow  because there be no high number that  m in  s morally  a monotonic function act like a bijection on a finite set be a permutation  and have very limit degree of freedom  PRON be likely to be limit to the identity  or or a counter  identity which revert the order of element in the set  outside these trivial example  monotonic function can not be bijection   in practice  PRON be due to finite  precision numerical computation  for instance 64bit float point   that especially affect function involve irrational number  integer or sum  of  power  of  two calculation may be less sensitive   if PRON dare to plot the numerical error  PRON get something like   with matlab code  x  linspace11000010000    y  explogxx   plotx  y        xlabelx     ylabely     so PRON can see that the maximum error spread with the value  if PRON want to do something  PRON can check and threshold a relative error  especially if PRON variable spread over a large scale   PRON get even weird  if PRON just swap the function  say   logexpxx  the error be in this case numerically zero   x  linspace170010000    y  logexpxx   plotx  y        xlabelx     ylabely     if PRON have the symbolic toolbox PRON can fix the problem by  sym x  y  explogxx   y  0  the basic issue be that generally  explnx   neq x when use a fix  precision float  point format  this be true independent of the accuracy with which the exponential and logarithm function be be compute  due to the pidgeonhole principle   without loss of generality  this effect be most easily demonstrate by use a binary logarithm log2 in conjunction with one of the ieee754 binary float  point format  with such a format  each binade comprise an identical number of encoding  if PRON compute log2 of two consecutive binade   48 and   816 all result will all fall into the single binade   24 since there be twice as many source encoding than target encoding be available  PRON must have plenty of collision by the pidgeonhole principle   all logarithm be contract operation  this mean that when use a fix  precision float  point computation  multiple input can map to the same output  information be lose due to this compression  and as a consequence  when PRON expand the number in   24 back to   416 with exp2  for many  x PRON can not get back the start value even if both function use a correctly round implementation   reverse the order of operation  that is  use of an expand operation follow by the contracting inverse operation  PRON can sometimes guarantee that every operand  x round  trip successfully  as long as there be no intermediate overflow or underflow  and assume correctly  round operation  PRON do not know  off  hand  whether this guarantee  lnexpx    x  but the graph in laurent duval s answer seem to suggest PRON  this paper show that the square root of the square of a float  point number  x be exactly  lvert x lvert  under the restriction state  
__label__linear-solver __label__sparse for an implicit scheme PRON want to solve system  ax  b  where  a be a non  symmetric square matrix  PRON want source code of large sparse matrix solver  eg lu  sgs  to use in PRON code which be in c language  PRON have try precondition gmres code from this site but PRON get slow and take huge amount of memorysometime program be get  kill  automatically    PRON do not say how big PRON system of equation be  but if PRON be interested in try direct sparse solver  here be link to two for non  symmetric system that be know to be of high quality and be write in c  suitesparse  superlu  PRON would look at petsc and trilinos  PRON have ton of different sparse solver available   PRON highly recommend on eigen  PRON can also look at taucs  
__label__sparse __label__fortran __label__constrained-optimization __label__least-squares PRON be wonder if there be a fortran library that contain a solver for the sparse lselinear equality  constrain least square  problem     minxcx  d2  text  subject to   ax  b     where  a and  c be large and sparse   for dense linear system  lapack have the subroutine dgglse  but PRON be look for a solver for sparse system   
__label__convex-optimization __label__constrained-optimization __label__complexity __label__semidefinite-programming this be a follow up question to this question   consider the follow sdp in standard form   beginalign    ampminxin sn  xgt0  operatornametrax   ampmboxsubject to  operatornametrbixleq bi  i1dot  m  endalign   here   ain sn  bi in sn   biin mathbbr and  m  on  can anyone please explain the expect storage complexity of this problem in the general case   PRON understanding be that first order solver like scs will form a solve a  n2  sized linear system use gradient information only  and hence the storage complexity will be  on4 which reflect the size of gradient matrix which will be  n2time n2  be this understanding correct  PRON confusion be regard the possibility of dual problem be small size  be that take care of by cvx automatically   for example  if  n1000  and  m1000   PRON seem like primal form as mention above would form the small problem   edit after johan s answer   PRON be currently use cvx  PRON try an example similar to what PRON give  use notation in PRON post above  PRON be use  n100m10  xin sn  if PRON submit PRON problem in format above  PRON get follow output   calling scs 117  5050 variable  10 equality constraint  lin  sys  sparse  indirect  nnz in a  50500  cg tol  1iter200   ep  135e06  alpha  150  maxiter  10000  normalize  1  scale  100  variables n  10  constraint m  5050  cones   sd var  5050  sd blk  1  setup time  302e03s  now if PRON submit PRON problem by form a dual of this  PRON get the message that cvx will solve the dual of PRON dual  ie the original problem   and PRON get exactly same number of constraint and variable as above   so PRON question be whether cvx be really solve the way PRON have describe   what be the expect space complexity give the above variable   no  PRON understanding be not correct  by introduce a slack  sin rm PRON can write the constraint in the form  operatornametrbix   si  bi PRON now have a standard semidefinite program  in primal form  over the product of a semidefinite cone of size  n  and an lp cone of size  m the linear system will be a linear system of size  m time m as there be  m equality in the primal   m variable in the dual   a very bad model would be to do what PRON implicitly hint  ie  to introduce the element of  x as variable  lead to a linear system of size  on2 when solve a primal  dual pair in semidefinite programming  PRON never explicitly solve for the element of the primal cone  the matrix as a whole be reconstruct from the solution of the line  search for the dual   the stuff here may be a relevant read  and the link paper there   edit  trivial example in yalmip  disclaimer  develop by PRON   PRON do not want yalmip to interpret this as a problem define by the individual variable in  x and fit datum into a dual description  which be default  lead to 125250 variable in dual 125250 equality in primal   x  sdpvar500    optimizexgt0tracex1tracerandn500xsdpsettingssolversc     instead  PRON want yalmip to interpret PRON from a primal point of view  1 equality in primal  1 variable in dual   optimizexgt0tracex1tracerandn500xsdpsettingssolverscsdualize1   
__label__visualization __label__paraview PRON have multiple csv file with point  coordinate and  possibly  multiple datum value attach to PRON  ie  the row look like  x y z data1 data2   be there any possibility to display say 10 of those csv file in a  time  series in paraview   if PRON manage to of load a single file and plot what PRON want from PRON PRON can  try to rename the file  in paraview  when PRON have multiple file with the same base name but consecutive enumeration PRON can plot PRON over time  
__label__lapack PRON would like to analyze some well  use scientific code that make heavy use of lapack  ie PRON be look for code that both spend a lot of time within lapack function and use lapack non  trivially  ie call more than just dgemm   PRON would be interested in application that use either sequential lapack or scalapack or any other implementation that expose the same interface   what be some good example that be easy to obtain  ideally PRON would be open source or available  on  request   atlas do a pretty brutal group of grind metric on lapack and blas   PRON may be able to snag some code out of there that meet PRON need   PRON may also suggest check out numpy and scipy as both have very large unit  test that likely span large part of that phase  space   additionally  PRON provide hook down to the lapack function if PRON want to call those directly on array of PRON creation   another tack to the problem be possibly search github for the lapack call of interest  and see what code surface   PRON be hard to know where to start  as there be so many code out there use lapack and PRON criterion be a bit … subjective   use lapack non  trivially    PRON will throw in some pointer   PRON can find here an old survey of lapack user  which give some information into who use PRON  and what be PRON use PRON for  most quantum chemistry code out there use lapack for PRON linear algebra  cpmd  cp2 k  octopus  to cite only a few open  source code   PRON could also go to the package list of PRON favorite linux distro  and check out what package depend on lapack  have a look at the radau iia solver by e hairer  httpwwwunigechhairersoftwarehtml   PRON use lapack as a backbone for an implicit runge  kutta method for ode   PRON write an electromagnetic simulation code s4 which be limit entirely by the ability to solve a dense nonsymmetric eigenvalue problem  zgeev   the code be open source and write in c  
__label__finite-element __label__mesh-generation PRON sincerely apologise if this question be a duplicate  though PRON be clearly a question that must have be ask and answer a 1000 time PRON can not find any reasonable solution   how do PRON take a simple 2d cad drawing  dwg file  of a 2d boundary and generate a 2d computational mesh suitable for fem analysis in any of the follow file format   xml  dolfin xml   elenode  triangle file format   mesh  medit generate by tetgen   mshgmsh  gmsh   grid  diffpack tetrahedral   inp  abaqus tetrahedral   eexoncdf  sandia exodus ii   vrtcell  star  cd   PRON be do this on PRON own time so PRON must be free software  any assistance would be greatly appreciate   derek  for PRON setup the most common strategy be to transfer datum from the cad program to the fem preprocessor via a neutral file format  eg ige or step  which be both support by gmsh   unfortunately PRON have no direct experience of draftsight  so please check if PRON be capable of export  saving  model in ige or step format   most cad program have native support for ige or step  so PRON would expect that also draftsight have this capability    PRON would add PRON two cent about mesh with gmsh   even though gmsh have some ability to define geometry  gmsh be not cad package  so for complex model PRON need to use any third  party cad software  for example   opencascade  library   salome  freecad  brl  cad  pythoncad  blendercad  archimedes  openscad  PRON would give PRON link for all of these package but stackexchange prohibit PRON to do this because PRON be quite new here  however PRON can find PRON here or just google by name   these package be all free and most of PRON be open  source   to bind the model from these package with gmsh  PRON need to export geometry in one of opencascade format  brep  step or ige  but note that iges format be not recommend   or any other compatible to gmsh format  stl and many other   if any of present package do not export a model in suitable format  PRON can save PRON in some cad format and then use cad exchanger for convert  
__label__finite-volume __label__discretization __label__advection-diffusion in the context of the solution of advection  diffusion equation by finite volume method  many numerical scheme  paper and book chapter be dedicate to address the numerical diffusion andor numerical dispersion that come from the discretization of the advection term   if PRON understand PRON correctly  the discretization of the diffusion term also create numerical diffusion andor dispersion  however  give the lack of literature about PRON  PRON seem not to be a problem   why be that so   what PRON know be that when discretiz adconvective  diffusive equation  this problem  numerical diffusion  face both of the adconvection term and the diffusion term due to the numerical approximation when compute the gradient  therefore  the numerical diffusion be a numerical  truncation error and be a function of a few parameter  mainly the grid size and the alignment of the streamline with the grid   PRON tend to think that  solve  this problem  numerical diffusion  for the adconvection term and the diffusion term be the same  note that high order approximation have less numerical diffusion  however  PRON have numerical oscillation   PRON be sure someone else would answer PRON question better than PRON do  PRON be still a graduate student in cfd    caring  be a modeling question  whether PRON care depend on the granularity of PRON model  the require precision  and the question PRON be try to answer  of course method can always be more precise  but that be not the purpose of a numerical approximation  a numerical approximation be a fast calculation that be a good enough solution of PRON equation to analyze PRON result  so there be no general answer   PRON need to ask PRON whether the amount of numerical diffusion due to PRON choose  dx   dt  spatial discretization method and order  etc  be an appropriate tradeoff in term of runtime and PRON development time  and whether the amount of error that be numerically introduce do not interfere with PRON modeling conclusion   numerical diffusion be not a  big  problem in diffusive equation solver because PRON be there in PRON model  if PRON have no diffusion in PRON model  say euler s equation or nondiffusive shallow water equation or any other hyperbolic equation  then numerical diffusion become an issue because PRON denature the numerical solution by give  or remove from  PRON properti that the exact solution have not  or have   for example  an exact solution that form shock will be approximate by a smear out numerical solution   from a mathematical point of view  the minimum PRON require from a numerical method be that a sequence of numerical solution correspond to a sequence of ever fine grid  or mesh  converge  in some sense  to the exact solution  if PRON have diffusion in the exact model  the exact solution be smooth and there be numerous convergence theorem that guarantee the convergence of the numerical method   however if there be diffusion in the exact model  PRON solution be more singular  PRON can have shock for example  or lack even a first derivative and must be interpret in some generalize sense   the few convergence result be restrict to special situation and the convergence rate be those dictate by the smoothness of the exact solution  which be low   and if there be too much diffusion then this convergence be further slow down  and in some case completely obliterated    so the reason PRON do not need to worry  too much  about numerical diffusion  for convection  diffusion equation be that the solution begin smooth  PRON be easy to approximate with smooth numerical solution  where smoothness of a discrete function can be measure by use discrete smooth norm    numerical dispersion be a different story  which PRON be not really entitle to comment about  but because PRON equation be not dispersive PRON may be important to think about PRON   however  generally speak numerical dispersion be much less of a problem when compare to numerical diffusion   PRON have the follow problem     fracpartial upartial tvcolorredfracpartial upartial xnucolorbluefracpartial2upartial x20 tag  the function  u may represent for example the concentration that propagate at velocity  vgt0  and dispers in a medium with viscosity  nugt0 since only PRON be discuss how term be discretis initial and boundary condition be unnecesary   imagine PRON discretise the equation      accord to the follow scheme     colorredfracpartial upartial x   fracui  ui1delta x    colorbluefracpartial2 upartial x2approx fracui12uiui1delta x2  give the follow numerical approximation     fracpartial upartial tvcolorredfracui  ui1delta xnucolorbluefracui12uiui1delta x20 tag  if PRON reverse the transformation  to find what equation       solf by mean of taylor expansion     uipm 1uipmfracpartial upartial xdelta xfrac12fracpartial2upartial x2delta x2pmfrac16fracpartial 3 upartial x3delta x3mathcalodelta x2  PRON find     fracpartial upartial tvleftcolorredfracpartial upartial xfrac12fracpartial2 upartial x2delta xmathcalodelta x2rightnuleftcolorbluefracpartial2upartial x2frac13fracpartial3upartial x3delta x2mathcalodelta x3right0     PRON be clear that the lead error term come from the discretisation of the convective term  mathcalodelta x  while the error generate by the discretisation of the diffusive term do not contribute due to PRON smallness   mathcalodelta x2    PRON can say that when use the scheme       PRON approximate the following equation exactly  up to an error  mathcalodelta x2      fracpartial upartial tvcolorredfracpartial upartial xleftnucolorredfracvdelta x2rightcolorbluefracpartial2upartial x2colorbluemathcalodelta x2tag  the add viscosity be then  vdelta x2  if PRON would have use a centre scheme for convective term  PRON would have solve equation      exactly up to second order  check PRON     this be the main reason of the fact that book do not stop to describe the add viscosity from the discretisation of the diffusive term  the lead one be the add by convective term for stable scheme  eg upwind   
__label__data can anybody tell PRON the formula how to find the number of false positive with respect to the first class   where  y be the truth  target and  ax be the prediction  PRON always find the notion of false positive and negative confusing  especially when PRON come to multi  class problem   a good rule of thumb that PRON come up with be the following  true positive   PRON predict that PRON be a certain class  and PRON be right   false positive   PRON predict that PRON be a certain class but PRON be wrong   true negative   PRON predict that PRON be not a certain class  and PRON be right   false negative   PRON predict that PRON be not a certain class  but PRON be wrong   so in PRON case  there be 31 true positive with regard to the first class since PRON predict 31 time that something be class 1 but PRON be wrong  
__label__r __label__gpu PRON have just instal a nvidia gt660 graphic card on PRON desktop  and  after some struggle  PRON manage to interface PRON with r  PRON have be play with several r package that use gpu   especially gputool  and PRON be compare the time take by PRON  gpu and cpu to perform some basic operation   invert matrix  cpu faster   qr decomposition  cpu faster   big correlation matrix  cpu faster   matrix multiplication  gpu much faster    notice that PRON have experiment mainly with gputool so maybe  other package perform better   in broad term PRON question be  what some routine statistical  operation that may be worth execute on a gpu rather than a cpu   multiple imputation method for missing data  like those in alice  ii  r    PRON think those tend to be often embarrassingly parallel and hence suitable to a gpu architecture  never try PRON PRON though   for all of the application PRON mention  gpu should be more capable  from a hardware perspective  than cpu for sufficiently large matrix   PRON do not know anything about r s implementation  but PRON have use cublas and magma with great success for inversion around  n  210 and multiplication  correlation for rectangular matrix with  n  m approx 210   k approx 214  PRON be an especially big surprise to PRON that large correlation matrix would be faster on the cpu use r  more broadly  PRON suspect most statistical operation that spend most of PRON time in dense linear algebra  blas  lapack functionality  can be efficiently implement on the gpu   gpu be sensitive beast  although nvidia s beefiest card can theoretically execute any of the operation PRON list 100x faster than the fast cpu  about a million thing can get in the way of that speedup  every part of the relevant algorithm  and of the program which run PRON  have to be extensively tweak and optimize in order to get anywhere near that theoretical maximum speedup  r be generally not know to be a particularly fast language  and so PRON do not surprise PRON that PRON default gpu implementation be not that great  at least in term of raw performance  however  the r gpu function may have optimization setting that PRON can tweak in order to regain some of that miss performance   if PRON be look into gpu because PRON have find that some calculation that PRON need to run be go to take week  month to finish  PRON may be worth PRON while to migrate from r to a more performance  friendly language  python be not too much hard to work with than r the numpy and scipy package have most of the same stat function as r  and pycuda can be use to implement PRON own gpu base function in a fairly straightforward way   if PRON really want to increase the speed at which PRON function run on gpu  PRON would consider implement PRON own function in a combination of c and cuda  the cublas library can be use to handle all of the linear algebra  relate heavy lifting  however  keep in mind that PRON can take quite a while to write such code  especially if PRON be PRON first time do so   and so this approach should be reserve only for those computation that take an extremely long time to run  month  andor that PRON be go to be repeat hundred of time   in broad term  algorithm that run faster on the gpu be one where PRON be do the same type of instruction on many different datum point   an easy example to illustrate this be with matrix multiplication   suppose PRON be do the matrix computation   a time  b  c  a simple cpu algorithm may look something like  start with c  0  for  int i  0  i  lt  cwidth  i     for  int j  0  j  lt  cheight  j     for  int k  0  k  lt  awidth  k     for  int l  0  l  lt  bheight  l     cj  i    aj  k   bl  i            the key thing to see here be that there be a lot of nest for loop and each step must be execute one after the other   see a diagram of this  notice that the calculation of each element of c do not depend on any of the other element  so PRON do not matter what order the calculation be do in   so on the gpu  these operation can be do concurrently   a gpu kernel for calulat a matrix multiplication would look something like    kernel void multiply      global float  a     global float  b     global float  c      const int x  getglobalid0    const int y  getglobalid1    for  int k  0  k  lt  awidth  k     for  int l  0  l  lt  bheight  l     cx  y    ax  k   bl  y          this kernel only have the two inner for loop  a program send this job to the gpu will tell the gpu to execute this kernel for each datum point in c the gpu will do each of these instruction concurrently on many thread   just like the old saying  cheaper by the dozen  gpu be design to be faster do the same thing lot of time   there be however some algorithm which will slow the gpu down  some be not well suit for the gpu   if for example  there be data dependency  ie  imagine the computation of each element of c depend on the previous element  the programmer would have to put a barrier in the kernel to wait for each previous computation to finish  this would be a major slow down   also  algorithm which have a lot of branch logic ie     kernel foo      if  somecondition     do something    else    do something completely different      tend to run slower on the gpu because the gpu be no longer do the same thing in each thread   this be a simplified explanation because there be many other factor to consider   for example  send datum between the cpu and gpu be also time consume  sometimes PRON be worth do a computation on the gpu even when PRON faster on the cpu  just to avoid the extra send time  and vice versa    also many modern cpu support concurrency now as well with hyperthreaded multicore processor   gpu s also seem to be not so good for recursion  see here which probably explain some of the problem with the qr algorithm  PRON believe that one have some recursive data dependency  
__label__categorical-data PRON be try to understand what level of measurement be good for describe the  number of room in a flat  feature   first of all  PRON think PRON be not a continuous feature  because rational value like 142  for example  do not make sense   to decide whether a feature be categorical or nominal  PRON should try to find an ordering between value  and here be PRON question  should PRON look for an order with respect to the response feature  in PRON case  price of a property     PRON can say a  1room flat  be cheap than a  2room flat   and so on  but that be not always the case  in general  maybe PRON be true  but there be case when a  1room flat  in the city centre be way more expensive than the one further away   so  a can not decide which representation to choose  categorical or nominal   PRON would stick with continuous   a number of 142 could mean that PRON may have a single room  but with significantly more area than the average room   PRON may even have some fraction if PRON need to share kitchen and bathroom with other   those thing may be easy to fit in a number than in yet another category PRON do not know how to order   PRON would say a discrete numerical variable   this be the same  type  of variable as  how many child do PRON have  in survey  definitely an integer  and PRON do not necessarily mean raise five kid be  54  time more expensive than raise four kid   but PRON of course depend on the environment  software PRON be use  survey PRON be give   r differentiate a numeric  ie a float  from an integer   PRON do not know where PRON be get this particular classification scheme from   continuous  categorical  nominal   but PRON ’ worth mention that this be not a great scheme for classify datum type  PRON have already encounter an example of a data type  count datum  natural number  that do not fit neatly into PRON   PRON be also worth mention that a lot of people will use  nominal  and  categorical  interchangeably to describe a discrete datum type with no natural ordering  probably the most widely  accept term to describe discrete datum with a natural ordering be ordinal datum   in this case   number of room in a flat  have an unambiguous ordering  in a very real and intuitive sense  a five  room flat have more room than a two  room flat  a one  room flat have few room than a three  room flat  PRON make sense to talk about one flat have a great or less number of room than another  and if PRON want to know if that ordering matter in some way  PRON need to preserve the ordering in PRON analysis  
__label__machine-learning __label__r __label__unsupervised-learning PRON have unsupervised datum  ie this data do not have any target variable through which PRON can learn PRON be prior behaviour  PRON be a mix of continuous and categorical datum  now PRON want to classify the test datum into three category on basis of PRON unsupervised datum   the approach PRON take be to first do the clustering of unsupervised datum  use this categorise datum as a base data for prepare a new  model that predict on top of PRON   PRON want know whether this approach correct or not or be there good way for classify test set  particular algorithm PRON need to follow for this   PRON be do this in r  the approach be to modify the training set datum so that this can be use to properly predict the test datum  here target variable be miss in train and test set   PRON have many option of algorithm to use for classification of unsupervised datum   this be a very broad topic  but if PRON need a specific algo recommendation  try to see if self  organize map  som  may help with PRON specific problem  in r  try the kohonen package   k  mean be another popular clustering algorithm   no matter which method PRON use  consider convert PRON categorical datum to numerical datum for clustering  as PRON may alleviate some of PRON mixed data  type woe   PRON be really a broad topic but PRON think PRON be go on a right track   PRON solve a similar problem couple of month back where PRON work on classification of document in multiple category use centroid base algorithm   here  PRON cluster training dataset use spherical k  means  and the result centroid of the cluster represent a category  later while predict a category of new document  PRON would compare the document with all the centroid and assign a category base of sse   michael be right  k  mean clustering could possibly work for PRON  but k  mean be not design to handle categorical variable   if PRON do not have too many category  then PRON could choose to represent PRON as dummy variable   here be a link to a post where PRON explain dummy variable in python pandas   PRON also find a stack overflow answer that explain how to create dummy variable in r 
__label__r __label__python __label__decision-trees give  a data frame of 25 million record and 25 column  which contain 1 group column with a variable number of group  eg 25 m record  10 group today  25 m record  50 group tomorrow    how can PRON build a decision tree for each group   PRON be go to do this use python but think there may be an easy way in r  tapply    here be PRON initial thought process in python   instantiate an empty dictionary to hold the prediction result  identify the unique id in the grouping column and iterate over the row of datum in a for loop  in each loop  build the decision tree  predict and store the result in the dictionary as a  groupingvar  prediction  pair  thought on how to do this faster or more efficiently in r   
__label__pde __label__parabolic-pde PRON remember see in the book by kreiss  time  dependent partial differential equation and PRON numerical solution  that if some elliptic differential operator satisfy    lu  uleq kuu for the equation  ut  luf with some boundary condition then PRON can be show that  the equation be stable  that be continuously depend on the initial datum  however  when PRON think of  lu and  usinnx on   0pi as a solution of  ut  un2sinnx  PRON have boundary condition as zero at both end independently of  n  then PRON can calculate  beginequation    lu  uuprimeprimeu   int uprimeprime  udx  n2 int sin2nxdx  endequation   and  beginequation    u  u int u2dx  int sin2nxdx  endequation   clearly  the inequality   lu  uleq ku  u do not hold as  n increase  even though second derivative be a proper elliptic operator  what be PRON miss here  PRON do not put the proof here but PRON be just a few line  however the example above contradict to the statement   when do the estimate   lu  uleq kuu hold then   PRON get  un2 u  and the condition hold with  k0  by the way  kreiss  conclusion hold for any linear operator  no ellipticity must be assume  
__label__python __label__regression __label__random-forest PRON read a lot about random forest and gradient boosting  but PRON do not know how these two algorithm really work   for example  see the simple picture about basketball  picture 1  from this link   how do random forest and how do gradient boosting work   have each tree in the random forest different training datum and different feature   sorry about m question but PRON don‘t find an easy non  mathematical answer   accord to mueller in introduction to machine learn with python  a guide for datum scientist a random forest consist in several decision trees whose input be a reorganization of the initial dataset  this reorganization be call bootstrap   still accord to PRON  each tree will be train on a feature space subset  during the training each tree will try to fit the datum accord to PRON feature subset   in gradient boosting tree  a part of what be call gradient boost machine  the idea be to take shallow tree  shallow here mean that each tree will have a very thin subset of the feature space  without  boostrap  the dataset  the different tree be set in a series circuit and each tree will correct the previous tree error  PRON may tune the learning rate between each tree for a good learning   ps  gradient boosting machine  in general  combine several  simple  model  in gradient boosting tree the simple model be shallow tree etc   PRON can have a look to this non mathematical article gb from scratch  PRON be not a complete answer but PRON hope PRON will be complete and still useful  random forest   build a decision tree   sample n example from PRON population with replacement  mean example can appear multiple time    at each node do the follow  select m predictor from all predictor  split base on predictor that perform good via some objective function  go to the next node  select another m predictor and repeat  combine all tree as an average or weight via some scheme   gradient boosting  one key note be that random forest tree essentially indepedent of each other   boost algorithm add a certain depedency to the model   initialize a model by find the minimizer of a certain objective function  for each iteration  compute the partial derivative of lyi   fxi with respect to  fxi for all i to n  fit a tree  hm to the the result from above   solve  lambdam  arg min sumi1nlyi   fm1xi    lambda hmxi  update model via   fmx   fm1   lambdam hm   x 
__label__deep-learning __label__convnet __label__backpropagation __label__computer-vision PRON need help in understand the gradient flow through a concatenation operation   PRON be implement a network  mostly a cnn  which have a concatenation operation  in pytorch   the network be define such that the response of pass two different image through a cnn be concatenate and pass through another cnn and the training be do end to end   since the first cnn be share between both of the input to the concatenation  PRON be wonder how the gradient should be distribute through the concatenation operation during backprop  PRON be not an expert on backprop and this be the first time PRON be tinker with a custom backward implementation so any pointer would be helpful   PRON can provide more detail if PRON guy need PRON   for concatenation  the gradient value during back propagation split to PRON respective source layer  there be no direct interaction between gradient in either of the source layer   the layer immediately after the concatenate layer do interact with both network  and PRON will have some weight parameter that multiply output from network a and some that multiply output from network b there will not be any parameter that multiply output from both layer  unless PRON be force PRON to be the same through weight sharing  but that will not be the case if for example PRON be stack feature from both start network    the only issue PRON may have be clearly identify which parameter link to each original network  that be an implementation detail  so PRON would need to share PRON code so far in order to debug that if PRON go wrong  
__label__matrix-equations __label__matrix PRON be build an application where PRON need to compare find datum with the actual data PRON should be   PRON have 5 set of datum  each with 3 variable a  b  c   let matrix a be a 3x1 matrix with datum a  b  c which represent the actual datum   let matrix b be a 3x3 matrix with unknown datum that need to be calculate   let matrix c be a 3x1 matrix with find datum a  b  c   the equation a  bc need to be solve so that PRON give the low difference for the 5 dataset   how would PRON start with solve this  do PRON just have to start with some number in b and try new one each iteration   let PRON see if PRON can restate the problem to answer PRON question   PRON have 5 set of datum   yi   xi such that for each  i  1  ldot  5    xi and  yi be both 3 by 1 vector  PRON also have a 3 by 3 matrix  b that be suppose to relate  xi to  yi via     yi   bxi     be this formulation correct   if so  the proper method for solve this sort of problem use linear regression   one linear regression method be ordinary least squaresols   ols will find a solution to the overdetermined set of equation      y1   ldot  y5    b cdot  x1   ldot  x5  with minimum residual  that is  PRON minimize the sum of squared distance between the datum  yi and the prediction  bxi  hence  least square   this sum of squared distance be  sumi15yi   bxi2  the norm use be a 2norm    for convenience  define the 3 by 5 matrix  x and  y by     y   y1   ldot  y5    x   x1   ldot  x5  so PRON be find a solution to the overdetermined system of equation    y  bx  with minimum sum  of  squared distance  this next part be important   the notation PRON have use to describe PRON problem be the transpose of what the wikipedia article on ordinary least square use  so if PRON try to compare everything  PRON be go to have to transpose every matrix   use some algebra  the value of  b that minimize the sum of squared distance be    b   xxt1xyt  PRON can check the dimension of each matrix to see that  b be 3 by 3   do not calculate  b this way   calculate matrix inverse directly be usually bad for a variety of reason  why PRON be bad be out of the scope of the question  if PRON need a matrix inverse  PRON be usually good to solve the system of equation relate to that inverse   a good way to calculate  b would be to solve the related linear system    xxtb  xyt  this system of equation be call the normal equation  plural  because the system of equation be refer to as a group    do not calculate  b use the normal equation either   solve the normal equation be not usually a good idea because this system of equation can be ill  condition  which will degrade the accuracy of  b  an even good way to calculate  b would be to use qr decomposition   let  xt    q1   q2r1   0t  obtain from a qr decomposition  use the method outline here     b  r11q1tyt  so PRON would solve the linear system    r1b  q1tyt  this last method should give PRON what PRON want  assume PRON do not make any mistake during PRON lunch break   
__label__finite-element __label__pde __label__weak-solution PRON wanna ask a question that confuse PRON quite a long time  PRON see many guy  in the context of computational mechanic   PRON seem to choose the virtual function or kinematic in a way that some kinematical quantity be keep  fix zero  but  PRON ask the remain  of  PRON  to act as role during the derivation of  a weak form equation  in the end  PRON can generally get a system of weak equation  PRON be all the time wonder whether PRON have a chance to change the nature of the original physical problem of a strong form   who can shed light on PRON doubt in a physcial or mathematical view   edit   say  u  u1u2u3 for whatev reason  PRON variation be delta u1  delta u2  delta u3  the weak form may be integral sigmau   sym gradientdelta u1   0  integral sigmau   sym gradientdelta u2   0  integral sigmau   sym gradientdelta u3   0   
__label__r __label__statistics __label__visualization to provide a full yet simple picture of a 3level  one  way anova  PRON use the follow visualization where variation within each group  the fill circle  and variation between the group  black arrow  be simple to be understand   but PRON be wonder if PRON could be possible to extend the current visualization to a 2 x 3 two  way anova  add another way with two group to the current visualization     note  the dashed vertical line denote each group s mean   PRON could go for a 3d plot  but PRON be only useful in specific situation  eg simple bar plot  a surface etc    and definitely not here  PRON good option  while keep the overall design of PRON plot  seem to be to split PRON datum by the factor with 2 level  and plot each separately in 2 panel side by side  that way  PRON can compare similarity  baseline shift  and interaction  
__label__nlp brat  brat rapid annotation tool  can be use for name  entity annotation   can brat be use for text classification annotation  ie  give the text  annotate whether PRON belong to some class   PRON think brat that fit some task as ner  postag   with classification problem  PRON can add keyword to text which separate with tab symbol  after  PRON can annotate for label   for example  label sentence  PRON can annotate for word  label  with some class  thank  accord to PRON documentation  brat do a lot of thing  but text classification be not one of PRON  brat be  too powerful  for that  PRON would recommend PRON use a tool like prodigy instead   this should do what PRON be look for   brat be not design with classification task in mind   like valentin mention  prodigy be a more modern annotation management system that fit well in a classification task workflow  PRON have be test PRON at work  PRON be promising  but also have PRON limit   PRON mention the potential need for crowdsourc  PRON be sure PRON know about aws mechanical turk  PRON also talk about datum  annotation integrity validation  PRON good bet may very well be to develop PRON own annotation management system to cater to these need  PRON have very similar requirement  and this be the route PRON have choose to take   PRON evaluate prodigy before PRON 10 release  PRON have quite a few quirk  plus not all feature be available at the time   PRON seem to be very good at quick prototyping and building mvp  but less adapt at serious large  scale annotation task  PRON be great to be able to answer quickly yes  no to machine  generate annotation  but in PRON experience in many task those be not accurate often enough and PRON would be more efficient to manually annotate  PRON seem PRON can now load PRON own model into PRON  so there would be a possibility to improve that if PRON would invest dev time into prodigy s framework   another very important aspect miss be any kind of distribute annotator management system  multi  user  conflict resolution  etc  add to that no support for integrity  validation check  prodigy team say PRON be all in the work though   PRON be also work on an extension library  the prodigy annotation manager  which will integrate with prodigy and will let PRON set up complex annotation project  manage multiple annotator  enforce quality control and keep track of the progress via an admin console  PRON can sign up to PRON mailing list to be notify about the private beta   edit  just stumble on webanno  which may be worth a look  
__label__matrices PRON need to permute the degree of freedom of a system and apply this permutation to a few sparse matrix in ccs  or crs  format  PRON could construct a permutation matrix and perform sparse matrix  matrix multiplie  but that seem like overkill  be there standard algorithm for permute sparse matrix  how do user of package like metis permute sparse matrix   a function write in c to permute a ccs matrix be show here   httpwwwciseufleduresearchsparsecsparsecsparsesourcecspermutec  the algorithm be fairly simple but there be not many comment  if PRON want  more detail  take a look at davis  book where the routine be present   httpwwwciseufleduresearchsparsecsparse 
__label__machine-learning __label__clustering __label__feature-extraction PRON be look to find cluster in a multidimensional datum set but PRON be particular that the result cluster also account for underlying feature interaction  do k  mean account for inter  dependency between feature  be there other cluster algorithm that do this   
__label__machine-learning __label__predictive-modeling let PRON get straigt into PRON   think of an schedule operation  there be a surgeon and  say  three other medical personel that carry out various task  for instance  a nurse  an anesthesiologist  and a backup   each one report to duty at a certain time  however  with some probability p  to be predict  one or many members  of the team will not show up for work  if an individual do not show up for work  but PRON particular skill set be still cover due to the backup  then the operation be do  otherwise  PRON be cancel  for instance  if the backup be a certify anesthesiologist  absence of the schedule anesthesiologist do not prevent the surgery from take place since a minimum require skill mix be give  if  however  the backup be a nurse  and the anesthesiologist do not show up  then the surgery have to be cancel   this problem involve two layer   do an individual team member show up for work   be an individual team member s skill critical for the execution of the surgery   PRON goal be to predict cancellation of the surgery  which involve step 1 and step 2 simulataneously  alternatively  PRON could think of step 2 be conditional on step 1    PRON have a good understanding of how to use standard machine learn algorithm to predict 1  but how do PRON generate final prediction take both layer into account  the problem set up seem reasonably general so that PRON would assume there exist a  standard  method   this problem be best address with discrete  event  stochastic simulation modeling   the prediction PRON get from such a model be a probability distribution on the surgery be cancel   PRON be easy to code up such a model give PRON problem statement   PRON need   a boolean function for each kind of surgery  which  give the cast of character and skill set as input  determine true  false if the surgery can proceed   a probability distribution for the appearance of each kind of character  when schedule   an object which be initialize with the schedule   the idea be to do this check  surgery  surgery cancel  many time  thereby grow a set of outcome for each policy PRON wish to explore   if PRON do not want to roll PRON own  there be package for many language such as simpy for python and simmer for r  the nice thing about discrete event simulation be that PRON do not need to make any simplify assumption about probability  and the business logic can be arbitrarily complex   for example  if the surgeon and one of the nurse be have an affair  the chance that one cancel on the same day as the other can be increase  
__label__image-classification __label__computer-vision __label__image-recognition during the past few year several important area of image processing and image classification or generation become dominate by convolutional neural network   PRON be interested if there be any method come from mathematical physics context  like method design for solve ill  pose problem  spectral analysis  image deblurring and dering   that outperform neural network  base approach in 2017 for some common computer vision or image processing problem  or method that do not have any neural network  base rival  maybe in the field of biomedical image analysis  just a guess    the deep and more specific the answer be  the good   
__label__linear-algebra __label__numerical-analysis __label__convergence __label__newton-method this be a follow  up to this answer   suppose PRON have a possibly very ill  condition matrix  a  and PRON compute PRON inverse with lu  ge to get  xtextluapprox a1  the newton  raphson iteration    f  x mapsto 2x  xax    generally converge to the matrix inverse of  a   do computing apply the newton  raphson iteration to  xtextlu  that is  compute  fxtextlu   ffxtextlu make the result matrix inverse more accurate  or less accurate   PRON be unable to find a reference directly  so PRON be ask if someone know for sure   the absolute error will be proportional to  f1  eg  accuracy and stability of numerical algorithms  chapter 25   but  f vanish at  x  a1 so presumably this mean that apply  f could only help when the error come from the growth factor in the lu factorization step  but do not that mean that apply  f hurt at other time since PRON rely on compute a root of a function with a vanish jacobian  which would make PRON less accurate than lu  for example  in the one  dimensional case with a root of multiplicity 2  newton s method would have error  osqrtepsilon  but lu  ge always have error  orho epsilon  PRON try PRON on all  16time 16  matrix in matrixdepotjl  and PRON seem like PRON almost always hurt accuracy  never help by more than  frac13   and often fail   PRON be not very clear what matric to test    module matrixinversenewtonraphson  use matrixdepot  fa  x   2x  xax  ra  x   normax  eyea   1   function goname  m   a  fullmatrixdepotname  m    b  inva   b1  fa  b   x  05a   normaa   2   for i in 12 m  x  fa  x   end   ra  b1   maxepseltypea    ra  b    ra  b   ra  x   name   end  function mainm16   e     for name in matrixdepotall    try  pushe  goname  m    catch err  printlnerr   end  end  sorte   end  end  
__label__neural-network __label__keras PRON be try to create an autoencoder where a few input can affect the other input  but not the other way around  an example of this be give by the following picture  PRON look into define PRON own keras layer  but do not know how to proceed   keras do give a chance to add custom layer  PRON do not know about the previous version but keras 113 be flexible to do this  have a look at this link  this also contain a good example for a custom layer  here  PRON can try to change the weight matrix accord to PRON need  PRON would also suggest PRON to try read source code of some basic layer to understand PRON good   PRON believe PRON can do this use the functional api and play around with PRON input  suppose PRON have the set of input feature  can be singleton set   aband  c for some  x in  x if PRON split PRON input into  xa   xb  and  xc  where all element in  x be transform to include only feature in  a  PRON can theoretically build this model   here be a code snippet demonstrating this idea  give  xa   xb  and  xc  and PRON correspond shape  PRON could widen this model by expand the fully  connect layer  by increase the variable set to 1 in the dense call   no promise on the code work off the bat  PRON have brush over some detail but this should give PRON a start on PRON problem   inputa  inputshape  shapea   inputb  inputshape  shapeb   inputc  inputshape  shapec   a  dense1inputa   a  dense1a   b  mergeinputa  inputb  inputc   modeconcat    b  dense1b   c  mergea  bmodeconcat   c  dense1c   b  mergea  bmodeconcat   b  dense1b   a  dense1a   from here  if PRON want one output  PRON can call merge on  a   b  and  c and then tack on a dense  fully  connected  layer with  for example  a softmax layer for multi  label classification  
__label__machine-learning __label__ensemble-modeling if PRON get 95   accuracy in normal model  should PRON still consider ensemble model  why should PRON choose ensemble model over normal model   firstly  welcome to the site   when do PRON use ensemble model   when there be 2 model which perform moderately then PRON combine PRON result to get a model which perform better  in PRON scenario PRON already have a model which give PRON good result what be the point of implement ensemble models   as tagoma say  PRON depend on PRON datum and PRON goal  for example  PRON be try to predict stock rate  every 001  matter  in such scenario PRON need to use complex algorithm to maintain balance on that slim line ie  not to over  fit  not to over  train and just predict   measure to check if PRON model be over  train be by give some random datum and see how PRON perform  add some noise in the training datum   one more important thing to do be  to check for the predictor importance and see if there be any highly correlate feature with the target variable  for example  PRON be plan to predict age and if PRON have dob as a feature in that yes of course PRON would predict with 9999  accuracy but that be not what PRON use ml for   if all of PRON be satisfied and achieve that accuracy then PRON mean that PRON model s performance be good   finally  implementation of ensemble be dependent on PRON business problem and PRON understanding on business   PRON be a problem specific and datum specific problem  for one problem  95  accuracy may be good  for some other PRON may not be very good and there may still be room for improvement  likewise  PRON may happen that the problem be well define  PRON architecture be good but due to a lot of noise in the datum  PRON can not do good than 95   in that case  PRON will say the result be good   ensemble  ensembling be a general term for combine many classifier by average or voting  PRON be a form of meta  learn in that PRON focus on how to merge result of arbitrary underlying classifier  generally  ensembl improve the final result but again  not necessarily  if PRON be ensembl two highly correlate model  PRON ensembl model will give almost the same result as PRON stand  alone model  
__label__machine-learning __label__svm __label__feature-extraction PRON be very new in machine learning  PRON have annotate datum with category  aspect  opinion word and sentiment  for example  for the bellow text   the apple be really tasty   PRON have categoryfood  aspect  apple  opinion word tasty and sentimentpositive  PRON have training datum like this format   how can PRON train a svm classifier use this format of training set   how to extract feature like n  gram  pos and sentiment word to train the classifier   could PRON please suggest any begin step for this aspect base sentiment analysis use machine learn algorithm   PRON would recommend PRON to start from read the draft of the introductory book  sentiment analysis and opinion mining  by bing liu  the draft in a pdf document format be available for free here   more detail about the new upcoming book of this author  as well as comprehensive information on the topic of aspect  base sentiment analysis  with reference and link to data set  be available at this page  httpwwwcsuiceduliubfbssentimentanalysishtml   another interesting resource be a survey book  opinion mining and sentiment analysis by bo pang and lillian lee  the book be available in print and as a downloadable pdf e  book in a publish version or an author  format version  which be almost identical in term of content   aleksandar blekh have give some really nice link about the big picture of how to do sentiment analysis  PRON will try to provide some link to software and talk about the nitty  gritty of how to make PRON work  PRON will point PRON to the example use scikit  learn  httpscikitlearnorgstable   a machine learn library in python   PRON would first want to take PRON dataset and load PRON into scikit  learn in a sparse format  this link  httpscikitlearnorgstabletutorialtextanalyticsworkingwithtextdatahtml  give example of how to load text in a bag  of  word representation  and the same module  scikitlearnfeatureextractiontext  can also count n  gram  PRON then describe how to run naive bayes and svm on that dataset  PRON can take that example and start play with PRON  
__label__symbolic-computation here a couple of question   1  do a cas exist that can manipulate cartesian base tensor and produce a simplified output that be hopefully in index notation   2  if not  be there some indirect method to accomplish this   PRON be talk about do rather simple manipulation such as computing      nabla2mathbfrleftfracarleftmathbbi   fracmathbfrmathbfrr2right   leftfracarright3leftmathbbi   3 fracmathbfrmathbfrr2rightright      in a 3d cartesian basis where  mathbfr   langle x  y  zrangle   mathbbi be the identity 3x3 matrix   r  mathbfr etc   what PRON have try   PRON have try use maxima to do this the  pedestrian  way  ie make vector and explicitly compute matricie etc     this just result in a complex mess of a big 3x3 matrix with long expression   this  work  but the main problem be that simplification be not easy in this case  or maybe PRON be not do PRON right    PRON be aware of other  tensor software   eg httpenwikipediaorgwikitensorsoftware  that do these sort of thing but PRON all seem to be make for more general or complex us and PRON not clear to PRON how to control PRON for PRON use   so PRON ask if anyone have any experience with this and could perhaps provide an example where simple manipulation like this in a cas can be perform   thank  
__label__machine-learning __label__python __label__random-forest PRON fit a dataset with a binary target class by the random forest  in python  PRON can do PRON either by randomforestclassifier or randomforestregressor   PRON can get the classification directly from randomforestclassifier or PRON could run randomforestregressor first and get back a set of estimate probability  then PRON can find a cutoff value to derive the predict class out of the set of probability  both method can achieve the same goal  ie predict the class for the test datum    also PRON can observe that  randomforestclassifierpredictprobaxtest1    be different from  randomforestregressorpredictxt   so PRON just wanna confirm that both method be valid and then which one be good in random forest application   if PRON be do a classification task  use random forest classifier  if PRON want probability use predictproba or PRON can use predict directly to get class  PRON may be get correct result but understand that the random forest regressor work on a different cost function  and be not constrain to give output between 0 and 1  so what PRON be get out of the regressor be not really probability  if PRON run PRON on enough dataset PRON may start give PRON output great than 1  hence  bottom line stick to classifier  also PRON will recommend read up a bit on the difference of regression and classification task   on the one hand  as himanshu rai have mention  if PRON be use randomforestregressor for classification task  the  probability  PRON get may great than 1  or less than 0  that be not what PRON expect   on the other hand  randomforestclassifier use accuracyscore as loss function  while randomforestregressor use r2score  that be a big difference   and some advice  reference  httpscikitlearnorgstablemodulesgeneratedsklearnensemblerandomforestclassifierhtmlsklearnensemblerandomforestclassifierscore  httpscikitlearnorgstablemodulesgeneratedsklearnensemblerandomforestregressorhtmlsklearnensemblerandomforestregressorscore 
__label__linear-algebra __label__eigensystem __label__arpack PRON be work on a generalize eigenvalue problem of the form     boldsymbolacdotboldsymbolxlambdaboldsymbolbcdotboldsymbolx      where  boldsymbolb be not symmetric positive  therefore PRON recast the problem to     boldsymbolb1cdotboldsymbolacdotboldsymbolxlambdaboldsymbolx      PRON be lucky  the matrix  boldsymbolb be block diagonal  therefore PRON can compute the inverse by an lu  factorization of the diagonal block   this eigenvalue problem be then solve with the arpack solver  arpack be use because of PRON feature to extract specific part of the spectrum like eigenvalue with the large real part  the method work well for small problem   however  if PRON increase the problem size  the performance of the arpack algorithman in term of iteration require increase  the condition number of  boldsymbolb1cdotboldsymbola increase heavily up to the order  10  6  be there a way of precondition the arpack algorithman to accelerate the convergence   PRON have summarize the comment thread of PRON original question into an answer   here be a few thing that PRON can try   increase the number of PRON arnoldi vector  ncv  generate at each iteration  here be what the arpack documentation for dsaupd say about this   at present there be no a  priori analysis to guide the selection of ncv relative to nev  the number of eigenvalue PRON be look for   the only formal requirement be that ncv  gt  nev  however  PRON be recommend that ncv  gt 2nev   if many problem of the same type be to be solve  one should experiment with increase ncv while keep nev fix for a give test problem  this will usually decrease the require number of opx operation but PRON also increase the work and storage require to maintain the orthogonal basis vector  the optimal  cross  over  with respect to cpu time be problem dependent and must be determine empirically   revisit PRON implementation of the linear operator  in PRON case  PRON should read something like    a   z leftarrow a v   b  solve  m w  z for  w  use the pre  compute factorization of  m  if PRON be look for the small eigenvalue  or eigenvalue close to a give real value  PRON be advisable to use the shift  and  invert mode of arpack  the shift  sigma can be zero  see the example PRON give here  or any other real  complex value  PRON linear operator become   a  sigma m1  m   for  m symmetric and indefinite    if scipy  arpack work well enough for PRON problem  then petsc  slepc be probably overkill  
__label__finite-element __label__numerical-analysis __label__error-estimation PRON have learn about finite element method  also a little on other numerical method  but PRON do not know what be exactly definition of these two error and difference between PRON   error estimate usually have the form    u  uh leq ch  where  u be the exact solution PRON be interested in   uh be a compute approximate solution   h be an approximation parameter PRON can control  and  ch be some function of  h  among other thing   in finite element method   u be the solution of a partial differential equation and  uh would be the finite element solution for a mesh with mesh size  h  but PRON have the same structure in inverse problem  with the regularization parameter  alpha in place of  h  or iterative method for solve equation or optimization problem  with the iteration index  k  or rather  1k  in place of  h    the point of such an estimate be to help answer the question  if PRON want to get within  say   103 of the exact solution  how small do PRON have to choose  h    the difference between a priori and a posterior estimate be in the form of the right  hand side  ch   in a priori estimate  the right  hand side depend on  h  usually explicitly  and  u  but not on  uh for example  a typical a priori estimate for the finite element approximation of poisson s equation  delta u  f would have the form    u  uhl2  leq c h2 uh2  with a constant  c depend on the geometry of the domain and the mesh  in principle  the right  hand side can be evaluate prior to compute  uh  hence the name   so PRON would be able to choose  h before solve anything  in practice  neither  c nor  uh2 be know   u be what PRON be look for in the first place   but PRON can sometimes get order  or  magnitude estimate for  c by carefully go through the proof and for  u use the datum  f  which be know   the main use be as a qualitative estimate  PRON tell PRON that if PRON want to make the error small by a factor of four  PRON need to halve  h  in a posteriori estimate  the right  hand side depend on  h and  uh  but not on  u a simple residual  base a posterior estimate for poisson s equation would be    u  uhl2  leq c h fdelta uhh1  which could in theory be evaluate after compute  uh in practice   the  h1 norm be problematic to compute  so PRON would further manipulate the right  hand side to get an element  wise bind    u  uhl2  leq c leftsumk  hk2 fdelta uhl2k    sumf  hk32  jnabla uhl2fright  where the first sum be over the element  k of the triangulation   hk be the size of  k  the second sum be over all element boundary  f  and  jnabla uh denote the jump of the normal derivative of  uh across  f this be now fully computable after obtain  uh  except for the constant  c so again the use be mainly qualitative  PRON tell PRON which element give a large error contribution than other  so instead of reduce  h uniformly  PRON just select some element with large error contribution and make those small by subdivide PRON  this be the basis of adaptive finite element method  
__label__python __label__clustering __label__text if PRON have many string from different source that tend to exhibit some common pattern  be there a way to extract these common motif   for example  in a list  of million  that include string    rs12346rs1212122  sxs  rs333  kgp222     be there a way to extract the follow pattern    rs  plus one or more digit   sxs  rs  plus one or more digit   kgp  plus one or more digit  some other parameter   clean up beforehand be not an option  some manual tweaking would be possible  eg manually change a pattern because of outside knowledge   exception  ie classify less than 100  of the string  can be tolerate  an ideal solution would be a build  in python library   PRON think the answer also depend on PRON usecase  if PRON just want to detect these kind of string PRON would focus on heuristic rule that suit PRON need  rather than create a system that learn to recognize the pattern of the string   however  if PRON aim be generate similar kind of string base on the current pattern or find new string in a stream of text  PRON should look for regex generator  there be this list of resource regard regex in general and PRON should focus on the generator for the task at hand   for a similar task in the past  PRON have use an online  free tool that will generate a regex  if possible  that satisfie as many of PRON sample string as possible  so PRON could give PRON a try  as a fast solution   either way  PRON would  first do some datum explorationeg  be number and letter also intermix or do number always come up after or letter  symbol etc    in order to get a good understanding for the problem at hand  also  this could lead to simple heuristic rule that could suffice as a crude solution or at least a simple baseline system against which PRON would  pitch  a machine learning model   assume that letter be indicative of  motif  and number be consider as digit and not exact number  this be what PRON would do   first  transform number into a digit placeholder     import re  s  resubds       then PRON would transform a string into a bag  of  bigrams vector in the char level  from sklearnfeatureextractiontext import countvectorizer  vec  countvectorizers  analyzercharwb   ngramrange13    after these 2 step  PRON get a sparse vector from any string    ab123  gt    a1ab1b2     1   next PRON want to convert those vector to a pairwise distance matrix and cluster by that distance   from sklearnmetricspairwise import pairwisedistance  from scipycluster import  hierarchy  def linkclusterx  threshold01  metriccosine   algoaverage     x  xtodense    z  hierarchylinkagex  algo  metric  metric   c  hierarchyfclusterzthreshold  criteriondistance    return c  PRON end result would be  c  linkclustervec  
__label__neural-networks __label__machine-learning __label__deep-network __label__deep-learning __label__pattern-recognition the question be about the architecture of deep residual networks  resnets   the model that win the 1st place at  large scale visual recognition challenge 2015   ilsvrc2015  in all five main track   imagenet classification   ultra  deep   quote yann  152layer net  imagenet detection  16  good than 2nd  imagenet localization  27  good than 2nd  coco detection  11  good than 2nd  coco segmentation  12  good than 2nd  source  msra  ilsvrc  amp  coco 2015 competition  presentation  2nd slide   this work be describe in the following article   deep residual learning for image recognition  2015  pdf   microsoft research team  developer of resnets  kaim PRON  xiangyu zhang  shaoqing ren  jian sun  in PRON article    identity mappings in deep residual networks  2016    state that depth play a key role    PRON obtain these result via a simple but essential concept  go deeper  these result demonstrate the potential of push the limit of depth    PRON be emphasize in PRON presentation also  deep  good      a deep model should not have high training error      deeper resnets have low training error  and also low test error      deeper resnets have low error      all benefit more from deep feature – cumulative gain      deeper be still good    here be the sctructure of 34layer residual  for reference    but recently PRON have find one theory that introduce a novel interpretation of residual network show PRON be exponential ensemble   residual networks be exponential ensembles of relatively shallow networks  2016   deep resnets be describe as many shallow network whose output be pool at various depth   there be a picture in the article  PRON attach PRON with explanation   residual networks be  conventionally show as  a   which be a natural representation of  equation  1   when PRON expand this formulation to equation  6   PRON  obtain an unraveled view of a 3block residual network  b   from this  view  PRON be apparent that residual network have o2n  implicit path  connect input and output and that add a block double the number  of path   in conclusion of the article PRON be state   PRON be not depth  but the ensemble that make residual network strong   residual network push the limit of network multiplicity  not network  depth  PRON propose unraveled view and the lesion study show that  residual network be an implicit ensemble of exponentially many  network  if most of the path that contribute gradient be very short  compare to the overall depth of the network  increase depth  alone can not be the key characteristic of residual network  PRON now  believe that multiplicity  the network ’s expressability in the  term of the number of path  play a key role   but PRON be only a recent theory that can be confirm or refute  PRON happen sometimes that some theory be refute and article be withdraw   PRON question   should PRON think of deep resnets as ensemble after all  ensemble or depth make residual network so strong  be PRON possible that even the developer PRON do not quite perceive what PRON own model represent and what be the key concept in PRON   imagine a genie grant PRON three wish  because PRON be an ambitious deep learning researcher PRON first wish be a perfect solution for a 1000layer nn for image net  which promptly appear on PRON laptop   now a genie induce solution do not give PRON any intuition how PRON may be interpret as an ensemble  but do PRON really believe that PRON need 1000 layer of abstraction to distinguish a cat from a dog  as the author of the  ensemble paper  mention PRON  this be definitely not true for biological system   of course PRON could waste PRON second wish on a decomposition of the solution into an ensemble of network  and PRON be pretty sure the genie would be able to oblige  the reason be that part of the power of a deep network will always come from the ensemble effect   so PRON be not surprising that two very successful trick to train deep network  dropout and residual network  have an immediate interpretation as implicit ensemble  therefore  PRON be not depth  but the ensemble  strike PRON as a false dichotomy  PRON would really only say that if PRON honestly believe that PRON need hundred or thousand of level of abstraction to classify image with human accuracy   PRON suggest PRON use the last wish for something else  maybe a pinacolada  
__label__matrices be there any reference  preferably available online as pdf  free would be good  which summarize the various matrix decomposition with PRON condition for use  usage  algorithm  complexity and underlying math  of course  a single resource may not have all of PRON but more the merry   PRON would love something which cover svd  cholesky  lu and ldu   PRON have occasionally look online for such thing  but PRON favorite reference  for just the basic  be trefethen and bau s numerical linear algebra textbook  PRON be  as such thing go  very short and eminently readable   PRON be one of the standard introductory book to algorithmic material on matrix decomposition and for a short introduction PRON know of nothing good  unfortunately  this also include free online material for comparison   the matrix cookbook and applied and computational linear algebra  a first course cover decomposition in chapter five  
__label__machine-learning __label__text-mining PRON be try to build a model that can be use to identify the word sale whenever a group of text be pass through the model  should PRON use azure or python for this  PRON have do the text preprocess and extract the n  gram feature in azure  what should PRON do next and which model should PRON use   PRON be always good to approach a problem use the simple possible tool  in PRON case there be no need to use a machine learning  ai  model to detect the word sale   to find the word sale within a long datastream  PRON would do the follow  parse through the entire string while look at segment of size 4 at a time  convert the substring to lowercase  determine if this substring be  sale   if PRON be save the index of the  s  into a list  return the list which will include the index of each start of the word sale within PRON text   public arraylistltintegergt  getindexsalestr inputstring    arraylistltintegergt  index  new arraylistltgt      string   queue  new string4    queue0   0   queue1   0   queue2   0   queue3   0   forint i  0  iltinputstringlength    i    queue0   queue1    queue1   queue2    queue2   queue3    queue3   inputstringcharati    string temp  javautilarraystostringqueuetolowercase     iftempequalssale     indexesaddi      return index     as mention by dan carter  if PRON do not care about the index of the word  sale  within PRON string then PRON be simple to use   public boolean getindexsalestr inputstring    ifinputstringtolowercasecontainssale    return true   return false    
__label__neural-networks __label__hypercomputation __label__recurrent-neural-networks PRON be prove that a recurrent neural net with rational weight can be a super  turing machine  can PRON achieve this in practice   PRON presume the proof the op be refer to can be find in this monograph by hava siegelmann   in PRON article  the myth of hypercomputation   the eminent computer scientist martin davis explain  p8  9  that there be nothing  super turing  about this formulation   edit  PRON be look like the claim about rational weight be super  turing be make in this  more recent paper by siegelmann  which introduce an additional assumption of plasticity  ie that weight can be dynamically update   PRON mean real number weight  specifically  irrational   this would require a machine that have unlimited precision over irrational value  PRON have see machine part that have many quality  PRON have never see one that have unlimited quality  qm may give PRON some magical transistor that can hold an unlimited number of different value  or by defer computation into the future and then teleport the result back into the past  PRON present   outside of that  for classical system  PRON would need a analog device that can output irrational value with unlimited precision  PRON do not think PRON have discover any device that can do that  
__label__fluid-dynamics __label__simulation this be not a technical question  and be ask just out of curiosity   be computational fluid dynamic  cfd  simulation use in animation movie   something like shallow water equation  explosion problem etc    in other word  can computational simulation of real world problem be use in art    PRON have tag the question with simulation  but please feel free to retag PRON as PRON can not find appropriate tag   after a bit of search  PRON discover that cfd be heavily use in film  game and special effect in medium  here be the wiki page of the relevant topic  reference therein be helpful as well   one of the leader in the field of use cfd for animation  ron fedkiw  have a web page with some fantastic example  include reference to the relevant publication   there be a paper in notice of the american mathematical society on this subject  crash waves  awesome explosions  turbulent smoke and beyond  applied mathematics and scientific computing in the visual effects industry  in particular  these commercial package constitute example of simulation software use in the film industry   yes a lot  see for example fumefx  3ds max plugin  grid base simulation  or realflow  shp simulation  and more   and there be still research go on in this field  see siggraph paper  PRON favorite this year be snow simulation  
__label__pde __label__finite-element __label__error-estimation let  omega be a convex polygonally bound lipschitz domain in  mathbb r2   let  f in l2omega  then the solution of the dirichlet problem  delta u  f in  omega   operatornametrace  u  0  on  partialomega have a unique solution in  h2  and be well  pose  ie for some constant  c PRON have  uh2  leq c fl2  for some finite element approximation  uh  say  with nodal element on a uniform grid  PRON have the error estimate    u  uh h1  leq c h  u h2  PRON seem  maybe PRON be wrong with that  that people usually do not use the obvious error estimate    u  uh h1  leq c h  f l2  which PRON can obtain by combination of the above two inequality  instead  a posteriori error estimater be develop in various form  the only objection PRON can imagine against the above equation be that the constant  c may in practice be too pessimistic or not reliably estimatable   the reason why people prefer to use the first estimate  in PRON opinion  be that the first one arise naturally from the galerkin orthogonality of the fem  interpolation approximation property  and most importantly the coercivity of the bilinear formfor poisson equation s boundary value problem  PRON be equivalent with the poincaré  friedrichs inequality for  h10  function       beginalign   u  uh2h1omega    ampleq c1  nabla  u  uh  2l2omega       nabla  u  uh  2l2omega    amp intomega  nablau uhcdot nablau uh      amp intomega  nablau uhcdot nablau mathcaliu      ampleq  nabla  u  uh  l2omega    nabla  u  mathcaliu  l2omega      rightarrow  nabla  u  uh  l2omega    ampleq  nabla  u  mathcaliu  l2omega   leq c2 h uh2omega    endalign      where  c1  depend on the constant in the poincaré  friedrichs inequality for  h10  function   mathcaliu be the interpolation of  u in the finite element space  and  c2  depend on the minimum angle of the mesh   while the elliptic regularity estimate  u h2omegaleq cfl2omega be solely on the pde level  have nothing to do with the approximation  plus above argument hold even when  fin h1 be a distribution   now move on to the reason why a posteriori error estimate be widely use  be mainly because   PRON be computable  there be no generic constant in the expression of the estimate   the estimator have PRON local form  which could be the local error indicator use in the adaptive mesh refining procedure  therefore  the problem with singularity or really  bad  geometry could be deal with   both of the a priori type estimate PRON list be valid  PRON provide PRON the information of the order of convergence  however none of PRON could be a local error indicator just for one triangle  tetrahedron  because neither of PRON be computable due to the constant  nor be PRON define locally   edit  for more of a general view of the fem for elliptic pde  PRON highly recommend read chapter 0 in brenner and scott s book  the mathematical theory of finite element methods  which consist only 20 page and cover briefly almost every aspect of finite element method  from the galerkin formulation from the pde  to the motivation why PRON would like to use adaptive fem to tackle some problem  hope this would help PRON more   PRON estimate be too pessimistic on two front  PRON have identify the first one already   c now not only include the interpolation constant but also the stability constant   the second one be that the error estimate really read     nabla el2  le c h  uh2     note that the right hand side have the  h2  seminorm  not norm  of course PRON can bound the rhs by the full norm  but PRON lose again this way  
__label__algorithms __label__spatial-data PRON have to write an algorithm  code to reconstruct an unknown 3d scalar potential  vmathbfr up to a specify threshold  ec  assume that the potential be monotonic   the problem have the follow rule   PRON start from the minimum of the potential and define a grid  PRON can only do  small  step  ie PRON can only move to the neighboring point with respect to where PRON be  random sampling be not possible   at the end of the procedure  all the point  mathbfri of the grid have  vmathbfriltec must be know   be the a algorithm or a keyword that PRON can look for in google for this kind of problem   thank  
__label__classification __label__feature-selection __label__feature-extraction PRON have a document classification project where PRON be get site content and then assign one of numerous label to the website accord to content   PRON find out that tf  idf could be very useful for this  however  PRON be unsure as to when exactly to use PRON   assumm a website that be concern with a specific topic make repeat mention of PRON  this be PRON current process   retrieve site content  parse for plain text  normalize and stem content  tokenize into unigram  maybe bigram too   retrieve a count of each unigram for the give document  filter low length and low occurrence word  train a classifier such as naivebayes on the result set  PRON question be the follow  where would tf  idf fit in here  before normalize  stem  after normalize but before tokeniz  after tokeniz   any insight would be greatly appreciate   edit   upon close inspection  PRON think PRON may have run into a misunderstanding at to how tf  idf operate  at the above step 4 that PRON describe  would PRON have to feed the entirety of PRON datum into tf  idf at once  if  for example  PRON data be as follow      tokenizedcontentsite1   categorystringsite1      tokenizedcontentsite2   categorystringsite2        tokenizedcontentsiten   categorystringsiten     here  the outermost structure be a list  contain tuple  contain a dictionary  or hashmap  and a string   would PRON have to feed the entirety of that datum into the tf  idf calculator at once to achieve the desire effect  specifically  PRON have be look at the scikit  learn tfidfvectorizer to do this  but PRON be a bit unsure as to PRON use as example be pretty sparse   as PRON have describe PRON  step 4 be where PRON want to use tf  idf  essentially  td  idf will count each term in each document  and assign a score give the relative frequency across the collection of document   there be one big step miss from PRON process  however  annotate a training set  before PRON train PRON classifier  PRON will need to manually annotate a sample of PRON datum with the label PRON want to be able to apply automatically use the classifier   to make all of this easy  PRON may want to consider use the stanford classifier  PRON will perform the feature extraction and build the classifi model  support several different machine learn algorithm   but PRON will still need to annotate the training datum by hand  
__label__numerical-analysis assume that PRON have a function  u define in a ball in a discrete way  PRON know only the value of  u in the node   i  j  k of spherical grid  where  i be a radius coordinate   j be a coordinate for angle  varphi   k be a coordinate for angle  psi  consider a vector  function    nabla ui  j  kleftfracpartial upartial ri  j  kfrac1risinpsikfracpartial upartial varphii  j  kfrac1rifracpartial upartial psii  j  kright  gradient of  u  PRON need to know the value of  nabla ui  j  k on z  axis in cartesian coordinate  which correspond to  psi0   axis in spherical coordinate  but PRON can not use the formula above  because in case  psi0  the second term turn to infinity   actually  PRON can find the value of  fracpartial upartial z with a help of the formula of numerical derivative  but PRON have a problem with find  fracpartial upartial xfracpartial upartial y  because the grid be not rectangular  could PRON help PRON with this stuff and advise PRON what to do   there be 3 way to avoid this situation  but before use one must check if this way be suitable due to computation error   1  green  gauss cell method  here the definition of gradient be use     nabla ui approxfrac1vi   intlimitspartial viu doverlinesapprox sumlimitsk1nufk  sk overlinenk     where  k  number of neighbour of cell  vi  2  least square method  the error    sumlimitsk1nfrac1dikei  k2    ei  knabla ui cdotdelta ri  kui  uk   must be minimize  hence PRON get the component of  nabla ui  3  interpolation method  the value of gradient be interpolate from the value of gradient vector  function  
__label__machine-learning __label__data-mining __label__cross-validation __label__evaluation PRON be wonder if sometimes  to validate a model  PRON be not good to use aucpr instead of aucroc  do these case only depend on the  domain   amp  business understanding    especially  PRON be think about the  unbalanced class problem  where  PRON seem more logical to use the aucpr because recall and precision be well  use metric for this problem   yes  PRON be correct that the dominant difference between the area under the curve of a receiver operator characteristic curve  roc  auc  and the area under the curve of a precision  recall curve  pr  auc  lie in PRON tractability for unbalanced class   PRON be very similar and have be show to contain essentially the same information  however pr curve be slightly more finicky  but a well draw curve give a more complete picture   the issue with pr  auc be that PRON difficult to interpolate between point in the pr curve and thus numerical integration to achieve an area under the curve become more difficult   check out this discussion of the difference and similarity   quote davis  2006 abstract   receiver operator characteristic  roc   curve be commonly use to present result  for binary decision problem in machine  learn  however  when deal  with highly skewed dataset  precision  recall   pr  curve give a more informative picture  of an algorithm ’s performance  PRON show that  a deep connection exist between roc space  and pr space  such that a curve dominate  in roc space if and only if PRON dominate  in pr space  a corollary be the notion of  an achievable pr curve  which have property  much like the convex hull in roc space   PRON show an efficient algorithm for compute  this curve  finally  PRON also note difference  in the two type of curve be significant for  algorithm design  for example  in pr space  PRON be incorrect to linearly interpolate between  point  furthermore  algorithm that optimize  the area under the roc curve be not  guarantee to optimize the area under the  pr curve   this be also discuss on kaggle recently   there be also some useful discussion on cross validated  
__label__software __label__eigensystem suppose solve sequential generalize eigenvalue problem    ai x lambda bx  i123ldot    in general setting  PRON always need to perform lu for matrix b   precondition  before to apply the rest iterative algorithm  be there a  numerical libraryi have programming experience with petscslepc  or a toolkit that can allow PRON to separate those two part  thus to perform lu only once   by default  lu factorization of  b be by direct solver  whose cost may be somewhat comparable  PRON suppose   update  thank to arnold  but PRON want to modify PRON problem a little  where  b have a null vector st    bmathbf1mathbf0quad mathrmrankb   n1   where  ai  b be both  ntime n sparse symmetric matrix  factor  pb  lu PRON and write a routine for evaluate  l1pau1x give  x  use two backsolf   then PRON can solve the problem   l1pau1lambda iz0  with a standard iterative solver for the ordinary eigenvalue problem   if  b be singular  compute a left null space basis consist of the row of  m  and a right null space basis consist of the column of  n  so that  mb0  and  bn0 then PRON can replace the eigenvalue problem by the modify problem with the matrix  apmatrixa  amp  sanma  amp  sc and  bpmatrixb  amp  anma  amp  c  with  c and  s arbitrary  if  x solve the original eigenvalue problem then  xpmatrixx0 be an eigenvector of the new problem with the same eigenvalue  now the kernel of the matrix  pmatrixbma be trivial since otherwise the eigenvalue problem be ill  pose  this imply that  b be a nonsingular matrix  thus one can apply the precede to the modify problem   the new eigenvalue problem also have the eigenvalue  s  attain for all vector of the form  xpmatrix0z therefore one should choose  s such that PRON lie somewhere in the middle of the  expect spectrum  
__label__matlab __label__finite-element __label__assembly PRON have be do finite element analysis use matlab  PRON look for many example and tutorial produce only the stiffness matrix let element be weightless  however  in PRON case i need to do a soil deformation where the self weight of element be significant   please please please PRON help and any guidance will be appreciate   a load due to gravity or self  wieght be commonly refer to as a body force in continuum  mechanic  finite element text often use this term when refer to this type of loading   for each finite element  an equivalent nodal force vector due to body force can be calculate  as     bf fei  int ni left  beginarrayc   bx  by  endarray  right  dv    where  ni be the element shape function at the ith node    bx the body force in the x direction  and  by be the body force  in the y direction  for a gravity load in the negative y direction   by  rho g  where  g be the acceleration due to gravity and  rho be the material  density   the element   bf fei be assemble into the global load vector  in a manner analogous to how element stiffness matrix be assemble   if PRON want to learn more  most finite element text that emphasize structural  analysis discuss this procedure in section describe calculation of  equivalent nodal load  for example  see section 42 in  httpwwwamazoncomfiniteelementprocedureskjbathedp097900490x 
__label__python __label__visualization __label__plotting  figured this be the most germane place for this question  but definitely let PRON know if not    two ask   how would PRON uniformly standardize the size and scale for each of the bivariate  ie  scatter plot   subplot  when use the pairplot functionality in seaborn  if the data be already standardized   by default PRON create the plot which be size the same  but scale differently   once the scale be uniform across all plot  PRON want all of the ax to be format such that each plot look like a coordinate plane  ie4 quadrant  both ax spine center    here be some add context on PRON specific use case   PRON have z  score for the world series win team for a set of variable  era  r  batting avg   etc    PRON want to plot all pairwise comparison for this select set of variable  and because all the datum be already standardize  the ax should range roughly from  35  35    PRON be less interested in regression  and more interested in  archetype   eg 1995 atlanta braves have the good pitching in baseball but be not exactly stellar hitter  PRON still win the world series  how many other ws win team would be in quadrant iii with PRON    here be code PRON have use to make one such plot  not use seaborn   but PRON want to take advantage of the power of do as many pairwise comparison be as possible   x  zscoreswswinnersteamba    y  zscoreswswinnersera    fig  pltfigure    ax  figaddsubplot111   axscatterx  y   plotaxesrange   3  3  3  3   axaxisplotaxesrange   equal    axspinesleftsetpositioncenter    axspinesrightsetcolornone    axspinesbottomsetpositionzero    axspinestopsetcolornone    axxaxissettickspositionbottom    axyaxissettickspositionleft    axaxhlinelinewidth1  colorblack    axaxvlinelinewidth1  colorblack    pltshow     this seem pretty tedious  and pairplot get PRON 90 of the way to what PRON desire    anyways  thank  in advance   
__label__topic-model __label__gensim __label__lsi what be mean by energy spectrum in lsilatent semantic indexing    PRON be do topic model with gensim s lsimodel  and part of the output per chunk be the following   info  prepare a new chunk of document  info  use 100 extra sample and 2 power iteration  info  1st phase  constructing  100000  600  action matrix  info  orthonormaliz  100000  600  action matrix  info  2nd phase  run dense svd on  600  20000  matrix  info  compute the final decomposition  info  keep 500 factor  discard 6560  of energy spectrum   info  merge projection   100000  500    100000  500   info  keep 500 factor  discard 0843  of energy spectrum   info  process document up to  1400000  the above output be from 1400000 document into the process   out of aproxx  3500000   and PRON appear to discard less and less for each chunk  the 2nd chunk be high   info  keep 500 factor  discard 6556  of energy spectrum   info  merge projection   100000  500    100000  500   info  keep 500 factor  discard 13469  of energy spectrum   info  process document up to  40000  PRON be not sure whether the discard x  of energy spectrum be good with high or low number  be the  energy  analogous to entropy  do discard mean PRON loose information  or do PRON mean the singular value decomposition be get good and good with more information   or none of the above   
__label__c++ __label__mpi __label__memory-management PRON be try to use the mpi share memory feature  PRON have several smp node  and each of PRON have four core  PRON need an array of size n for each node that should be access by all four core in each node  PRON plan be to construct a share window of size n4 use mpiwinallocateshar  and PRON expect that memory usage of each node would be n in the example below  n be 4x10  9 byte  but the memory usage of each node be not 4 gb but 16 gb  be PRON miss something    include  ltiostreamgt    include  ltmpihgt   int mainint argc  char   argv    mpiinitampargc   ampargv    int rankall   int ranksm   int sizesm    all communicator  mpicomm commsm   mpicommrankmpicommworld   amprankall     share memory communicator  mpicommsplittypempicommworld  mpicommtypeshared  0  mpiinfonull   ampcommsm    mpicommrankcommsm   ampranksm    mpicommsizecommsm   ampsizesm    stdsizet localwindowcount1000000000    char  baseptr   mpiwin winsm   int dispunitsizeofchar     mpiwinallocatesharedlocalwindowcount  dispunit  dispunit  mpiinfonull  commsm   ampbaseptr   ampwinsm     write  char buffer   if  ranksm   0    buffer   a      else if  ranksm   1    buffer   c      else if  ranksm   2    buffer   g      else   buffer   t      mpiwinfence0  winsm    for  stdsizet PRON  0  PRON  lt  localwindowcount  it    baseptrit   buffer     mpiwinfence0  winsm     read  long long int indexstart1  ranksm  localwindowcount    long long int indexendsizesm  ranksm   localwindowcount  1    for  long long int itrel  indexstart  itrel  lt  indexend  itrel    buffer  baseptritrel    if  itrel   indexstart    stdcout  ltlt  ranksm  ltlt   start    ltlt  buffer  ltlt  stdendl     else if  itrel    indexend  1     stdcout  ltlt  ranksm  ltlt   end    ltlt  buffer  ltlt  stdendl       mpifinalize     return 0     
__label__optimization __label__pde __label__numerical-analysis __label__discretization __label__elliptic-pde PRON be solve elliptic pde problem  for which  euler scheme look as follow     nabla  gamma  nabla u2  nabla u   0  where    gammanabla u2    1  nabla u212    PRON be aim to define minimization problem for problem above  do anybody have any suggestion how PRON should look like  or some recommend literature   let PRON assume  that PRON have discretiz problem and on every point PRON obtain  evaluation  gij    therefore  gu   0  be PRON system of nonlinear equation   now let  gamma to be norm function define as  gammax    gx2  then PRON nonlinear eq  problem be equivalent to the problem    textminimize     gammax    for the particular equation PRON be solve  call the minimal surface equation   the functional PRON be try to minimize be     ju   intomega sqrt1  nabla u2    dx      PRON can find a derivation of the equation  as well as a discussion of solution approach in lecture 315 and follow here  httpwwwmathtamuedubangerthvideoshtml  of course there be many other source for the same material as well  for example  nearly every book about the calculus of variation will have PRON example  
__label__machine-learning __label__deep-learning __label__nlp deep learning  dl  be now day be use in various field  PRON may like to know any idea  approach on usin dl for signature extraction from email   there be some paper which use ml like httpkegcstsinghuaeducnjietangpublicationsf142tangpdf  httpwwwcscmueduvitorpaperssigfilepaperfinalversionpdf   nbspnbsp  there be few more   there be some open source library in python like talon which use nlp  regular expression and other technique  and use ml algorithm  svm  for the same   PRON have do a little research but PRON be not get any resource or any idea on how to proceed with dl for this task   PRON think of use seq2seq model but PRON be not sure PRON can be of much help for this task  PRON be not sure whether summarization technique can be useful   any idea or resource for the same can be helpful   detect a signature in an email be more like a detection problem in an image rather than a natural language processing  nlp  problem  PRON do not particularly matter what the signature say  PRON simply want to be able to detect PRON   in such a case  detection in image be currently dominate by convolutional neural network  cnn   this be a very powerful technique which have prove to be the good for a multitude of image  base task  however  PRON require a very large dataset  so PRON will need to collect a lot of varied example and annotate PRON PRON   the output of the cnn can provide either a binary output 01 determine if a signature exist  or PRON can provide an output which be the coordinate of the center of the signature  x  y coordinate  such that PRON can know where PRON be place in the document   in this problem  the signature in each email be relate to the text that be before the signature that be why PRON would suggest use recurrent neural network like lstms or gru especially if PRON be a classification task  PRON can also try the lstm seq to seq model if PRON task be to output the signature  
__label__python __label__fortran __label__performance PRON try to use f2py  cython and numba to make a simple loop structure be faster in python  python implementation   def pythona    b  00  for i in range100    for j in range100    for k in range100    for l in range100    b  b  ai  j  k  l   return b  numa implementation   jitdoubledouble            nopython  true   def numbaa    b  00  for i in range100    for j in range100    for k in range100    for l in range100    b  b  ai  j  k  l   return b  cython implementation   cdef double cythondouble          a    cdef double b  cdef int i  j  k  l  b  00  for i in range0100    for j in range0100    for k in range0100    for l in range0100    b  b  ai  j  k  l   return b  f2py implementation   subroutine f2pyruna  b   integer   i  j  k  l  real8   dimension100100100100   intentin    a  real8   intentout    b  b  00d0  do i1  100  do j1  100  do k1  100  do l1100  b  b  al  k  j  i   end do  end do  end do  end do  end subroutine f2pyrun  cython compilation   setupextmodule  cythonizeextensionruntestruntestpyx      includedirsnumpygetinclude      f2py compilation   f2py c runfortran  f90 f90flags  o3 m fortranrun  the function be run by pass a nprandomrnd100100100100  into the function  the timing be as follow   22998 sec  python  0114 sec  cython  2678 sec  f2py  0106 sec  numba  PRON know that for this specific problem PRON have set up  PRON can use npeinsum    npeinsum   have a timing of 0061 sec  but for PRON  real  code the problem can not be solve with einsum  PRON can be see that cython and numba execute at about the same speed  whereas f2py be much slow  the f2py function be write in fortran90  so PRON would have think PRON would have be atleast as fast  PRON question be thus  be PRON use f2py wrongly  or do f2py have speed problem   update 1  by suggestion from stefano m  PRON make a f2py perfermance report      lt  f2py performance report  gt     overall time spend in    a  wrap  fortran  c  function    105 msec   b  f2py interface   1 call    2925 msec   c  call  back  python  function    0 msec   d  f2py call  back interface   0 call    0 msec   e  wrap  fortran  c  function  acctual    105 msec  as PRON can be see almost all the time be use in the interface  here be how to function be call   import numpy as np  import time  import fortranrun  a  nprandomrand100100100100   start  timetime    b  fortranrunf2pyruna   printtimetimestart  b   f2py    PRON think that the problem be link to the way in which f2py generate the fortran interface  the argument to fortranrunf2py should be store as a fcontiguous array  otherwise the interface will create an internal copy with the correct storage order   python 362  default  jul 22 2017  211922    gcc 711 20170516  on linux  type  help    copyright    credit  or  license  for more information    gtgtgt  import numpy as np   gtgtgt  import fortranrun   gtgtgt  a  nprandomrand100100100100    gtgtgt  aflag  ccontiguous  true  fcontiguous  false  owndata  true  writeable  true  aligned  true  updateifcopy  false   gtgtgt  atflag  ccontiguous  false  fcontiguous  true  owndata  false  writeable  true  aligned  true  updateifcopy  false   gtgtgt  b  fortranrunf2pyat    gtgtgt      lt  f2py performance report  gt     overall time spend in    a  wrap  fortran  c  function    761 msec   b  f2py interface   1 call    0 msec   c  call  back  python  function    0 msec   d  f2py call  back interface   0 call    0 msec   e  wrap  fortran  c  function  acctual    761 msec  here PRON see that a be ccontiguous  while the transpose at be fcontiguous  and call b  fortranrunf2pyat  have no f2py interface overhead   on the contrary  when PRON call b  fortranrunf2pya   since a be not fcontiguous  the f2py interface will allocate scratch memory and copy the original array a in the correct storage order   memory  order be a common topic for c  fortran interface  PRON should either construct a as a fcontiguous array  or pass at to PRON fortran routine  but remember that PRON be operate on the transpose array   warning about array argument copy can be generate with compile option  df2pyreportonarraycopy1  see also internal organization of numpy array 
__label__rnn PRON have lack of understanding this issue  could anybody explain PRON or give an advice to a good literature regard PRON  PRON do not understand what be a explicit density model and how PRON differ from an implicit density one  could anyone exemplify the difference between these two kind of model   picture source  httpsarxivorgabs170100160  the main difference between explicit density model and implicit density model be that explicit density model  use an explicit density function and implicit density model do not  in other word explicit model assume some prior distribution about the datum  for example the work of dinh et  al   which in the reference link be classify as an explicit model  use an isotropic unit norm gaussian as the prior  page 7  of the above link   one example of implicit density model be the work of bengio et  al  also mention in the reference link  to estimate the distribution of the datum PRON learn the transition operator of a markov  chain whose stationary distribution estimate the datum distribution   this paragraph in this paper may be of help  methink  
__label__python __label__markov PRON have look at hmmlearn but PRON be not sure if PRON be the good one   sklearn have an amazing array of hmm implementation  and because the library be very heavily use  odd be PRON can find tutorial and other stackoverflow comment about PRON  so definitely a good start   httpscikitlearnsourceforgenetstablemoduleshmmhtml 
__label__dimensionality-reduction should one apply dimensionality reduction method to the datum set before or after train  test splitting   anyway  in case of train a model with preprocess by dim  red  one should apply the same dim  red to the future instance to be predict  right  but how could PRON reduce the dimensionality of one single instance in the same way   PRON depend on the type of algortithm PRON use for dimensionality reduction  in case PRON use pca  PRON should build PRON pca on PRON train set  then PRON need to set PRON principal component to transform PRON point in test set into the same space  this way PRON can then use train and test set in the same reduct space   PRON should always use just PRON training datum and then apply the same transformation on PRON test datum  this be a much fair representation of how PRON model will perform when real new unseen sample be feed to PRON model  the point of a test set be try to estimate generalization on unseen datum  use PRON test datum leak information into PRON training set  plus  if PRON can not apply the transformation to PRON test datum  how will PRON apply PRON to new datum  
__label__k-means __label__orange __label__pca PRON have try to repeat the same result with the same flow  and PRON do not understand the result be different in each situation   PRON describe the situation PRON have a file with 192 instance and 37 feature  y select in all case the same column and preprocess by median and stddev  PRON compute the pca with 7 principal component  the follow step be to run the k  means algorithm  k be between 2 and 8 from this  new  dataset  the scatter plot show the result for k5   PRON attach different image with PRON flow   the first one be the original flow  PRON be paint of yellow color   which PRON would like to repeat without the rest of the option  the second image    however  when PRON try to do PRON  PRON see that the result be different  the third image  of course the color do not determine the difference  however the cluster be different  in addition the slhouette scores be different too for the different flow   k  mean initialize with the kmean and PRON have the question if PRON can  control  this  or if the way to initialize k  means be always randomly  PRON see in other programme that there be an option call seed which be use to control that an experiment can be repeat but PRON do not see this option here or something similar   PRON do not understand what be happen here and if there be some way to obtain always the same result with the same flow   
__label__sampling PRON be make a hierarchical model which consist of   a matrix  xij of bernoulli random variable  for each  i  all random variable  xij be iid  with probability of equal  1  equal  pi   pi sim betaalpha  beta for the same  alpha and  beta each time    alpha  mumu2  mu  sigma2sigma2    beta   mu  1mu2  mu  sigma2sigma2   the prior on  mu be the uniform distribution on   01  the prior on  sigma2  be condition on  mu  and be the uniform distribution on   0  mu1mu  PRON have try estimate  mu and  sigma2  in two way  one way be via numerical integration  and that be go ok except PRON need to come up with a new integral or sum if PRON want to find the posterior of  fmu for any  f the other way be via mcmc use pymc2  but PRON be get out a roughly uniform posterior on  mu over   01  and that be clearly wrong   PRON question be  why be PRON get a uniform posterior  the posterior should be unimodal and very concentrated   
__label__python __label__numpy __label__special-functions PRON have the follow function that have be vectoriz so that for every element in input array t  an array be output   npvectorize  def hamt    d  nparraynpcostnpsqrtt01dtype  npcomplex128   return d  PRON be get an error  typeerror  only length1 array can be convert to python scalar  PRON believe this error happen when there be conflict with numpy and python  but PRON can not see where that would be  also  if the dtype be not give  the error be valueerror  set an array element with a sequence  PRON be guess PRON can not assign array inside array in python  but how else can PRON do this   the problem be that npcost  and npsqrtt  generate array with the length of t  whereas the second row   01   maintain the same size   to use npvectorize with PRON function  PRON have to define the output type  and npvectorize  be not really mean as a decorator except for the simple case   in this way however PRON can generate the function with the right type   def hamt    d  nparraynpcostnpsqrtt01dtype  npcomplex128   return d  hamvec  npvectorizeham  otypesnpndarray    now PRON can use hamvec as a function    gtgtgt  x  nparray123     gtgtgt  hamvecx   array   array    054030231  0j   100000000  0j     000000000  0j   100000000  0j      array041614684  0j   141421356  0j     000000000  0j   100000000  0j      array098999250  0j   173205081  0j     000000000  0j   100000000  0j      dtype  object   note   the npvectorize  be just a convenience function  PRON do not actually make code run any faster   this question may have be more suitable for stackoverflow   edit  as an answer to the follow up question  the result value of the matrix be of type numpycomplex128    gtgtgt  y  hamvecx    gtgtgt  typey000     lttype  numpycomplex128gt   and PRON can do for example    gtgtgt  ynpcomplex3  2j    array   array    162090692  108060461j   300000000  2j      000000000  0j    300000000  2j       array124844051  083229367j   424264069  282842712j     000000000  0j    300000000  2j       array296997749  197998499j   519615242  346410162j     000000000  0j    300000000  2j       dtype  object  
__label__bigdata term like  data science  and  data scientist  be increasingly use these day   many company be hire  data scientist   but PRON do not think PRON be a completely new job   data have exist from the past and someone have to deal with datum   PRON guess the term  data scientist  become more popular because PRON sound more fancy and  sexy   how be datum scientist call in the past   in reverse chronological order  datum miner  statistician   apply  mathematician   PRON do think PRON be new job  basically datum scientist have to apply mathematical algorithm on datum with considerable constraint in term 1  run time of the application 2  resource use of the application  if these constraint be not present  PRON would not call the job data science  moreover  these algorithm be often need to be run on distribute system  which be another dimension of the problem   of course  this have be do before  in some combination of statistic  mathematic and programming  but PRON be not wide spread to give rise to the new term  the real rise of data science be from the ability to gather large amount of datum  thus need to need to process PRON   term that cover more or less the same topic that data science cover today   pattern recognition  machine learning  data mining  quantitative method  also   business intelligence developer   an ideal data scientist be 60  70  statistician and 30  40  a computer scientist and so the old name of  data scientist  be somebody who be part statistician and part computer science guy   in several subfield  some be simply call analyst  if PRON go back earlier in time  in a pre  science era  PRON tend to believe that people involve in divination or astrology  several of PRON because PRON be pay for that  much more than for serious science  be precursor   on a light note  PRON like the neologism dedomenology  in french  dédoménologie   and PRON tend to consider that hugo steinhaus  the inventor of  kmeans  be  due  to PRON interest in many applied field  one of the first to spark the fire of data science   some really nice answer already  however  PRON would break the entire process of break the work of a data scientist into who actually do those   get the datum from database and other source  generally  PRON use to be the dba who get the datum from the db s and the people who collect datum from other source be call datum guy  PRON do not really have a specific name  atleast in india   and the scraping and crawl script be write by software engineer who be hire especially for that purpose   analytics and prediction  do by people call the statistician or the mathematician   visualizations and reporting  do by people call business analyst or the mba guy in the company   big data and pipelin stuff  do by software engineer hire especially for thar particular purpose  
__label__neural-network __label__convnet __label__convolution PRON recently read real  time single image and video super  resolution use an efficient sub  pixel convolutional neural network by wenzhe shi et al  PRON can not understand the difference between deconvolution  mention in section 21  and the efficient sub  pixel convolution layer  escl for short   section 22   section 22 define the escl as perform convolution with a fractional stride  be not this what deconvolution be  why be a distinction be make   thank  
__label__languages PRON be not try to start a language flame war here  PRON merely want to ask a simple question   be there a good resource for comparison for language for scientific computing   by comparison  PRON mean on the ground of   number of stable library which can be use   programming effort  speed  etc   PRON find a couple of comparison online but all seem to be relevant to general computer programmer  not for numerical computing    also  PRON be ask the question for general scientific computing  not necessarily for matrix algebra  for instance  this also include statistic  engineering etc   PRON be not try to ask a  which language should PRON learn  type of question  PRON be ask for good comparison   PRON be especially interested in c  java  cc  and python  the goal be to build usable code in variety of scientific application with different hardware  include android  use as many stable library along the way   PRON also read parallel scientific computation software development language  but PRON be too focussed on matrix algebra   PRON be possible to do any feasible computation in any turing  complete language   thus  the comparison be not between language but between compiler  runtime  and library set  this assume that those who devise and implement the algorithm be competent enough in the choose language  the runtime and library  as  ship  should be use the low complexity algorithm   the benchmarks  game be a fantastic resource that could be use for educational purpose  really an eye  opener for performance  obsess type  and if PRON be deal with realistically  sized problem  PRON be one of PRON    however speed be not everything  the real devil lie in small detail  correctness    how be float  point number parse  rounded  and output   be there any library function to force a specific rounding mode   be math function work for corner case  plus and minus zero  etc     be there any handling for underflow   do build  in facility for handle complex number conform to accept practice and standard   must say PRON question be mislead  there be precious few thing in statistic and engineering and whatev other discipline that can be accomplish without matrix computation   PRON would reccomend such an approach  PRON should learn one general purpose  generate fast code  compile  language like c  c or fortran  PRON favourite be c  on each of those PRON can use easily dll s  PRON have blas  lapack libs  also there be cuda  fortran and c  or opencl  look like c  extension for compute on gpu   on the other hand PRON be convinient to use a so  call script language that be orient for scientific computing  so PRON could easily check if something will work at all  to make nice plot easily and so on  PRON have use scilab  and now PRON be try to learn mathematica for this purpose   PRON choice be  c  a lot of available numerical  blas  lapack  gnu math library  cgal    and visualization  vtk  lib  for high efficiency work  not hpc computing  mathematica or maple be quite useful but be sometimes annoying  but really powerful   PRON advise to student be always this  the most precious resource of all be PRON own time  if there be a programming language a that allow PRON to write a program in half the time PRON would take PRON with language b  but the program be 20  slow  then this be still almost always a win  unless PRON need to run the program for month   this be the reason why most modern scientific computing library be write in c and no longer in fortran or plain c  because PRON be so much fast to write and debug code in c than in fortran  and because of the huge number of other library PRON can then use  including  in particular  the c standard library  and that make programming so much faster  
__label__machine-learning __label__deep-learning __label__nlp __label__keras PRON be still new to machine learning and just come across powerful deep learning library  keras   PRON have read keras document and try few keras example on github here  PRON have also study some basic knowledge and concept of deep learning from several source but still have not really have solid understanding in cnn and rnn which look to be very powerful network   so  to prove PRON assumption  PRON download reutersmlppy example from keras github which originally use simple mlp network as a model  PRON combine the idea of cnn which PRON get from imdbcnnpy example to reutersmlppy example and then observe the result   surprisingly  the result do not come out like PRON expect  cnn perform bad than simple mlp network  can someone please explain why the accuracy of cnn be low than the simple mlp network   here be the output  tensorflow as backend   8982 train sequence  2246 test sequence  46 class  numwords1000  mlp  sequencestomatrix  mode  bianry    epoch 15  89828982                                  3s  loss  13236  acc  06984  epoch 25  89828982                                  2s  loss  07182  acc  08250  epoch 35  89828982                                  2s  loss  04544  acc  08864  epoch 45  89828982                                  2s  loss  03197  acc  09192  epoch 55  89828982                                  2s  loss  02511  acc  09356  19202246                              eta  0s  test loss  105213204963 test accuracy  0785396260071  cnn  padsequence    epoch 15  89828982                                  81s  loss  19794  acc  05181  epoch 25  89828982                                  78s  loss  14289  acc  06591  epoch 35  89828982                                  79s  loss  11546  acc  07175  epoch 45  89828982                                  78s  loss  09639  acc  07663  epoch 55  89828982                                  77  loss  08378  acc  07935  22402246                                  eta  0s  test loss  0960687935512  test accuracy  0764470169243  cnn  and rnn  model be not general improvement to the mlp design  PRON be specific choice that match certain type of problem  the cnn design work best when there be some local pattern in the datum  which may repeat in other location   and this be often the case when the input be image  audio or other similar signal   the reuter example look like a  bag of word  input  there be no local pattern or repeat relationship in that datum that a cnn can take advantage of   PRON result with a cnn on this datum set look reasonable to PRON  PRON have not make a mistake  but learn how a cnn really work on this data  
__label__machine-learning __label__deep-learning at the bad case scenario  PRON could treat the pretrain weight as a random initialization  same as what PRON would do for training from scratch  right  if that be the case  then would not PRON be good to always start with a pretrain model  as the low layer of weight have already probably learn general pattern of image that be transferable across all data set   PRON worry be what if the dataset PRON want to use for finetuning be highly specialize  highly unnatural and very different from the dataset the pretrain model be train on  would this still mean finetun from a pretrain model would not be the good idea   eg training on x  ray image instead of natural image for a cifar1000 pretrain model    
__label__word2vec __label__word-embeddings __label__gensim of all the example PRON have find for doc2vec training  the document be uniquely label  what happen when many document share the same label   taggeddocument of gensim accept a list label for the same text  PRON imply PRON can have multiple label for the same text  however  PRON be not clear to PRON if PRON be a good practice to have fragment text under the same label  PRON can still train and get the embedding  but be PRON good   for example  the question PRON be post here have a title  a detailed description  and a list of tag  how do PRON model PRON for doc2vec to find similar question   note that some of the tag be not in the title nor the description  what be the good way to include PRON in the doc2vec training  shuffle PRON and concatenate with title and description  or keep PRON as separate entry under the same label   for the example PRON pose  the answer and discussion on this question may be helpful  some good point on option around labelling   doc2vec  how to label the paragraph  gensim   PRON have try to explain the logic behind label use in document vector in doc2vec  how to label the paragraph  gensim   to answer PRON question   1  when two document share the same label  then doc2vec algorithm determine the semantic meaning of the label from both the document  note that doc2vec learn the semantic meaning of label not individual document   2  again  PRON be not learn document  PRON be instruct doc2vec to learn the embedding for the label  so  if multiple label be give for a document  all receive the same semantic meaning from the document and when a part of label when use in other document  keep on learn more semantic meaning from PRON   for instance  doc1  hunt  bite  eat  flesh doc2  life  love  eat  money  PRON be clear that doc1 be about animal and doc2 be about human  the label eat will have semantic meaning from both of PRON   3  if PRON goal be to find similar question such as this  then PRON should probably give just a single label for the whole question and then find a question with a label have close cosine distance   4  never confuse label with word  in word2vec  word learn embedding and in doc2vec  label learn embedding from the word use in document  if PRON would like to add some more semantic meaning to a document  then PRON could add PRON to the particular document as word  if PRON want to add semantic meaning to a label  the word have to be add to each document which carry that label if PRON want the label to have strong affinity  but add word manually  do not sound like a good option to PRON personally   
__label__machine-learning __label__r __label__predictive-modeling __label__decision-trees PRON be try to build a regression tree with 70 attribute where the business team want to fix the first two level namely country and product type  to achieve this  PRON have two proposal   1build a separate tree for each combination of country  amp  product type and use subset of the datum accordingly and pass on to respective tree for prediction  see here in comment  PRON have 88 level in country and 3 level in product type so PRON will generate 264 tree   2build a basic tree with two variable namely country and product type with appropriate cp value to generate all combination as leaf nodes264build a second tree with rest all variable and stack tree one upon tree two as a single decision tree   PRON do not think the first one be the right way to do  also  strike on how to stack the tree in second approach  even if PRON be not the right way would love to know how to achieve this   please guide PRON to approach the problem  thank   depend which tree algorithm PRON want to use PRON could manually construct the two first level of the tree  PRON can just follow the pseudo code explain for example here for the c45 tree  once PRON have do this PRON can remove the two feature from the datum set and create tree for the remaining part of the tree  if PRON want to create a rpart object PRON would be require to take some part of the source and this may be a bit more demanding  depend on what tree algorithm PRON use PRON will just have a binary split at both level so PRON will only need to build 4 separate tree and not 264  note that PRON may not have the optimal decision tree since after step through the first two level  the country and product type may still be variable that because a split  but without see the data be impossible to tell   side note  PRON may be valuable to explain the business that country and product type be not the most sensible variable to have in the top of the decision tree  sometimes PRON be good to educate the end user than to force machine learn to do something inaccurate  in PRON experience end user prefer to have a correct solution than a solution that work because people have a gut feeling that PRON should be in a certain way   PRON think PRON could do this fairly automatically if PRON be open to use python  a library call automl  have a feature call categorical ensembling  where PRON can explicitly say  PRON want a model build for each level of this feature   if PRON make a feature that be country  product type and use that as PRON category  the rest should be pretty easy    disclosure  PRON have make minor contribution to automl  PRON be fos under the mit license  
__label__reinforcement-learning __label__ai-basics in order to model a card game as an exercise PRON be think an elementary setting as a multiarm bandit  each lever be the distribution of expect reward of an specific card   but of course the player only have some card in the hand each round  or equivalently for a give round PRON have available a number  n of arm randomly select from the total number  n of lever  be this just a  contextual bandit  or have PRON some specific  narrower  name that PRON could use to look up in the literature   
__label__machine-learning __label__neural-network __label__deep-learning __label__lstm PRON be try to learn how lstm network work  and even if PRON get the basic  the detail of the internal structure be not clear for PRON   on this blog link  PRON find this scheme of a lstm architecture  where apparently  every circle should correspond to an individual lstm unit like this  be this correct   be each unit in the cell independent from the other  or do PRON share information   imagine PRON have the follow configuration   number of sample  1000  number of time  step  10  number of feature  5  in this case  each unit in a cell will take as input a vector of size 5 right   but what will be the size of the output for one unit  1   thank  PRON think PRON image be mislabel   PRON think each blue box be an lstm layer  compose of multiple cell  unit  each of which accept a vector input xt   with that  the answer to PRON question be   1  yes  2  yes  PRON be independent  at a single time step  PRON share information with each other between time step   3  yes  each unit cell will take an input of size 5  PRON think the output size be always 1  similar to neural network node  like sigmoidal unit  that combine and then activate   actually the shape be for simplification  if PRON want to know the correct behavior PRON have to take a look at the formula of each lstm cell  to answer PRON first question  there may be different answer  what the picture be depict belong to task which be many to many  and for each input PRON need exactly one output  there be different task for sequence that can be define   one to one  one to many  many to one  many to many  PRON can take a look at here   for illustrate the formula of each lstm cell PRON have give the following picture from professor andrew ng course about deep learning   as PRON can see  each node in the lstm cell can be connect indirectly to the output of the adjacent cell of the previous time  step  PRON be indirect because there be gate between PRON  also consider that lstm cell share the weight for all the input of different time step  consequently each neuron in lstm cell be dependent to the input of the current time  step and the output of the adjacent node of the previous time  step   about the third question  the input size will be equal to the number of feature of the input for each time  step  the number of the output depend on PRON task as PRON refer to at first  take a look at the first link    be this correct   yes the diagram both look correct to PRON  the key thing to understand both diagram be that the input and output of an lstm cell be vector   the circle in the first diagram do represent the concept that the layer contain multiple individual artificial neuron  and that may make PRON assume that the second diagram be a picture of one of those neuron  arguably there be multiple  neuron  or sub  layer with different role inside a cell  because there be multiple place in which calculation of the form  fwmathbfx   b occur  perform slightly different role  PRON think the term  cell  be use to refer to this architecture of neuron  as a short  hand when PRON say  neuron   PRON tend to think of the output stage of the hidden layer   however  in the second diagram  all of the operation show work with vector  most importantly the leave  to  right arrow in the second diagram represent vector of hidden state from timestep to timestep from the whole layer  so each neuron in the cell be recurrently connect to every other neuron in that cell  twice as PRON happen in an lstm  because lstm have both an internal cell state and a layer output   be each unit in the cell independent from the other  or do PRON share information   to match PRON description of the diagram  let PRON define a  unit  as a collection of one of each type of neuron  gate use to make up the cell  that in theory could be wire together to make a work lstm cell layer with a single scalar cell state and output value   these unit be independent in that each have PRON own weight parameter  there be no share parameter for the connection between input and the unit  or for the recursive connection that forward state from one time step to the next  in that sense the unit do not share information   however  the connection do mean that on each time step  input datum and hide state plus output from last output from all other unit in the cell be combine be use in calculation  any cell unit can base PRON new internal state plus PRON output on the value of all other output and internal state from other unit in the cell  in this sense  the unit do share information  PRON guess from PRON question that PRON be probably this second issue that PRON be concerned about  as the second diagram make PRON think of a wiring diagram for a single neuron  but as explain above that be not the case   imagine PRON have the follow configuration  number of sample  1000 number of time  step  10 number of feature  5  in this case  each unit in a cell will take as input a vector of size 5 right   almost  each neuron inside the cell will take an input of 5 from  mathbfx  plus an input of the hidden layer output   mathbfh so if in PRON case the lstm cell size be 10  then each neuron would take a combined vector of 15  in addition  a second cell state vector be maintain  not label in PRON diagram  that be not use directly as an input to any neuron  ie component of the form  fwmathbfxb   but do interact with the other value and can get change PRON  through the various gate  in PRON second diagram  PRON be the uppermost arrow go leave to right   but what will be the size of the output for one unit  1   the cell as a whole will have the output of whatev size PRON have make the layer  that be what the diagram number 2 try to show  however  use PRON work definition of  unit   the output of each unit will be two scalar value  the hidden layer output  and the cell state  which will be part of PRON respective vector show in the diagram  
__label__neural-networks __label__deep-learning PRON be train a generative model and the validation loss always decrease then increase after several epoch  PRON be wonder if this be normal   
__label__reference-request __label__optimization __label__machine-learning PRON be look to learn more about sparse optimization and apply PRON to machine learning problem  could PRON please recommend some book  resource on this topic  both theoretical and apply be fine   PRON sound like PRON be interested in iterative method in optimize solution for a sparse system  if so  PRON can consult the follow book   httpwwwuserscsumnedusaaditermethbook2ndedpdf  first  just to clarify thing  PRON be talk about solve optimization problem of the form   min  x 1  subject to    ax  b 2  leq delta  and related form  right   there be many different application in which problem be formulate as the minimization of the 1norm of a vector subject to linear or least square constraint   an important area of theoretical research be prove condition under which solve the  l1 minimization problem will recover the sparse solution   many folk work in convex optimization have gravitate to this area because of the new find demand for solver for these problem   the method that PRON develop can be use in many different application  and the solver be effectively black box to most user of the code   PRON be not clear whether PRON be more interested in the application of sparse optimization to machine learning or in theoretical question or in method for actually solve the result optimization problem   these be really very disjoint though related area of research   as a starting point  PRON would suggest look at the list of resource for compressive sensing at   httpdspriceeducs  if PRON can supply more detail about what PRON be PRON want to learn about  and also give PRON some idea of PRON background in optimization and other area of mathematic  then perhaps PRON could suggest more specific reference  
__label__statistics __label__nlp __label__text-mining __label__time-series PRON will set the question up with an example  PRON be analyse news coverage text datum from 2014  and find that a term appear less often in the third quarter of 2014 than the final quarter  let PRON imagine PRON be the term  christmas    unfortunately  there be also far less news article in the third quarter than in the second  due to the lack of news in the summer   so how do PRON accurately compare the count in each quarter  PRON assume that there will be a great number of occurrence in the fourth quarter  but how much do the magnitude of this difference depend on the change in size of the underlying text   heap s law show the relationship between text size and number of unique term  PRON be non  linearity imply that the rate of new  unique word introduce by the text decrease as PRON increase the size of the text  and the proportion of the text take up by each exist word therefore increase  this apply give document take from the same  distribution  of text  in other word the underlie zipfian distribution of word rank be identical  see wiki    in PRON example above this be obviously not the case  since the underlie topic  and resultant term distribution  will change between summer and winter  especially with regard to the term  christmas   but take the same term count but over the whole of 2013 and 2014  PRON would reasonably expect the general underlying term distribution to be the same in each period  so heap s law apply  but what if the volume of text have change  simply normalising by the size of the text  or the number of document  do not  as far as PRON can tell  account for the relative change in expect value of the term count   PRON have a hunch that this may be a simple application of heap s or zipf s law  but PRON can not see how to apply PRON to this particular question  appreciate any insight   if PRON be look to calculate the relative importance of a word in this scenario  PRON may consider do an inverse document frequency score on a quarterly basis   tf  idf  once PRON have inverse frequency score for all the word in the corpus from quarter to quarter  PRON can normalize by the range of all score  and then do a quarter to quarter comparison   if PRON be try to do a prediction  historical datum from previous year would be very useful   PRON can find trend over the course of a year by do a regression on historical datum and normalize to the predict document output for that year   the approach suggest by jagartner be good if PRON want to analyze the quarter  to  quarter change in frequency for a give term  consider only that term in isolation   what PRON do not do be evaluate the relative frequency compare to all other term in the corpus   for this PRON could compare frequency rank for all term quarter  to  quarter  this be the analysis use in zipf s law   another advantage of this analysis be that PRON can test whether the generate process be or be not stationary  eg govern by same distribution quarter to quarter    the advantage of compare frequency rank be that PRON do not depend on the relative size of the corpus for each quarter  as long as PRON be all  large     in simple language  the frequency rank of the word  christmas  in the 4th quarter will probably be the same  regardless of whether the  us english news  corpus be 10000 article or 1000000 article   both previous answer take an interesting approach  but neither really tackle exactly what PRON be ask  how to compare a give term frequency over time  and compare with other term over the same period  in a statistically rigorous way that account for the power law distribution of term frequency with collection size   the answer below be PRON work so far on the problem  PRON be not  by any mean  a complete answer  however PRON post PRON here to elicit feedback and suggestion for improvement   take the definition of zipf s law      fr  sim zmax  ralpha      here   r be the rank of a give term   zmax be the frequency of the most frequent term   alpha be zipf s exponent  and  fr be the frequency of the r  th rank term   take the log of both side  PRON get a linear relationship between  logf and  logr  with gradient  alpha and intercept  zmax   logf  sim logzmax    alpha logr  PRON want to find the expect frequency of a single term over a give period  first  split the period into equal sized window  for the first window   i  PRON know the frequency of PRON term  fi  and the frequency of the large term  zmax  i and  alpha PRON can use these to find the rank of PRON term    r sim leftfracfirzmax  irightalpha  this rank can then be use to calculate the  expect  frequency for all subsequent time window  which PRON will indicate with  hatf    hatfi1r  sim zmax  i1  ralphai1     the ratio  frachatff represent the normalise frequency   some observation   for a stationary underlie distribution  this approach identify minor deviation from the expect frequency of a term  a move window could be use if the underlie text distribution change dramatically  or exhibit seasonal behaviour   the initial choice of  r could be calculate for the entire period  rather than the first window    alpha could be calculate from the raw term and total frequency count use heap s law  see lu et al  2010 for a derivation of heap s law from zipf s    this approach assume a linear relationship between  f and  r in log space  
__label__cross-validation PRON be new to machine learning  though PRON have a background in statistic  but PRON have a question about  kfold cross  validation  so PRON understand the basic idea that PRON divide the dataset into  k partition and then train a model on  k1  partition while test on the  kth partition that be leave out  so PRON do not want to train the model over the entire dataset  but instead over just PRON  k1  partition   PRON question be how do PRON handle change in the model parameter with each fold in the process  another way of ask this question be how do PRON choose the model to test  so when PRON train on  k1  fold  in each iteration there will be a subtle change in the parameter estimate  namely the  beta0  ps will change  so when PRON test the model against the testing partition  PRON be not test the same model each time  because the parameter coefficient be different   give that the  betas change for each fold  how do PRON choose which model to use  and once PRON choose a model  do PRON need to test only this model for the  k  fold cross  validation  for otherwise PRON be test different model and then average PRON prediction error   any clarification would be appreciate   there be two main  set of  thing PRON model need to learn from training  parameter and hyperparameters   parameter be those that can be infer  learnt  from datum  these be internal to the model and will be calculate in the training phase during  the hyperparameter however  can not be learn from datum and need to be decide  make assumption  before training  as PRON come from statistic  an example could be model a distribution from a set of sample  parameter could be mean and std  and hyperparameter could be select a normal distribution or a beta distribution   if PRON do not have hyperparameter  eg simple regression model   PRON only need to train the model and let the algorithm find the optimal weight  parameter  use the datum  however  if PRON want to use let PRON say support vector machines  then PRON need to provide some hyperparameter such as the cost  kernel  parameter of the kernel  and so on  before start the training   cross validation be use to find the optimal hyperparameter  PRON be do by create a grid of potential hyperparameter value and train each combination of PRON against the whole datum perform k training of k1 portion of datum  each time the model be evaluate use one specific combination of hyperparameter and PRON get a number of prediction equal to the number of training sample  the combination that yield good result base on the specify metric  be consider the good option  final model    have a look at this nice blog for further clarification   cross validation procedure   PRON think titoort get at the bulk of the confusion here  PRON be try to explain how to get back the training parameter   when PRON seem like the point the author be make be that PRON can use x  fold  for avoid confusion with PRON example knn  to find a hyperparameter   let PRON better explain with k  nearest neighbor   say PRON be try to find the good k to use with the iris set  here be error rate for k  2  3  4  5    from sklearn import cluster  dataset  mixture  from sklearnneighbor import kneighborsclassifier  from sklearnmodelselection import traintestsplit  datum  sklearndatasetsloadiris    x  datadata    y  datatarget    accuracy     for n in range2  6     70  holdout for train  xtrain  xt  ytrain  yt  traintestsplitx  y  testsize07   model  kneighborsclassifiernneighbor  n   modelfitxtrain  ytrain   score  modelscorext  yt   accuracyappendn  score     accuracy  number of cluster and the accuracy of the model     2  096190476190476193       3  092380952380952386       4  097142857142857142       5  094285714285714284    however this bring up a really easy criticism  how do PRON know PRON do not just get lucky in pick the datum to pass into the model when k be set at 4    thankfully for this example  that be probably the seeing as there be only suppose to be 3 class    so in order to overcome this PRON use x  fold to average out the bias that come from sample datum  and thus good trust the accuracy that PRON get from pick k  accuracy     for n in range2  6    kaccuracy      splitting and run this loop 10 time  kf  kfoldnsplits10   for trainindex  testindex in kfsplitx    model  kneighborsclassifiernneighbor  n   modelfitxtrainindex   ytrainindex    score  modelscorextestindex   ytestindex    kaccuracyappendscore    get the averaging of each 10 kfold run  score  sumkaccuracy   10  accuracyappendn  score     accuracy     2  093333333333333335       3  094666666666666688       4  092666666666666675       5  093333333333333335    first of all PRON have eliminate the k4 option  and the actual k3 emerge as the winner   with more fold PRON accuracy can be better trust as well   let PRON connect this back to PRON original question though   each radius attach to each cluster will be different every time PRON run this algorithm because PRON be fit to the datum   so yes PRON be right that PRON will never be able to exactly replicate the radius  parameter  choice that be make to get a certain accuracy  but that be not the goal of cross  validation   the more important aspect of folding be to pick the good k  once PRON know that PRON can train to PRON heart content   the primary use of cv be for tune the model   PRON use the average cv value to see how PRON model may behave on new datum  once PRON have settle for the good hyper parameter  PRON retrain on the whole train datum   then people notice that PRON can also use the model train on each fold  by average PRON prediction on the test set   that ’ the second way of use cv   the good approach depend son the datum   sometimes retrain on the full set be good than average fold prediction  sometimes the converse be true  
__label__machine-learning __label__classification __label__data-cleaning __label__preprocessing __label__survival-analysis PRON be work on build prediction model for disk failure  time take to occur a disk failure and what parameter could strongly affect disk failure   PRON be bit confused on  what datum preprocess step should PRON perform  the dataset be  highly imbalanced  500 failure and 40000 non  failure   what type of machine learning model should PRON take into consideration as datum be highly imbalanced   few day back  PRON read about survival analysis  and now PRON be in conundrum whether the problem would be of survival  analysis or machine learning   PRON be currently work with dataset provide by backblazehttpswwwbackblazecom  b2hard  drive  test  datahtml    PRON would be really great PRON PRON could get some direction    PRON could think about rebalanc the dataset use undersampling of the majority class  oversampl the minority dataset or by apply smote to the dataset   if PRON have get the dataset more balanced then either logistic regression  random forest be not a bad start place  random forests PRON believe be good at handle unbalanced classification problem  but PRON may still have trouble with the degree of unbalance that PRON be talk about   survival analysis generally have an aspect of time to the problem  ie  when may a hard drive fail   so if time be include in PRON datum then PRON definitely could frame PRON as a survival analysis problem  however if time do not enter the problem then PRON may be easy to just analyse as a ml problem   some algorithm  such as svm or logistical regression  have possibility to add a weight to certain class  therefore fix the unbalanced issue   this really sound like a job for survival analysis  which be especially design to answer question like  when machine x fail  or  which attribute influence the most the failure   PRON can simply start by plot the kaplan  meier curve and then further stratify PRON by some attribute  then PRON can try cox regression model  PRON be useful to see the influence of an attribute on survival  the hazard ratio   but do not forget to verify the assumption  functional form and proportional hazard    in r the survival analysis be implement very well  so do not be affraid   there be simple and short tutorial which may help  
__label__finite-difference __label__python __label__numerical-analysis __label__wave-propagation the original equation be    frac1c2  fracpartial2 upartial t2   fracpartial2 upartial r2   frac1rfracpartial upartial r   frac1r2fracpartial2 upartial phi2  and the finite difference version use be    uti  j    c2 delta t2leftfrac12ridelta r   ut1i1j   ut1i1j    frac1delta r2ut1i1j   ut1i1j   2ut1i  j    frac1ri2 delta phi2ut1i  j1   ut1i  j1   2ut1i  jright   ut2i  j   2ut1i  j  with  c2025    delta r01    delta phi01256    delta t005 the total radius be 5   r  nplinespace0  radius  50   phi  nplinspace0  2nppi  50   r  phi  npmeshgridr  phi   x  rnpcosphi   y  rnpsinphi   thus dr55001  and dphi2nppi5001256   PRON initial condition look like  PRON create r5050   phi5050   and the same space for x  y   x  rcosphi   y  rsinphi    PRON do not know why  but PRON get strange result   also PRON make a model for a cartesian system  and PRON work   PRON think that PRON have two problem   PRON timestep be too big  the cfl condition be not satisfied   and  PRON be not update the value for  phi2pi  PRON be not sure about the cfl condition for polar coordinate  but PRON believe that PRON should be    2cfracdelta tdelta r delta phi  leq cmax enspace     what imply that  delta t  lt  fracdelta r delta phi2c  regard the value for  phi2pi  PRON need to update for  phi0  and  phi2pi although PRON be not loop until the last angular value   these thing be correct in the code below  and PRON work   one thing that be not work terribly good be the solution for  r0   but PRON have no idea now on how solve PRON  but PRON think that PRON need to be solve as a particular case   PRON attach some image for the mode   11 of vibration  since PRON be not good with the animation thing in python    import numpy as np  from scipyspecial import jv  jnzeros  from mayavi import mlab     parameter  nr  50  nphi  50  nsteps  1000  radius  5   c  05  dphi  2nppi  nphi  dr  5nr  dt  0005  if dtlt  drdphi2c    the maximum value for dt be drdphi2c   dt  drdphi4c      initial condition  r  nplinspace0  radius  nr   phi  nplinspace0  2nppi  nphi   r  phi  npmeshgridr  phi   x  rnpcosphi   y  rnpsinphi   kthzero  jnzeros1  1   z  npcosphi   jv1  kthzeror  radius   t  npzerosnstep  nr  nphi    t0      zt  t1      zt     stepping  k1  cdt2dr2  for t in range2  nsteps    for i in range0  nr1    for j in range0  nphi1    ri  maxri   05dr    to avoid the singularity at r0  k2  cdt22ridr   k3  cdt2dphiri2  tt  i  j   2tt1  i  j   tt2  i  j     k1tt1  i1  j   2tt1  i  j   tt1  i1  j   k2tt1  i1  j   tt1  i1  j   k3tt1  i  j1   2tt1  i  j   tt1  i  j1    tt  i  1   tt  i  0    update the value for phi2pi      surf  mlabmeshxt  yt  10t999   colormaprdylbu    mlabshow    timestep  0  timestep  199  timestep  399  timestep  599  timestep  799  timestep  999  ps  please  generate video and share PRON in the comment  PRON want to see   and  use a different colormap  the one PRON use be not so good  
__label__matlab __label__ode __label__plotting PRON be try to apply the  restore force surface  method to a dynamic linear system   the idea behind this method be that  know acceleration  displacement  velocity and input force PRON be possible to establish if the system under study be linear or not by plot a surface in the phase  space   to verify PRON code PRON apply the method to a system PRON already know  therefore PRON generate the need data   ddotydotyy  solve the following equation    ddoty   40 doty   104  y  ft    where  ft   30 sin10 t  this be the code PRON write to do that    t  u   ode45odefun0 600 0     vel  u2    spost  u1    finput  30sin10t    acc  zerossizevel     obtain the acceleration from the velocity  acc1   0   for i  2lengthvel   h  titi1    acci    veliveli1h   end  where the  odefun be   function  f   odefunt  y   f  zerossizey     f1   y2    f2   10  4y140y230sin10t    end  for the application of the method PRON have to compute the so call restore force as   zydoty    ft   ddoty    therefore PRON can plot the triplet   ydotyz as a surface where  y and  doty represent a point in the phase  space and  z the height above that point    surface  xplot  linspaceminspostmaxspost200    yplot  linspaceminvelmaxvel200    zgrid  gridfitsortspostsortvelsortz   xplot  yplot     trisurf should work as well instead of gridfit   sort   use not to have oscillatory datum  figure    surfxplot  yplot  zgrid   titlerestoring force surface    xlabeldisplacement    ylabelvelocity    zlabelforce    once PRON have obtain the surface PRON need to extract from PRON the cross section  one for  y  0  and another one for  doty   0 the slope of the cross section at  doty   0  should represent the stiffness of the system  therefore PRON should be  k  10  4    whereas the slope of the cross section obatin at  y  0  should represent the damping    40     but here PRON have problem  infact  since the system be know to be linear  the coefficient in the equation be constant   PRON expect to have straight line as cross section whereas the plot of  velocity  force be not a straight line as PRON should be   to exctract a cross  section PRON save the value of  y and  z correspond to  doty  lt  delta   surface section to plot force  displacement  deltav  00001   xx  zeroslengthspost1    fx  zeroslengthz1    for i  1lengthvel   if absveli    lt deltav  xxi1   sposti    fxi1   zi    else  xxi1   999   just to give some value to be discard later on  fxi1   999   end  end  xxxx   999       elimination of point outside the range  deltav  deltav    fxfx   999       xsort  sortxx    fxsort  sortfx    figure    plotxsort  fxsort   xlabeldisplacement    ylabelforce     b  bint   polyfitxsort  fxsort1    errk  10  4b1    PRON should be 0  the same for the  y  0    deltas  001   xxdot  zeroslengthvel1    fv  zeroslengthz1    for i  1lengthspost   if abssposti    lt deltas  xxdoti1   veli    fvi1   zi    else  xxdoti1   999   fvi1   999   end  end  xxdotxxdot   999       fvfv   999       xdotsort  sortxxdot    fvsort  sortfv    figure    plotxdotsort  fvsort   xlabelvelocity    ylabelforce    this last plot be not even a straight line and this mean the damp be not linear but PRON know PRON must be linear since PRON start from a linear equation   any idea   PRON think PRON do not need to use sort function and PRON deltas be too high  PRON would suggest that reduce the deltas to 0000005 and remove all PRON sort function and PRON will get right slope for damp  
__label__tensorflow __label__keras if PRON be train on a gru model  be there a way PRON can output the learnt parameter so that when PRON train next time with more datum  PRON can initialize with those learnt parameter as a starting point   there be a way to do this  see the documentation  PRON be very good   PRON assume PRON mean that PRON build a network use keras  which contain recurrent gru layer  and would like to save the model after some training  then restart from the same point eg with new datum or just to push the model further   example  assume PRON want to analyse some image of shape  150  150  3    from kerasmodel import model  loadmodel  from keraslayer import input  gru  dense  mymodel  model    myinput  inputshape150  150  3     use tensorflow s dim   channel last   grulayer  gruinput   output  dense32grulayer    tell the model class which layer be the start and end of the model  model  modelinput  myinput  output  myoutput    save PRON   mymodelsavemymodelh5     some time later  a new python session  myreloadedmodel  loadmodelmymodelh5     continue use the model  explanation  at the point the model be build  PRON can compile and train PRON the usual way eg mymodelfit     once training be finish  PRON can save the model two different way   just the model weight  use mymodelsaveweightsmyweightsh5    the whole model and PRON meta datum  use mymodelsavemymodelh5    in either case  PRON will need the library h5py  see the documentation   option 2 preserve more information from the model  quote the documentation  PRON save   the architecture of the model  allow to re  create the model  the weight of the model  the training configuration  loss  optimizer   the state of the optimizer  allow to resume train exactly where PRON leave off   to continue on the with model where PRON end and save  PRON be as simple as   mymodel  kerasmodelsloadmodelmymodelsh5    now PRON can train PRON further or introspect the model and so on   link  detail on the model class  details of gru  details of save model 
__label__tools PRON be run an experiment where PRON need to collect and analyse participant  browse and search history  the design of the experiment be similar to an  instrumented user panel   describe here  httpciteseerxistpsuedu  viewdoc  downloaddoi10111048971amprep  rep1amptype  pdf  in the classic case  participant must install some kind of logger on PRON computer  which collect and send browse datum to the researcher behind the scene  find such tool be where PRON get stuck   PRON could  of course  just ask PRON participant to export PRON browse history and send PRON to PRON every night  but PRON be hop there would be something more automate out there  with potential bell and whistle such as annotation   note  please let PRON know if this be the wrong stack exchange  PRON must be really struggle with find the right keyword for this   
__label__optimization __label__algorithms __label__mpi in a parallel mpi code  PRON have load balance issue  PRON 2d computational domain be distribute on a 2d mpi cartesian topology  which lead to equal sized 2d sub  domain per mpi process  however  the amount of work to do per domain be determine by the number of  compute point  belong to each domain  and the density of these compute point be very uneven  lead to some process have much much more of PRON than other  therefore  the overall parallel efficiency of PRON code may become quite poor   to solve this issue  PRON want to resize PRON 2d domain decomposition  to re  balance the number of compute point across PRON mpi process  PRON be therefore look for an algorithm permit PRON to achieve this optimisation process  the constrain be   the x and y dimension  gx  gy  of the global grid to distribute be fix  integer represent the number of grid point to distribute across mpi process in each dimension   the total number of mpi process be fix  let ntot be this number  then PRON have to keep nx  ny  ntot for any valid  nx  ny  solution   the number of x and y grid point for each mpi process can be different  the only limit be that the sum of the individual x grid point for all mpi process should be equal to gx  likewise for gy   the density of compute point on the grid be know a priori  each grid point may have anywhere between zero and a ten such compute point   any idea of any method  algorithm to solve this problem   PRON be fully aware that there may be no perfect solution  but any mapping of the grid permit to improve the load balancing would already be good to take   this be typically do with graph partition algorithm  and package such as metis and parmetis  same link  be often use to do so  
__label__data-mining __label__clustering __label__nlp __label__nltk PRON want to implement  template  base information extraction without the template  paper  for the first step  PRON have to do  cluster event   PRON have the muc dataset  which PRON have parse and tokeniz  but there be something that PRON can not understand  in the paper  PRON say that   PRON cluster event pattern to create template   an event pattern be either   1  a verb    2  a noun in wordnet under the event synset  or   3  a verb and the head word of PRON syntactic object   example of each include   1  ‘ explode’    2   explosion’  and   3   explode  bomb’   PRON be wonder what be these three condition  how can PRON apply PRON to cluster PRON datum  PRON mean  should PRON first go search in wordnet and remove some word   
__label__neural-network PRON have just learn about regularisation as an approach to control over  fitting  and PRON would like to incorporate the idea into a simple implementation of backpropagation and multilayer perceptron  mlp  that PRON put together   currently to avoid over  fitting  PRON cross  validate and keep the network with good score so far on the validation set  this work ok  but add regularisation would benefit PRON in that correct choice of the regularisation algorithm and parameter would make PRON network converge on a non  overfit model more systematically   the formula PRON have for the update term  from coursera ml course  be state as a batch update eg for each weight  after sum all the applicable delta for the whole training set from error propagation  an adjustment of lambda  currentweight be add as well before the combine delta be subtract at the end of the batch  where lambda be the regularisation parameter   PRON implementation of backpropagation use per  item weight update  PRON be concerned that PRON can not just copy the batch approach  although PRON look ok intuitively to PRON  do a small regularisation term per item work just as well   for instance lambda  currentweight  n where n be size of training set  at first glance this look reasonable  PRON could not find anything on the subject though  and PRON wonder if that be because regularisation do not work as well with a per  item update  or even go under a different name or altered formula   regularization be relevant in per  item learning as well  PRON would suggest to start with a basic validation approach for find out lambda  whether PRON be do batch or per  item learning  this be the easy and safe approach  try manually with a number of different value  eg 0001  0003  001  003  01 etc  and see how PRON validation set behaf  later on PRON may automate this process by introduce a linear or local search method   as a side note  PRON believe the value of lambda should be consider in relation to the update of the parameter vector  rather than the training set size  for batch training PRON have one parameter update per dataset pass  while for online one update per sample  regardless of the training set size    PRON recently stumble upon this crossvalidated question  which seem quite similar to PRON  there be a link to a paper about a new sgd algorithm  with some relevant content  PRON may be useful to take a look  especially page 1742  1743    to complement what insy say   regularization be use when compute the backpropagation for all the weight in PRON mlp   therefore  instead of compute the gradient in regard to all the input of the training set  batch  PRON only use some  one items   stochastic or semi  stochastic    PRON will end up limit a result of the update in regard to one item instead of all which be also correct   also  if i remember correctly  andrew ng use l2regularization   the n in lambda  currentweight  n be not mandatory  PRON just help rescale the input  however if PRON choose not to use PRON  PRON will have  in most of the case  to select another value for lambda   PRON can also use the grid  search algorithm to choose the good value for lambda  the hyperparameter   the one PRON have to choose   
__label__neural-networks __label__machine-learning __label__prediction __label__linear-regression __label__structured-data recently PRON be work on a problem to do some cost analysis of PRON expenditure for some particular resource   PRON usually make some manual decision from the analysis and plan accordingly   PRON have a big datum set in excel format and with hundred of column  define the use of the resource in various time frame and typesother various detailed use    PRON also have information about PRON previous 4 year of datum and actual resource usage and cost incur accordingly   PRON be hop to train a nn to predict PRON cost beforehand and plan even before PRON can manually do the cost analysis   but the big problem PRON be face be the need to identify the feature for such analysis  PRON be hop there be some way to identify the feature from the datum set   ps  PRON have idea about pca and some other feature set reduction technique  what PRON be look at be the way to identify PRON in the first place   
__label__pde __label__fenics PRON be interesting in solve the follow nonlinear  time  dependent pde in 2 spatial dimension  complex gross  pitaevskii eq      i fracpartial psipartial t   left  nabla2   1i sigmapsi2  1  right  psi  the goal be to find steady state solution for the function  psix  y  t  for different parameter  sigma here  sigma be a positive real  value parameter  the boundary condition can initially be of the dirichlet type  set the function  psi to zero on the contour of a square  but later on PRON plan to implement some absorb bc  perhaps use the perfectly match layer method    PRON be also consider solve the equation in a different geometry later on  on a disk use polar coordinate   for the time  dependence  PRON plan to use the gryphon module of erik skare  httpslaunchpadnetgryphonproject   which be basically a runge  kutta solver and for the spatial part fenics   so the question be  do PRON think this be feasible to do with fenic  and if so  how would one proceed   PRON would try push this form  v  vectorfucntiospacemesh   cg   1  dim2   u  functionv   v  testfunctionv   sigma  constant01   f  innergradu   gradvdx     inneru  u10u0sigmau1v0    u1sigmau0v1dx  into nonlinear  poisson demo  but note that this would yield trivial solution if PRON start from zero field with zero dirichlet  
__label__nlp __label__linear-algebra how be eigenvector and eigenvalue can be apply  applicable to natural language processing problem  any example   latent semantic analysis  lsa  rely on linear  algebraic decomposition  eg svd   which in turn involve eigenvector  value  see here    not sure if this be quite what PRON question be drive at  but in general  eigenvector be useful in datum analysis because PRON define some  natural  direction in the datum  for example  the eigenvector of the covariance matrix of some datum be all perpendicular to one another  and the ith one point in the direction of ith great variance in the datum  in other word  the first eigenvector point in the direction where the data s variance be great  the second one in  perpendicular  direction of second  great variance  and so on   this be why eigenvector  base decomposition can be so useful in feature selection and dimensionality reduction  in informal term  PRON transform the datum into perpendicular feature that be align with some  natural  ax in the datum  
__label__machine-learning __label__neural-network __label__deep-learning __label__lstm __label__natural-language-process PRON be new in machine learning and PRON be work on a problem relate to text  PRON know that in ml PRON can use feature as numerical value as input to neural network  but PRON do not know how to use feature as word  in some paper PRON read that PRON take feature to be n word with some property  PRON really do not understand how be that possible  please  if PRON be not a problem  just to tell PRON some good paper or textbook or link where PRON be explain how to do that   PRON need to make a dictionary of word  PRON mean PRON have to make a dictionary which PRON assign each word a unique value  then PRON can use one  hot  encoding to represent each word uniquely  if this be what PRON need PRON will do what PRON want  but this have a big problem  when PRON think about cat and dog  PRON may find similarity and difference between PRON  this be because PRON have more knowledge than the only representation of word in PRON brain  consequently  PRON should use approach to assign a unique number to each word  and put near concept as neighbor  for the first part take a look at here and the second part  take a look at here  
__label__fluid-dynamics __label__sparse __label__matrix PRON be implement a fluid simulator as PRON numerical method course project and PRON have to compute pressure at each simulation step  basically  that mean solve an equation  ap  b  where  a be a sparse matrix  all PRON entry be zero except   PRON main diagonal  diagonals before and after main diagonal  for some  k  the  kth diagonal before and the  kth diagonal after main  somewhat like this     beginbmatrix   a11   amp  a12   amp  0  amp  dot  amp  a1k   amp  0  amp  0  amp  dot  amp  0   a21   amp  a22   amp  a23   amp  dot  amp  0  amp  a2k1    amp  0  amp  dot  amp  0   0  amp  a32   amp  a33   amp  dot  amp  0  amp  0  amp  a3k2    amp  dot  amp  0   0   amp  0  amp  a43   amp  dot  amp  0  amp  0  amp  0  amp  dot  amp  0   dot  amp  dot  amp  dot  amp  dot  amp  dot  amp  dot  amp  dot  amp  dot  amp  dot   ak1   amp  0  amp  0  amp  dot  amp  dot  amp  dot  amp  dot  amp  dot  amp  dot   0  amp  ak  12   amp  0  amp  dot  amp  dot  amp  dot  amp  dot  amp  dot  amp  dot   0  amp  0  amp  ak  23   amp  dot  amp  dot  amp  dot  amp  dot  amp  dot  amp  dot   dot  amp  dot  amp  dot  amp  dot  amp  dot  amp  dot  amp  dot  amp  dot  amp  dots  0  amp  0  amp  0  amp  dot  amp  0  amp  0  amp  an1n2    amp  an1n1    amp  an  1  n    0  amp  0  amp  0  amp  dot  amp  0  amp  0  amp  0  amp  ann1    amp  ann   endbmatrix  since PRON sumulate the fluid without free surface  the matrix be not change during the simulation  so PRON want to invert PRON before the simulation and then just compute  a1  b at each step  however  PRON believe there be fast algorithm for do PRON than gaussian elimination which could exploit sparsity and the structure of such a matrix   so  PRON question be   be there a commonly use name for such a matrix   be there any effective algorithm for invert PRON   something similar to tridiagonal matrix algorithm  maybe    if 2 be positive  be there any java library do that    PRON suppose PRON be solve the so call pressure poisson equation and that PRON  a be a discrete representative of a laplacian   typically  PRON do not want to have the inverse compute and store  cf  the comment to this question  the main reason be that the inverse will be a dense matrix  PRON rather suggest PRON compute a sparse lu factorization of  a  so that  ax  lux  b can be solve for  x very fast  for the 2d laplacian the factor  l and  u be know to be computable with little fill in   routine for the sparse lu decomposition be available here or as a part of high performant library like umfpack  however  PRON be not able to find a java implementation within 3 minute of google   finally  as PRON  a be typically positive definite  PRON can go with the cg algorithm which take only a few line of code to implement   PRON matrix be not  banded  in the sense of  banded  direct solver  so do not bother with those   multigrid be absolutely the good way to solve these  pressure poisson  problem   there be lot of library and PRON be not difficult to implement a simple multigrid algorithm for structured grid   cg and direct solver be fundamentally non  scalable  so the cost will increase superlinearly as PRON increase resolution  
__label__python __label__xgboost __label__cross-validation __label__evaluation PRON be try to create a custom evaluation metric  feval  function for xgboostcv  PRON should process some of the training feature  however PRON can not find a way to extract feature from xgboost  dmatrix   object  only label    as PRON be suggest PRON can be do with first make a dmatrix slice then save PRON to a svmlight file and finally read PRON with scikit  learn  but be there a more elegant way if do PRON    currently PRON can not direct extract datum matrix from dmatrix  mainly due to dmatrix be a internal data structure that may or maynot sit in memory   by tqchen comment on 3 aug 2015   source  
__label__bigdata PRON be a recent graduate  currently PRON work in a late stage startup  product company  as a software engineer where PRON responsibility be to interact with customer  understand PRON environment and write code so that PRON can use PRON product  PRON work on c   web service and relational database  PRON also work rarely on big data space to migrate some small component of PRON product to big datum platform  PRON have get an opportunity to work in a large firm as a big datum engineer to work on apache project like yarn and hadoop and solve customer issue  and also to contribute to open source community  in curreny company  there be no one to review PRON work and mentor PRON   will PRON be a wise decision to leave a company where PRON have an opportunity to see product from end to end and go to a company where PRON will not the same visibility of product   be PRON next opportunity a good one if PRON want to make PRON career in big datum space   please share PRON input to help PRON take PRON decision   PRON think PRON question be off  topic  but PRON will still try to answer because PRON may help PRON  get a job in data science be not something everybody can do  PRON be a competitive field  well do  for land PRON in datum science  however  data science be a big area  so PRON will need to think what PRON want to do and plan ahead   do PRON want to get PRON into machine learn  do PRON want to get PRON into statistic  right now  PRON be more like a software engineer in the field  still not quite be a data scientist  do PRON want to become a real data scientist  or be PRON happy be a software engineer in the field  if PRON be plan to become a data scientist  PRON should train PRON hard in statistic  machine learning etc  be a hadoop user be simply not enough to step PRON fully into the field   PRON give PRON PRON own experience  PRON be a software engineer PRON  PRON be not easy to get a job in data science  PRON have to do a postgraduate degree in mathematic  train PRON as much as possible  in the end  PRON land PRON a job as a bioinformatician   recommendation  do not leave just because nobody mentor PRON  learn as much as possible and prepare PRON  once PRON be comfortable  move on   PRON be vote to close PRON as off  topic    studentt have already give a wonderful answer    firstly  PRON be up to PRON to take the decision   PRON have to have a passion for datum  and willing to get PRON foot dirty at PRON   PRON be not the end  if PRON colleague be not mentor PRON  no one mentor PRON when PRON be prepare PRON  PRON have write an answer on quora about how PRON disturb a lot of people till PRON mentor PRON  in fact  PRON be very happy to   will PRON be a wise decision to leave a company where PRON have an  opportunity to see product from end to end and go to a company where  PRON will not the same visibility of product   the domain of data science be very difficult  let PRON cite an example from PRON team   PRON  be the lead  have an idea of the entire product end  to  end  however the one in PRON team work on customer success analytic do not have the complete idea on the entire productin fact  PRON do not need to   and vice versa   so  PRON be completely dependant on the role   be PRON next opportunity a good one if PRON want to make PRON career in big  datum space   big datum space   yes  in data science  maybe  but PRON have a lot of ground to  cover   read this short free ebook may give PRON some insight on what to do   analyze the analyzers 
__label__machine-learning __label__feature-selection __label__correlation PRON be do a naive bayes prediction model where PRON have 666 feature to select from   PRON have try the selectkb chi2 test but PRON be a feature  label test whereas what PRON be try to do be to find out the independency and also correlation feature  feature  as the assumption to use naive bayes be that variable be all independent to each other  PRON have to remove feature that be dependent  correlate to each other   what be the other independence test that PRON should look into apart from  chi2    PRON have continuous feature   this package be very useful  this do the feature engg  for PRON  give PRON the important variable  sample code would be something like this   setseed123   borutainput  lt borutatarget variable  datum  trainingdata  dotrace  2   printborutainput    plot a graph for good understand  plotborutainput  xlab     xaxt   n    lzlt  lapply1ncolborutainputimphistoryfunctioni   borutainputimphistoryisfiniteborutainputimphistoryii    nameslz   lt colnamesborutainputimphistory   labels  lt sortsapplylz  median    axisside  1las2label  nameslabel    at  1ncolborutainputimphistory   cexaxis  07   by use this PRON can get all the important feature but the downfall be PRON take time if PRON have more datum  in PRON case PRON data consist 40 feature and 200000 record PRON take almost 2 hour but the result be good   for good understanding PRON can go through this link    PRON think PRON can use this test too  in the description PRON tell that PRON can be apply on numerical datum too   if PRON have datum which be nominal datum  PRON can use g  test  attach one more link along with this  which consist of test for different kind of feature  may be helpful in the future link  PRON have a question  why be not PRON apply pcaor any dr techniques  on the data  set   assume that PRON dint try any dr techniques   if PRON apply dr techniques  PRON think PRON may get good components which would aid PRON in get good result  good understanding of datum   as PRON mention that PRON be use nb classifier  why do PRON choose that  be there any specific reason   as PRON know that there be many other classifiers which may outrun nb classifier   in PRON case PRON use nb classifier for twitter sentiment mining  as those be short sentence and nb classifier best suit for PRON analysis   hope PRON answer be helpful   nb  naive baye  dr  dimensionality reduction 
__label__optimization __label__numerical-analysis __label__jacobian __label__calculus for an optimization routine PRON need to compute the derivative of the right  hand side    fuxk  uk of a discrete  time system  xk1   fxk  uk since    fuxk  uk include term that be divide by  uk  PRON be not possible to evaluate  fxk  uk0 from the physics  PRON be pretty clear that a limit must exist  the numeric pose a problem here  though  any idea  how PRON could circumvent PRON   to illustrate the behavior  PRON add a double logarithmic plot show  fuxk  uk as  u rightarrow 0 PRON do not make any sense that  fu escape here  what the plot do not show be that there be multiple sign change of  fu as  u drop below  104  let PRON say   the function  fuxk  uk be define as follow      fux  u   partialu int0t int0t sinx5  x6 tau  u fractau22     dtau      PRON have a cas do the job of analytically integrate and derivat the sine term  as already point out this produce some division by  u which cause the problem when PRON come to evaluate at  u0  give that  u fractau22  ll 1   one way of tackle the numerical oscillation  even before the actual term emerge  be a taylor approximation in  u of the sine term  thank to kirill for the full series      sinunderbracex5  x6 tauu0   underbraceu fractau22delta u    sinx5  x6 tau   sumn1n frac1lfloor n2 rfloor2nn1    tau2nqnun   odelta un  where    qn   begincas   cosx5  x6 tau   amp  n text  odd    sinx5  x6 tau   amp  textelse    endcases      introduce this approximation into the original equation and exchange order of integration and differentiation give     beginarrayll   fux  u   amp   int0t int0t partialu left  sinx5  x6 tau   sumn1n frac1lfloor n2 rfloor2nn1    tau2nqnun   odelta un   right    dtau    amp   int0t int0t sumn1n frac1lfloor n2 rfloor2nn1    tau2nqnun1   odelta un1     dtau   endarray     a first  order approximation   n1   be then give as     beginarrayll   fux  u   amp  approx int0t int0t cosx5  x6 tau  fractau22    dtau    amp   frac3 cosx5x6  4   frac6t2x6  2cosx5  tx6   2tx6sinx5   2 sinx5  tx62x6  4    endarray      which do not depend on  u  naturally  
__label__finite-difference __label__numerical-analysis __label__convergence __label__stability in a book that PRON course be follow this semester  the theorem give be only in one direction   if the scheme be both consistent and stable  then the scheme be convergent   however  since this be not a double implication theorem  then be there convergent scheme that be either consistent but not stable  or stable but not consistent   what PRON be think about be the lax equivalence theorem  PRON be a double implication  if and only if   PRON can prove generally use a functional analysis approach  though that may be more in depth and  further from practical method  than an approach most class would take  which be probably why only the forward implication be prove   in general  PRON agree with the previous answer by chris rackauckas  however  to be more precise  the answer depend on the definition of stability and convergence   in stetter  analysis of discretization method for ordinary differential equation   in chapter 1  there be simple example for scheme which be convergent  consistent but not stable or stable but not convergent  PRON be in a sense degenerate because a bad choice for the space be make  
__label__neural-network __label__keras PRON have be use keras to do some timeserie predictive neural net   one thing that be have PRON stagger be how PRON be get an almost 99  accuracy rate on very very noisy and uncorrelated datum  PRON be look for accuracy of around 50  to be good     what PRON have now find  be that after PRON train PRON model on the training set  PRON be casually run modelevaluatext  ytest  before then run a manual backtest on the datum  the manual backt on xt and compare PRON to the error in yt then show almost perfect accuracy   be PRON then correct in assume that keras when use the evaluate   function in keras the original model be actually update with the testing datum as well  this seem a bit strange and counterintuitive to PRON   keras do not update PRON model with testing datum   PRON may be that PRON label have be provide wrong in the test datum  check the modelpredictclass   to get the output of PRON class and crosscheck PRON with PRON actual output manually by pick random subset  
__label__machine-learning __label__python __label__data-mining __label__nlp __label__text-mining PRON m new in text analysis and need PRON advice to help medical student to write properly and correctly  the student describe sickness as PRON  observe PRON  however  PRON must use an  official sickness description   PRON have the datum collect from the student and the correct datum  let PRON explain   PRON have a csv table1 contain 300 k row  one column  each row be the description of health condition write by student  PRON can be redundant    PRON have an other table2 contain almost 200 k row and 3 column  column 1  the official name of the condition  the correct one the student should use   column 2  a code  number   column 3  the translation of  the official name of the condition  in another language   the idea be to start from table1 and match each row with n official name  with PRON code and translation   let PRON say 5 official name rank by distance computation   PRON be confused   should PRON go for a recommendation engine or a search  rank algorithm   what be the step PRON can identify to accomplish this task   ps  the final output may be an api where student start to write the description of the sickness and obtain a list of choice  recommendation where PRON can pick one   PRON hope PRON be clear  thank  fuzzywuzzy would be a good python library for this purpose  a code example from the github readme    gtgtgt  choice    atlanta falcons    new york jets    new york giants    dallas cowboys     gtgtgt  processextractnew york jet   choice  limit2      new york jets   100     new york giants   78     gtgtgt  processextractonecowboy   choice     dallas cowboys   90   PRON imagine PRON would like to use the extract function with limit 5 with choice be set equal to all the  official name  from PRON table2 csv file   detail on installation and use be at the link above    the idea be to start from table1 and match each row with n official name  with PRON code and translation   let PRON say 5 official name rank by distance computation   PRON would recommend against use an ml classifier because PRON be likely to exclude some of the 200k official name if only 5 option be give back   there be many fuzzy text match algorithm to match PRON row to an official name  fuzzywuzzy s and several other algorithm be base on the levenshtein distance  PRON can use a for  loop to go through the 200k official name  depend on how much text there be this may take a while  sort the list and slice  pop value that have to low score  and PRON will have the top 5 good match official name for a row  PRON guess PRON want to rank the frequency of each official name and return the highest rank as a first result   because the health description give by the student can be redundant  PRON may want run a zipf rank algorithm to filter more common word   this be answer be far from complete  comment be welcome   thank PRON all PRON will definitely test the fuzzywuzzy lib  meanwhile please take a look on the code PRON have make yesterday  PRON can be enhance  go ahead if PRON can make PRON good  –   define function to read the csv file  def getdocumentlink    dataframe  pdreadcsvlink  encode   iso8859  1   sep       return dataframe   define function convert text  csv data to list  def columntolistcolumn    return columntolist     read document csv  document1  getdocumentcuser  smegrhi  desktop  doc1csv    document2  getdocumentcuser  smegrhi  desktop  doc2csv     get column into list  list1  columntolistdocument1dialbl     list2  columntolistdocument2icd10lblfr      data pre  process  def processtextsentence     convert to lower case  sentence  sentencelower     convert www   or https  to url  sentence  resubwwwshttpsssentence    convert  sentence  resubssentence    remove additional white space  sentence  resubs      sentence    replace  sentence  resubrs    r1   sentence   return sentence   execute  for i  text in enumeratelist1    re  processtexttext   list1i   res   vectorize the datum  word  recompilerw     function vectorize datum  def texttovectortext    word  wordfindalltext   return counterwords    calculate similarity  def getcosinevec1  vec2    intersection  setvec1key     amp  setvec2key     numerator  sumvec1x   vec2x  for x in intersection    sum1  sumvec1x2 for x in vec1keys      sum2  sumvec2x2 for x in vec2key      denominator  mathsqrtsum1   mathsqrtsum2   if not denominator   return 00  else   return floatnumerator   denominator   compare sentence from list 1 with list 2  choose indication from data  doc1  text1  list11    text1  shigellose à shigella dysenteriae  vector1  texttovectortext1   listcosine     for i  text in enumeratelist2    vector2  texttovectortext   cosine  getcosinevector1  vector2   listcosineappendcosine   the result be not that bad neither good   PRON suggestion  
__label__linear-algebra __label__linear-solver __label__poisson __label__multigrid __label__spectral-method let PRON say PRON be solve a simple poisson problem use a mixed  dg  finite element method   if PRON use orthogonal polynomial as basis function PRON can write the finite  dimensional linear system as     leftlbrackbeginarraycci1amp  s11    s11mathrmt   endarray  rightrbrack leftlbrackbeginarrayc  q1u1 endarray  rightrbrack  leftlbrack beginarrayc  f1g1 endarray  rightrbrack      for appropriately sized matrix  s11i1  and right hand side  f1  and  g1   suppose PRON solve this linear system and obtain solution  hatq1  and  hatu1  now consider the same underlie pde with an enriched basis  polynomial of one degree high  say   this will result in a new linear system with the follow structure      leftlbrack  beginarraycc   i1amp  0amp  s11amp  s12  0ampi2amp  s21amps22  s11mathrmt   amp  s21mathrmt    amp  0  amp0  s12mathrmt   amp  s22mathrmtamp  0  amp  0  endarray   rightrbrack  leftlbrackbeginarrayccq1q2u1u2 endarray  rightrbrack  leftlbrackbeginarrayccf1f2g1g2 endarray  rightrbrack      where the vector  q2  and  u2  contain all the new degree of freedom   PRON question be as follow   how can PRON use the solution to the small problem to construct the solution of the large problem   recursion  suppose PRON have a fast way to compute the solution of the small problem  for all right hand side  how can PRON use this method to find solution of the large problem  PRON have a vague idea of some sort of preconditioner along the line of multigrid   note   PRON can rewrite the large linear system as     leftlbrack  beginarraycc   i1amp  0amp  s11amp  s12  0ampi2amp  s21amps22  s11mathrmt   amp  s21mathrmt    amp  0  amp0  s12mathrmt   amp  s22mathrmtamp  0  amp  0  endarray   rightrbrack  leftlbrackbeginarrayccq1hatq1 q2 u1hatu1 u2 endarray  rightrbrack   leftlbrack beginarrayc0f20g2 endarray  rightrbrack     and after shuffle the order of the equation and unknown get     leftlbrack beginarraycccc   i1amp  s11amp0amp  s12  s11mathrmtamp  0  amp  s21mathrmtamp  0  0amp  s21   ampi2amps22  s12mathrmtamp0  amp  s22mathrmt   amp0  endarray  rightrbrack  leftlbrack beginarrayc  q1hatq1 u1hatu1 q2 u2endarray  rightrbrack   leftlbrackbeginarrayc0 0 f2g2 endarray  rightrbrack      the thing to notice here be that the original linear system appear in the upper left hand corner and the  presumably small  relate linear system appear in the bottom right hand corner   the  i matrix be really mass matrix  but since PRON choose orthogonal polynomial as the basis function PRON be diagonal and therefore trivial to invert  PRON be especially interested in link to the literature or a name of a technique like this that PRON can use to search the literature   for the first question PRON know that PRON could use previous solution as a guess for an iterative solver but PRON be look for something more clever than this   what PRON be think of be something that use the structure of the augment matrix to make solution of the system simple  for example  one could be tempt to think of form the schur complement with regard to the bottom right  2times 2  block of the rewrite system   but PRON do not think anything like this exist  if PRON would  then PRON would have discover a purely algebraic way to efficiently solve the linear system that arise from high order discretization  PRON do not have such algorithm  though  the good algorithm PRON have use multigrid  but these algorithm make very specific use of the property and entry of the matrix PRON be try to invert  as oppose to just PRON algebraic structure  in other word  PRON do not think that by just reorder term PRON will be able to come up with an efficient solver  
__label__monte-carlo __label__simulation PRON need to compute the helicity modulus as a function of temperature for a three  dimensional x  y model  see nk kultanov  yu  e lozovik   the critical behavior of the 3d x  y model and PRON relation with fractal property of the vortex excitation    PRON be resort to use a monte carlo simulation because PRON can not find a closed expression for the superfluid density in a 3d mc model in the literature  PRON have write a simple monte carlo simulation and PRON be experience some problem  ie long thermalization time and noisy result even when average on a huge number of simulation   since PRON code be simple  be there any trick PRON could use to make the computation more efficient  reference and source code would be very much appreciate  PRON be primarily interested in the 3d case  since PRON code seem to work well for the 2d case   if anyone be interested PRON have upload PRON code  in c  here  httppastebincomscrmeaq8  the long thermalization time that PRON be run into be a generic problem that typically go under the name  critical slow down  and be common to the local  update scheme that PRON be use  PRON update by locally change a single spin at a time    once PRON realize that  the way out be to do good sampling  local update be out so PRON have to invent global update  two great way of do this be as follow   1  cluster update use the wolff algorithm   httpprlapsorgabstractprlv62i4p3611   sorry  no free version available   2  find a  dual  model to the one PRON be study  with the same partition function but new degree of freedom  for ising and xy model if PRON play this game PRON end up with  bond  current  model where the current be update globally use a worm algorithm detail nicely in  httparxivorgabscondmat0103146v1  oh  and as PRON google for that prokofiev reference  PRON see that someone even implement PRON with cute graphic in mathematica   httpdemonstrationswolframcomwormalgorithmforjcurrentmodel  just take a quick look at PRON code  there be a few thing that could be improve   PRON use three random number call to pick a site  when PRON could just use modulo arithmetic with one number from 1 to  n  nx time ny time nz  and compute the value of  i   j  and  k  similarly  in updateconfiguration  PRON be perform loop that lead to a function call  PRON would probably be good off if PRON reorganize the function to contain the loop instead  that way PRON can take good advantage of memory structure and datum parallelism than PRON have now  
__label__machine-learning __label__optimization __label__gradient-descent __label__cost-function for many optimization problem  the cost function to minimize take this form     frac  1nsample   sumi1nsample   loss  fxiyi   beta theta2 2    when use an algorithm like sgd for example  PRON only consider the loss for the specific input  xi and proceed to modify the parameter   PRON question be  when use sgd  be the loss to consider   loss  fxiyi   beta theta2 2    or   frac  1nsample   loss  fxiyi   beta theta2 2    thank   
__label__computer-vision PRON be new to computer vision and be give this problem  PRON be hop someone could help PRON with PRON   PRON be give a cube  c  PRON center be at point   202020 on camera ’s  coordinate frame  the size of the cube be  2   length of one edge  and one of PRON face be orient parallel to the image plane  if the effective focal length of the camera be  f10   give the coordinate of the projection of the cube ’s vertex  eight in total  on the image assume perspective projection   
__label__deep-learning __label__nlp __label__keras __label__machine-translation have anyone see this model s implementation in keras   httpsarxivorgabs170603762  inb4   tensorflow  httpsgithubcomkyubyongtransformer  pytorch  httpsgithubcomjadore801120attentionisallyouneedpytorch  thank in advance   
__label__python __label__dataset __label__k-means can PRON please recommend PRON for a big dataset for k mean   PRON would be cool if PRON will integrate easily with python  but any thing will be good   for the very first start PRON would recoment synthetical datum  simple draw a k set of random distribute number with different mean and require dimensionality   the big advantage be that PRON know the result clustering and PRON can easily verify the result  also scale the datum be not problem  
__label__data-mining __label__classification __label__data-cleaning PRON be new to datum mining  so this may sound like a very simple task to some   so  PRON work in reliability engineering in aviation  amp  PRON have a set of datum that be generate on a daily basis regard system failure  amp  failure rectification  this data be categorize use numerical tag of maintenance manual task  reference datum  by chapter  section and paragraph  however  since the data be enter manually by people  sometimes  the wrong chapter  section tag enter  amp  would require to be manually check to insure the datum s validity   the failure  resolution data be available in table format  csv  excel   and PRON also have the maintenance manual datum keyword include PRON chapter  section in table format   PRON question be  be PRON possible use rapidminer to take these table  cross check some keyword in the text  failure  rectification   amp  compare PRON with the reference datum  and output PRON with the proper reference tag  of the chapter  section    take into account spelling error  acronym  amp  abbreviation  or be there a program  application that be more specialized than rapidminer to do these function   example  failure on system x be rectify and log on the database  system x be under chapter 4 section 33  however  when enter the datum  the person put PRON under chapter 3 section 44  PRON have a document that have the reference for system x be under chapter 4 section 33  be PRON possible for rapidminer to check the text in the failure text  amp  the rectification text  amp  cross  check PRON with a predefined list where system x be under chapter 4 section 33  amp  give PRON the output with the right chapter  section  take into account spelling mistake  amp  the fact that some people write abbreviation and acronym differently  example  ibm  ibm  ibm   
__label__stability assume PRON have a real value function  fx1ldot  xn of some variable  xi which PRON want to evaluate numerically  in general the formula for  f can contain product  rational  trancendental function etc  and  will be to long to investigate PRON numerical stability analytically  or PRON will at least be to time consume to do PRON in practice  assume PRON do not have a short equivalent with guaruanteed stability  be there a methodical procedure to analyse the numerical stability of  f PRON think of compare PRON to arbitrary precission result obtain use a computer algebra system  say the function will be implement in c use stdlib function and single or double precision  which quantity should PRON compare to quantify the quality of the approximation at finite precission  how do PRON determine critical value of the variable  how can PRON choose the compiler and the compiler optimization so other people can easily reproduce the result   PRON know that the problem set be probably to generic to give good answer  but PRON still think that this be a common problem in computer science and wonder if there be reference which propose standard to perform such analysis   what PRON be look for be what be call  automatic error analysis  and be the subject of chapter 26 of higham s book  accuracy and stability of numerical algorithms   2nd ed   siam publishers   one technique PRON describe be use direct search optimization  try to formulate PRON problem as an optimisation problem and use the optimization algorithm to find coefficient or parameter value that maximize or minimize a quantity relate to the accuracy of PRON algorithm  formula  PRON use the example of the growth factor in gaussian elimination  what matrix maximiz this growth factor  or the root of a cubic  as PRON answer in one of PRON previous question    PRON would suggest that PRON obtain a copy of this book  read the introductory chapter and this chapter 26 and the reference therein   what gertvde describe be the numerical error  this may be what PRON be look for  but PRON be not the same as numerical stability as indicate in the title of the question  numerical stability essentially ask whether nearby value of PRON input variable yield nearby value of the output  in other word  whether a formula like   fxvarepsilonfx le c varepsilon hold for some moderate constant  c  for this  PRON can analyze the derivative of  f or  if PRON function do not vary in PRON property wildly over PRON domain  simply try this for a bunch of  xepsilon pair   evaluate PRON function a few time  3 be typically enough  with all input slightly randomly perturb by  pm 1ulp  the standard deviation of the three result will give PRON a crude  but usually sufficient  measure of numerical sensitivity  PRON can compare this to the expect sensitivity from linearization  and form the quotient to get a stability estimate   note that numerical stability ask for how much bad the actual error at evaluation of a particular  x be compare to the error expect from sensitivity analysis when change the input by  pm1  ulp  the latter error express in ulp define the problem condition  condition can be very poor for a stable algorithm  example   1x near  x0   and stability can be poor for a very well  condition function  example   11x11x near  x0    
__label__machine-learning __label__python __label__deep-learning __label__keras PRON be try to implement this paper httpjournalsplosorgplosonearticleid101371journalpone0137036 on a set of medical image  PRON be do PRON in keras  the network essentially consist of 4 conv and max  pool layer  follow by a fully connect layer  follow by a soft max classifier  as far as PRON can see  PRON have follow the architecture mention in the paper  however  the validation loss and accuracy just remain flat throughout  the accuracy seem to be fix at 575   any help on where PRON could be go wrong would be greatly appreciate   from kerasmodel import sequential  from keraslayer import activation  dropout  dense  flatten  from keraslayer import convolution2d  maxpooling2d  from kerasoptimizer import sgd  from kerasutil import nputil  from pil import image  import numpy as np  from sklearnutil import shuffle  from sklearncrossvalidation import traintestsplit  import theano  import os  import glob as glob  import cv2  from matplotlib import pyplot as plt  nbclass  2  imgrow  imgcol  100100  imgchannel  3                      data directory setting                        datum   home  raghuram  desktop  data   oschdirdata   filelist  oslistdirdata                                                                        test line   PRON  cv2imreadfilelist1000     print npshapei        nonresponderfilelist  globglob0flairpng    responderfilelist  globglob1flairpng    print lennonresponderfilelistlenresponderfilelist   label  nponeslenfilelistdtype  int   labels0lennonresponderfilelist    0  immatrix  nparraynparraycv2imreaddataimageflatten   for image in filelist     img  immatrix1000reshape1001003    pltimshowimgcmap   gray    datum  label  shuffleimmatrix  label  randomstate2   traindata   datum  label   x  y   traindata0traindata1     also need to look at how to preserve spatial extent in the conv network  xtrain  xt  ytrain  yt  traintestsplitx  y  testsize03  randomstate4   xtrain  xtrainreshapextrainshape0   3  imgrow  imgcols   xt  xtestreshapextestshape0   3  imgrow  imgcols   xtrain  xtrainastypefloat32    xt  xtestastypefloat32    xtrain  255  xtest  255  ytrain  nputilstocategoricalytrain  nbclasses   yt  nputilstocategoricalyt  nbclass   model  sequential      first conv layer and PRON activation follow by the max  pool layer   modeladdconvolution2d1655  bordermode   valid   subsample   11   init   glorotnormalinputshape   3100100     glorot normal be similar to xavier initialization  modeladdactivationrelu     modeladdmaxpooling2dpoolsize   22strides  none     output be 48x48  print  first layer setup                              second conv layer                                   modeladdconvolution2d3233bordermode   same   subsample   11init   glorotnormal     modeladdactivationrelu     modeladddropout06    modeladdmaxpooling2dpoolsize   22strides  none                                                                                  print  second layer setup    output be 2x24                            third conv layer                                     modeladdconvolution2d6433  bordermode   same   subsample   11   init   glorotnormal     modeladdactivationrelu     modeladddropout06    modeladdmaxpooling2dpoolsize   22strides  none                                                                                   output be 12x12  print  third layer setup                                  fourth conv layer                               modeladdconvolution2d12833  bordermode   same   subsample   11   init   glorotnormal     modeladdactivationrelu     modeladddropout06    modeladdmaxpooling2dpoolsize   22strides  none                                                                                  print  fourth layer setup    output be 6x6x128   create the fc layer of size 128x6x6   modeladdflatten     modeladddense2init   glorotnormalinputdim  128  6  6    modeladddropout06    modeladdactivationsoftmax     print  set up fully connected layer   print  now compile the network   sgd  sgdlr001  decay1e4  momentum06  nesterov  true   modelcompileloss   mseoptimizer   sgd   metricsaccuracy      fit the network to the datum   print  network setup successfully  now fit the network to the datum   model  fitxtrain  ytrain  batchsize  100  nbepoch  20  validationsplit  none  verbose  1   print  testing   loss  accuracy  modelevaluatextestytestbatchsize  32verbose  1   print  test fraction correct  accuracy     2fformataccuracy   PRON seem that PRON use mse as the loss function  from a glimpse on the paper PRON seem PRON use nll  cross entropy   mse be consider prone to be sensitive to datum imbalance among other issue and PRON may be the because of the problem PRON experience  PRON would try training use categoricalcrossentropy loss in PRON case  moreover learn rate of 001 seem too large PRON would try to play with PRON and try 0001 or even 00001 
__label__machine-learning __label__bigdata __label__time-series __label__tensorflow __label__data this may seem like a silly question  but as PRON be go through the documentation for both service PRON be difficult to disentangle what each do  specifically  from what PRON have gather  bluemix be essentially an all  in  one cloud platform for access various ibm analytics api and PRON can code in various language  while  data science experience be sort of an rstudio on steroid  PRON even allow PRON to use rstudio   where PRON can process massive data set use ibm s resource instead of PRON own  but PRON can also code thing in python if PRON want   for PRON need  PRON will be use several different type of datum  include time  series physiological datum and natural language text  sound like PRON will likely need watson   PRON would like to be able to use tensorflow for PRON work as well   if PRON be comfortable with advanced coding and familiar with model technic in r and python  PRON would recommend to use the datascience experience  since PRON will do all the coing   but if PRON want to have ready  to  use model and api s  and will not be bother to tweak or modify some advanced parameter  bluemix be a good choice for PRON  
__label__neural-network __label__deep-learning __label__gpu PRON have see discussion about the  overhead  of a gpu  and that for  small  network  PRON may actually be fast to train on a cpu  or network of cpu  than a gpu   what be mean by  small    for example  would a single  layer mlp with 100 hidden unit be  small    do PRON definition of  small  change for recurrent architecture   be there any other criterion that should be consider when decide whether to train on cpu or gpu   edit 1   PRON just find a blog post  possibly outdated  PRON be from 2014      most network cards  only work with memory that be register with the cpu and so the gpu to gpu transfer between two node would be like this  gpu 1 to cpu 1 to network card 1 to network card 2 to cpu 2 to gpu 2  what this mean be  if one choose a slow network card then there may be no speedup over a single computer  even with fast network card  if the cluster be large  one do not even get speedup from gpu when compare to cpu as the gpu just work too fast for the network card to keep up with PRON   this be the reason why many big company like google and microsoft be use cpu rather than gpu cluster to train PRON big neural network    so at some point  accord to this post  PRON could have be faster to use cpu  be this still the case   edit 2  yes  that blog post may very well be outdat because   now PRON seem that gpu within a node be connect via pcie bus  so communication can happen at about 6gib  s   for example  httpswwwyoutubecomwatchvel1islp1uos  about 35 minute in   the speaker imply that this be fast than go from gpu1 to cpu to gpu2  PRON would mean the network card be no longer the bottleneck   PRON will first reference some quote from similar question   when PRON come to matrix operation  PRON do not think twice  PRON always opt for gpu  source   zwnj   the parallel architecture in a gpu be well adapt for vector and matrix operation   source  so if PRON read through these question  PRON will see that PRON advise to use gpu regardless of the case  PRON will always provide some improvement   the reason PRON may have read that  small  network should be train with cpu  be because implement gpu training for just a small network may take more time than simply train with cpu  that do not mean gpu will be slow   a 100hidden unit network be kind of small  PRON would call PRON a small network relative to the big deep network out there  recurrent architecture  mostly  have more synapsis thant feed forward network  so a 100hidden unit rnn be  big  than a 100hidden unit ffn   the cpu be the manager of the branch  PRON can do a bit of everything  but PRON be not great at much except delegate task  however  the gpu be a dedicated mathematician hiding in PRON machine  if PRON be do any math heavy process then PRON should use PRON gpu  always   if PRON be use any popular programming language for machine learn such as python or matlab PRON be a one  liner of code to tell PRON computer that PRON want the operation to run on PRON gpu   PRON should also make sure to use all the core of PRON machine  this mean make use of parallel computing  especially for neural network where operation can be do independently  this be go to increase PRON speed immensely   unlike some of the other answer  PRON would highly advice against always train on gpu without any second thought  this be drive by the usage of deep learning method on image and text  where the data be very rich  eg a lot of pixel  a lot of variable  and the model similarly have many million of parameter  for other domain  this may not be the case   what be mean by  small   for example  would a single  layer mlp with 100 hidden unit be  small    yes  that be definitely very small by modern standard  unless PRON have a gpu suit perfectly for training  eg nvidia 1080 or nvidia titan   PRON would not be surprised to find that PRON cpu be fast   note that the complexity of PRON neural network also depend on PRON number of input feature  not just the number of unit in PRON hidden layer  if PRON hidden layer have 100 unit and each observation in PRON dataset have 4 input feature  then PRON network be tiny  400 parameter   if each observation instead have 1 m input feature as in some medical  biotech context  then PRON network be pretty big in term of number of parameter  for the remainder of PRON answer PRON be assume PRON have quite few input feature pr  observation   one good example PRON have find of compare cpu vs gpu performance be when PRON train a poker bot use reinforcement learning  for reinforcement learn PRON often do not want that many layer in PRON neural network and PRON find that PRON only need a few layer with few parameter  moreover  the number of input feature be quite low  initially PRON train on a gpu  nvidia titan   but PRON be take a long time as reinforcement learning require a lot of iteration  luckily  PRON find that training on PRON cpu instead make PRON training go 10x as fast  this be just to say that cpu s can sometimes be good for training   be there any other criterion that should be consider when decide whether to train on cpu or gpu   PRON be important to note that while on a gpu PRON will always want to fill up the entire gpu memory by increase PRON batch size  that be not the case on the cpu  on the cpu an increase in batch size will increase the time pr  batch  therefore  if PRON be important for PRON to have a very large batch size  eg due to a very noisy signal   PRON can be beneficial to use a gpu  PRON have not experience this in practice though and normally small batch size be prefer  
__label__machine-learning __label__python __label__deep-learning __label__keras __label__confusion-matrix in the keras blog on train convnet from scratch  the code show only the network run on training and validation datum  what about test datum  be the validation datum the same as test datum  PRON think not   if there be a separate test folder on similar line as the train and validation folder  how do PRON get a confusion matrix for the test datum  PRON know that PRON have to use scikit learn or some other package to do this  but how do PRON get something along the line of class wise probability for test datum  PRON be hop to use this for the confusion matrix   for confusion matrix PRON have to use sklearn package  PRON do not think keras can provide a confusion matrix  for predict value on the test set  simply call the modelpredict   method to generate prediction for the test set  the type of output value depend on PRON model type ie either discrete or probability   to get a confusion matrix from the test datum PRON should go to two step   make prediction for the test datum  for example  use modelpredictgenerator to predict the first 2000 probability from the test generator   generator  datagenflowfromdirectory    datum  test    targetsize150  150    batchsize16   classmode  none    only datum  no label  shuffle  false    keep datum in same order as label  probability  modelpredictgeneratorgenerator  2000   compute the confusion matrix base on the label prediction  for example  compare the probability with the case that there be 1000 cat and 1000 dog respectively   from sklearnmetric import confusionmatrix  ytrue  nparray0   1000   1   1000   ypr  probability  gt  05  confusionmatrixytrue  ypr   additional note on test and validation datum  the keras documentation use three different set of datum  training datum  validation datum and test datum  training datum be use to optimize the model parameter  the validation data be use to make choice about the meta  parameter  eg the number of epoch  after optimize a model with optimal meta  parameter the test data be use to get a fair estimate of the model performance  
__label__machine-learning __label__r __label__python the problem  PRON have a huge dataset which be more than 200gbs in size  the dataset contain around 200 column  predictor   the task at hand be to predict which product to promote to the customer so that PRON  PRON ultimately buy PRON and thus maximize the revenue of the company   PRON be a cross sell situation but PRON need to look at product recommendation as well as revenue maximization   from what PRON understand  revenue maximization would require a regression model   correct PRON if PRON be wrong   PRON be not sure how to recommend product since there be more than 2000 unique product   dummy coding would require tremendous amount of time and resource PRON feel    due to the sheer size of the datum  PRON be plan to use python for handle the datum   suggestion about r also welcome   ps  forgive PRON if the problem seem too basic  but PRON have just start learn   update    the data be in longnarrow  format  PRON can also use r in order to tackle this  products be identify by PRON unique product id  2000  unique product id   headers   date  time  pid  custid    amount  tax  netrevenue  net revenue  continuous variable  product id  continuous but to be treat as nominal  custid  continuous but to be treat as nominal  PRON question right now be really vague  however  if PRON have to make a recommendation base on what PRON have write PRON would suggest do a market basket analysis to determine which product be commonly purchase together  once PRON understand which product be purchase together PRON can do a regression analysis of the combo find in the market basket analysis in order to predict revenue maximization  if PRON understand PRON question correctly  PRON need to recommend one product out of 2000  PRON want to pick the one with the high revenue expectation  this can be decompose in the probability of make the sale time the revenue give that be a sale  the revenue for a sale be know  the probability be what be need  PRON assume PRON have historic datum of which customer  with the give feature  buy which of these product  now PRON can train regression model to estimate the probability of a sale for a specific product  logistic regression and neural networks with a sigmoid activation function be well suited for unbiased probability estimate  by optimize the log  loss cost function   with this amount of datum PRON propose to start on a subset of PRON datum and use logistic regression  since this be a generalize linear model PRON will likely need to do some feature engineer to increase performance  since PRON mention PRON use python  scikit  learn have a lot of function for logistic regression optimization and feature engineering so take a look at that   once PRON have train 2000 model  for a new recommendation PRON would run the feature through the model  get all the probability  multiply PRON with revenue correspond to this product and pick the high one   depend on the volume and density of the information PRON have about PRON customer and PRON purchase  PRON may be able to run a collaborative filtering  mean find the cluster of similar customer and see what each cluster tend to buy   then  for each individual PRON can recommend what PRON have not buy before and have a great metric  revenue in this case    from the business standpoint  there may be other metric PRON may also want to take into account  net profit  time in stock  etc   
__label__finite-difference __label__discretization PRON want to write a matlab code for solve shallow water equations numerically use finite difference method  can anyone help PRON with that  these be PRON equation  PRON have already discretiz PRON eqution use fdm forward time central space scheme  want to apply newtons method to the discretiz equation   PRON discretized equation be   so to solve these equation use newtons method  what would be the dimension of jacobian matrix form   
__label__fluid-dynamics __label__mesh __label__oscillations if one have to simulate oscillating plate  solid wall  sinusoidal function of time  in a domain  a simple piston movement in  y  direction   the obvious way would be scenario 1  for brevity  assume there be no topological change involve   scenario 1   to use dynamic mesh and move the mesh point via the prescribed equation   scenario 2   the fluid near wall take up a zero velocity  assume no slip   now  with this in mind  use an oscillating velocity boundary condition in  u  at the plate  solid wall  work out the velocity of the fluid for the plate s displacement amplitude  as the fluid should obey the no slip condition and produce the flow feature similar to a move mesh   will the result of scenario 2 be different to scenario 1  be scenario 2 the right way to represent a near wall fluid with no slip  what happen to  k  and  pressure  in scenario 2   one and two do not lead to the same flow  one be like a piston that actually compress the flow  two will behave differently  the detail be dependent on the inlet flow characteristic  but PRON should not expect the same behavior  if the flow be slow   m ll 1    PRON may be able to get away with a short  time simulation of the starting condition in a fix box  but the two case will rapidly diverge  PRON may also be able to simulate a speaker  diaphragm this way  but solve the compressible navier  stokes equation for very small amplitude be usually overkill  
__label__machine-learning __label__deep-learning __label__time-series PRON be get into lstm  and PRON have one technical question  PRON have time  series in 1 minute interval and 2 week datum   PRON need predict every next minute in the future  one slide window have 120 minute length  PRON question be  if  the past datum outside  of one slide window  have an influence on training and testing in current window of a lstm   different way   could be  the slide window from the 2 week mix randomly for training  out of any order   thanks   
__label__classification __label__keras PRON know that there must be a simple solution to this  PRON just can not come up with PRON right now  but how do PRON get from a datum collection that have category to something that can be use by keras and categoricalcrossentropy   PRON know that PRON need something like y  kerasutilstocategoricaltraininglabel  but PRON be expect traininglabel to be a list of integer  how do PRON get to that step from a set of label that be currently in text   
__label__computational-geometry __label__regression __label__geometry __label__curve-fitting PRON have a question regard quadric fit to a set of point and correspond normal  or equivalently  tangent   fit quadric surface to point datum be well explore  some work be as follow   type  constrain direct fitting of quadric surfaces  james andrews  carlo h sequin  computer  aided design  amp  application  10a   2013  bbb  ccc  algebraic fitting of quadric surface to datum  i al  subaihi and g a watson  university of dundee  fit to projective contour be also cover by some work  such as this one   from all these work  PRON think taubin s method for quadric fitting be pretty popular   g taubin   estimation of planar curves  surfaces and nonplanar space curves define by implicit equations  with application to edge and range image segmentation   ieee trans  pami  vol  13  1991  pp1115  1138   let PRON briefly summarize  a quadric  q can be write in the algebraic form      fmathbfcmathbfx    a x2  by2  cz2  2dxy  2exz  2fyz  2gx  2hy  2iz  j     where  mathbfc be the coefficient vector and  mathbfx be the 3d coordinate  any point  mathbfx lie on the quadric  q if  mathbfxtqmathbfx0   where      q   beginbmatrix   a  amp  b  amp  c  amp  d   b  amp  e  amp  f  amp  g   c  amp  f  amp  h  amp  PRON   d  amp  g  amp  PRON  amp  j   endbmatrix      algebraic fit in principle  PRON would like to solve for the parameter that  minimize the sum of squared geometric distance between the point and the quadratic surface  unfortunately  PRON turn out that this be a non  convex optimization problem with no known analytical solution  instead  a standard approach be to solve for an algebraic fit  that be to solve for the parameter   mathbfc that minimize      sumlimitsi1n  fmathbfcmathbfxi2  mathbfct  m mathbfc      with     m  sumlimitsi1n  lmathbfxilmathbfxit     where  mathbfxi be the point in the point cloud and     l   x2  y2  z2  xy  xz  yz  x  y  z  1t     notice that such direct minimization would yield the trivial solution with  mathbfc at the origin  this question have be study extensively in the  literature  one resolution that have be find to work well in practice be taubin ’s method  cite above   introduce the constraint       nablax fmathbfcmathbfxi  2  1     this can be solve as follow  let      n  sumlimitsi1n lxmathbfxilxmathbfxit lymathbfxilymathbfxit  lzmathbfxilzmathbfxit     where subscript denote the derivative  the solution be give by the generalize eigen decomposition    m −lambda n  mathbfc   0 the best  fit parameter vector be equal to the eigenvector correspond to the small eigenvalue   main question  in many application  the normal of the point cloud be available  or compute   the normal of the quadric  mathbfnx can also be calculate by differentiate and normalize the implicit surface      mathbfnx   fracnabla fmathbfcmathbfxnabla fmathbfcmathbfx      where     nabla fmathbfcmathbfx    2  beginbmatrix   ax  dy  fz  g   by  dx  ez  h   cz  ey  fx  i   endbmatrix      however  taubin s method utilize only the point geometry  and not the tangent space  and PRON be not aware of many method  which be suitable for fitting quadric such that the tangent of the quadric also match the tangent of underlying point cloud  PRON be look for potential extension of the method above  or any other to cover these first order derivative   what PRON would like to achieve be maybe address partially in low dimensional space  with more primitive surface  curve  type  for example  fit line to image edge  take into consideration the gradient information be cover here  fitting plane  a simple type of quadric  to 3d cloud be very common  link 1  or fit sphere or cylinder can be fit to orient point set  link 2   so what PRON be wonder be something similar  but the fit primitive be a quadric   PRON would also welcome the analysis of the proposed method such as   what be the minimum number of orient point require   what be the degenerate case   can anything be say about robustness   update   PRON would like to present a direction to follow  formally  what PRON desire to achieve       nabla f  mathbfn    0     at the point  mathbfx maybe PRON may be possible to fuse PRON with taubin s method to come up with an additional constraint and minimize use lagrange multiplier   PRON know of one example where the normal have be include in the fitting procedure  PRON be not a direct quadric fitting though  a locally parametriz patch be fit to the point and normal  use normal give more equation in the fitting problem  allow high order polynomial to be use   a novel cubic  order algorithm for approximate principal direction vector  see also this paper  hermite interpolation of implicit surfaces with radial basis functions  and ist extension  hermite radial basis functions implicits 
__label__information-theory PRON want to understand the meaning of the difference of two information entropy value   PRON have the following scenario  let  x be a number of hour a user spend on some video sharing website  thus  PRON may have the set    xa   x1x2cdot  xna  and   xb   x1x2cdot  xnb that represent the number of hour the user of  a and  b spend on the website  a and  b  respectively   now  PRON can calculate the cumulative distribution function   cdf  probability value  describe here  as follow   for a real  value random variable x  the cdf be define as     fax   pxleq x forall x in xa  similarly  PRON find  fbx  then PRON calculate the entropy value  ea and  eb for the probability   fax and  fbx obtain above  as describe here      ea   sumlimitsi1napxilogpxi  similarly  PRON find  eb  now PRON question be  what do  ea and  eb mean in this scenario  and what PRON difference also mean   
__label__optimization __label__r PRON be a beginner user of r PRON be try to maximize log likelihood function with the bounded parameter  the function be a kind of gamma mixture model which try to capture unobserved heterogeneity across individual  PRON choose the optim s l  bfgs  b give that the function contain two parameter  ie  alpha and r  which should have positive value  however  the optimization do not take place and return the initial parameter with the  message below    sample datum  datalt  matrixc136547254464392415411411065101038ncol3byrow  true   colnamesdatalt  cidfreqtime    beginalign    ampminr  alpha  r log alpha  sumj0ni1logrj    ni  rlogtis   tic   alpha   amptextsuch that   ampqquad r  gt  0quad textshape parameter   ampqquad alpha  gt  0quad textscale parameter    endalign   in the code below PRON use follow term for function   ni  datai2    ni be the frequency of individual i  tistic  datai3    tistic be the total time of individual i   likelihood function PRON want to optimize  will  lt functiontheta    alphalt  theta1    scale parameter  gt0  rlt  theta2   shape parameter  gt0  lli  c    s  c    fori in 15    forj in 0datai21    sj1lt  logrj1     lliilt  rlogalphasumsdatai2rlogalphadatai3    s  c      printlli   lllt  sumlli   returnll      initial parameter  initlt  calpha1r5    l  bfgs  b optimization  anslt  optimpar  init  fn  ll  methodl  bfgs  blower  c1e101e10upper  cinf  inf     message   1   convergence  norm of projected gradient  lt pgtol   any hint towards a successful optimization   
__label__numerical-analysis __label__reference-request __label__runge-kutta PRON try to go to the primary source in order to understand how to use butcher table to simplify the algebra PRON need to do when use taylor series to find the order of accuracy of a scheme  for instance   however  maybe because of a lack of relevant background  PRON find PRON particularly tough to understand how to utilize butcher table from butcher s book   be there good  relatively self  contain  ie minimum prerequisite  book or tutorial that cover the necessary mathematic PRON need to utilize butcher table   PRON sound like there be two thing PRON may want to  use  PRON for   to implement a method   any reference will give PRON a clear algorithmic description that should make this easy   to check the order of a method   PRON be not sure why PRON need to do this  but PRON be just a matter of look up the order condition  equation  and plug in number   PRON can find both in almost any reference  though the order condition for very high order method be only in more specialized source  like butcher s book    just to give PRON something concrete  PRON recommend chapter 7 of leveque s book on finite difference   for a python implementation of the algorithm and the order condition  up to order 14  see PRON nodepy package  
__label__pde __label__advection __label__operator-splitting what be the implication of apply a direction  splitting within each stage of an ssp  rk scheme   for instance  give a standard advective transport type equation      partialtq  operatornamedivmathbfuq   0     PRON be popular to make use of an explicit ssp  rk style time  step scheme  such as the well  know ssp  rk2 method      q1    qn    delta t operatornamedivmathbfuqn     1ex   q2    tfrac12qn    tfrac12q1    tfrac12delta t operatornamedivmathbfuq1     1ex   qn1   q2       but if the flux term  can only be evaluate via a direction  splitting      operatornamedivmathbfuq   partialxuq   partialyvq      what be the implication of advance each rk stage accord to a simple direction split      q1a    qn   1ex   q1b    q1a    delta t   partialxuq1a     1ex   q1c    q1b    delta t   partialyvq1b     2ex   q2a    tfrac12qn   tfrac12q1c    1ex   q2b    q2a    tfrac12delta t   partialyvq2a     1ex   q2c    q2b    tfrac12delta t   partialxuq2b     2ex   qn1   q2c       be the order of the method reduce  do PRON loose PRON ssp property  be PRON possible to combine such an approach with ssp  rk3 to get a third  order accurate method   
__label__machine-learning factorization machine be a great way to model interaction between different feature  in a way that be more efficient than full  blow quadratic interaction  there exist efficient implementation like libfm and vowpal wabbit s lrq option   PRON question be about high order interaction  the fm paper mention that PRON be  easy  to extend PRON to high order  tensor fm    see  eq   5  in the paper  be there any good implementation of 3way factorization machine    move from cross  validated   a tensor flow implementation  httpsgithubcomgeffytffm  PRON support arbitrary order    2  factorization machines  dense and sparse input  the interface be in scikit  learn style   see jupyter notebook with example   PRON have recently start polylearn  a scikit  learn  contrib package for efficient implementation of factorization machine and polynomial network  in python with cython  polylearn currently coordinate descent solver for 2nd and 3rd order factorization machine  and for arbitrary order polynomial network  use the approach describe in the paper cite on the polylearn website   PRON be currently work on an implementation of arbitrary high order fm use sg and cd style algorithm  use the approach from this recent paper  PRON should have PRON ready within a week   polylearn have not have a release yet  so PRON would love to hear any feedback on the api and functionality  in order to round thing off before the 01 release  
__label__clustering __label__algorithms __label__similarity assume that PRON have a set of element e and a similarity  not distance  function simei  ej  between two element ei  ej ∈ e  how could PRON  efficiently  cluster the element of e  use sim   k  mean  for example  require a give k  canopy clustering require two threshold value  what if PRON do not want such predefined parameter   note  that sim be not neccessarily a metric  ie the triangle inequality may  or may not hold   moreover  PRON do not matter if the cluster be disjoint  partition of e     PRON think a number of cluster algorithm that normally use a metric  do not actually rely on the metric property  other than commutativity  but PRON think PRON would have that here    for example  dbscan use epsilon  neighborhood around a point  there be nothing in there that specifically say the triangle inequality matter   so PRON can probably use dbscan  although PRON may have to do some kind of nonstandard spatial index to do efficient lookup in PRON case   PRON version of epsilon  neighborhood will likely be sim  1epsilon rather than the other way around   same story with k  mean and related algorithm   can PRON construct a metric from PRON similarity   one possibility  distei  ej   min  simei  ek   simek  ej   for all k   alternately  can PRON provide an upper  bind such that simei  ej   lt  simei  ek   simek  ej   d  for all k and some positive constant d   intuitively  large sim value mean closer together  be 1sim metric  like   what about 1sim  constant    what about min  1simei  ek   1simek  ej   for all k   that last be guarantee to be a metric  btw   an alternate construction of a metric be to do an embedding   as a first step  PRON can try to map PRON point ei   xi  such that xi minimize sum  abs  simei  ej   f  distxi  xj     for some suitable function f and metric dist   the function f convert distance in the embedding to a similarity  like value  PRON would have to experiment a bit  but 1dist or expdist be good starting point   PRON would also have to experiment on the good dimension for xi   from there  PRON can use conventional clustering on xi   the idea here be that PRON can almost  in a good fit sense  convert PRON distance in the embedding to similarity value  so PRON would cluster correctly   on the use of predefined parameter  all algorithm have some tuning   dbscan can find the number of cluster  but PRON still need to give PRON some parameter   in general  tuning require multiple run of the algorithm with different value for the tunable parameter  together with some function that evaluate goodness  of  clustering  either calculate separately  provide by the clustering algorithm PRON  or just eyeball   if the character of PRON data do not change  PRON can tune once and then use those fix parameter  if PRON change then PRON have to tune for each run   PRON can find that out by tune for each run and then compare how well the parameter from one run work on another  compare to the parameter specifically tune for that   topological data analysis be a method explicitly design for the setting PRON describe  rather than a global distance metric  PRON rely only on a local metric of proximity or neighborhood  see  topology and datum and extract insight from the shape of complex datum use topology  PRON can find additional resource at the website for ayasdi   alex make a number of good point  though PRON may have to push back a bit on PRON implication that dbscan be the good clustering algorithm to use here  depend on PRON implementation  and whether or not PRON be use accelerated index  many implementation do not   PRON time and space complexity will both be on2   which be far from ideal   personally  PRON go  to cluster algorithm be openord for winner  take  all clustering and flame for fuzzy clustering  both method be indifferent to whether the metric use be similarity or distance  flame in particular be nearly identical in both construction   the implementation of openord in gephi be onlogn  and be know to be more scalable than any of the other cluster algorithm present in the gephi package   flame on the other hand be great if PRON be look for a fuzzy clustering method  while the complexity of flame be a little hard to determine since PRON be an iterative process  PRON have be show to be sub  quadratic  and similar in run  speed to knn   dbscan  see also  generalized dbscan  do not require a distance   all PRON need be a binary decision  commonly  one would use  distance  lt  epsilon  but nothing say PRON can not use  similarity  epsilon  instead  triangle inequality etc  be not require   affinity propagation  as the name say  use similarity   hierarchical clustering  except for maybe ward linkage  do not make any assumption  in many implementation PRON can just use negative distance when PRON have similarity  and PRON will work just fine  because all that be need be min  max  and  lt  kernel k  mean could work if PRON similarity be a good kernel function  think of PRON as compute k  mean in a different vector space  where euclidean distance correspond to PRON similarity function  but then PRON need to know k  pam  k  medoid  should work  assign each object to the most similary medoid  then choose the object with the high average similarity as new medoid  no triangle inequality need    and probably many many more  there be literally hundred of cluster algorithm  most should work imho  very few seem to actually require metric property  k  mean have probably the strong requirement  PRON minimize variance  not distance  or similarity   and PRON must be able to compute mean  
__label__neural-networks __label__machine-learning __label__deep-learning __label__computer-vision __label__object-recognition from PRON understanding and text PRON find in research paper online   1  pixel base object recognition   neural network be train to locate individual object base directly on pixel datum   2  feature base object recognition   contents of a window be map to a feature space that be provide as input to a neural classifier   what do the above 2 definition imply in a simple language  especially point number 2   also  can someone point PRON to paper  resource  article that would explain the above approach in great detail   pixel base object recognition  like the name say  work by analyze the individual pixel of an image  for example  PRON analyze an image with a lot of different shade of blue and some grey pixel  PRON may assume that this be the picture of a plane in the sky or a ship in the water   PRON could also look for similar picture by calculate the difference between each individual pixel of 2 image and sum up the difference  the small the sum of the difference  the more the picture look alike   PRON depend on PRON field of work  but those method will most likely not be good enough for solid image recognition   PRON also mention neural network  of course PRON can feed the raw pixel datum to a neural network  but that be not state of the art  PRON will almost always use a cnn  which bring PRON to PRON next point   feature base object recognition  like the name say  try to extract certain feature from an image to perform classification  PRON use several convolutional layer for analysis before PRON feed the output to a neural network  this architecture be call cnn  convolutional neural network    what those feature actually look like be hard to explain without show example  PRON recommend to watch the follow two lecture from the stanford course cs231n   lecture 2  image classification  lecture 5  convolutional neural networks  the first one explain image classification in general and show some implementation for pixel base algorithm  the second one dive into cnn and explain feature base approach  PRON highly recommend to watch the full series  but if PRON be in a hurry  those 2 lecture will tell PRON what PRON want to know  
__label__pde __label__finite-difference __label__python __label__discretization __label__numpy the explicit 4th order discretization for the 2d scalar wave equation be give by   begineqnarray   ujkn1     left  fracdelta t  vjk    delta s  right  2 left   suma  nn wa uja kn  suma  nn wa uj kan right   2 ujkn   ujkn1   nonumber   ujkn1     left  fracdelta t  vjkdelta s  right  2  suma  nn  wa left  uja kn  uj kan right   2 ujkn   ujkn1  label1   endeqnarray   where  delta s  delta x  delta z   accord to jing  bo cheen1 the stability limit be give by  where  v be  maxvjk for  n  2  forth order for space derivative     delta t leq frac2 delta s   v sqrtsuma  nn   wa1  wa2       where  wa be the center difference weight  in the code bellow PRON be define be     wa   1012  403  502  403  1012     for the code bellow  PRON write use numpy  PRON have the following     delta s  10      v  20000    grid dimension     nz  nx  20    source   triangular wavelet place at  10  10  with 23 sample or  23 ms    amplitude of peak 10  number of iteration  enough to see some propagation      numberiter  200    then PRON would expect that     delta t leq 00030618621784789723    this case  should be ok no  since PRON be     delta t  0001    update    after read some post PRON guess be something relate to not have an smooth sorce or use spatial derivative high order  still PRON do not have all the skill to make this here work   what PRON be see be a lot of dispersion  just in 6 iteration PRON maximum value on PRON grid  non zero  be on e14 order   also look on a specific point of PRON grid  close to the source  1313  PRON get the following picture as time evolf  first 50 iteration   what be PRON do wrong  suggetion to make PRON stable will be very appreaciated  import numpy as np  import sys  def trianglefc  dt  n  none    r     triangle wave one period   define by frequency and sample rate or by size   fc   maximum desire frequency   dt   sample rate   n   half length of triangle      ifnnone    n  int1floatfcdt    t  nparange010n  1  10n   y  1t  y  npappendy  00   y   1t1   y   npinserty  0  00   return npappendy  npappend1  y    nz  nx  20  dt  0001  ds  10  numberiter  200  source  triangle90  0001   uprevious  npzerosnz  nx    previous time  ucurrent  npzerosnz  nx    current time  ufuture  npzerosnz  nx    future time  v  npzerosnz  nx    v        20000   additional not need  simulation   npzerosnumberiter  nz  nx     source activation  center of grid  uprevious1010   source0   for i in range1  numberiter1     tringular source position center grid  ifi  lt  npsizesource     ucurrent1010   sourcei   for k in rangenz    for j in rangenx     u0k u1kujku3k u4k   boundary fix 0 outside  u0k  u1k  u3k  u4k00  uj0uj1uj3uj400  ujk  ucurrentkj   ifj2  gt  1    u0k  ucurrentkj2   ifj1  gt  1    u1k  ucurrentkj1   ifj1  lt  nx    u3k  ucurrentkj1   ifj2  lt  nx    u4k  ucurrentkj2   ifk2  gt  1    uj0  ucurrentk2j   ifk1  gt  1    uj1  ucurrentk1j   ifk1  lt  nz    uj3  ucurrentk1j   ifk2  lt  nz    uj4  ucurrentk2j   d2udx2   u0k16u1k30ujk16u3k  u4k120  d2udz2   uj0  16uj1  30ujk16uj3uj4120  ufuturekj    d2udx2d2udz2dtvkjds2  ufuturekj    2ucurrentkjupreviouskj    make the update in the time stack  uprevious  ucurrent  ucurrent  ufuture  simulationi1   ucurrent  sysstdoutwriter  d    i    1 a stability formula for lax  wendroff method with fourth  order in time and general  order in space for the scalar wave equation  geophysics vol  76 no  2 2011  after more than 2 month and no one find the answer to PRON problem  PRON find PRON the error and the solution  to help the community iam share PRON   the source term be wrong  the correct discrete equation use a source term  st be give by   beginmultline   ujkn1     left  fracdelta t  vjkdelta s  right  2  left  suma  nn  wa left  uja kn  uj kan right  right   sjkn delta t2 vjk2  2 ujkn   ujkn1  labelxb1   endmultline   not by the equation give in the beginning that do not take in account the source term  the code do not implement  above   PRON just set s  u1010n  st    that be not right   python class  pointer error  big mistake   when PRON do the time stack update  bellow   PRON be not copy the numpy array content  PRON be just change the class pointer    make the update in the time stack  uprevious  ucurrent   uprevious be point to ucurrent  previous ok    ucurrent  ufuture  ucurrent be point to ufuture   simulationi1   ucurrent  so what the laplacian loop be do   ufuturekj    2ucurrentkjupreviouskj   be in fact   ufuturekj    2ufuturekjupreviouskj   content copy would be   ucurrent        ufuture        run the code above PRON get  simple animation   httpwwwyoutubecomwatchvma7rwcq3z9e 
__label__optimization PRON need help program a script  in matlab  that for assign value compute the optimal transmission angle of a crank  rocker mechanism   the parameter be    l  crank s angle of oscillation  for example from 20 ° to 150 °   m  rocker s angle of oscillation  PRON be a fix value    t    tanl2   u   tanl  m2   v   tanm2  there be a condition on  l   llt90m2  the equation be a cubic     q3  2q2  t2 q fract21t2u2  where  q  t2x2  the root of  q must be real and positive    x be the variable and PRON should satisfy the follow inequality    x2 geq 1  and  x2 leq u2 t2  now the length of the 4bar be  beginalign   a  amp sqrtfracv21  v2cr  b  amp sqrtfracx2 v21  v2cr  c  amp sqrtfracx2  t21  t2cr  d  amp sqrtfracu2  x21  u2    endalign   PRON solve the equation for  q then calculate  x now the transmission angle  max and min  depend on  ax   bx   cx and  dx PRON have a different transmission angle  max and min  for all the different value of crank s angle of oscillation  for example as  istat previously from  20  to   90m2  the low bind be discretionary     beginalign   mathitangtrasmminamp arccosleftfracb2  c2d  a22bcright     mathitangtrasmmax   amp arccosleftfracb2  c2d  a22bcright   endaligned      PRON code be the follow   compute transmission angle and length of bar  clear all  m  32    rocker ’s oscillation  i  1   f  1   for l  1   90  m2     crank ’s oscillation pi  there be a problem here i can not start from 20 for example   li   l  180    crank ’s oscillation  t  tandli2    u  tandlim2    v  tandm2    p   1 2 t2  t2   1  t2u2      if li    m  fprintfvalue of crank ’s oscillation be not acceptable n   ll     else  q  rootsp    for j  13  if isrealqj    ampamp  qjgt0  x  abst  sqrtqj      if x2  gt   1  ampamp  x2  lt   u2t2  a  sqrtv21  v2     b  sqrtx2v21  v2     c  sqrtx2  t21  t2     d  sqrtu2  x21  u2     angtrasmminf   acosdb2  c2d  a22bc     angtrasmmaxf   acosdb2  c2d  a22bc     scartominf   abs90angtrasmminf     scartomaxf   abs90angtrasmmaxf     thetaf   li    r1f   a   r2f   b   r3f   c   r4f   d   fprintfthe max transmision angle be  d when theta be  dnangtrasmmaxfli     fprintfthe max transmision angle be  d when theta be  dnangtrasmminfli     fprintf  n     f  f  1   end  end  end  end  i  i  1   end  
__label__data-visualization __label__visualization __label__vtk suppose PRON have two vtk file  both in structured grid format  the structured grid be the same  PRON have the same list of point  in the same order   and there be a field  call PRON  phi   in each vtk file  PRON want to create a third vtk file  again with the same structured grid  and plot a field that be the difference between phi in the first vtk file and phi in the second vtk file   PRON know how to do this manually  PRON can parse the raw text in the two vtk file  copy the datum into array  subtract one array from the other  and then dump the datum in the right format into a new file  be there a good way to calculate this difference and export PRON to vtk  a solution in python  or in visualization software like visit or paraview would be preferable to use a compile language like c   the purpose of calculate this difference be to compare different numerical method for calculate the solution of a pde  since PRON be use the same software to generate the solution  PRON can guarantee that all datum but the field phi will be the same in each file PRON generate   the easy way PRON could find to subtract two field from different vtk file with the same structured grid be to use a programmable filter in paraview  which let PRON manipulate datum use python script   in the programmable filter dialogue box  PRON can subtract the two array and write to output with the code   phi0  inputs0celldataphi    phi1  inputs1celldataphi    output  celldataappendphi1  phi0   difference    in this instance  the field phi happen to be cell datum  if PRON field be point datum  replace celldata everywhere in the script with pointdata  see httppublickitwarecompipermailparaview2010april016667html for further detail   PRON do not have a particularly good approach  but PRON would copy the  phi  field from one vtk file into the other and name PRON  phiprime  or something  in both paraview and visit  PRON have the option of define new field by a formula that use the value of other field  PRON can then define a field  error  as  error  phi  phiprime  in the field editor  and plot this field  error  as either a surface  a contour plot  or whatev PRON be interested in   the step of copy the block of datum from one file to the other be clearly awkward  but PRON be the good PRON can come up with   in paraview there be the append attributes filter which can be use for this  PRON require that the same number of point be in the datum set for append point datum properly and the same number of cell be in the datum set for append cell datum properly  PRON will have problem though with array of the same name  ie phi in PRON example   PRON can copy that array easily with the calculator filter though before use the append attributes filter  then PRON can use another calculator filter to do the subtraction  this be probably less efficient than use paraview s python programmable filter though   besides that  PRON could use the vtkpython executable to do PRON manually since PRON would have direct access to both grid and PRON attribute   PRON realize this be a bit old  but PRON though PRON may be interested in the visit solution   PRON can do this in visit with something call a connectively based cross mesh field expression  that be a mouthful  but be basically machinery to map field between database  in PRON case vtk file    the  connectivity base   conncmfe  be use when the topology be the same between the file  like in PRON case   there be also a  position base   poscmfe  that sample between mesh with different topology   for PRON case  open the first file  amp  use the expressions window to define an expression  myphidiff    phi  conncmfeltfile2vtk  phigt   mesh   then PRON can plot  myphidiff  with a pseudocolor plot   there be also wizard that PRON can use to help define the expression   options menudata level comparisons    here be some more info   httpvisitusersorgindexphptitlecmfe 
__label__machine-learning __label__deep-learning PRON have a multiclass7 label  classification problem implement in mlp  PRON be try to classify 7 cancer base on some datum  the overall accuracy be quite low  around 58   however  some of the cancer be classify with accuracy around 90  for different parameter  below cancer 123 etc mean different type of cancer  for example 1 breast cancer  2  lung cancer etc  now  for different parameter setting PRON get different classification accuracy  for example   1  hyper parameter  learningrate  0001  trainingepoch  10  batchsize  100  hiddensize  256   overall accuracy 53   cancer 2 accuracy 91   cancer 5 accuracy 88     cancer 6 accuracy 89   2  hyper parameter  learningrate  001  trainingepoch  30  batchsize  100  hiddensize  128   overall accuracy 56   cancer 2 accuracy 86   cancer 5 accuracy 93     caner 6 accuracy 75   as PRON can see  for different parameter setting PRON be get totally different result  cancer 1347 have very low accuracy  so PRON exclude PRON  but cancer 2  56 have comparatively good result  but  for cancer 6  the result vary by great number depend on the parameter setting   an important note be  here overall accuracy be not important but if PRON can classify 2  3 cancer with more than 90  accuracy that be more important  so PRON question be  how do PRON interpret the result  in PRON paper how should PRON show the result  which parameter setting should PRON show  use  or should PRON show different parameter setting for different cancer type  so basically  how to handle this type of situation   
__label__beginner __label__career PRON be a 26 year old guy with an mba and PRON work as an erp system administrator   PRON have be interested in the field of data science for a while now  PRON have always like statistic and various analytical task   PRON would really like to try and work within this field but PRON feel a bit overwhelming  people mention that PRON have to learn python  r  sql  machine learning  advanced algebra  data modeling  big datum  hadoop etc   predictive analytic  various business intelligence tool  vba  mathlab etc etc   as of today PRON have some sql knowledge and have a genereal understanding of bi  big datum  good excel skill etc  PRON be willing to learn some of the abovementioned area but PRON do not have the money or time to get back to full time university study again   so here be PRON question  be there any recognize  light version  of datum scientist on the market  what be PRON usually call  what skill do PRON need to master  what should PRON learn in order to work with big datum set and analytic without have to study full time for another 5 year   PRON live in scandinavia so the job market be probably different here but PRON think PRON would be interesting to hear some answer   business intelligence be perfect for PRON  PRON already have the business background  if PRON want to become a bona fide data scientist brush up on PRON computer science  linear algebra  and statistic  PRON consider these the bare essential   PRON do not know about scandinavia  but in the us  data science cover a broad spectrum of task range from full  time software development to full  time datum analysis  often with domain expertise require in various niche  such as experimental design  PRON have to decide where PRON strength and interest lie to pick a position on this spectrum  and prepare accordingly  useful activity include participate in kaggle competition  and contribute to open source data science library   the business analyst flavour of datum science be something PRON be nicely suit for   as far as PRON have see business analyst and business intelligence engineer in the industry  most of PRON work be center around derive insight from excel sheet and write sql query to dig out the appropriate datum  PRON do write script  but that be generally for just the visualization purpose  and not for high lever analytic like machine learning   PRON can also see a nice future for PRON in the financial analytic  quant domain  PRON be also a domain where the learning curve be a bit steep  but totally worth PRON  here be PRON answer on quora about get into the field of quant   however  if PRON want to get up to speed with datum science  then PRON have to slowly build up strong linear algebra skill  along with a very keen and valuable domain knowledge in whatev domain PRON would be work with  the latter be often underrated  but from PRON  short but valuable  experience in the industry  PRON vouch for that fact   bonus resource   quora data science topic wiki  metacademy learning path  ocw math course  if PRON be okay with start with a full blast ground up learn marathon for data science  then this be the path PRON would recommend   single variable calculus  multi variable calculus  differential equations  linear algebra  probability theory and combinatoric basics  statistics  algorithms  all the above course be available in the ocw catalogue  if not  then PRON can find PRON in other mooc aggregator too  
__label__deep-learning __label__deep-network __label__computer-vision PRON be try to implement some image super  resolution model on medical image  after read a set of paper  PRON find that none of the exist model use any activation layer for the last layer   what be the rationale behind that   as discuss here   httpswwwresearchgatenetpostwhatshouldbemyactivationfunctionforlastlayerofneuralnetwork  linear be the preferred activation function   but then linear activation function be equivalent to no activation function at all   httpsdatasciencestackexchangecomquestions13696lackofactivationfunctioninoutputlayeratregression  in term of a neural net  a linear activation be like a pass through or no activation function  depend on how the algorithm have be program  PRON may  for ease and clarity  require the application of an activation function  if PRON want the last layer to simply be the dot product of that layer ’s weight and the precede layer  eg without a tanh activation function  the linear activation serve this purpose  
__label__python __label__preprocessing PRON be look for a tool  library that will take a numpy or pandas matrix and generate a list of statistic for the matrix and column  specifically  for each column  PRON would want info like the follow   assumed datum type  numerical vs string   assume PRON be determined to be a numerical column   mean and std deviation  max and min value  number of nan s   of nan s  assume PRON be determined to be a string column   number of distinct string value  so the tool take the matrix and output some kind of report along these line  the goal be to do some sanity check on PRON input datum set as well as check datum transformation along the way  for example  if PRON be do some kind of transformation and value in column a go from 1  nan to 60  nan  then PRON do something really bad   do such a tool exist   PRON should take a look at pandasprofiling  PRON do not think PRON work with numpy array but PRON do exactly what PRON want for pandas dataframe  PRON can output to pdf or to a nice look html format within PRON jupyter notebooks   httpsgithubcomjospolflietpandasprofil  PRON could use pandasdescribe   import panda as pd  import numpy as np  df   pd  dataframefirst    1234np  nan    second    100200np  nan  np  nan4     dfdescribeincludeall    regard the percentage of nas  PRON could use just a small formula for that   countnan  lendf   dfcount    countnan  dfshape1  
__label__r __label__random-forest PRON want to assess the importance of variable in PRON model use the importance   function of r randomforest package  PRON have a binary response variable  class and binary feature value   mytreerf  lt randomforestclass    datum  mydata  ntree1500keepfor  false  importance  true   importancemytreerf   the output matrix contain the meandecreaseaccuracy and meandecreasegini  PRON understand those two   PRON problem be with two other column in the output  one simply say  true  the other one  false   neither in the documentation nor online PRON be able to find an answer what those value be and how PRON be calculate   can anyone help PRON out   cheer  edit 1  thank to david answer PRON realize true and false be PRON class  name   PRON still do not understand how the value give in the matrix be calculate though  can anyone help with that   edit 2  thank to david again  PRON turn out the answer be in the documentation  but PRON can not be find in the chapter about the importance   function  but rather in the description of object of class randomforest  importance be one of these object      a matrix with nclass  2  for classification  or two  for regression  column   for classification  the first nclass column be the class  specific measure compute  as mean descrease in accuracy  the nclass  1st column be the mean  descrease in accuracy over all class  the last column be the mean decrease  in gini index   PRON do not see any column like that  here be a simple example   libraryrandomfor   datairis   irisrf  lt randomforestspecie    datum  iris  importance  true   colnamesimportanceirisrf      1   setosa    versicolor     3   virginica    meandecreaseaccuracy     5   meandecreasegini   if PRON be see column name  true  and  false  in the matrix return by importance  PRON good guess be that those be the name of column in PRON training datum  
__label__image-processing PRON need to write a report about some image processing topic  PRON want to describe some kind of density  eg    rho  fractextvoxel with greyvalue 128textnumber of voxels  be there a common way how to describe this in a more mathematical sense  PRON would use for example the cardinality symbol and define some function for the voxel    rho  fracv128v  denote  v as the voxels   but in PRON opinion the  v128 look terrible and unintuitive   in one publication PRON read for example  o128 where  o be a function of greyvalue   but PRON think this be also not commonly use    this question may be off  topic here  PRON be not sure    PRON be simply a question of notation  let PRON say PRON have previously define  v128 as the set     vi  k  vk  i       in other word  as the set of index  k for which the  kth voxel value  vk equal  i   then  vi be clearly the size of the set  in other word  the number of voxel with the desire grayscale value  the point be that PRON must make clear beforehand that  vi be a set  and then write  vi make sense   other notation would use  vi to denote the number of element of a set  which be of course the same as the size of the set  
__label__r __label__clustering PRON be ask this question because the previous one be not very helpful and PRON ask about a different solution for the same problem   the problem  PRON have lateral position  xcoord  of vehicle over time which be record as the distance from the right edge of the road  this can be see for one vehicle in the follow plot   each point on the plot represent the position of the front center of the vehicle  when the vehicle change the lane  lane number not show  there be a drastic change in the position as see after the  start of lane change  on the plot   the datum behind this plot be like below   vehicle  id frame  id  xcoord lane  1  2  13 1646700  2  2  2  14 1644669  2  3  2  15 1642600  2  4  2  16 1640540  2  5  2  17 1638486  2  6  2  18 1636433  2  PRON want to identify the start and end data point of a lane change by cluster the datum as show in the plot  the data point in the plot circle in red be more similar to each other because the variation between PRON be small compare to the data point in the middle which see large variation in position  xcoord    PRON question be  be PRON possible to apply any clustering technique to segment these datum so that PRON could identify the start and end point of a lane change  if yes  which technique would be most suitable   PRON use r PRON have try hierarchical clustering before but do not know how to apply PRON in this context  please help   PRON doubt any of the cluster algorithm will work well   instead  PRON should look into   segmentation  yes  this be something different   specifically time series segmentation  change detection  as PRON say  there be a rather constant distribution first  then a change  then a rather constant distribution again  segment  wise regression may also work  try to find the good fit that be constant  linearly change  and constant again  PRON be essentially four parameter to optimize in this restricted model  average before and after  beginning and end of transition  
__label__linear-algebra __label__algorithms __label__eigenvalues __label__matrix PRON be try to find the large eigenvalue of very large  n time n matrix    n  1010 and large   the matrix be not sparse but the multiplication operation be fast   for now  PRON be use method without explicit matrix storage  from the simple power method to krylov  schur  use petsc  slepc in c   but all these method require to store at least a vector of size  n  be there a diagonalisation algorithm which do not need as much memory   
__label__keras __label__multitask-learning multi task learn model accept three input  PRON be use keras datum generator  be PRON possible to pass three datum generator to modelfitgenerator function    problem definition  PRON be work a classification problem  the dataset i be use be painters by number  a competition host by kaggle  the task be to identify painter  style and genre give painting   PRON have develop individual model to perform each task  now  i would like to incorporate multi task learning  see if PRON outperform individual model   model  no of class  softmax       model predict painter  8  give painting  model predict style  10  give painting  model predict genre  23  give painting  the above table detail the individual model and the no of output class for each model   now  i want to do multi task learning  so PRON come up with below simple architecture   credit  kajalgupta  multi task learn architecture   style   inputshape64643    genre   inputshape64643    painter  inputshape64643    sharedconv  convolution2d   filter  5   5 feature map  kernelsize   55    stride  1   sharedconvlayera  sharedconvstyle   sharedconvlayerb  sharedconvgenre   sharedconvlayerc  sharedconvpainter   mergedlayer  keraslayersconcatenatesharedconvlayerasharedconvlayerbsharedconvlayercaxis1   pooling  maxpooling2d   poolsize   22    stride  2    mergedlayer   dense  flattenpool   outstyle  dense   noclassesstyle   kernelinitializer  glorotnormalseed  seedval    biasinitializer   zero    kernelregularizer  l2l00001    activation   softmax      dense   outgenre  dense   noclassesgenre   kernelinitializer  glorotnormalseed  seedval    biasinitializer   zero    kernelregularizer  l2l00001    activation   softmax      dense   outpainter  dense   noclassespainter   kernelinitializer  glorotnormalseed  seedval    biasinitializer   zero    kernelregularizer  l2l00001    activation   softmax      dense   multitaskingmodel  modelinputsstyle  genre  painteroutputsoutstyle  outgenre  outpainter    multitaskingmodelsummary    multitaskingmodelcompile   loss   categoricalcrossentropy    optimizer  adamlr00001  beta109  beta20999  epsilon000000001    metricsaccuracy      now i want to pass three kera image datum generator  the below throw an error  history  multitaskingmodelfitgenerator   generator   trainstyledatagenerator   traingenredatagenerator   trainpainterdatagenerator    stepsperepoch lentrainstyledatagenerator    epoch  noepoch   validationdata   validationstyledatagenerator   validationgenredatagenerator   validationpainterdatagenerator      be there any alternative way to achieve the above  any suggestion or tip would be greatly helpful please    
__label__matlab PRON want to create a stair plot with solid horizontal line and dash vertical line  PRON can see an example of what PRON be try to do in this question  scroll down a little bit   in the example  however  PRON use python  but PRON want to do this in matlab   PRON  simple  code be  k414   a  2k  12   t4014   figure    stairsk  ar    hold on  plott  t   PRON plot become  PRON have no idea what to do  any help be appreciate   as mention in the comment to PRON original post  with matlab PRON will have to draw the individual line segment  try this   k414   a2k  12   t4014   n  sizek2    figure1    xb  yb   stairsk  a    give PRON the x and y coordinate  stairsk  ar     the plot  hold on  for i  122n2   sizexb    sizeyb   2n1   overplot black line segment  plotxbi  i1ybi  i1k    end  the end result look as follow  
__label__matrices __label__factorization look for some software to deal with 50kx50k sparse matrix apply non  negative matrix factorization  do PRON know any   
__label__finite-element __label__pde __label__discontinuous-galerkin __label__solid-mechanics PRON be interested to study crack propagation in a hyperelastic material in a variational setting  the crack surface exhibit a jump discontinuity  the function space for displacement field should consist of discontinuous function   the discontinuous galerkin finite element  dgfem  approximation space consist of piecewise discontinuous function which seem to a valid choice to solve crack problem  however  the require degree of freedom be extremely large for a high resolution discretization   the recently propose  enrich galerkin finite element method  egfem  enrich the continuous galerkin approximation space with a piecewise constant function  this allow discontinuity along the element face  for example  a one dimensional function  fx can be approximate in a unit interval as    fx   f1 phi1x   f2 phi2x   f3   where  phi1x and  phi2x be basis function  egfem seem to be a good choice to solve crack problem as require degree of freedom be quite less than for dgfem   egfem do introduce the discontinuity along the edge  face  but in the deformed configuration the disjoint edge  face would always be parallel  this be one of the limitation of egfem   before go to implement these method  PRON would like to understand advantage and disadvantage of dgfem and egfem approach to solve the hyperelastic energy minimization problem   what all point PRON should consider to evaluate these method   
__label__data-mining __label__dataset __label__algorithms __label__data __label__svm PRON be currently work on datum imbalance use smote for binary and other algorithm for the multi  class problem   PRON have the idea how to create the synthetic example to bring noticeable accuracy on a give dataset   PRON want to go into deep and understand how a classifier  especially svm handle the datum with the synthetic example to classify more accurately  PRON would also helpful to know for other technique like boost algorithm  random forest etc   any kind of guidance on above question will be very much helpful   PRON have already ask this question on stackexchange here but do not get any answer   how about use a gan  generative adversarial network  to generate undisdinguishable datum for PRON imbalanced dataset  an example for this can be see here   httpsgithubcomoshkerasgan    PRON think that in order to understand how the svm handle the new synthetic  datum  PRON should look at the loss function svm use  ie hinge loss and the behavior on an imbalanced dataset  intuitively this function will try to fit the hyperplane that good separate the datum  for example imagine PRON have a dataset that be not linearly separable and PRON have to class a  with a 1000 sample  and b  with 50 sample   so the svm will  move  the hyperplane to classify right all sample of a even if this mean fail to classify correctly some sample of class bthen when PRON create new sample of the class b the classifier will try to find a balance between the error of the both class  a more detailed explanation can be find in the 63 section fo this article  in general all classifier tend to classify correctly the most dominating class because of the loss function PRON use  one way of deal with PRON be generate synthetic datum  other way be use a cost sensitive classifier  this be the cost of misclassification of the minority class be high than the majority class  more detail can be find in the follow link   httpssvdscomlearningimbalancedclasses  httpmachinelearningmasterycomtacticstocombatimbalancedclassesinyourmachinelearningdataset 
__label__machine-learning __label__algorithm __label__swarm-intelligence to tune the parameter of particle swarm optimization  pso   there be two method offline and online  in offline manner  the meta  optimization be use to tune the parameter of pso by use another overlie optimizer  in the online manner  there be two technique  self  adaptation   consist of add some or all of the optimizer behavioural parameter to the search  space  thus make PRON subject to optimization along with the problem at hand   another technique be meta  adaptation   in which an overlay optimizer be try to tune the parameter of another optimizer in an online manner during the optimization of a problem     the concept of meta  optimization  a black  box optimizer be use in an offline manner as an overlay meta  optimizer for find good behavioural parameter of another optimization method  which in turn be use to optimize one or more actual problem    in standard pso the particle be initialize by use uniform random number and these particle be update use update equation  the good solution be select base on the good value of objective function   in PRON work  PRON have two datum set  training and theoretical dataset and PRON need to initialize the particle by use training datum instead of random number   in this case  how can PRON tune the parameter of pso use training and theoretical datum set   also  PRON have a problem which be  PRON get the good cost in the initial step of pso and in the initial step there be no parameter or update equation   be PRON possible to tune the parameter use machine learning method  how can PRON do this   
__label__matrices __label__performance __label__matlab PRON have to make a coordinate transformation between two reference system  ax   for that  three matrix   3times3   have to be multiply due to some intermediate ax be use   PRON have think about two approach to resolve this   method  1  make the multiplication directly  that is     vf  r1 r2 r3 vi  method  2  split into step    v3i   r3 vi   v23   r2 v3i   vf  r1 v23  where    r1    r2  and  r3  be  3times3  matrix   vfvi   v3i   v23 be  3times1  vector  PRON would like to know what method be more efficient computationally  less time  to do the transformation  this will be make a lot of time    for starter  PRON would not use intermediate variable  but bracket  unless  of course  PRON be interested in the intermediate result  but PRON be guess not   PRON try the following in matlab    gtgt  n  500    gtgt  a  randn   b  randn   c  randn   v  randn1     gtgt  tic  for k1100  abcv  end  toc  elapse time be 3207299 second    gtgt  tic  for k1100  abcv    end  toc  elapse time be 0108095 second   PRON have to say  though  that this be quite frightening  PRON have always assume that matlab would be smart about the matrix multiplication order  as this be a know problem with a simple and efficient solution   since the matrix be so small  all of the cost be go to be in call overhead   if PRON will do the transformation many time  PRON will be fast to precompute d  abc once and then for each vector apply vf  dvi   PRON could also consider bring this out to a mex file   matlab interpret sequence of multiplication andor division from leave to right  hence  abcv be much more expensive than  abcv  as PRON have two matrix product and one matrix  vecor product in place of three matrix  vector product   on the other hand   abcv should be slightly fast than if PRON save the intermediate in separate vector  as PRON second method suggest   to find out in general how to measure the impact of small programming  difference on large  scale computation  write at the matlab prompt   help profile   
__label__lammps PRON want to implement a basic example from the book  the art of molecular dynamic simulation  in lammps  PRON describe the 2d  movement of molecule in a lennard  jones  potential  define as    urijleftbeginmatrix4varepsilonleftleftfracsigmarijright12leftfracsigmarijright6rightvarepsilonamprijltrc216sigma0amprijgeq rcendmatrixright     use temperature and density as parameter  the dimension of the 2d  area be  left20  20right  have a molecule sit at each of the node  result in 400 molecule  the value measure be the total kinetic energy  the total potential energy and the total pressure   PRON implementation in lammp thus be   first test  if PRON be able to write a lammp script on PRON own   init  clear  unit metal  dimension 2  boundary p p p  atomstyle body nparticle 2 6  atommodify map arrayarray or hash   create atoms  lattice sq 1  region box block 0 20 0 20 0 1 unit lattice  createbox 1 box  lattice sq 1 orient x 1 0 0 orient y 0 1 0 orient z 0 0 1  createatom 1 box  replicate 1 1 1   interatomic potential  pairstyle lj  cut 1122466  paircoeff   1 1  neighbor 04 bin  velocity all create 144 87287 loop geom   define settings  compute eng all pe  atom  compute keng all ke  atom  compute eatom all reduce sum ceng  compute keatom all reduce sum ckeng   compute peng all pressure   run minimization  resettimestep 0  fix 1 all box  relax iso 00 vmax 0001  thermo 10  thermostyle custom step pe lx ly lz press pxx pyy pzz ceatom  minstyle cg  minimize 1e25 1e25 5000 10000  variable natom equal  countall    variable teng equal  ceatom   variable length equal  lx   variable ecoh equal  vteng  vnatom   print  total energy  ev     teng     print  number of atom    natom     print  lattice constant  angstom     length     print  cohesive energy  ev     ecoh     print  all do    be that implementation approximately correct  or be there something completely wrong   when execute the file  the execution stop with the error  error  illegal variable command   variablecpp512   how can PRON debug that   PRON seem PRON question be more on  how to debug that error  than on the error PRON  if PRON wish PRON to look into the error specifically  PRON will do this  PRON promise    otherwise  PRON be great to learn from PRON own mistake and to learn how to debug PRON   the good way to debug a lammp script be to use the command   echo both  at the beggining of the script  this command will output each line process by lammp one by one to the terminal   PRON must understand that the script be read and interpret line by line  so if PRON put echo both  each line that be interpret will be display as PRON be interpret  this way  when the code crash  the line above PRON will be the last one that have be process and the error will most likely reside within that line  in PRON case  PRON believe PRON error be most likely relate to one of PRON compute command  
__label__hidden-layers why anybody would want to use the  hidden layer   how PRON enhance the learn ability of the network in comparison to the network which do not have PRON  linear model    hidden layer by PRON be not useful  if PRON have hide layer that be linear  the end result would still be a linear function of the input  and so PRON could collapse an arbitrary number of linear layer down to a single layer   this be why PRON use nonlinear activation function  like relu  this allow PRON to add a level of nonlinear complexity with each hidden layer  and with arbitrarily many hidden layer PRON can construct arbitrarily complicate nonlinear function   because PRON can  at least in theory  capture any degree of complexity  PRON think of neural network as  universal learner   in that a large enough network could mimic any function    hidden  layer really be not all that special  a hidden layer be really no more than any layer that be not input or output   so even a very simple 3 layer nn have 1 hide layer   so PRON think the question be not really  how do hidden layer help   as much as  why be deep network better     and the answer to that latter question be an area of active research   even top expert like geoffrey hinton and andrew ng will freely admit that PRON do not really understand why deep neural network work   that is  PRON do not understand PRON in complete detail anyway   that say  the theory  as PRON understand PRON go something like this   successive layer of the network learn successively more sophisticated feature  which build on the feature from precede layer   so  for example  an nn use for facial recognition may work like this  the first layer detect edge and nothing else   the next layer up recognize geometric shape  box  circle  etc    the next layer up recognize primitive feature of a face  like eye  nose  jaw  etc   the next layer up then recognize composite base on combination of  eye  feature   nose  feature  and so on   so  in theory  deep network  more hidden layer  be good in that PRON develop a more granular  detailed representation of  thing  be recognize  
__label__machine-learning PRON be new in ml and ai  actually learn right now  but PRON be think about start take part in some project in ml  be kaggle be a good place to find project in ml to work on   yes  kaggle be a great place to start work on ml project   PRON will start use ml algorithm  at first  use scikit  learn or any other ml library of PRON choice   learn the concept of cross  validation  very important concept   PRON will cope with imbalanced class  various size of dataset  moreover  PRON will be able to ask question and receive feedback from the community   PRON be a great place to start and keep learn  but do not limit PRON at use ml algorithm blindly  try to understand PRON  even re  write PRON if PRON want   but PRON be definitely a lot of fun and there be always several competition run so PRON can choose whatev dataset PRON prefer   yes  definitely PRON be   PRON put PRON in front of real  world problem  theoretical question and practical issue that be not necessarily what PRON deal with in college  university task and project   kaggle be a great website  the website provide challenge that allow PRON to see how various component of ml interplay with each other  for example  large data set  standard debugging of ml code  much like a  standard  computer programmer   and see the breadth of application of ml to everyday problem be just a few of these component   coupling work on kaggle project with theoretical study of ai  ml be an effective way to learn ml quickly  
__label__logistic-regression __label__feature-extraction __label__feature-engineering __label__kaggle PRON be try to practice  learn some new data science skill by take the now end allstate purchase prediction challenge and see how well PRON can score  in the dataset give PRON see a customer history as PRON go to an allstate store and look at a specific policy as give by a collection of categorical variable  PRON do this by have column label a  b  c  d  e  f  g which each have a finite collection of possible integer value   the final entry for each customer be the plan that PRON end up purchasing  PRON question be  how can PRON convert the plan history into a readily consumable feature for a logistic regression model  one observation PRON make be that in the training set the maximal length of entry for a specific customer be only 13 and the test set do not go beyond this length   one plan PRON think of be create a collection of binary feature which encode the day  eg  a21  a22  a23    a213  which encode whether or not the custom choose a2 on the first  second  to thirteenth day  unfortunately  this create a lot of binary categorical feature and PRON be not sure whether or not this be the most efficient tool for summarize the customer s history while go to the store  be there any other method that PRON should look at for encode this data   
__label__apache-spark __label__apache-hadoop __label__apache-mahout PRON instal hadoop  mahout and spark  PRON be able to see the hadoop and spark masterwebui  moreover  PRON can also run the follow command    hadoopmuildevcel01 mahout bin  mahout  however  PRON PRON try run the spark  shell PRON run in the problem state below    hadoopmuildevcel01 mahout bin  mahout spark  shell  exception in thread  main  javalangnoclassdeffounderror  org  apache  spark  repl  sparkiloop  at orgapachemahoutsparkbindingsshellmainmainmainscala   cause by  javalangclassnotfoundexception  orgapachesparkreplsparkiloop  at javaneturlclassloaderfindclassurlclassloaderjava381   at javalangclassloaderloadclassclassloaderjava424   at sunmisclauncherappclassloaderloadclasslauncherjava331   at javalangclassloaderloadclassclassloaderjava357    1 more  question  any suggestion how PRON could resolve PRON problem   this error be common when the sparkhome environment variable be not set   in the shell type export sparkhomepath  to  PRON  spark 
__label__image-recognition __label__convolutional-neural-networks PRON be currently in the process of learn about use cnn in image recognition  many of the different resource PRON read that be explain the motivation refer to the fact that these network be  to some degree  translationally invariant  PRON understanding be as follow   fully connected network be ill  suit to image recognition because of the high dimensionality of the datum and especially because PRON do not preserve the spatial relationship between pixel  PRON care about the state of the pixel surround pixel x as well as x PRON   cnn remedy this because PRON look at the image in 2 dimension so that surround pixel be be consider as well  the kernel do this   however  while PRON care about what be around x  PRON do not care where x be  so  PRON apply the kernel the same everywhere and PRON make a bunch of layer so that PRON can get a bunch of kernel   PRON also use pooling  this reduce the data dimension but also add some of that translation invariance   so  PRON question be  what feature of the cnn be cause the invariance  PRON see some explanation say PRON be a result of the map basically activate when a certain feature show up  regardless of where  other say that the pooling mean that if a horizontal line  for instance  show up in one place vs a pixel over  the pooling would activate the same for both  PRON seem like the first reason would be totally sufficient and the second not really adequate but PRON could also see PRON be both   PRON also read a paper  httpsarxivorgpdf160609549pdf  about fully  convolutional network  in section 21  the author explain that PRON intentionally choose to make PRON fully  convolutional because that allow PRON to  compute the similarity at all translate sub  window on a dense grid in a single evaluation    that sentence make PRON think that PRON be more the first explanation  anyways  PRON hope to gain a better intuitive understanding of how the different part of the cnn come together to work particularly well on image  thank   the translational invariance come from the fact that the kernel be apply everywhere  basically the kernel be  translate   ie shift  and that mean the feature PRON detect be detect everywhere   pooling may help with that in some case  because in the end PRON need a dog at the right side to activate the same neuron as a dog on the left  so there need to be funnel of the information  but PRON be not strictly necessary  PRON could also imagine a cnn with a dog kernel in a late layer  which just recognise dog everywhere  with no pooling necessary   fully connected network be wasteful in that PRON do not share weight  to get the necessary translational invariance PRON therefore need to learn pretty much the same weight at all position  but PRON do preserve the spatial relationship between pixel   pooling reduce the dimensionality  but PRON also forget the precise position of a feature  this may help with generalisation  but PRON be not sure whether that be a big factor  
__label__machine-learning __label__python PRON have a daily set of datum that PRON need to detect if there be any anomaly in PRON  these be variable relate to the stock market so PRON would just give an example  totally fictitious  here   date  type  notional underlying expiry marketvalue dv01 x  y  z  112018 bonds 100000  ustreasury mar18  100000  25  25  50 150  122018 bonds 150000  ustreasury mar18  149000  40  40  80 120  what PRON be try to say be that there be some unknown correlation a  b  c between dv01 and parameter x  y and z however PRON be not entirely sure what the relation be  for simplicity sake PRON be just let a1  b2  c3  for other type of instrument such as options  the correlation will be entirely different  what PRON hope to achieve here be to be able to pick out value which do not conform to these relation  if the follow data come in the next day  the 500 in z should be flag   date  type  notional underlying expiry marketvalue dv01 x  y  z  132018 bonds 200000  ustreasury mar18  200500  55  55  110 500  unfortunately a  b  c be not constant  PRON change with time  the model should be able to learn from the past  say 10 day  and establish what type of relation PRON be and check the next day if there be any anomaly  what would be a good way to start   
__label__machine-learning PRON be try to build a predicting model use machine learn algorithm   PRON have a use case where the input datum have a very high dimension  each sample point have 20000 feature   PRON have a decent training sample set with around 1 million training sample  if necessary  PRON can acquire more  say  2 or 3 million   PRON be not very speed sensitive  PRON be not like a recommendation system that need to respond within a second  the application allow PRON to spend up to a couple of minute for one prediction  nevertheless  PRON hope the algorithm can be run in parallel mode in the future   give above description  what kind of algorithm would PRON suggest   PRON big concern be over fit  with so many feature  PRON seem that PRON be doom to over  fit   PRON be try to do something alone the line of near neighbor  but with these many feature  calculate distance sound like a mission impossible  maybe PRON should do pca to do dimension reduction first   any comment be welcome   PRON do not say whether PRON be build a regression or classification model  but here go anyway   as ever  PRON depend  though several neural network approach  such as deep learning or rbf network  have show promise with high dimensional datum   PRON be possible to have knn approach which use representative point  either as cluster centre or class boundary  to reduce the computation burden   as a test PRON try compute the euclidean distance between a single vector and  20000  20000 feature vector  this take around 7 second on a single core of a desktop machine use mathematica  if PRON have the ram and several core knn should be feasible in PRON time scale   feature engineering may gain PRON speed benefit but PRON would want to tune that in concert with prediction  classification accuracy   there be several method for achieve regularisation if PRON suspect PRON be overfitt   PRON may want to explore ridge regression  tikhonov regularisation or early stop if PRON be follow the neural network path   good luck   there be no one  size  fit  all  also  datum dimensionality be not the same everywhere  text datum  which be inherently sparse  have a very different intrinsic dimensionality than eg random gaussians   for text datum  linear svm be know to work very well   rbf kernel do not work well with high  dimensional datum  because PRON be distance  base at the core  and choose the sigma parameter become next to impossible   if PRON can  fold  dimension  PRON also get very different behavior  PRON be image recognition  PRON usually have thousand of pixel  however  PRON never look at all of PRON at once  instead  PRON use convolutional kernel that move over the data space  and PRON may have only say 32x32 pixel  that be still 1024 dimension  but not million anymore  
__label__matlab __label__convex-hull PRON be aware of a matlab function which take a set of point as an input and select the point which compose the convex hull   exemples from mathworks website   xx  1051   yy  abssqrtxx      x  y   pol2cartxx  yy    k  convhullx  y    plotxkykrx  yb     now  PRON question be  do there exist a matlab function or what could be the method to follow in order to compute the intersection of two convex set define by the set of point of PRON convex hull   let the  xcoordinate for the point in the convex hull of the first convex region be store in the array x1  and let the  ycoordinate of those same point be store in the array y1  let the array x2 and y2 store analogous information for the second convex region   if PRON have matlab s mapping toolbox  PRON can use the polybool function to return the coordinate of a polygonal approximation of the intersection of PRON two region via  x  y   polyboolintersection   x1  y1  x2  y2   PRON be not familiar with the algorithm behind the function  so PRON can not comment on that   there be a polygonintersection script on the matlab file exchange which will compute the intersection of one or more polygon  in PRON case the convex hull of PRON two set be the polygon PRON wish to intersect  
__label__machine-learning __label__classification __label__feature-selection PRON be have a dataset which all the feature be from 0 to 1real number  and the output be 0 or 1integer number   example   var1  var2  var3  output  001  01  07  1  001  01  07  1  01  02  03  0  02  04  04  0  04  01  09  1  which classification algorithm be recommend when the variable be  normalize  from 0 to 1  do svm or logistic regression  react  good in these type of datum   PRON have notice that most of the people that be use svm  nn or logistic regression when make feature scale PRON use stardadization   value  meanstd    be there a reason not to rescale the value between from 0 to 1   re  scaling or any other form of standardization  normalization be very useful when deal with model that be train with gradient descent   svm  nn  logreg    this question explain the effect of normalization on the gradient rather well  httpsstatsstackexchangecomquestions111467isitnecessarytoscalethetargetvalueinadditiontoscalingfeaturesforre  decision tree for example be invariant to linear transformation  therefore feature scaling  in theory  do not effect the model in any way   in the case of PRON set  up  PRON have a binary classification problem   PRON would recommend try out first a linear classifier on PRON datum  logistic regression be a good first choice  if PRON decide that logreg be not work as well as PRON would like PRON to then PRON can move on to more complicated model  PRON would recommend use decision tree couple with a gradient boosting model   in problem that be similar to what PRON describe  specifically when there be no inherent structure in the datum  as there be in image classification  speech recognition  etc    gradient boosting model tend to outperform neural network   hope this help  
__label__machine-learning __label__svm __label__feature-extraction __label__feature-engineering __label__feature-construction PRON be try to classify  heading  image and image caption  of a webpage  PRON be prepare datum by scrap select urlsaround 1000  use xpath of dom element PRON need  each datum row in the csv file contain tag name  x  coordinate  y  coordinate  text  size etc with three target label heading  image  image caption but a text simply will not become image caption until unless PRON be under an image  be there any way to create this dependency among row   PRON assume that PRON be solve a supervised classification problem  that is  PRON train PRON model on a label sample  PRON can think of two approach to this problem   i classify tag  use neighbor tag for the feature  for each tag PRON can calculate feature like    x  y  distance to the close image   x  y  distance to the close image above this tag  number of image near this tag    such feature could be feed into any classifier like svm or decision tree   ii classify pair of tag  for each tag  i  PRON can consider all other tag  j and predict probability that  i be a caption of image  j this prediction may be base on concatenation of   feature of  i  feature of  j  joint feature of  i and  j  like vertical and horizontal distance from  i to  j  after these probability be predict  PRON can classify  i as a caption indeed  if any of these probability exceed some threshold  
__label__clustering PRON have the follow label cluster  which be what an ideal clustering algorithm would generate   now  PRON have apply a basic k  means clustering algorithm to the datum  and the outcome be as follow   PRON recognize that this be a tough problem to properly cluster because some of the class be so similar   but PRON be wonder if there be any alternative algorithm that may help PRON improve the separability of the cluster  and improve how well PRON unsupervised clustering algorithm would work on new datum   PRON data do not appear to be easily separable  in general  one could apply some kind of transformation that pull apart the distribution for each class  have label available make PRON possible  in principle  to learn such a transformation  as emre metnion in the comment   but  there be a couple issue with PRON particular data set  1  PRON do not appear to have many datum point  unless PRON have only plot a small subset   this would limit PRON to very simple transformation  otherwise PRON would probably get severe overfitting   2  the point be simply overlap  a transformation can only work base on PRON input and  if the coordinate be indistinguishable  there be nothing that can be do  in the good case  PRON may be able to pull the lower leave turquoise cluster and the yellow point further from the main mass  but the rest of the point be pretty much intermingle  any transformation that could manage to separate PRON in the training datum would be very complicated  and probably just reflect sample noise  ie PRON would probably be completely overfit  and not generalize to new datum    the ideal thing would be to find  measure additional  relevant  variable  in this case  the class may become separable in the high dimensional space  for example  imagine addit a third axis  where the red point become  lift  above the blue point   PRON forget the most important step   preprocess   look at the ax  scale PRON the same way  and PRON will realize that PRON y axis have zero effect  PRON datum really look like this squeezed slice   do not blindly run clustering  PRON work be likely 70  figure out how to adequately preprocess PRON datum  10  clustering  and 20  make sense of the outcome   do not underestimate how difficult preprocessing be  the way PRON use k  mean  PRON assume 1 gallon of water  1 second  that assumption be probably wrong   last but not least  since PRON have label  why use cluster at all   clustering can not inherently extract label class  if PRON have label then PRON should use those with a supervised algorithm  there be no reason that any clustering should agree with pre  provide class label  imagine a data set that be perfectly uniformly distribute  one could have class label for that datum that can be quite arbitrary  PRON could even restrict to arbitrary convex region   what cluster result should PRON expect for perfectly uniformly distribute datum  be there any reason that should ever be able to match any particular class labelling other than by chance  cluster correspond to the something about the distributional property of PRON data set  unless the class label happen to align with those distributional property  and there be no reason that PRON should  then cluster can not recover the label  
__label__statistics an algorithm PRON be write need to compute roll quantile of a time series  currently PRON do this in the naive way  for a window of size w and a vector x of size n  for t from w to n   qt     quantilesxt  w1t    however  PRON seem that there should be a fast way  give that PRON know the previous quantile qt1   the new datum xt  and the datum xt  w  that have just drop out of the window  PRON be think of something along the line of the well    know incremental mean algorithm   for t from w to n   mt   mt1    xt   xt  w    w  which avoid recalculate the mean at every stage  even an approximation would be good  PRON have see approximation for compute quantile on stream  but PRON be important to PRON that PRON have a roll window  not just an expand window   one solution would be to keep PRON roll window of datum sort use a self  balance binary tree or a hash table   on every update  PRON pay a constant insertion cost  a  on deletion cost  and a  on traversal cost to update the quintile  
__label__feature-selection __label__scikit-learn __label__pca __label__randomized-algorithms PRON be use scikit  learn to do a genome  wide association study with a feature vector of about 100 k snp  PRON goal be to tell the biologist which snp be  interesting    randomizedpca really improve PRON model  but PRON be have trouble interpret the result  can scikit  learn tell PRON which feature be use in each component   yes  through the component  property   import numpy  seaborn  panda  sklearndecomposition  datum  numpyrandomrandn1000  3   numpyrandomrandn33   seabornpairplotpandasdataframedata  columnsx    y    z       sklearndecompositionrandomizedpcafitdatacomponents   gt  array    043929754   081097276   038644644     054977152   058291122  059830243     071047094  005037554  070192119     sklearndecompositionrandomizedpca2fitdatacomponents   gt  array    043929754   081097276   038644644     054977152   058291122  059830243     PRON see that the truncate decomposition be simply the truncation of the full decomposition  each row contain the coefficient of the corresponding principal component  
__label__pde __label__finite-element __label__reference-request __label__elliptic-pde PRON be look for example of helmholtz and biharmonic equation in cartesian co  ordinate with exact solution  in order to compare PRON numerical solution with PRON   PRON be able to find quite a few example on the internet  where the problem with boundary condition be precisely define  those be  unfortunately only illustrative example and exact solution be not show   PRON be encourage about manufacture the solution  like on mathstackexchangecom  and PRON do that successfully   PRON be afraid in that case some interesting example the specialist in pde be aware of would not be treat  like some solution give by infinite series  which PRON would truncate when some level of accuracy be reach   for example  the one give on wikipeda article on elliptic bvps be interesting   any particular example  or a useful link to a web  page or a paper be appreciate   do an axis align rectangular box  length  width  height  a  b  c  with dirichlet boundary condition   phi0   on the wall admit a close form  exact solution  maybe a tensor product of sinusoid  eg  phix  y  z   sinkx xsinky ysinkz z pick  kx  ky  kz judiciously to realize the dirichlet condition  eg  kx  npi  a   ky  mpi  b   kz  ppi  c  for some integer  n  m  p   plug that solution into the  div   grad operator  then the result equation   divgrad   phi  k2 phi  should give PRON the separation condition for  kx  ky  kz  probably  kx2ky2kz2k2     this be kinda standard fare for the vector wave equation  maxwell s equation  electromagnetic   PRON have not mess around much with the scalar helmholtz equation but PRON would expect PRON to work very similarly  for electromagnetic resonator  the vwe  PRON would recommend balanis   advanced engineering electromagnetics   there be probably a comparable reference for the the scalar helmholtz equation  a graduate level acoustic text  perhaps   but PRON would not know what PRON be   PRON have no experience with the biharmonic equation   look for the book vibration of plate by arthur leissa  PRON have explicit solution for square and circular plate  include table with approximate eigenvalue for different boundary condition  
__label__graph-theory suppose PRON have a set of small subgraph  a  gi of an original direct acyclic graph  g  typically  giltltg  which together span the original graph     g   cup gi     PRON question be then if PRON take a arbitrary subset of these graph  a  subset a  and a single subgraph from this graph   a in a  be the any simple way  or know algorithm  of reassemble the subgraph  g  subset g give by that portion of the union of  a connect to  a   in word  this be rather like a jigsaw problem where  a be the total collection of piece that originally come in the box   a be the subset leave after half of PRON get lose and  a in a the random selected piece PRON put down the start the puzzle off  the question be then what be the large connected graph  connected subset of piece  x in a  that PRON can lay down all on the board   the actual application arise where each subgraph  gi of  a represent a rule and  eg  x wedge y rightarrow z  and the objective be to find the large rule imply transitively from an initial seed rule  a in a  subset a and the remain rule contain in thin out subset  a  subset a  PRON think similar thing be possible in declarative language such as prolog but PRON suspect that prolog can actually do any more  any good up  to  date reference on declarative programming language would also be very useful   
__label__statistics in present geochemical datum  PRON would like to try a statistical method that present the datum in an isocon diagram  this method require scale all the datum to be the same distance from the origin  ie  normalizing so that the sum of square  1   result  all data point lie along an arc of a circle center on the origin   in other word  data point  element  be scale down in such a way that the squared sum of value for altered  y  axis  and unaltered  x  axis  rock become unity  allow the element to plot along an arc of a circle center on the origin at a uniform distance of 1 unit   would anyone be helpful in explain what PRON would mean and necessary step to normalize PRON datum so that the sum of square  1  this will be use on an x  y scatterplot  which software would process this type of operation best   so far  PRON have calculate for each element for both the x  axis  unaltered  and y  axis  altered  material   mean   degree of freedom   sum of squared error   variance  and  standard deviation   PRON be not entirely clear to PRON what step PRON struggle with  but for the sake of explanation  assume PRON have datum  xi  i1  n and PRON want to compute the normalize datum  yi  i1  n from the  xi then     yi  fracxisqrtsumj1n xj2     the denominator be of course the same for every  i  so PRON only have to compute PRON once  
__label__ode __label__time-integration __label__symmetry if PRON have a physical system which contain a time reversal symmetry  for example a hamiltonian  hx  pp22 m  vx with  vx real  and PRON want to solve the differential equation which describe this system  which solver for ode should PRON use in order to keep the time reversal symmetry  for example in mathematica   which solver break this symmetry   edit  PRON want to extend this question  let PRON consider a system of couple first order differential equation   dota1  t   f1a1a2a3ldot  ant    dota2t   f2a1a2a3ldot  ant    dota3t   f3a1a2a3ldot  ant   vdots  what integration method be best use if the underlie system contain a time reversal symmetry   what one usually want in this situation be to preserve a discrete analog of time symmetry  namely  if the time discretization be apply to solve first forward and then backward in time  the initial condition be recover   this be true if the method be invariant under the following substitution     delta t to delta t    anj  to an  j   here  an be the numerical approximation of the solution  atn  so the second substitution be imply by the first    PRON will give two example to illustrate   the explicit euler method    an1   an  delta t fan  do not preserve time symmetry  apply backward in time PRON become the implicit euler method     an  an1   delta t fan  on the other hand the midpoint  or leapfrog  method    an1   an1   2delta t fan  do preserve time  reversal symmetry   other well  know method that preserve time  reversal symmetry include the trapezoidal method and  as mention in the comment  the verlet method  
__label__research what be the most advanced ai software human have make to date and what do PRON do   in PRON opinion  this would be phaeaco  which be develop by harry foundalis at douglas hofstadter s crcc research group   PRON take noisy photographic image of bongard problem as input and  use a variant of hofstadter s  fluid concepts  architecture  successfully deduce the require rule in many case   hofstadter have describe the related success of copycat as be  like a little kid do a somersault   ie PRON do not have the flashy appeal of system like alphago  what PRON do however have be a much more flexible  ie not precann  approach to perception of problem structure than other system  which hofstadter claim  and many include peter norvig agree  be the really hard problem   in PRON opinion this would be the google search engine   PRON search the web   alphago be the most sophisticated and closet human creation towards an artificial general intelligence  agi   PRON be a computer program that be develop by google deepmind to play the board game  go   the game be different than other game  as the number of potential legal board position be great than the number of atom in the universe  PRON have way more legal board position than the chess  so  alphago require different technique for PRON be development   program s victory against the good player in the world in march 2016 be consider a major break through in the field of ai  go be previously consider to be a hard problem and many expert believe that current technology be not enough  expert be say that PRON will take atleast 5 year  or may be 10 year  before PRON will have a well develop go software player   the game use sophisticated algorithm of deep learning and reinforcement learning in order to learn the game  what make this game different from other board game  like chess  reversi  etc   be that move be often base on intuition  if PRON ask a chess player why PRON make a certain move  PRON will always be hear an answer where PRON will explain PRON how PRON think this move can increase in change of win  every move use certain heuristic  strategy and or trick  this be not the case with go  some move be often take because of intuition  cod an ai software that can play a game  where intuition be a integral part of the game make PRON different from other ai that PRON have today   at present alphago be the close ai software to artificial general intelligence   PRON can go through these link for more information   1  first  2  second  in addition to the answer already post  PRON think ibm s watson deserve a mention   PRON do something pretty impressive with PRON jeopardy win  possibly as impressive as alphago   sadly  since then  there do not seem to have be a lot of really public demo of watson  as ibm be position the technology as a tool for company and other organization  and most of PRON be pretty secretive about the detail of what PRON be do   PRON think PRON do publicize a bit of information about use PRON for medical diagnosis  but that be the only other application PRON can think of off hand   PRON be sure there be more though  
__label__python __label__visualization __label__social-network-analysis PRON would import  tweepy  package for access twitter datum for analysis and visualization in python  afterwards PRON do setup oauth authentication and everything else but on the next step  which be streaming tweets for PRON be use mystreamlistener  this cause PRON a name error  nameerror  name  mystreamlistener  be not define  here be PRON code   api authentication   import tweepy library  import tweepy   store oauth authentication credential in relevant variable  accesstoken   provide   accesstokensecret   provide   consumerkey   provide   consumersecret   provide    pass oauth detail to tweepy s oauth handler  auth  tweepy  oauthhandlerconsumerkey  consumersecret   authsetaccesstokenaccesstoken  accesstokensecret    streaming tweets   initialize stream listener  l  mystreamlistener     create PRON stream object with authentication  stream  tweepy  streamauth  l    filter twitter streams to capture datum by the keyword   streamfiltertrack    clinton    trump    sander    cruz     what should PRON do now  any thought   cheers   PRON need to override streamlistener   class    api authentication   import tweepy library  import tweepy   store oauth authentication credential in relevant variable  accesstoken   provide   accesstokensecret   provide   consumerkey   provide   consumersecret   provide    pass oauth detail to tweepy s oauth handler  auth  tweepy  oauthhandlerconsumerkey  consumersecret   authsetaccesstokenaccesstoken  accesstokensecret    streaming tweets   override tweepy  streamlistener to add logic to onstatus  class mystreamlistenertweepy  streamlistener    def onstatusself  status    printstatustext    initialize stream listener  l  mystreamlistener     create PRON stream object with authentication  stream  tweepy  streamauth  l    filter twitter streams to capture datum by the keyword   streamfiltertrack    clinton    trump    sander    cruz    
__label__nlp __label__python PRON be work on a problem where PRON need to determine whether two sentence be similar or not   PRON implement a solution use bm25 algorithm and wordnet synset for determine syntactic  amp  semantic similarity   the solution be work adequately  and even if the word order in the sentence be jumble  PRON be measure that two sentence be similar eg   1  python be a good language   2  language a good python be   PRON solution be determine that these two sentence be similar   what could be the possible solution for structural similarity   how will PRON maintain structure of sentence   firstly  before PRON commence PRON recommend that PRON refer to similar question on the network such as httpsdatasciencestackexchangecomquestions25053bestpracticalalgorithmforsentencesimilarity and httpsstackoverflowcomquestions62328isthereanalgorithmthattellsthesemanticsimilarityoftwophras  to determine the similarity of sentence PRON need to consider what kind of datum PRON have  for example if PRON have a label dataset ie similar sentence and disimilar sentence then a straight forward approach could have be to use a supervised algorithm to classify the  sentence   an approach that could determine sentence structural similarity would be to average the word vector generate by word embed algorithm ie word2vec  these algorithm create a vector for each word and the cosine similarity among PRON represent semantic similarity among word   daniel l 2017   use word vector PRON can use the follow metric to determine the similarity of word   cosine distance between word embedding of the word  euclidean distance between word embedding of the word  cosine similarity be a measure of the similarity between two non  zero vector of an inner product space that measure the cosine of the angle between PRON  the cosine angle be the measure of overlap between the sentence in term of PRON content   the euclidean distance  between two word vector provide an effective method for measure the linguistic or semantic similarity of the correspond word   frank d 2015   alternatively PRON could calculate the eigenvector of the sentence to determine sentence similarity   eigenvector be a special set of vector associate with a linear system of equation  ie matrix equation   here a sentence similarity matrix be generate for each cluster and the eigenvector for the matrix be calculate  PRON can read more on eigenvector base approach to sentence rank on this paper httpspdfssemanticscholarorgca73bbc99be157074d8aad17ca8535e2cd956815pdf  for source code siraj rawal have a python notebook to create a set of word vector  the word vector can then be use to find the similarity between word  the source code be available here httpsgithubcomllsourcellwordvectorsgameofthroneslive  another option be a tutorial from oreily that utilize the gensin python library to determine the similarity between document  this tutorial use nltk to tokenize then create a tf  idf  term frequency  inverse document frequency  model from the corpus   the tf  idf be then use to determine the similarity of the document  the tutorial be available here httpswwworeillycomlearninghowdoicomparedocumentsimilarityusingpython 
__label__categorical-data __label__probability __label__metadata the formula for information give by a data of occur with probability p be   PRON  log2   p  this formula give the bit if information need to know the outcome of the event   this formula capture the intuition that the information need to know the outcome of an event with probability 1 be 0 as PRON already know that outcome of the event   so should not the formula give the information as 0 for the event with probability 0 as PRON know the outcome of the event   so should not the graph of the PRON vs p be a symmetric as the value of p close to 1 will give similar information as the event with p close to 0   
__label__data-mining __label__statistics __label__clustering __label__predictive-modeling PRON be work for a logistics firm and there be approx  750  customer who avail PRON service  PRON be in the process of build and generate some insight for the business base on the payment make by these customer for the last 1 year  some make payment on time whereas some be late  amp  some be extremely late  could PRON please advice which modeling technique or statistical approach would be good in this case   PRON can think of create cluster base on the payment history and highlight  amp  place all defaulter in one cluster where PRON company can focus   please suggest   here be some thing that come to mind   1  arma  arima model  these may show if there be any seasonal trend in PRON payment history   2  logistic regression  will allow PRON model the conditional probability of a miss payment base off of number of previous miss payment   from here PRON need to determine what the threshold for risk be since that be a little more subjective  
__label__classification __label__genetic-algorithms __label__evolutionary-algorithms __label__game-theory for instance strength  sizespeed  where size and speed refer to memory and processing   PRON now have very strong  narrow ai  but PRON tend to run on fast hardware without volume restriction   to understand why PRON be ask  this article on bbc may provide some insight   which life form dominate earth     if PRON be a bet man  PRON would put money on tardigrade outlast human  and the secret of PRON success be that PRON require minimal resource and processing power  unlike high  order automaton    
__label__machine-learning __label__statistics what be the significance of the square root in root  mean  square  error  in effect  PRON question be what be the difference between  rms error  and  rm error2   PRON depend on what PRON be use the rmse for  if PRON be merely try to compare two model  estimator  then there be no significance to the square root  however  if PRON be try to plot the error in term of the same unit as PRON make the measurement  estimate  then PRON need to take the square root to transform the squared unit to the original unit  much like variance vs standard deviation   the square in rmse be use because PRON always give a positive value for error  so avoid error cancel each other out  and afford great weight to value further from the target function  so emphasise point for which the estimator be poor   the square root be use to remove the effect of the squaring   PRON could look at use the mean absolute error  mae  which do not have the distance weighting effect of the rmse and just take the average of the absolute value of the error   if the set that PRON be use the rmse on be a linear space  a good reason to use the square root be that PRON turn the set into a metric space  the square root ensure the right scale property  essentially  the rmse be equivalent to the euclidean norm  as a benefit  PRON be possible to use result of the general theory of metric space  
__label__matrix PRON be know that numpy basic matrix slicing will generate a view  whereas advanced slice a copy  be this true in cvxopt  PRON try  from cvxopt import spmatrix  import numpy as np  a  spmatrix2122143    1202320    0011234    aview  a0303    basic slicing  acp  a012012     advanced slicing  npmaysharememoryaaview   return false  npmaysharememoryaacp    return false  that be a sparse matrix   numpy do not have sparse matrix  and PRON think neither scipysparse nor cvxopt use memoryview for sparse matrix   even for dense matrix PRON appear that cvxopt use a low level python api instead of something like cython   PRON do not see any reference to memoryview interface so perhaps even dense matrix in cvxopt use copying instead of view   this may be the relevant function for sparse matrix indexing in the cvxopt source  where PRON will find a lot of copying   httpsgithubcomcvxoptcvxoptblobmastersrccsparsecl2791 
__label__fortran __label__floating-point recently  PRON have encounter a bizarre problem with fortran95   PRON initialize variable x and y as follow   x10  y01  later PRON add PRON together and print the result   110000000149012  after examine the variable  PRON seem as though 01 be not represent in double precision with full accuracy   be there any way to avoid this   PRON declare the variable as double precision  but PRON initialize PRON with single precision value   PRON could have write   x10d0  y10d1  barron s answer below be another way of make a literal double precision  with the advantage that PRON allow PRON to change the precision of PRON variable at a later time   another way to do this be to first explicitly specify the precision PRON desire in the variable use the selectedrealkind intrinsic and then use this to define and initialize the variable  something like   integer  parameter   dp  selectedrealkind15   realdp    x  x  10dp  a nice advantage to do PRON this way be that PRON can store the definition of dp in a module  then use that module where need  now if PRON ever want to change the precision of PRON program  PRON only have to change the definition of dp in that one place instead of search and replace all the d0 at the end of PRON variable initialization   this be also why PRON would recommend not use the 10d1 syntax to define y as suggest  PRON work  but make PRON hard to find and change all instance in the future    this page on the fortran wiki give some good additional information on selectedrealkind  
__label__neural-network __label__deep-learning PRON would like to implement a neural network allow to make captcha recognition  actually  PRON be new in deep learning that be the first neural network PRON be build   PRON have see a another similar project on github  httpsdeepmlblogwordpresscom20160103howtobreakacaptchasystem  however  PRON be not able to understand PRON  PRON do not know what be a vgg base neural network model  etc   PRON be wonder if there be a course or course that PRON could follow in order to be able to implement such a neural network  PRON do not know where to start   PRON do not aim to become an expert for the moment  PRON just want to first concretely discover this science that seem exciting   thank PRON for PRON help  PRON recommondation be a course from stanford  convolutional neural networks for visual recognition   but  this will not provide PRON easy to understand code to break captcha  PRON will learn the concept of convolutional neural networks  cnn   which be popular in the classification of image   as a reminder  captchas be build to prevent computer from automatically fill PRON in  even if nowadays technique may be able to solve PRON  PRON project may be a bad idea  especially from the legal perspective   
__label__regression __label__survival-analysis PRON be try to create a regression model that predict the duration of a task  the training datum PRON have consist of roughly 40 thousand complete task with these variable   who perform the task  250 different people   what part  subproject  of the project the task be perform on  20 different part   the type of task  the start date of the task  10 year worth of datum   how long the person who have to do the task estimate PRON will take  the actual duration this task take to finish  the duration can vary between half an hour to a couple of hundred of hour  but be heavily right skewed  most task be complete within 10 hour   on log scale the distribution be still slightly right skewed   the prediction do not have to be perfect  but PRON be try to improve the people s estimation  one question to ask be  what measure can PRON use the define beter   PRON think the good measure would be the mean squared error  mse  since PRON weigh large error much bad than small error   before PRON turn to machine learn PRON try some simple approach such as adjust the estimation by the average or median error  adjust PRON by the average  median error group by person  group by subproject but each of these happen to perform bad   with machine learning  one of the first problem PRON encounter be the number of categorical variable since for most model these have to be encode someway  eg one  hot   anyway  PRON try to apply some linear model  for example with stochastic gradient descent PRON approach would be   one  hot encode the categorical feature  the convert the date to unix timestamp  normalize all the feature that be not already between 0 and 1  split the datum in 8020 learn and test set   with grid search cross validation and the learn set try to find the good hyper parameter and fit the model   predict with the test set  calculate the error  score  now one thing PRON notice be that the result vary quite a bit  on one run the mse be close to double of another run  150 and 280   another thing be that the mse of the people s estimate be about 80  so PRON model perform a bit bad   during PRON effort to improve the performance PRON stumble across this question where someone suggest to use survival model  now PRON be unfamilliar with these kind of model but PRON sound promising but during PRON initial test with this PRON turn out to be way too slow for PRON purpose  too large of a dataset    in the same datascience answer that suggest to use the survival model  and the wikipedia page  PRON also mention poisson regression  but PRON be not sure how PRON would apply this to PRON case   so a long story short  PRON have just two question  1  be PRON approach of use sgd  correct  and do PRON think PRON can improve the result with that  2  be other model better suit for this kind of prediction and if so  can PRON explain a bit how PRON would use PRON   most likely   sgd be not a limit factor for PRON  but  have PRON consider take a classification rather than regression approach   PRON look like PRON be predict real value as oppose to class   since PRON  state that the prediction do not have to be perfect  why not try group PRON outcome variable into bin  then predict the bin   PRON will have a far less granular solution  but PRON may find PRON work   PRON think the analysis which PRON have do be good   regard the survival analysis procedure  PRON think use PRON in PRON scenario be good enough  even PRON may take time but the result from that be good and very insightful   since PRON have apply survival analysis on the datum  PRON need to make sure that these assumption be meet   there be several different way to estimate a survival function or  a survival curve  there be a number of popular parametric method  that be use to model survival datum  and PRON differ in term of  the assumption that be make about the distribution of survival  time in the population  some popular distribution include the  exponential  weibull  gompertz and log  normal distribution   perhaps the most popular be the exponential distribution  which  assume that a participant s likelihood of suffer the event of  interest be independent of how long that person have be event  free   other distribution make different assumption about the probability  of an individual develop an event  ie  PRON may increase   decrease or change over time   more detail on parametric method  for survival analysis can be find in hosmer and lemeshow and lee  and wang1   here on two nonparametric method  which make no assumption about  how the probability that a person develop the event change over  time  use nonparametric method  PRON estimate and plot the survival  distribution or the survival curve  survival curve be often  plot as step function  as show in the figure below  time be  show on the x  axis and survival  proportion of people at risk  be  show on the y  axis  note that the percentage of participant  survive do not always represent the percentage who be alive   which assume that the outcome of interest be death    survival   can also refer to the proportion who be free of another outcome  event  eg  percentage free of mi or cardiovascular disease   or PRON  can also represent the percentage who do not experience a healthy  outcome  eg  cancer remission    PRON can go through this link for good understanding   regard poisson distribution  do PRON plot and check whether the data be follow poisson distribution like   the poisson distribution be an appropriate model if the follow assumption be true   k be the number of time an event occur in an interval and k can  take value 0  1  2  …   the occurrence of one event do not affect the probability that a  second event will occur  that is  event occur independently   the rate at which event occur be constant  the rate can not be high  in some interval and low in other interval   two event can not occur at exactly the same instant  instead  at each  very small sub  interval exactly one event either occur or do not  occur   the probability of an event in a small sub  interval be proportional  to the length of the sub  interval   or  the actual probability distribution be give by a binomial  distribution and the number of trial be sufficiently big than the  number of success one be ask about  if these condition be meet then PRON can use poisson model  go through this link  implementation of this in r  python   finally  to address PRON 2 question   PRON approach be correct there be no problem with use that method  to improve PRON result PRON need to work on feature enggderiv new variablessince PRON PRON be consider the duration as a continuous variabledid PRON perform that log transformation  which PRON have state in the beginning    in PRON scenario PRON think survival and poisson give PRON good result  if PRON think these take more time then try get sample of datum and get PRON work do  if PRON be consider the out  come as a continuous variable then PRON can use random forest  xgboost  all method which be use for predict a continuous variablebut if be PRON PRON would spend more time in fit survival and poisson and then shift to other prediction technique   let PRON know if PRON have any issue  
__label__r PRON want to build a home server  workstation to run PRON r project  base on what PRON have gather  PRON should probably be linux base  PRON want to buy the hardware now  but PRON be confuse with the many available option for processor  ram  motherboard  PRON want to be able to use parallel processing  at least 64 gb  of memory and enough storage space  10 tb    software wise  ubuntu   r  rstudio  postgresql  some nosql database  probably hadoop  PRON do a lot of text  geospatial  network analytic that be resource intensive  budget 3000us   PRON questions   what could an ideal configuration look like   hardware  software   what type of processor   note   no  PRON do not want to use a cloud solution   PRON know PRON be a vague question  but any thought will help  please   if PRON be off  topic or too vague  PRON will gladly delete   cheers b  there be no ideal configuration  for r or in general  product selection be always a difficult task and many factor be at play  PRON think that the solution be rather simple  get the good computer that PRON budget allow   have say that  since PRON want to focus on r development and one of r s press issue be PRON critical dependence on the amount of available physical memory  ram   PRON would suggest favor more ram to other parameter  the second most important parameter  in PRON opinion  would be number of core  or processor  see detail below   due to PRON potential multiprocessing focus  finally  the two next most important criterion PRON would pay attention to would be compatibility with linux and system  manufacturer s quality   as far as the storage go  PRON suggest consider solid state drive  ssd   if PRON would rather prefer to have a bit more more speed than more space  however  if PRON work will involve intensive disk operation  PRON may want to investigate the issue of ssd reliability or consult with people  knowledgeable in this matter   however  PRON think that for r  focus work  disk operation be much less critical than memory one  as PRON have mention above   when choose a specific linux distribution  PRON suggest use a well  support one  such as debian or  even better  ubuntu  if PRON care more about support  choose PRON lts version   PRON would rather not buy part and assemble custom box  but some people would definitely prefer that route  for that PRON really need to know hardware well  but potential compatibility could still be an issue  the next paragraph provide some example for both commercial  off  the  shelf  cots  and custom solution   should PRON be interested in the custom system route  this discussion may be worth read  as PRON contain some interesting pricing number  just to get an idea of potential saving  and also shed some light on multiprocessor vs multi  core alternative  obviously  the context be different  but nevertheless could be useful   as PRON say  PRON would go the cots route  mainly due to reliability and compatibility issue  in term of single  processor multi  core system  PRON budget be more than enough  however  when PRON go to multiprocessor workstation  PRON be not even talk about server   even two  processor configuration can go over PRON budget easily  some  not far away  such as hp z820 workstation  PRON start from 2439 usd  but in minimal configuration  when PRON upgrade PRON to match PRON desire spec  if PRON be even possible   PRON be sure that PRON will be talk about 5 k usd price range  extrapolate from the series  high  level model   what PRON like about hp z820  though  be the fact that this system be ubuntu certify  consider system compatibility and assume PRON desire to run ubuntu  the good way to approach PRON problem be to go through ubuntu  certify hardware list and shortlist system that PRON like  just for the sake of completeness  take a look at this interesting multiprocessor system  which in compatible configuration may cost less than from hp or other major vendor  however  PRON be multimedia  orient as well as PRON be reliability and compatibility be unknown  not to mention that PRON be way over PRON specified budget   in term of r and r  focus software  PRON highly recommend PRON to use rstudio server instead of rstudio  as that will provide PRON with an opportunity to be able to work from any internet  enable location  provide PRON computer will be run  obviously   another advice that PRON have be to keep an eye on alternative r distribution  PRON be not talk about commercial expensive one  but about emerge open source project  such as pqr  httpwwwpqrprojectorg  will update as need  PRON hope this be helpful  
__label__machine-learning __label__classification __label__statistics __label__svm __label__naive-bayes-classifier PRON would like to develop a soccer field segmentation method  for this purpose  PRON prepare a training image datum set and annotated field and non  field pixel  follow be a gr  chromacity plot of all training sample  color with respect to PRON label   PRON want to train a classifier for infer the label of a new sample  the first approach come to PRON mind be use gaussian mixture model to model both distribution  would PRON recommend another method for this purpose   PRON would not suggest gmm at this point as the distribution of point in the space be not well  shape enough  even if PRON want to use PRON PRON be good to look at PRON datum in pc space  ie use pca   PRON suggestion would be   1  think of PRON feature  what be PRON  be PRON go to use these gr  chromacity as feature  if yes PRON should know that kernel method work better on this as the feature be highly nonlinear  the image show that PRON need a feature mapping anyway   2  PRON seem PRON have already think of kernel method as PRON put svm as a tag  PRON can use PRON for classification  may work better than gmm here  also think of probabilistic graphical model as PRON have be use intensively for image segmentation and PRON image be structure enough  a football field have PRON fix position in the image anyway    3  if PRON have raw label dataset  PRON would recommend to think of smart feature for segmentation  in gr  chromacity PRON already loose some information about color which be the most important thing for PRON here  PRON would recommend take the position of pixel into account as well  then a pca on the new datum may reveal some more linearly separate class  
__label__graph-theory PRON have a graph  with  sim mathcalo1000 verticie that have the topology of  s4  but with degenerate edge  that is  there can be multiple edge between two vertex   there be no loop  and no disconnected piece  hence the topology   the graph consist of vertex which always have five edge attach   PRON be try to find subgraph  under the name  head   of this graph that have a give number of vertex  but restrict to those that only have five outgoing edge  a  neck   from the subgraph to the rest of the graph   PRON know PRON can only find head that have an odd number of vertex in the subgraph  since PRON must only have five  an odd number  of edge leave the subgraph  the neck    the algorithm PRON put together find all subgraph of the desire size  through recursion  that contain a particular vertex  and while find PRON check to see the size of PRON neck   there be some optimization PRON have include while build the subgraph  for instance at each step PRON know the maximum and minimum number of edge that can be leave the subgraph since at the final step PRON must only have five   this algorithm become extremely time consume when the subgraph size get near 21 vertex  since the number of subgraph of that size that contain a single particular vertex be enormous  and PRON guess there be at least a few more head of large size base on the number PRON have find at each size   do anyone know of an efficient algorithm to find subgraph with a particular valency in PRON edge  or do PRON brute force algorithm seem to be the only way   thank  let PRON know if PRON need more detail   
__label__r __label__social-network-analysis PRON have the follow dataset     strenght  movie1  movie2    23  2  3  80  1  2  10  4  3  and PRON want to create a graph with the relationship between movie have the first column as the strenght of the relationship  how can PRON do this use r   many thank   try this r code   libraryigraph   dfr  lt dataframeidmovie1c214   idmovie2c3  2  3   strength  c238010    igr  lt igraphgraphdataframedfr   plotx  igr   edgecurvedfalse  edgewidthlogedgeattrigrstrength   edgelabeledgeattrigrstrength   maingraph of movie strengths    work only for small daaset  visualization get ugly quickly  
__label__classification __label__deep-learning __label__model-selection PRON want to create a deep learning model for a binary classification problem  PRON have 16 feature and 50k sample  how would PRON select number of layer and number of neuron of each layer  for PRON problem   please share the link which explain the same   there be no rule to infer neural network hyper  parameter from a problem description  with only number of feature  number of example and the fact PRON have a binary classification problem  PRON be far too little to even make an educated guess   first  be PRON sure PRON be ready to build a deep learning model for PRON datum  have PRON look at the datum  or scatter plot PRON reduce to two dimension use pca or t  sne  to get a feel for how easy PRON data be to separate  easy to separate raw data imply use simple  shallow model   PRON could try a basic model such as logistic regression or svm in order to establish a benchmark so PRON can tell whether the deep model be do anything useful   assume PRON be ready to go ahead  the time  honour approach be to try out variation  and measure the result use cross  validation  PRON can do this methodically  by for instance start with one hidden layer with 64 neuron  and either add  remove layer or add  remove neuron in each layer  generally these search do not cover all possible variation  but stop once PRON have try those that seem interesting and have reach a reasonable result for PRON problem   do note that other hyper  parameter will affect result and can interact with PRON choice for number of neuron and layer  PRON can not isolate PRON choice of network depth from other choice  such as optimisation method  activation function  regularisation  when start to build a model  PRON be just as reasonable to spend a long time explore these other factor as PRON be to look at network size  shape   PRON be very easy to get a deep nn to over  fit PRON datum  cross  validation be therefore a necessity as PRON explore hyper  parameter for PRON problem  one reasonable method of search against number of neuron be to increase PRON until PRON start to notice over  fitting  then adjust regularisation to stop the over  fit  at that point  with all other factor remain the same  there be probably no need to explore network with many more neuron  although deep  shallow network may still be worth explore  and if PRON do so  PRON will once again want to explore number of neuron per layer   
__label__lstm look at the literature  there be 2 distinct approach to lstm  some people use recurrent weight with input  forget  output  notice  PRON equation do not even mention blockinput  PRON start from describe the  f or  i gate  1   wikipedia   2   lke this   other people use recurrent weight with blockinput  z   input  forget  output  1   2    3    4   like this   have try the first approach  PRON still use standard weight on blockinput but no recurrent weight in that place  seem fine but a little odd  network converge well  but very rarely jump over local mimima  as if too high learning rate   personally PRON like the second approach  recurrent weight on all 4 entry point   which one do PRON use   edit   to make thing bad  the  lstm peephole paper   page 121 describe 3 connection  no peephole for blockinput    this destroy the equal  size of PRON matrix  because now 3 gate use the usual  recurrent and peephole weight  but the very first gate only use the usual weight  there be more than two or three variant with regard to lstm   a paper that explore these variant can be read here   lstm  a search space odyssey 
__label__sampling how can PRON understand  markov chain monte carlo allow sample from a large class of distribution and which scale well with the dimensionality of the sample space   
__label__apache-spark __label__apache-hadoop __label__association-rules PRON be do PRON master thesis on big data analytics  PRON be try to develop a algorithm to identify association between some product in a supermakert  imagine that PRON have this dataset   purchaseid productid  purchasevalue  1  2  45  1  3  12  2  3  14  2  1  35  2  2  73  3  2  05  3  3  10  what PRON want to conclude be that    every people that by productid 2 also buy 3   anyone know if exist any code algorithm available to use in spark mllib  PRON already search on internet but PRON do not find anything   anyone can help PRON   many thank   this sound like a classic frequent pattern mining problem  where PRON be try to find set of item that be frequently find together within user  set of purchase   start here  httpssparkapacheorgdocslatestmllibfrequentpatternmininghtml 
__label__clustering __label__labels __label__tsne let PRON say PRON begin with an exceptionally large dataframe  eg import  mung from tsv file   several of these column be categorical label    as a more concrete example  let PRON imagine a group of student in a school district  pre  school to high  school    now  PRON begin use sklearn and instantiate a t  sne model  similar to the example here   httpscikitlearnorgstablemodulesgeneratedsklearnmanifoldtsnehtml  import numpy as np  from sklearnmanifold import tsne  x   PRON datum  model  tsnencomponents2  randomstate0   npsetprintoptionssuppresstrue   modelfittransformx   and then PRON plot this  the plot may look something like this  httpimgurcoma3amkj  here be PRON problem  with real dataset  after use t  sne to learn  cluster  PRON will have a number of  cluster   then  use the categorical label  PRON try to go through each of these  and try to figure out what structure the t  sne plot be give PRON   for PRON school example  PRON would get the t  sne output  then PRON would label the datapoint   let PRON assume that the cluster be actually representative of age  classroom  eg the first  grader group together  the second  grader be a group  etc    if PRON try to color this plot with  grade   PRON will see that the grade do not really explain the structure of this plot   why  because every class  level have student with as  bs  cs  etc   then PRON may try height  that do pretty good  because there be a correlation between short students  pre  school  tall student   high school senior    how do one use a t  sne plot to infer the  most correct  label of the datum  how do one use t  sne plot to explain  and further explore  the plot structure   with t  sne none of the input parameter be weight more than any other parameter so the difference PRON want to see like student form island by grade level will not happen because there be so much other datum present to pull those student  data point in different direction   PRON highly encourage PRON to have a specific question in mind and tailor PRON input category so that PRON question can be answer by the t  sne map   PRON may try ask a specific question and change which input category PRON ask t  sne to look at  for example  do tall student get good grade  feed in height and grade  category while leave out grade level and age  this be a silly example but PRON hope PRON give an idea of how PRON can use t  sne to help PRON learn about PRON datum   PRON may also find that there be category that mask meaningful finding  height may not be very useful for pull out meaningful information and since the height range be go to be much large than the a  f grade range  PRON will likely influence the t  sne map more   look at the datum as PRON describe on a color scale for each category be a good place to start with each new t  sne run   PRON be okay to run multiple t  sne with different parameter  to be sure PRON be get a meaningful answer  PRON recommend run t  sne multiple time with the same parameter as well   if PRON have enough student  PRON would be great to include half in PRON training set where PRON explore and figure out what question to ask and then when PRON think PRON have find something meaningful  apply those condition to the other student in the testing set to see if PRON hold true  
__label__machine-learning __label__algorithms __label__feature-selection __label__feature-extraction __label__tensorflow PRON have m  around omillion   of row of type  timestamp  val  ind1  ind2  ind3   k entry  PRON task be to predict the value of  val  for any future give time  how should PRON model this as a machine learning problem  note that PRON want the value of  val  give the past datum without know the current value of  ind1  ind2  ind3   etc   of the same row  this be different in that sense   formally  assume that PRON have first i rowseach entry be know  of the table  and PRON want to predict the value of  valj  j  i  ie near future   how will PRON do that  PRON have not do these kind of prediction use ml or tensorflow earlier  if PRON be give  ind1j  ind2j   etc but not  valj this kind of prediction would have be easy   even if PRON model PRON val as a function of ind1  ind2  etc of previous index  how will PRON get PRON for different future time   about datum PRON do not know much about datum  all PRON know be that PRON be relate to stock and exchange  maybe  val  be the entity which be dependent on ind1  ind2  etc  PRON assume whatev be suitable apart from that   PRON have m  around omillion   of row of type  timestamp  val  ind1  ind2  ind3   k entry  PRON task be to predict the value of  val  for any future give time  how should PRON model this as a machine learning problem  note that PRON want the value of  val  give the past datum without know the current value of  ind1  ind2  ind3   etc   of the same row  this be different in that sense   formally  assume that PRON have first i rowseach entry be know  of the table  and PRON want to predict the value of  valj  j  i  ie near future   how will PRON do that  PRON have not do these kind of prediction use ml or tensorflow earlier  if PRON be give  ind1j  ind2j   etc but not  valj this kind of prediction would have be easy   even if PRON model PRON val as a function of ind1  ind2  etc of previous index  how will PRON get PRON for different future time   about datum PRON do not know much about datum  all PRON know be that PRON be relate to stock and exchange  maybe  val  be the entity which be dependent on ind1  ind2  etc  PRON assume whatev be suitable apart from that   and please give any idea  how will system learn iteratively as new datum row be add  PRON do not want to do the whole training again and again   PRON model should be base on what PRON data represent   if PRON want meaningful result  PRON should get some idea of what PRON data be represent before try to answer this question   for instance  PRON could use PRON ind  column to predict the value of PRON indj row  then use the indjs to predict the valj  but this would only make sense if there be a predictive relationship between the indjs   and PRON would definite want to have some idea of what that relationship may be in order to choose an appropriate algorithm for each particular column   furthermore  the same would apply for  jth row valj prediction   at the very least PRON would want to  make an educated guess about the relationship between PRON indjs and PRON valj column so that an appropriate algorithm could be choose   as another example  PRON may be able to use the previous row as a predictor for the next value in the time series  ie indj1s    valj  but again  this only make sense if PRON be able to make reasonable assumption about the relationship of the datum  because PRON ultimately need to choose the algorithm PRON use to make the prediction  
__label__classification __label__algorithms __label__encoding PRON understand that compression method may be split into two main set   global  local  the first set work regardless of the datum be process  ie  PRON do not rely on any characteristic of the datum  and thus need not to perform any preprocessing on any part of the dataset  before the compression PRON   on the other hand  local method analyze the datum  extract information that usually improve the compression rate   while read about some of these method  PRON notice that the unary method be not universal  which surprise PRON since PRON think  globality  and  universality  refer to the same thing  the unary method do not rely on characteristic of the datum to yield PRON encoding  ie  PRON be a global method   and therefore PRON should be global  universal  should not PRON   PRON primary question   what be the difference between universal and global method   be not these classification synonym   
__label__regression __label__marketing PRON be work on a market mix model ad be try to apply diminish return to grp value for PRON medium activity  PRON be confused on how to go about PRON  ie how to decide the parameter for the for the diminishing fn ex negative exponential function y  α⋅1 − e−β⋅x   β  0 have 2 parameter alpha  amp  beta  there be any number of combination  what be the approach to find a reasonable estimate for these value then   
__label__classification __label__svm __label__text PRON have build binary text classifier use svm on tf  idf for news articlessport  non  sports    but PRON not sure how to classify new document use this model  since tf  idf be calculate base on the occurrence of a word in all other document   do PRON have merge test and train datum every time PRON receive a new document for classification  PRON will change the model as well every time   be PRON miss something  PRON think  although svm on tf  idf give good result PRON can not be use in production   be there any other way to tackle this issue   lets take an example  training set   doc1  chelsea win the match   sport   doc2  india win the third test match against austrailia  sports   doc3  PRON want to sleep  non  sport   doc4  13 palace to see in auckland  non  sport   new testing set   doc5  climate change impact in austrailia  now how can PRON find idf score of  austrailia  in doc5 without merge this document with training set   since doc5 contain the word  austrailia   PRON will change the idf score of  australia  in doc1 will also change  thus model need retrain  what be PRON model build in   most popular library have a score function separate from the training part  PRON should be able to just pass the new document to the score function of the train model and get back the predict class   so PRON tf  idf be train only use the training set  PRON will determine the frequency of the occurrence of word  if PRON show the tf  idf a new word PRON have not yet see then PRON will simply ignore PRON  PRON will only use word that be in PRON training set  so  no PRON do not retrain PRON model after PRON have build PRON  once  PRON go through the training stage for PRON tf  idf that be the library of word that PRON algorithm can detect   through tf  idf PRON have get the important word which PRON have use to train PRON svm model  so when PRON pass in the test datum  only the wordsfeature  that be select through tf  idf be important and will be use by the svm model to predict the label  just make sure that the matrix use to train svm be similar for both training and testing 
__label__numerical-analysis __label__least-squares __label__projection __label__matrix-factorization projection of a vector  v onto the column space of a matrix  a be give by  aadagger v from the definition of moore  penrose inverse PRON know that  aadagger v   atdagger at v    below be the code for implement the projection of a random vector onto the space of a random matrix  this be relate to PRON other question backward stable algorithm to get orthogonal projection onto the column space of a matrix  PRON do not get an answer to that question   PRON would like to know why be there a huge difference between the two method of calculate the projection    testingprojfrostackexchange  clear   m  1400   n  1300   r  1   a  randm  n    u  randm  r    projln  pinvaauthis be projection through least norm  projls  apinvauthis be projection through least square   q r   qra    q  q1n    z1  qquthis be the actual projection  displayprojection through qqt   projls     normz1projlnnormprojln   displayprojection through qqt   projln     normz1projlsnormprojl   output   gtgt  stackexchange   projection through qqt   projls  ans   21569e13   projection through qqt   projln  an   83546e15  the actual projection which be give by  q1nq1nt  where  q be from the housholder decomposition of  a  PRON get the projection from  aadagger and   atdagger at and find that  aadagger be much good when  a be a tall matrix   
__label__neural-network __label__deep-learning __label__reinforcement-learning PRON question be regard the paper learning to communicate with deep multi  agent reinforcement learning  httpsarxivorgabs160506676   can anyone explain what be the significance of colour  digit mnist game in the paper  PRON understand from here  httpswwwredditcomrmachinelearningcomments4kzmislearningtocommunicatewithdeepmultiagent  that the agent have to choose an action that represent if  case 1color of agent 1   parity of agent 2 or  case 2color of agent 2   parity of agent 1  the reward for case 1 be twice that of case 2  hence  PRON optimal case  both agent should learn  case 1  to communicate PRON parity to other agent via the message and learn to check the received message against PRON own color  but if this seem to mean that case 2 be just act like a noise or additional challenge in attain optimal solution  even if the case 2 term do not exist  still the agent have a learn non  trivial communication protocol  have say that  do case 2 term have any fundamental implication  need   
__label__nlp __label__gensim PRON understand how doc2vec work  but PRON be unclear the good practice on feed in datum   suppose PRON have a document with multiple sentence    PRON really love football  peyton manning be a great player    if PRON feed this into the algorithm as is  the window for  peyton  could include   lovefootballmanningwas    this do not make intuitive sense to PRON  however  since the word come from different sentence   any suggestion   the aim of doc2vec be to produce document level embedding  thus even if word be sentence  separate if PRON include PRON in the same document PRON have to be consider part of the same semantic source for word similarity  if PRON do not want such a behavior  PRON may want to separate PRON document in a sentence  level and maybe aggregate the embedding later in different group to form PRON initial paragraph  depend on PRON plan    with doc2vec  each string will be treat as a separate document regardless of any format such as sentence or paragraph  so for an analysis of a book from project gutenberg  PRON could have each chapter of the book treat as a document or PRON could treat each paragraph as a document   so  for a two sentence paragraph like PRON example    PRON really love football  peyton manning be a great player   and use a sentence tokenizer such as the nltk package in python s nltktokenizesenttokenizer   function which will divide PRON string into separate sentence   PRON would then run the tokeniz sentence through doc2vec and have each sentence as a different document   see this example for an example with a sentence tokenizer in python  the choice of which level of text to treat as a document should be base on what PRON research design be and what unit of analysis PRON want to interpret for the end product   PRON tag the question with gensim  so PRON assume PRON be try to do a topic model  PRON have to weigh whether PRON be interested on change in topic from sentence to sentence  in paragraph chunk  or some other hierarchy within the text  
__label__finite-element __label__parallel-computing __label__discontinuous-galerkin PRON have always hear that easy parallelization be one of the advantage of dg method  but PRON do not really see why any of those reason do not also apply to continuous galerkin   just as PRON be the case with most general statement about numerical scheme  the answer depend on the exact circumstance PRON be look at  first of all  the advantage of dg concern parallelization mainly pay off in case of explicit time  integration scheme because of  the cell  local mass matrix of dg scheme  so PRON do not have to apply the inverse of the mass matrix globally   a favorable ratio of cpu  local work  volume integral  to communication related work  edge integral   especially for high order  while these statement apply to generic dg discretization  real hpc application  say  use several thousand processor  require some more effort concern the parallelization strategy in order maintain a good scaling  this paper show  for example  how one can achieve almost perfect scaling up to one cell per processor  this be certainly something PRON can not expect from a continuous fem  but as mention before  implicit scheme be totally different thing   when assemble a stiffness matrix  the datum store in an element in continuous  nodal  fem have to be communicate to all PRON nodal neighbour  in contrast  dgfem require the element datum to be communicate to all PRON face neighbour  in 1d nodal and face neighbour be identical  but in 3d the difference can be quite large  for a regular hexahedral mesh  there be 26 nodal neighbour but only 6 face neighbour  for irregular mesh with many high  valence vertex  the situation get bad for cg  while PRON stay the same for dg   from PRON many year write fem software  PRON believe that the statement that dg scheme be better suit to parallelization than cg scheme be apocryphal  PRON be frequently use in introduction of dg paper as a justification for dg method  but PRON have never see PRON substantiate with a reference that actually investigate the question  PRON be similar to every nsf proposal on a number theory project reference  cryptography  as an area of broad impact  a statement that in this generality be also never substantiate   in fact  PRON believe that with the one exception of explicit time step scheme and  possibly  problem where PRON have to invert a mass matrix  dg scheme be no good or bad than cg scheme if one investigate the cost of communication involve in either  PRON do mean this to be in a practical sense  sure  one may have to communicate less datum  but PRON would imagine the difference in wallclock time to be negligible to all the other operation program do on this datum   of course  PRON would be delight if anyone take up the challenge to prove PRON wrong   one reason dg method may receive more attention as a parallel method be that PRON be readily see that the method be inherently local to an element  the coupling in dg method be weak  as PRON only occur through adjacent edge  or face in 3d   so  for triangle or quad dg will communicate to three or four processer at most  respectively  whereas cg method will include element corner  thus the valence of an element s corner become important  depend on the mesh generator use  this valence can be eight processer  possibly high   so the cost of assemble the time derivative can be consider  high  for cg method  this be paricularly of concern for spectral method  where the volume of information to be communicate can be quite large  and latency hiding can become more difficult as the size of each partition shrink    but this extra cost for cg to assemble PRON be time derivative could be compensate for by a different load balance strategy  different mesh partitioning strategy  PRON be most farmiliar with metis  allow the user to balance the load through various metric  eg  ensure each partitiom have approximately the same number of element or limit the amount of communication between partition  PRON feel a reason the colloquisium of dg be readily parallelizable be that a naive splitting of the problem into equal piece can give a very efficient parallel implementation  even present super linear speedup in certain case due to cache effect  see for example baggag et  al  or altmann et  al    whereas cg may require a more clever partitioning method  so PRON may be the case that change spacial discretization from dg to cg  say  would require one to also reconsider how to divide the mesh into subproblem   dg for hyperbolic pde can be use as a replacement for finite volume scheme  in finite volume as in finite difference  when PRON increase the order of the scheme  PRON stencil increase  this make parallelization more difficult  since for each scheme order  PRON have different stencil  at partition boundary  PRON must ensure that all require cell from the neigbouring partition for the particular order of the scheme be available  but with dg  no matter what order the scheme be  each cell communicate only with PRON face neighbour  so compare between finite volume  difference and dg  one can say the dg be easy to parallelize  
__label__machine-learning __label__classification __label__data-mining __label__predictive-modeling __label__time-series PRON have time  series datum of 10days for the same time interval as show in below figure  here PRON show one  hour power consumption for 10 day  data be sample at 10 minute rate   PRON need to show this 10day usage with a single baseline  representative curve  PRON can calculate baseline curve simply by take mean  median of these 10 day datum  but before that PRON need to answer follow question   the baseline should not represent the outlier  abnormal  usage day  here in figure  PRON see that day 1 and 2 be follow an unusual pattern from the rest of day  PRON think the usage of these day should not be use in baseline calculation  how should PRON exclude these day from baseline calculation automatically   how should PRON find the most similar usage out of these 10day usage for PRON baseline calculation  PRON think the most  maximum similar usage day represent the day use for baseline calculation   ad 1  assume the measurement at any give time be normally distribute  PRON shape approximately a bell curve   PRON could use simple standard deviation to detect outlier  specifically  for any give time  PRON can calculate the mean and standard error  then PRON calculate the mean san outlier by take into account only the measurement that fall at most some pre  set distance from the mean  eg give normal distribution  68  of measurement fall within one standard deviation from the mean    pseudo  code example    measurement at time t0 for all 10 day  t0  nparray01  01  14  9  125  125  15  01  03  175     get mean and standard error  mean0  std0  t0mean    t0std     inlier be within one sigma from the mean  inlier  nplogicalandmean0  std0  lt  t0   mean0  std0  gt  t0       gt   0  0  1  1  1  1  0  0  1  0    and the baseline mean at time t0 be  baseline0  t0inliersmean        gt  102  ad 2  PRON can find the most similar day to the baseline by use any appropriate distance measure  ie for time series  euclidean or dynamic time warp   the result  then  consist of those day where distance be the least  
__label__data-mining __label__bigdata __label__algorithms __label__data __label__outlier PRON be work on a dataset which be not normally distribute  the dataset contain three dimension like cost  discount and profit   PRON be try to find possible outlier in all these dimension  PRON use z  score to detect outlier in single dimension to find which high cost be cause outlier   as a next step PRON try to find outlier with high cost and high profit and low discount   PRON come up with a formula of   zscorecost   zscoreprofit   zscorediscount   negative sign because PRON want to find outlier with low discount   be this approach meaningful to do  or be there any further prove way to achieve this   because  PRON  data be  normal distribute  gaussian  PRON could  easy  try to implement  in PRON desire  language  this  alghoritm  from  coursera  mooc   httpsclasscourseraorgml005lecture91  use z  score can be ok if PRON be sure about what PRON be look for  PRON can also be just a way to transform PRON datum before use some ml on PRON  be aware that z  score apply to normally distribute datum  which PRON say be not the case   be also aware that look for outlier in 3 dimension be not as simple as look 3 time for outlier in 1 dimension  PRON should plot PRON datum in 3d  and try to find where may be the outlier   otherwise  one  class svm be pretty good at anomaly  outlier detection  take a look at the introduction here  also  any clustering algorithm may be helpful to get a good insight  dbscan  for example  help PRON to find cluster base on the density of the datum   have PRON consider use the mahalanobis distance  PRON be can be think of as the magnitude of a multi  dimensional z  score   the way PRON prefer to view the the mahalanobis distance be as the square root of the exponent of the multivariate normal distribution  this be similar to the z  score  which be the exponent of the univarate normal distribution before the square be apply  a big difference between these two be the z  score be sign  while the mahalanobis distance be unsigned  which do not matter for find outlier anyway   PRON understand PRON do not have normally distribute datum  but sometimes pretend PRON datum be normally distribute can have good result  so use the mahalanobis distance can be worth investigate  
__label__openfoam PRON be try to follow the openfoam user guide but PRON be stick at the first tutorial 1   in fact  when PRON try to run the blockmesh command  use the administrator account   the follow appear   writing polymesh    foam fatal error   fail write polymesh   from function blockmesh  in file blockmeshapp  c at line 325   foam exit  PRON try to follow instruction give in this thread 2 but none of the two here propose can help PRON   any advice   thank PRON in advance   filippo  PRON would check if PRON be in the proper case directory and not in any other directory  such as  case  constant  polymesh or something similar  have PRON copy the tutorial from  foamtutorial to PRON home directory or somewhere else  where PRON have proper permission to read and write  PRON generally suggest not to use the administrator s account for PRON daily work   hope this help  otherwise please provide more information  openfoam version  etc   
__label__performance __label__complexity __label__floating-point when evaluate the number of flop in a simple function  one can often just go down the expression tally basic arithmetic operator   however  in the case of mathematical statement involve even division  one can not do this and expect to be able to compare with flop count from function with only addition and multiplication   the situation be even bad when the operation be implement in a library   therefore  PRON be imperative to have some reasonable notion of the performance of the special function   by special function  PRON mean thing like   exp    sqrt    sin  cos  tan    which be usually provide by system library   determine the complexity of these be confound even further by the fact that many of PRON be adaptive and have input  dependent complexity   for example  numerically stable implementation of exp    often adaptively rescale and use look  up   PRON initial impression here be that the good one may do in this case be ascertain the average behavior of the function   this entire discussion be  of course  highly dependent on the architecture   for this discussion PRON can restrict PRON to traditional general purpose architecture and exclude those with special function unit  gpu  etc    one can find fairly simple attempt to standardize these for particular architecture for the sake of system vs system comparison  but this be not acceptable if one care about method vs method performance  which methodology for determine the flop complexity of these function be consider to be acceptable   be there any major pitfall   PRON could count PRON on real system use papi  which grant access to hardware counter  and simple test program   PRON favorite papi interface  wrapper be ipm  integrated performance monitor  but other solution exist  tau  for example    this should give a fairly stable method  to  method comparison   PRON be go to answer this question as if PRON ask    how do PRON analytically compare or predict the performance of algorithm that heavily rely on special function  instead of the traditional multiply  add  carry flop count that come from numerical linear algebra   PRON agree with PRON first premise  that the performance of many special function be architecture  dependent  and that although PRON can usually treat each of these function as have constant cost  the size of the constant will vary  even between two processor from the same company but with different architecture  see agner fog s instruction timing table for reference    PRON disagree  though  that the focus of the comparison should be on the cost of the individual float point operation   PRON think that count flops be to some extent still useful  but that there be several much more important consideration that may make the cost of special function less relevant when compare two potential algorithm  and these should be explicitly examine first before go to a comparison of float  point operation   scalability  algorithms feature task that can be implement efficiently on parallel architecture will dominate the scientific computing arena for the foreseeable future   an algorithm with a good  scalability   be PRON through low communication  less need for synchronization  or good natural load balance  may employ more slow special function and therefore be slow for small number of process  but will eventually catch up as the number of processor be increase   temporal locality of reference  do the algorithm reuse datum between task  allow the processor to avoid unnecessary memory traffic   each level of the memory hierarchy that an algorithm traverse add another order of magnitude cost  roughly  to each memory access   as a result  an algorithm with high density of special operation will likely be significantly fast than an algorithm with the equivalent number of simple function operation over a large region of memory   memory footprint  this be strongly related to the previous point  but as computer grow large and large  the amount of memory per core be actually trend downward   there be two benefit to a small memory footprint   the first be that a small amount of program datum will likely be able to fit completely within the processor cache   the second be that  for very large problem  an algorithm with a small memory footprint may be able to fit into processor memory  allow problem to be solve that would otherwise exceed the capability of the computer   PRON sound like PRON want a way to evaluate how fpu  bind PRON code be  or how effectively PRON be use the fpu  rather than to count the number of flop accord to same anachronistic definition of a  flop   in other word  PRON want a metric that reach the same peak if every float point unit be run at full capacity every cycle  let PRON look at an intel sandy bridge to see how this may shake out   hardware  support float point operation  this chip support avx instruction  so register be 32 byte long  hold 4 double   the superscalar architecture allow instruction to overlap  with most arithmetic instruction take a few cycle to complete  even though a new instruction may be able to start on the next cycle  these semantic be usually abbreviate by write latency  inverse throughput  a value of 52 would mean that the instruction take 5 cycle to complete  but PRON can start a new instruction every other cycle  assume that the operand be available  so no data dependence and not wait for memory    there be three float point arithmetic unit per core  but the third be not relevant to PRON discussion  PRON will call the relevant two the a and m unit because PRON primary function be addition and multiplication  example instruction  see agner fog s table   vaddpd  pack addition  occupy unit a for 1 cycle  latency  inverse throughput be 31  vmulpd  pack multiplication  unit m  51  vmaxpd  pack select pairwise maximum  unit a  31  vdivpd  pack divide  unit m  and some a   2120 to 4544 depend on input  vsqrtpd  pack square root  some a and m  2121 to 4343 depend on input  vrsqrtps  pack low  accuracy reciprocal square root for single precision input  8 float   the precise semantic for what can overlap with vdivpd and vsqrtpd be apparently subtle and afaik  not document anywhere  in most us  PRON think there be little possibility for overlap  though the wording in the manual suggest that multiple thread may offer more possibility for overlap in this instruction  PRON can hit peak flop if PRON start a vaddpd and vmulpd on every cycle  for a total of 8 flop per cycle  dense matrix  matrix multiply  dgemm  can get reasonably close to this peak   when count flop for special instruction  PRON would look at how much of the fpu be occupy  suppose for argument that in PRON range of input  vdivpd take an average of 24 cycle to complete  fully occupy unit m  but addition could  if PRON be available  be execute concurrently for half the cycle  the fpu be capable of perform 24 packed multiplie and 24 packed addition during those cycle  perfectly interleaved vaddpd and vmulpd   but with a vdivpd  the good PRON can do be 12 additional pack add  if PRON suppose that the good possible way to do division be to use the hardware  reasonable   PRON may count the vdivpd as 36 pack  flop   indicate that PRON should count each scalar divide as 36  flop    with reciprocal square root  PRON be sometimes possible to beat the hardware  especially if full accuracy be not need  or if the range of input be narrow  as mention above  the vrsqrtps instruction be very inexpensive  so  if in single precision  PRON could do one vrsqrtps follow by one or two newton iteration to clean up  these newton iteration be just  y    3  xyy05   if many of these operation need to be perform  this can be significantly fast than naive evaluation of y  1sqrtx   prior to the availability of hardware approximate reciprocal square root  some performance  sensitive code use infamous integer operation to find an initial guess for the newton iteration   library  provide math function  PRON can apply a similar heuristic to library  provide math function  PRON can profile to determine the number of sse instruction  but as PRON have discuss  that be not the whole story and a program that spend all PRON time evaluate special function may not appear to get close to peak  which may be true  but be not useful for tell PRON that all the time be spend out of PRON control on the fpu   PRON suggest use a good vector math library as the baseline  eg intel s vml  part of mkl   measure the number of cycle for each call and multiply by peak achievable flop over that number of cycle  so if a packed exponential take 50 cycle to evaluate  count PRON as 100 flop time the register width  unfortunately  vector math library be sometimes hard to call and do not have all the special function  so PRON may end up do scalar math  in which case PRON would count PRON hypothetical scalar exponential as 100 flop  even though PRON probably still take 50 cycle  so PRON would only be get 25  of  peak  if all the time be spend evaluate these exponential    as other have mention  PRON can count cycle and hardware event counter use papi or various interface  for simple cycle counting  PRON can read the cycle counter directly use the rdtsc instruction with a snippet of inline assembly   why bother counting flop   just count cycle for every operation and PRON will have something that be universal  
__label__c++ __label__c __label__special-functions PRON need to evaluate kummer s confluent hypergeometric function for imaginary argument      1f1a  bix  where  i be the imaginary unit   a  b  x be real  and  a  bgt0 be there a routine available in c  c  PRON already see gsl  but PRON only compute   1f1  for real argument   one possibility be to wrap the arbitrary precision interval implementation in arb  this will not be as fast as a dedicated double precision implementation  but PRON may still be fast enough   note 1  this code require arb version 280 or later    include  acbhypgeomh   void  hyp1f1ixdouble  re  double  PRON be  double a  double b  double x     long prec   acbt aa  bb  xx  rr   acbinitaa   acbinitbb   acbinitxx   acbinitrr    acbsetdaa  a    acbsetdbb  b    acbsetdxx  x    acbmuloneixx  xx    for  prec  64   prec   2     acbhypgeommrr  aa  bb  xx  0  prec    if  acbrelaccuracybitsrr   gt 53   break      re  arfgetdarbmidrefacbrealrefrr    arfrnddown     PRON be  arfgetdarbmidrefacbimagrefrr    arfrnddown    acbclearaa   acbclearbb   acbclearxx   acbclearrr      int main      double re  PRON be   hyp1f1ixampre   ampim  314  278  20151130    printf15 g  15gn   re  PRON be      note 2  PRON have prepare a file arbcmathh  httpsgithubcomfredrikjohanssonarbcmath  that wrap 1f1 and many other function  2f1  bessel  incomplete gamma  etc   for use with c99 complex double  so PRON do not need to implement the wrapper code PRON    include  arbcmathh   int main      double complex w   w  achyp1f1314  278  20151130i    printf15 g   15gin   crealw   cimagw       the follow code    g hgcc o hg l  usr  local  lib lgsl lgslcblas lm  ampamp  hg   include  ltstdiohgt    include  ltgsl  gslcomplexhgt    include  ltgsl  gslcomplexmathhgt      gslcomplex operator   gslcomplex  z1  const gslcomplex  amp  z2    return gslcomplexdivz1  z2      gslcomplex operator    gslcomplex  z1  const gslcomplex  amp  z2    return gslcomplexmulz1  z2      gslcomplex operator   gslcomplex  z1  const gslcomplex  amp  z2    return gslcomplexaddz1  z2      gslcomplex operator   gslcomplex z2   const double   amp  x    return gslcomplexadd  z2   gslcomplexrectx0       gslcomplex operator   gslcomplex z2   const double   amp  x    return gslcomplexdiv  z2   gslcomplexrectx0          gslcomplex hyge  const gslcomplex  amp  a    const gslcomplex  amp  b    const gslcomplex  amp  c    const gslcomplex  amp  z    unsigned int n  100   accuracy    unsigned int i  0   itteration step    gslcomplex t  gslcomplexrect10   coefficient t for internal calculation     gslcomplex  tnext   aibicii1   t   return  in1   gslcomplexrect00   tgslcomplexpowrealz  ihygea  b  c  z  n  i1tnext         int main  void     gslcomplex a  gslcomplexrect   1   0    gslcomplex b  gslcomplexrect   2   0    gslcomplex c  gslcomplexrect   2   2    gslcomplex z  gslcomplexrect0201    gslcomplex zout  hygea  b  c  z    printf    10f10f i n    gslreal  zout   gslimag  zout     return 0        calculate   11833113578  00657194591 i  in PRON case  PRON may set a1  hyge1b  c  z   
__label__machine-learning __label__nlp __label__word-embeddings __label__word2vec say PRON have a document and PRON want to assign all  or most  of the word to pre  assign topic  so PRON could take a random selection of word and manually assign PRON to the appropriate topic  then PRON want an algorithm to assign the remain word to the appropriate topic base on how closely PRON occur to the manually assign phrase   be there an algorithm or technique that do this  PRON have look into topic modelling and word2vec and PRON seem to assign  topic  in some arbitrary space without any room for manual training and designation of topic   there be technique like singular value decomposition  sdv  and principal component analysis  pca  that when apply to many document  or many sentence in PRON case if PRON have only one document  can produce vector of word that occur together  these vector would then represent the topic   svd and pca be commonly use text analysis technique that have be show to be capable of identify different section  topic of newspaper  eg  sport  finance  world  etc  when apply to many news article   latent dirichlet allocation  lda  be a generative statistical model that allow set of observation to be explain by unobserved group that explain why some part of the datum be similar  lda do exactly the same thing PRON want to do  
__label__finite-difference __label__fluid-dynamics __label__numerical-analysis __label__machine-learning background   PRON have only build one work numeric solution to 2d navier  stokes  for a course   PRON be a solution for lid  drive cavity flow   the course  however  discuss a number of schema for spatial discretization and time discretization   PRON have also take more symbol  manipulation coursework apply to ns   some of the numeric approach to handle conversion of the analytic  symbolic equation from pde to finite difference include   euler ftfs  ftcs  btcs  lax  midpoint leapfrog  lax  wendroff  maccormack  offset grid  spatial diffusion allow information to spread   tvd  to PRON  at the time  these seem like  insert  name find a scheme and PRON happen to work    many of these be from before the time of  plentiful silicon    PRON be all approximation   in the limit PRON  in theory  lead to the pde s   while direct numerical simulation  dns  be fun  and reynolds averaged navier  stokes  rans  be also fun  PRON be the two  endpoint  of the continuum between computationally tractable  and fully represent the phenomenon   there be multiple family of approach that live interior to these   PRON have have cfd professor say  in lecture  that most cfd solver make pretty picture  but for the most part  those picture do not represent reality and that PRON can be very tough  and take lot of work  to get a solver solution that do represent reality   the sequence of development  as PRON understand PRON  not exhaustive  be   start with the govern equation   pde s  determine PRON spatial and temporal discretization   grid and fd rule  apply to the domain include initial condition and boundary condition  solve  lot of variation on matrix inversion   perform gross reality check  fit to known solution  etc   build some simple physical model derive from analytic result  test PRON  analyze  and evaluate  iterate  jump back to either step 6  3  or 2   thought   PRON have recently be work with cart model  oblique tree  random forest  and gradient boost tree   PRON follow more mathematically derive rule  and the math drive the shape of the tree   PRON work to make discretized form well   although these human  create numeric approach work somewhat  there be extensive  voodoo  need to connect PRON result to the physical phenomenon PRON be mean to model  often the simulation do not substantially replace real  world testing and verification   PRON be easy to use the wrong parameter  or not account for variation in geometry or application parameter experience in the real world   questions   have there be any approach to let the nature of the problem define  the appropriate discretization  spatial and temporal differencing scheme   initial condition  or solution   can a high definition solution couple with the technique of  machine learning be use to make a differenc scheme that have much  large step size but retain convergence  accuracy  and  such   all of these scheme be accessibly  humanly tractable to derive    PRON have a handful of element   be there a differenc scheme  with thousand of element that do a good job   how be PRON derive   note  PRON will follow up with the empirically intializ and empirically derive  as oppose to analytically  in a separate question   update   use of deep learning to accelerate lattice boltzmann flow   give 9x speedup for PRON particular case  hennigh  o  in press   lat  net  compressed lattice boltzmann flow simulations use deep neural networks  retrieve from   httpsarxivorgpdf170509036pdf  repo with code  PRON think    httpsgithubcomloliverhennighphynet  about 2 order of magnitude faster than gpu  4 order of magnitude  or o10000x  faster than cpu  and same hardware   guo  x  li  w  amp  ioiro  f convolutional neural networks for steady flow approximation  retrieve from   httpsautodeskresearchcompublicationsconvolutionalneuralnetworkssteadyflowapproximation  other who have look into the topic about 20 year ago   muller  s  milano  m  amp  koumoutsakos p application of machine learn algorithm to flow modeling and optimization   center for turbulence research annual research briefs 1999 retrieve from   httpswebstanfordedugroupctrresbriefs99petrospdf  update  2017    this characterise the use of non  gradient method in deep learning  an arena which have be exclusively gradient base   while the direct implication of activity be in deep learning  PRON also suggest that ga can be use as an equivalent in solve a very hard  very deep  very complex problem at the level consistent with or superior to gradient descent base method   within the scope of this question  PRON may suggest that a large  scale  machine  learning base attack may allow  template  in time and space that substantially accelerate convergence of gradient  domain method   the article go as far as to say that sometimes go in the direction of gradient descent move away from the solution   while in any problem with local optima or pathological trajectory  most high  value real  world problem have some of these  PRON be expect that the gradient be not globally informative  PRON be still nice to have PRON quantify and validate empirically as PRON be in this paper and the ability to  jump the bound  without require  reduction of learning  as PRON get in momentum or under  relaxation   PRON think PRON be mix a couple different idea that be cause confusion   yes  there be a wide variety of way to discretize a give problem   choose an appropriate way may look like  voodoo  when PRON be learn these thing in class  but when researcher choose PRON  PRON do so draw on the combined experience of the field  as publish in literature   therefore PRON make much more informed choice than a student could   question 1   if PRON be solve a problem  and PRON switch from one scheme to another  PRON run time will change  the convergence criterion may change  or PRON asymptotic behavior  but a very important point be that PRON final converged solution should not change   if PRON do  PRON either need to refine PRON mesh  or there be something wrong with PRON numerical scheme   perhaps PRON could use some optimization algorithm to create PRON numerical scheme and improve performance for a specific class of problem  but many time the hand derive scheme be create with mathematically provable optimal convergence  asymptotic behavior for the number of term involve or mesh type use   now the above paragraph do not account for thing like different turbulence model  which be different mathematical formulation  approximation of the physics  so be expect to have different solution   these be again highly study in the literature  and PRON do not think program be at the point that PRON can look at physical phenomenon and produce a mathematical model that properly predict the response of similar physical system   question 2   yes  PRON could derive a scheme that use the entire mesh at once  use some computer code to do PRON   PRON even feel safe say that for some mesh  such code exist  and could give PRON PRON scheme in a matter of a couple hour  once PRON find the code that be    the problem be that PRON will never beat nyquist   there be a limit to how large of time step PRON take  depend on the max frequency of the response of PRON system  and a limit to how large of mesh cell  element PRON can have depend on the spatial frequency of the solution   that do not even account for the fact that often the computational work involve in use a more complex scheme be often non  linear with complexity   the reason most student learn rk4 method for time integration be that when PRON start go to method with a high order than that  PRON gain more evaluation of PRON derivative faster than PRON gain order of PRON method   in the spatial realm  high order method greatly increase matrix fill  in  so PRON need less mesh point  but the work PRON do invert the sparse matrix increase greatly  at least partially offset PRON gain   PRON be not sure what PRON be refer to in question three   be PRON talk about turn close solution to a problem into good solution   if so PRON recommend some light reading on multigrid   if PRON be ask about turn decent numerical scheme into amazing one  PRON think the rest of PRON answer at least touch on that   PRON be a long  run joke that cfd stand for  colorful fluid dynamic   nevertheless  PRON be use  and useful  in a wide range of application   PRON believe PRON discontent stem from not sufficiently distinguish between two interconnect but different step  create a mathematical model of a physical process and solve PRON numerically  let PRON comment on these a bit   no mathematical  or really  any  model of physical reality be ever correct  at good PRON be useful for predict the result of measurement in a very precisely demarcate  but hopefully large  set of circumstance  this include the fact that PRON must be possible to obtain such a prediction give a specific configuration  this be why PRON have a whole hierarchy of model from quantum field theory to newtonian mechanic  in particular  the navier  stokes equation do not describe fluid flow  PRON give a prediction of specific aspect of the behavior of certain fluid under certain condition   for the more complicated mathematical model  such as the navier  stokes equation   PRON can never obtain an exact solution  and hence prediction   but only a numerical approximation  this be not such a bad thing as PRON sound  since the measurement PRON want to compare PRON with be PRON never exact  just as in the choice of model  there be a trade  off between accuracy and tractability  PRON make no sense to spend time or money on get a more accurate solution than need  at this point  PRON become purely a question on how to approximate numerically the solution of  in this case  a partial differential equation  which be the subject of a whole mathematical field  numerical analysis  this field be concern with prove error estimate for certain numerical method  again  under certain  explicitly specify  condition   PRON statement  insert  name find a scheme and PRON happen to work   be grossly unfair  PRON should be  insert  name find a scheme and prove that PRON work   also  these scheme be not pull out of thin air  PRON be derive from well  understand mathematical principle    for example  finite difference scheme can be derive use taylor  approximation of a give order  PRON be certainly possible to  and some people do  obtain very high  order difference scheme and implement PRON  but there be a law of diminish return  this can only be automate partially  and hence take a lot of effort  and certain increasingly restrictive condition must be satisfied to actually get the correspond high accuracy out of PRON  also  at some point PRON be good to use a different scheme altogether such as spectral method    the common theme here be that both model and numerical scheme come with a range of applicability  and PRON be important to pick the right combination for a give purpose  this be precisely why a computational scientist need to know both the domain science  to know which model be valid in which situation  and mathematic  to know which method be applicable to which model  and to which accuracy   ignore these  use only as direct  label lead to produce the kind of  computational bullshit   in the technical sense of harry frankfurt  PRON cfd professor refer to   as to why use a computational model when PRON have a physical model  such as a wind tunnel   one reason be that run software can be order of magnitude cheap than create a model and put PRON in a wind tunnel  also  PRON be usually not an either  or  for example  when design a car or an airplane  PRON would run hundred or thousand of simulation to narrow thing down  and then only for the final candidates  put a model into a wind tunnel   update   use machine learn instead of numerical simulation be like say  have no model be good than have an approximate model   which PRON doubt anyone in fluid dynamic  or any other field  would agree with  that be say  PRON be certainly possible  and actually do  to use machine learn to select unknown  geometry or application parameter  base on agreement with measure datum  however  here as well model  base method such as uncertainty quantification or  bayesian  inverse problem usually perform much good  and be base on rigorous mathematical principle   select numerical parameter such as step size or order of the method use machine learning be also possible in principle  but PRON fail to see the benefit since there be a mathematical theory that tell PRON precisely how to pick these parameter base on PRON  mathematical  model   update 2   the paper PRON link to be about computer graphic  not computational science  PRON goal be not to have an accurate simulation  ie  a numerical solution of a mathematical model  of a physical process  but something that just look like one to the naked eye  an extreme case of  colorful fluid dynamic     that be a very different matter  in particular  there be no error bind for the output of the train network compare to the correspond solution to the navier  stokes equation  which be an indispensable part of any numerical method    and PRON first question start from a false premise  in every approach  the problem determine the model  the model determine the discretization  the discretization determine the solver    httpswwwncbinlmnihgovpubmed27079692  a machine  learn approach for computation of fractional flow reserve from coronary compute tomography  
__label__data-mining __label__finance __label__market-basket-analysis can anyone please tell PRON what s the difference between  business analyst  financial analyst  data analyst  and what should one need to learn in order to achieve either of these profile   business analyst  a business analyst be one who understand the specific domain of the project  ex  retail  merchandise to be specific  supply chain etc    PRON  PRON role be to understand the business problem  analyze the current state and capture requirement use various tool like survey  interview  group discussion and then provide recommendation and create a requirements document for sign off   business analyst skillset  communication   analytical thinking  domain knowledge  generic  technical knowledge  problem solve skill  decision solving skill   managerial skills  negotiation and persuasion skills   financial analyst  financial analyst use financial datum to spot trend and extrapolate into the future  help PRON employer and client make the good investing decision  business rely on financial analyst to determine when PRON be an auspicious time to buy or sell specific security and  in some case  company use report put together by financial analyst to determine if the entire business should be sell   financial analyst skillset  financial analyst should be great problem  solver  excel at the use  of logic and possess strong skill in quantitative analysis  in  addition  successful financial analyst have an in  depth understanding  of various financial market and investment product   data analyst  a data analyst s role be one that work with lot of datum to derive meaningful insight to either address business problem or discover hidden trend and pattern that can be leverage to meet the business objective   data analyst skillset  data analyst also need similar skill with some additional skill  like to analyze datum like sql  data mining  olap  report etc   note  strong people skill  leadership ability  and teamwork be beneficial for each type career   for more info  PRON can visit these resources  career advice  financial analyst vs  data analyst  what be the difference between data analyst and business analyst   the differences between a business analyst  amp  a data analyst  hope PRON help  cheer   
__label__fluid-dynamics __label__simulation PRON be work on a 2d inviscid fluid simulation use a  panel method   with potential be use to enforce the no  through boundary condition   PRON be try to incorporate the kutta condition  which say that the pressure above and below an airfoil be equal when the stream meet at the trailing edge  or equivalently that the velocity be smoothly leave the trailing edge from the top and bottom of the airfoil in the same direction   PRON be usually invoke relate to airfoil  and most literature on the subject  eg  aerodynamic or modelling of steady  amp  unsteady flow around 2d airfoils use panel methods  assume PRON know beforehand where PRON trailing edge be   however  PRON want to be able to simulate any arbitrary 2d surface  well  as arbitrary as PRON can get with simple polygon  in any arbitrary unsteady flow   be there a generalize version of the kutta condition that PRON can use   something that would work for bluff body  airfoil  and any other random shape and still give the proper result   or alternatively  be PRON actually necessary to enforce PRON if PRON be aim for a purely inviscid flow   a couple of different source have hint that PRON be actually a fudge that enable PRON to take one of the important aspect of viscous flow without all the rest of the overhead   
__label__bigdata __label__research PRON be wonder if there be any research or study make to calculate the volume of space be use by all scientific article  PRON could be in pdf  txt  compressed  or any other format  be there even a way to measure PRON   can some one point PRON towards realize this study   regard and thank   perhaps PRON be look to quantify the amount of filespace use by a specific subset of datum that PRON will label as  academic publication    well  to estimate  PRON could find stat on how many publication be house at all the lead library  jstor  ebsco  academichost  etc  and then get the mean average size of each  multiply that by the number of article and whamo  PRON have get PRON an estimate   here be the problem  though  pdf file store the text from string s differently  in size  than  say  a text document store that same string  likewise  a compressed jpeg will store an amount of information i differently than a non  compress jpeg  so PRON see PRON could have two of the same article contain the same information i but take up different amount of memory m  be PRON look to get a wordcount on the amount of scientific literature   be PRON look to get an approximation of file system space use to store all academically publish content in the world  
__label__dataset PRON have no knowledge about the climate or soil  and PRON just want to find out more about these kind of dataset  PRON hear that climate corporation ask PRON candidate to perform statistical analysis on various climate dataset   this be why PRON be ask this question  please do not get PRON wrong  PRON be not try to get the dataset to prepare PRON for an interview  as PRON know PRON give out different dataset to people from different background   PRON know that climate corporation only hire phd  which PRON be not  PRON only want to play around with PRON dataset such that PRON can learn and implement time series analysis  that be PRON   so  if anyone do not mind share PRON dataset  please post the link PRON below  thank PRON very much   
__label__machine-learning __label__feature-extraction __label__correlation __label__score i be new to machine learn i have a question in PRON mind be there any correlation coefficientskendall  spearsman  mutual information etc   that can tell PRON for a particular string variable like  houston tx  PRON co relation with the label for example  be people who speak dual language have good opportunity to secure job in us    
__label__regression PRON be study  via simulation  a system that have several input parameter  the output of the system be influence by the input parameter  PRON goal be to identify the parameter that have the most impact on the output of the system  PRON run a large amount of simulation and for each one of PRON PRON change the value of each parameter use a random algorithm  to identify the parameter that have the most impact on the output PRON be plan on do a basic multivariate linear regression include all the parameter  however  PRON turn out to have a low r2 and low f  stat   PRON be work a little bit out of PRON element here so PRON be look for suggestion regard what PRON should investigate next to achieve PRON goal   the context of the study be build energy efficiency and the main difficulty be that some input parameter have multiple effect on the system  for example  if PRON lower the total lighting power use in the simulation PRON will have for effect to decrease the cool load of PRON building but if at the same time PRON be increase the cool efficiency of PRON cooling system the energy save by increase the efficiency would be less than if the lighting power be not decrease  in this example PRON would like to show which have more impact in regard to the energy consumption  however  in what PRON be try to do PRON have a lot more input parameter   sorry PRON can not comment yet  and this be not an answer  do PRON change all parameter at same time or do PRON have a strategy to change parameter per parameter  experiment    with PRON background on data analysis and nonlinear regression  PRON question seem to be very general and blurry  PRON guess what PRON be look for be call sensitivity analysis in some context   PRON need to understand that parameter may have different order of impact on different sub  space of variable  and parameter  space   PRON may also want to generate multiple sub  model base on this sub  space  so in each sub  model only a few of PRON parameter have the main effect and the rest could be neglect   again PRON need to make PRON question a little more detailed  for example tell PRON why PRON want to do this analysis  because PRON final goal totally impact the solution  
__label__numerical-analysis __label__iterative-method  variant of stats exercise to protect the innocent    start for  x  0  PRON can do one of two operation  pm 1  or  pm 4  let  t be one iteration of this step   what be the mean and standard deviation after 10 step mod 10  ie the distribution of  t100  mod 10   what be the mean and standard deviation after 100 step mod 100  ie t PRON distribution of  t1000  mod 100   what be the mean and standard deviation after 1000 step mod 1000  ie t PRON distribution of  t10000  mod 1000   the exam have ask for 10 decimal place accuracy   also the problem have be completely alter but original in spirit   notice these do not look for limit   can this be classify under a general theme   one way to do this be to note that since step size be independent of step direction  conditional on know the sequence of step size  the distribution of  tn0 be a sum of two random walk of  k and  n  k step with step size 1 and 4  respectively  both of which be independent binomial random variable   where  k be the number of step of size 1  which PRON have a binomial distribution  independent of the random walk  since the limit on the problem be pretty small  the probability of observe a give value of  tn0 can be calculate directly in time  on3 from the binomial pmf s  and then expectation and standard deviation modulo  n calculate from that   PRON think this can be classify as a question about iterate expectation   for reference  for  n10  this give PRON mean  43798828125  and sd   2871979661 
__label__petsc __label__compiling PRON write a function to read in the runtime parameter PRON program take  this function be in a different file than PRON main program  the two object file be link after compilation   when PRON compile the file that contain the function PRON use to read in parameter  PRON get the follow warning   ioc43   warn  120  return value type do not match the function type  chkerrqierr      this only seem to affect the chekrrq   macro  and PRON do not get this warning when PRON compile the example include with petsc   the only obvious difference be that the example be entirely within one file whereas PRON program be break up into multiple file   do petsc support separate compilation  or do some aspect of the chkerrq   macro because problem   chkerrq can not be use inside function other than main   that do not return petscerrorcode  this be because the chkerrq   macro be basically  ifierr   petscerror  ierr  return ierr    from the documentation   although typical usage resemble  void chkerrqpetscerrorcode   as describe above  for certain us PRON be highly inappropriate to use PRON in this manner as PRON invoke returnpetscerrorcode   in particular  PRON can not be use in function which returnvoid  or any other datatype  in these type of function  PRON can use chkerrv   which return without an error code  bad idea since the error be ignore or if  n   petscerror     returnyourreturntype    where PRON may pass back a petscnull to indicate an error  PRON can also call chkerrabortcomm  n  to have mpiabort   return immediately   basically  any function that use chkerrq   should either return petscerrorcode PRON or use chkerrv   instead   chkerrq can only be use in function that return petscerrorcode   the petsc documentation be reasonably clear here   chkerrq  checks error code  if non  zero PRON call the error handler and then return  chkerrqn  be fundamentally a macro replacement for   if  n  returnpetscerror  n       although typical usage resemble void chkerrqpetscerrorcode  as describe above  for certain us PRON be highly inappropriate to use PRON in this manner as PRON invoke returnpetscerrorcode   in particular  PRON can not be use in function which returnvoid  or any other datatype  in these type of function  PRON can use chkerrv   which return without an error code  bad idea since the error be ignore or if  n   petscerror     returnyourreturntype    where PRON may pass back a petscnull to indicate an error  PRON can also call chkerrabortcomm  n  to have mpiabort   return immediately  
__label__publications __label__journals PRON have hear that some journal be rat more highly than other   be this true  and if so  what be the criterion for judge the value of one peer review journal over another   how do PRON find out PRON rating   will PRON publication be of less  worth  if PRON be accept in a less reputable journal than  say  the siam review   PRON really depend on how much PRON believe in journal rating  the most popular metric PRON be aware of be the impact factor  this wikipedia article describe the impact factor  how PRON be calculate  and how PRON be susceptible to manipulation  as well as alternative journal rating metric  the most important thing worth note be that impact factor for journal in different field be not necessarily comparable  even though the impact factor rating system try to collapse the  quality  of each journal onto a single metric   for an interesting discussion on why such an idea can lead to mislead or easily manipulable result  see malcolm gladwell s new yorker essay  the order of thing  which discuss college ranking and car ranking    the impact factor be the average number of citation receive per paper publish in that journal during the two precede year  so the criterion for judge the value of one peer review journal over another be how many time PRON recent article get cite on an average  per article basis  this metric do mean that journal like science and nature be rank highly  as one would expect  but PRON also mean that international journal of nonlinear sciences and simulation  3100  get rank higher than siam journal of scientific computing  3016   even though the latter journal be a lot more recognizable   as for PRON publication be of less  worth   PRON depend on the person  PRON have be tell by PRON more senior colleague that have article in more prestigious journal be helpful for thing like tenure and hire decision  not have experience with such matter firsthand  PRON defer to the comment of people who be have more background with those sort of situation  PRON depend on the community of people PRON be try to reach or impress  if PRON publish something in a math journal  do not expect that people outside mathematic will necessarily read PRON   the same advice apply to other subject   also  PRON may decide to publish in a less prestigious journal because PRON want to get a result out quickly rather than wait a long time for PRON to be review by more prestigious journal  PRON have be tell that the siam journal can take a year or more to review a paper and publish PRON  which be back up by look at the time between the submit date and the publish date on the paper  submission to a particular journal be also dependent on aim and scope  politic  who be on the editorial board  who be likely to review PRON paper   what community PRON want to reach out to  sort of go with the advice about subject area above   PRON still consult other people on these matter   when PRON look at article  the  prestige  of a journal  to PRON  be really PRON internal feeling that reflect the value opinion of other people PRON know in science  PRON adviser  colleague  etc    PRON be more inclined to trust an article in a journal that be more prestigious because PRON feel PRON be more likely to be reliable  but there be certainly bad article in prestigious journal  so prestige be not a replacement for do PRON own due diligence  for example  there be some pretty serious typo in an otherwise good journal article PRON have cite from siam review   anyway  that be PRON rambling opinion  and as a  nearly  freshly  mint phd  PRON be sure that other with more experience will have much more perspective and insight to add   in an ideal world  PRON would only consider one thing when choose where to submit a manuscript   which journal s readership include the broad audience that would  be interested in this work   in practice  PRON also consider other factor  perhaps most significantly  how much will publish in this journal help PRON career   anyone who can understand PRON work well will judge PRON by PRON actual significance  but those who be too far remove may use the quality of the journal as a proxy for the quality of PRON paper   how be journal quality determine   experienced professional have PRON own opinion base on experience with each journal and the article PRON read   in some field  like mathematic  this expert opinion can only be synthesize by talk to a lot of expert   in other field  like computer science  this have be explicitly codify into a tiered ranking system   the most widely available metric for journal quality be the so  call impact factor   the danger of have a single  widely use metric for anything be that PRON may be manipulate or misapply  or may simply not be useful for some of the thing PRON be apply to   for instance  since PRON only measure citation over two year  PRON do not make much sense for mathematic paper  which typically get most of PRON citation after two year  often much later   PRON also seem to be poorly correlate with expert opinion   there be alternative  for instance eigenfactor   and many people be now propose the use of social web metric to rank journal   PRON do not use the impact factor or other such metric in PRON own decision  as PRON feel that PRON direct experience give PRON a more accurate measure of a journal s significance   an interesting exercise be to go through PRON personal library of reference  eg  in a bibtex file  and see the number of article PRON contain from different journal   that will probably give PRON the good indication of where PRON should publish   by the way  there be a proposal to create an expert  base ranking of math journal  see this report and this official imu blog   PRON should also be aware that there have recently be an explosion of new low  quality journal base on an author  pay model   PRON publisher frequently spam large community of scientist  often include many who be unqualified  in an attempt to get reasonably respectable name on PRON editorial board and solicit paper submission   as far as PRON can tell  PRON be profit  orient vanity press   what factor determine where PRON will publish a paper   will the people PRON want to read this paper see PRON  if PRON be follow up on the work of another group  perhaps to show a different viewpoint  sometimes to show algorithmic improvement or to fix problem with a previous paper   PRON will want to submit the paper to the same journal  even if there be an impact factor issue   as a young computational scientist still in the career development phase of PRON career  PRON would submit that there be an additional  and perhaps even more critical aspect to this question      will people who may be in a position to evaluate PRON see this paper  PRON have often speak with peer in the computational science field about the need to have a  home turf   computational science be a highly interdisciplinary field  unfortunately  PRON be not really able to be consider as computational scientist when PRON come time to be consider for a permanent position  in the absence of work in a computational science department  PRON will have to apply for tenure in an exist department  which usually mean that PRON  peer  will be other engineer  scientist  and mathematician — many of whom do not really have a strong background in computational science  this mean that even if PRON want to go in the cross  fertilize direction  PRON still need to focus some of PRON publication in the go  to journal for PRON discipline  this be a challenge that many of PRON peer will not necessarily have to face  and PRON be an additional complication in PRON life  but PRON be something PRON have to be aware of before PRON start work   how much competition do PRON have right now  the more crowd a field  the more important PRON be to get the result in early  while PRON be great to try to go to nature or science with every paper out of PRON group  presume PRON be not in pure math  or something similar   be first out of the gate in a  hot  field matter much more than publish in the  good  journal in a field   how important be this paper  a paper that represent a body of work that provide a lot of new datum  but not really much in the way of ground  break insight  probably do not merit go to a top  level journal  PRON be probably good to look for a reputable journal  however  if PRON have really find something big  shoot high  so long as PRON be not worried about the time crunch that affect the big journal   PRON can take longer  for instance  to publish in physical review letters than in one of the other physical review series  which be of comparable quantity    after all of these be take into account  then PRON will start worry about issue like impact factor  but then only as a loose quality control measure  difference of 10  20  be essentially meaningless  but a 10 versus a 20  or a 20 versus a 30  do represent a measurable level of difference between journal   rating be necessarily a subjective activity  if PRON care about how other will rate PRON paper  then PRON need to figure out what PRON criterion be  as a rule of thumb  the less competence PRON have in PRON field of work  the more PRON will rely on journal rating rather than rat PRON work  and the more PRON will rely on bibliometry  impact factor  h factor    rather than personal experience   PRON short answer be  unfortunately  there be  almost    there be the rating by the aust ms for mathematical journal and PRON think there be other rating as well  however  in PRON opinion this be not helpful  PRON quote from PRON own blog   PRON opinion be that rating be possible but both useless and dangerous  PRON be possible  since PRON can ask experienced mathematician and PRON will get a reliable answer  PRON be useless  since PRON either know which journal be good or PRON can ask a colleague  which be basically the same reason as the previous one   PRON be dangerous  since PRON offer the possibility to form decision on tenure or grant on these number and shift the focus from what PRON publish to where PRON publish    this be a conversation PRON should be have with the people above PRON that be go to evaluate PRON  if PRON be a phd student  then PRON should be talk to PRON advisor and other professor about how PRON department and other department at other institution evaluate the work of tenure  track faculty when make PRON tenure decision  if PRON be tenure  track faculty now  PRON should have already have this conversation with PRON department chair and other member of the department s budget council or similar body about how PRON evaluate the work of tenure candidate  if PRON be not a professor of some fashion  or if PRON be an academic in a place without tenure  PRON should ask the same question of PRON supervisor about promotion and retention at PRON institution   PRON experience have be that most researcher do not really read or try to keep up with publication in journal relevant to PRON area directly as PRON come out  not many people have personal subscription to ijnmf  say  and read each month s issue cover to cover  there be just too many venue to keep up with  more often  PRON find  people be search for related work  use email or google alert to seek out related item  and follow the work of colleague and competitor directly  so  the important thing be not try to get PRON work see via journal  but to publish in the good possible venue as determine by those who will be judge PRON career   the way to get PRON work see be to aggressively market PRON through conference  seminar  free software  PRON website  etc  this probably will not impact PRON promotion committee s decision  or the like   but if people be see and use PRON work through other venue  and PRON give PRON preprint and link to where PRON be publish in the journal  then that will increase PRON citation count  which be something important to those above PRON   there be some rank system for citation  which one may argue be decent measure of prestige  important paper get cite more and the like  the most common of these be  impact factor   but these all have PRON own problem  PRON be not constant by field  PRON be susceptible to manipulation by clever researcher  etc  other that be slightly good be the h  index and g  index  but  meh   generally  PRON have find there be a  feel  in the field for what be good and what be not  and more importantly  what be appropriate and what be not for PRON paper  for example  PRON have one paper that manage to get into a very good journal  but PRON utterly inappropriate  so PRON languish a bit   PRON evaluator or colleague would probably be the good source to talk to  experience be often the good judge of what count as  good  for a particular paper that would lend PRON well to a particular journal  general field  biology  medicine  etc   ranking  or even bad  academia  wide ranking may not be of any use  
__label__dataset __label__feature-selection __label__correlation how do PRON use a correlation score between two variable for analyse datum   PRON have a set of 20 feature and need to predict 21st feature  now be PRON necessary that correlation between any two feature should be close to 1  if PRON have 2 feature with corr score close to 1  then do this mean that PRON be contradict and thereby decrease the accuracy   so how do PRON use a correlation score in analysis   correlation should be as less as possible between different feature  because correlate feature mean that those feature be give out same kind of information  trend for the predictor to learn  thus only one of PRON be actually useful for prediction   keep more number of uninformative feature  correlate feature  would result in degraded accuracy if PRON sample size be similar to PRON feature set size  feature selection use recursive feature elimination or pca etc  can help PRON reduce PRON feature set to optimal size   PRON calculate correlation score in predictive analysis between feature and target variable  when use linear regression to model a data set  PRON first see if the plot between different feature and target variable value follow an upward   ve correlation  or downward trend  ve correlation  and not scatter randomly  if such a relationship exist then regression modelling on the datum would work well  
__label__machine-learning be there a general adversarial network that can take multiple low quality image of a subject to create a high quality image of the subject  srgan just take a single low r image and make PRON high re but PRON need something that can take multiple low quality  not necessarily low re  image to create a high quality image  in low quality PRON mean PRON could have one image with a reflection or partially blurry or have a small obstruction that another image may compensate for  would be good be if PRON could combine the low quality image to create on high quality image  be there any gan that could do this   
__label__data-visualization __label__gnuplot PRON want to plot a few set of datum point on the same x  axis that have different unit  how can PRON set different ax for each incompatible quantity   gnuplot have the ability to have 2 x and 2 y  ax per plot  the additional ax be call x2 and y2  accord to gnuplotinfo faq  here be a demo of PRON be use   also have a read of help plot from within the gnuplot program  
__label__mesh __label__projection PRON have a mesh which represent a 0genus model  PRON goal be to construct a homeomorphism from that mesh to PRON bounding sphere   PRON be try to understand a paragraph in httpciteseerxistpsueduviewdocdownloaddoi1011422762ampreprep1amptypepdf   the basic idea  of  the algorithm to project a 0genus mesh on the sphere  refer below  be to let the object grow towards PRON convex  hull and then project PRON directly onto the sphere      in PRON implementation PRON use a variation of this principle   PRON first compute a voxelization of the give model  the  gradient of the correspond volumetric distance function  be then use to guide the vertex of the give mesh towards  a convex configuration  this process can be consider as  some kind of  inverse  deformable surface technique   here be the link to the article PRON cite   httpdesignosueducarlsonhistorypdfsshapekentpdf  can someone please explain PRON   how to compute a voxelization of a give model   what be the  correspond volumetric distance function    how to use PRON gradient to  guide the vertex of the give mesh to a convex configuration    once PRON have get a  convex configuration   projection on the sphere be easy   any open source implementation be welcome   here be some element of answer to the three question and reference to alternative method for spherical parameterization   1  how to compute a voxelization of a give model   what PRON mean   PRON mean embed PRON surface into a 3d voxel grid  and determine which voxel have an intersection with some triangle of the surface   how to do that   there be several method to do that  the simple one be for each triangle  iterate on the voxel inside the bounding box of the triangle  and test for each voxel whether PRON have an intersection with the triangle  more sophisticated implementation can use the gpu  that be well suited to this kind of thing  1   PRON be also possible to use some variant of the bresenham algorithm  2  to  rasterize  the triangle in 3d  2  compute the distance function  what PRON mean  start from a voxel grid with voxel that be either empty or that have an intersection with the surface  obtain at step 1    populate each empty voxel with the distance between PRON center and the surface   how to do that  there be an efficient algorithm  see survey in  3   that operate by several  sweeping  over the volume  PRON be quite easy to implement and very efficient   3  use the gradient to guide the vertex of the mesh to a convex configuration  this mean solve an evolution equation that will morph the initial surface to a convex surface  the evolution equation use at each step the gradient of the distance to the initial surface   alternative for spherical parameterization  there be other method that PRON may use  on the theory side  the approach in  5  be elegant  but probably difficult to implement   if PRON want a method that be as simple as possible  cut PRON mesh into two half and parameterize each half use planar tutte  floater parameterization  6  then glue the half  PRON can then further optimize the mapping to make the seam between the half disappear   there be an implementation of this approach in PRON graphite software  7   reference    1  httpsdevelopernvidiacomcontentbasicsgpuvoxelization   2  httpsstackoverflowcomquestions21663613trianglerasterizationbresenhamsin3d   3  3d distance field  a survey of technique and application  jones etal  ieee trans  on vis  and computer graphics  2006   4  mathnyuedubkleinermeanconvexflowpdf   5  fundamental of spherical parameterization  gotsmann etal  acm siggraph 2003   6  parameterization and smooth approximation of surface triangulation  floater  1996   7  implementation of spherical parameterization in graphite ver  2   httpsgforgeinriafrfrsdownloadphpfile33290graphite2a6tarbz2  src  package  ogf  parameterizer  algos  sphericalstretchmapparameterizerhcpp 
__label__fluid-dynamics __label__boundary-conditions __label__finite-volume PRON be solve  or try to   an advection  diffusion  reaction 1d equation  the later have a mixed boundary condition in PRON left boundary  and a dirichlet bc in PRON right boundary   to solve the mixed bc  first PRON discretize the mixed bc as follow    2ka  dxtp  tw   htw  tref   0  where k be the difussion coeficient  a be the volume area  tp be the temperature at boundary volume centroid p  tw be the temperature at  west face of boundary volume  h be the regulation coeficient and tref be the room temperature   and the question be  how to go on   by the discretization just show  PRON get that tp and tw be both unknown   what PRON have to do then   thank a lot    
__label__ode __label__software PRON have a small system of stiff ode describe a chemical reaction  the right  hand side be quite complicated  as well as the jacobian  this equation will be solve many time with different initial condition  give that PRON have already select a stiffness  orient method  be PRON realystic to create more effective code than some existing solver  for example  be PRON worth try to invest into optimize lu  decomposition for PRON small system by loop unrolling  or whatev   in this thread some general idea be give  but maybe there be some tip about stiff ode solution   update  there be only three equation  PRON be use fortran and compare the efficiency of radau  implicit rk   rodas  rosenbrock  and dlsode  bdf method  code  rodas seem to work a bit faster   PRON assume PRON have already verify that PRON system be stiff  otherwise PRON  may be pay a significant performance penalty for use an implicit solver   the next thing PRON suggest be to gather more datum from the ode solver PRON be use   typically this kind of data be available in some additional output variable or  by turn on some kind of verbose output flag   total number of time step in the solution   minimum and maximum time step size   number of time in the solution where the time step be change   number of time the jacobian be update and factor   PRON say PRON rhs be quite complicated so PRON be guess PRON problem may be  very nonlinear  the algorithm will change time step size either because the  problem be very nonlinear and the algorithm be have difficulty obtain  a converged solution or because PRON do not meet the specify accuracy  requirement  have PRON experiment with different accuracy tolerance    if there be a large number of jacobian formation and factorization  those routine  may be candidate for optimization   PRON may want to implement a simple fix time  step backward euler  solver and see how the solution time compare with the off  the  shelf solver   finally  if PRON able to get some insight about the characteristic of PRON  ode system from the above experiment  PRON could try to profile PRON code   if PRON be run on linux  PRON can use gprof or valgrind  callgrind   httpvalgrindorgdocsmanualclmanualhtml  to  obtain datum about where the time be be spend  
__label__python __label__tensorflow __label__image-classification __label__inception PRON be try to retrain inception final layer on new set of image  PRON be use docker tensorflow image on windows environment  below be the step that PRON be follow   install docker toolbox for window   pull the tensorflow docker image   docker run it gcriotensorflowtensorflowlatestdevel  link tensorflow to the folder on host machine where new set of image be store   docker run it v    pwdtffile  sample gcriotensorflowtensorflowlatestdevel  tffile  sample contain the set of image on which PRON want to retrain the model   cd tensorflow  git pull  then PRON be run the below script   python tensorflow  example  imageretraining  retrainpy   bottleneckdirtffiles  bottleneck   howmanytrainingsteps 500   modeldirtffiles  inception   outputgraphtffile  retrainedgraphpb   outputlabelstffiles  retrainedlabelstxt   imagedir tffiles  sample  inception model be be download properly but after that PRON be get an error which say   tensorflowpythonframeworkerrorsimplpermissiondeniederror  tffile  sample  appdata  local  application data  operation not permit  below be the exact error that PRON be get   root54808ee651a1tensorflow  python tensorflow  example  imageretraining  retra  inpy    gt  bottleneckdirtffiles  bottleneck    gt  howmanytrainingsteps 500    gt  modeldirtffiles  inception    gt  outputgraphtffiles  retrainedgraphpb    gt  outputlabelstffiles  retrainedlabelstxt    gt  imagedir tffiles  sample  traceback  most recent call last    file  tensorflow  example  imageretraining  retrainpy   line 1326  in  ltmodulegt   tfapprunmainmain  argvsysargv0    unparsed   file  usr  local  lib  python27dist  package  tensorflow  python  platform  apppy   line 48  in run   sysexitmainsysargv1   flagspassthrough    file  tensorflow  example  imageretraining  retrainpy   line 988  in main  flagsvalidationpercentage   file  tensorflow  example  imageretraining  retrainpy   line 144  in createimagelist  subdir   x0  for x in gfile  walkimagedir    file  usr  local  lib  python27dist  package  tensorflow  python  lib  io  fileiopy   line 518  in walk  for subitem in walkospathjointop  subdir   inorder    file  usr  local  lib  python27dist  package  tensorflow  python  lib  io  fileiopy   line 518  in walk  for subitem in walkospathjointop  subdir   inorder    file  usr  local  lib  python27dist  package  tensorflow  python  lib  io  fileiopy   line 518  in walk  for subitem in walkospathjointop  subdir   inorder    file  usr  local  lib  python27dist  package  tensorflow  python  lib  io  fileiopy   line 499  in walk  list  listdirectorytop   file  usr  local  lib  python27dist  package  tensorflow  python  lib  io  fileiopy   line 478  in listdirectory  compatasbytesdirname   status   file  usr  local  lib  python27dist  package  tensorflow  python  framework  errorsimplpy   line 473  in   exit    capi  tfgetcodeselfstatusstatus    tensorflowpythonframeworkerrorsimplpermissiondeniederror  tffile  sample  appdata  local  application data  operation not permit  
__label__machine-learning __label__unsupervised-learning __label__detecting-patterns PRON have a customer purchasing dataset and the datum set be from a retailer have an online store and offline store  so  customer have two option in PRON shopping channel  online or offline  in an online shopping  there be a shipping fee however if a basket size be large than  50 there be no shipping fee   PRON find piece of evidence that customer be try to add some of item to make PRON basket size large than  50 when PRON basket be near and a little bit below the  50  because PRON shipping fee can be waive by do that   in this situation  PRON be try to identify and characterize item that be purchase only because of the shipping threshold by use a machine learn algorithm   if there be no shipping threshold   50  the customer would not purchase the item  but PRON purchase some item to make PRON basket size large than  50  PRON have not observe those kind of item  add item because of the shipping threshold    be there any machine learn algorithm that PRON can identify those kind of item   PRON think PRON need to use some of unsupervised machine learn algorithm   another challenging part be that each customer have different characteristic so PRON probably need to consider PRON as well  how can PRON detect those kind of item    since PRON comment would not fit  PRON will answer the question   PRON think this problem will be even more better solve if PRON have the web datum as well  say after add the require necessity in the cart the customer will check the total amount  and if PRON be below 50   the person will add some more item  so check this can give PRON a good clue   another data PRON must have for good guess be the order in which the person have add item to PRON cart  this will also provide PRON an important clue about the cheap item the customer be add to cart to try to cross the minimum threshold   PRON will also have to segregate cheap item from pricey one for a customer  this datum with the above 2 approach be a definite giveaway   this may be a personal opinion  but whenever PRON need to cross the shipping fee threshold PRON usually add item which PRON have already buy the previous time to escape shipping fee  PRON will need more datum to see if this be true in most case   and if PRON do not have any of the above datum  the pure machine learning approach would be to pick an item which be costly and somewhat popular and an item which will str together people with likewise interest  say  a great book on ml be this item   PRON find out all the customer who buy this book  check all the similarity between all the other thing buy by the customer  like say one customer add a book on ai after this while another buy a book on python  so these be relate by computer science  go on until PRON find all the thing that be generally dissimilar between all the customer  since now PRON be buy to cross the threshold and each will buy accord to PRON own requirement without a common interest in mind  there PRON have all the datum PRON need   or PRON can use the converse of this approach  find a non  significant thing like sugar  string together all the customer who buy this item  check PRON interest to see if PRON have anything to do with sugar  if not send PRON to one group  now in this group  match the interest of the customer on other item  if dissimilar PRON get a good idea that this item be buy to cross threshold   PRON understand this be a kind of opinionat answer  but PRON think these be trade secret not reveal by company  so PRON have to figure out algorithm PRON  also  simple machine learning will not work  lot of logical programming be also require  and PRON also think PRON need to have a good understanding of human psychology work  PRON can interview PRON friend and family about what PRON would do and get a general intuition  hope this help  
__label__machine-learning __label__python PRON be try to use arma  arima with the statsmodel python package  in order to predict the gas consumption  PRON try with a dataset of this format   use only the gas column   from pandastseriesoffset import   armamod20  smtsaarmajanuaryfebgas  m3       53fit    predictsunspot  armamod20predict2012  01  13    2012  01  14   dynamic  true   ax  januaryfebix2012  01  13 0000002012  01  15 220000gas  m3plotfigsize128    ax  predictsunspotsplotax  ax  styler   labeldynamic prediction     axlegend     why be the prediction so bad   PRON be not an expert on time series  but PRON have a general advice  may PRON suggest PRON to try other package  and various parameter  to see  if there be any difference in result   also  unless PRON have to use python  PRON would recommend to take a look at the r s extensive ecosystem for time series analysis  see httpwwwstatmethodsnetadvstatstimeserieshtml and httpcranrprojectorgwebviewstimeserieshtml   in particular  PRON may want to check the standard stat package  include function arima   and arima0   as well as some other package  fitarma  httpcranrprojectorgwebpackagesfitarma   forecast  httpcranrprojectorgwebpackagesforecast  and education  focus farma  cranrprojectorgwebpackagesfarma   to mention just a few  PRON hope this be helpful   gas usage have a daily cycle but there be also secondary weekly and annual cycle that the arima may not be able to capture   there be a very noticeable difference between the weekday and saturday datum  try create a subset of the datum for each day of the week or splitting the datum into weekday and weekend and apply the model   if PRON can obtain temperature datum for the same period check if there be a correlation between the temperature and gas usage   as aleksandr blekh say r do have good package for arima model 
__label__machine-learning __label__r __label__python which one will be the dominate programming language for next 5 year for analytic  machine learning  r verses python verses sas  advantage and disadvantage   there be a great survey publish by oreilly collect at strata   PRON can see that sas be not widely popular  and there be no reason why that should change at this point  one can rule that out   r be barely ahead of python  43  vs 41   PRON can find many blog express the rise of python in data science  PRON would go with python in the near future   but 5 year be a very long time  PRON think golang will steal a lot of developer from python in general  this may spill over to data science usage as well  code can be write to execute in parallel very easily  which make PRON a perfect vehicle for big data processing  julia s benchmark for technical computing be even more impressive  and PRON can have ipython like stuff with ijulia  hence python be likely to lose some steam to both  but there be way to call julia function from r and python  so PRON can experiment use good side of each   due to the very big increase in big data  pun intend  and the desire for robust stable scalable application PRON actually believe PRON to be scala   spark will inevitably become the main big data machine learning tool  and PRON be main api be in scala   furthermore PRON simply can not build a product with scripting language like python and r  one can only experiment with these language   what scala bring be a way to both experiment and produce a product   more reason  think functionally  write fast code and more readable code  scala mean the end of the two team development cycle  so good product ownership  more agile cross functional team  and half as many employee require to make a product as PRON will no longer need both a  research  team and an engineering team  data scientists will be able to do both   this be because scala be   a production quality language  static typing  but with the flexibility of dynamic typing due to implicit  interoperable with rest of java world  so apache commons math  databases  cassandra  hbase  hdfs  akka  storm  many many database  more spark component  eg graphx  sparkstream   step into spark code easily and understand PRON  also help with debug  scala be awesome   amazing ide support due to static typing  property base test with scalacheck  insane unit testing  very concise language  suits mathematician perfectly  especially pure mathematicians   a little more efficient as compile not interpret  python spark api sit on scala api and therefore will always be behind scala api  much easy to do mathematics in scala as PRON be a scalable language where one can easily define dsl and due to be so functional  akka  another way other than storm to do high velocity  pimp PRON library pattern make add method to spark rdd really easy 
__label__computational-chemistry PRON want to extract the geometry for every single step of a gaussian09 optimization from the log file  PRON need the result in cartesian xyz format  either in one file or in multiple file   first PRON try to do this in open babel  but when PRON run  obabel ig09 gaussianoutputlog oxyz m o geometryxyz  the program seem to fetch only the last geometry   any other suggestion for a quick solution   
__label__finite-difference __label__interpolation __label__spectral-method __label__b-spline the context of PRON question be how to compute high order derivat on direct numerical simulation of turbulent channel flow  PRON be of particular interest for fluid dynamic and turbulence research   PRON want to find the coefficient of a b  spline curve give the value of the result function on a set of collocatioin point  be there any way to do PRON without resort to solve a linear system each time PRON need to find the coefficient   PRON specific case be to interpolate use a 7th order b  spline scheme with knot give  but the value be set on  greville abscisae    the knot be give in this address   httpturbulencephajhuedudocschannelyknotstxt  and the collocation ponit in this   httpturbulencephajhuedudocschannelytxt  though not necessary  PRON can query any example of such coefficient on this website  just input the collocation point on this website   httpturbulencephajhueduwebqueryqueryaspx  the quantity PRON be seek to interpolate be the velocity field for the channel dataset   PRON have never work with b  spline before  so PRON may be miss something  but PRON have not find anywere a way to compute the coefficient from the collocation point  and this be bother PRON   solve a linear system for each line  in this case  a 512 x 506  would be bothersome  since PRON would need to do this for each vertical line on each snapshot of the simulation  and PRON would like to avoid this as much as possible  but PRON be not find any other alternative   
__label__finite-element __label__mesh-generation when read literatur about finite element method  the term  hang node  can often be encounter  could anyone tell PRON what indeed be a hanging node   the following picture illustrate a mesh with a hanging node and a mesh contain no hanging node   usually with a finite element mesh the vertex be share with PRON other neighbouring element  but the circled node do not belong to the bottom triangle  PRON call this node a hanging node  this commonly occur during the process of adaptive mesh refinement  hang node be either remove  as be do in the above picture by connect PRON to another vertex and thus create two new element  or by impose constraint on the system  
__label__machine-learning __label__neural-network PRON be implement a cnn with numpy and scipy to solidify PRON understanding  but PRON be encounter a rather strange problem   PRON layer be  input   conv   pool   fc  here be the relevant function   def unpacktheta  theta  filtersize  numfilters  pooleddim  outputsize    a  filtersize   2  numfilters  b  a  numfilter  c  b  pooleddim   2  numfilters  outputsize  d  c  outputsize  return  npreshapetheta0  a    filtersize  filtersize  numfilters     npreshapethetaa  b    numfilters     npreshapethetab  c    pooleddim   2  numfilter  outputsize     npreshapethetac  d    outputsize     def getconvolution  feature  wc  bc  numfilters  numimages  filteredsize    conv  npzerosfilteredsize  filteredsize  numfilters  numimages    for imagenum in rangenumimage    image  feature      imagenum   for filternum in rangenumfilter    filterlayer  nprot90wc      filternum   2   conv      filternum  imagenum   sigmoidconvolve2dimage  filterlayer   valid    bcfilternum    return conv  def getpooled  feature  poolsize  pooledsize  filteredsize  numfilters  numimages    ret  npemptypooledsize  pooledsize  numfilters  numimages    poolfilter  nponespoolsize  poolsize    poolsize   2  for i in rangenumfilter    for j in rangenumimage    pooledlayer  convolve2dfeature      i  j   poolfilter   valid    for x in range0  filteredsize  poolsize    for y in range0  filteredsize  poolsize    retx  poolsize  y  poolsize  i  j   pooledlayerx  y   return ret  def getcost  theta   arg      initialize variable  feature  inputsize  filtersize  numfilters  poolsize  outputsize  reg  arg  filteredsize  inputsize  filtersize  1  pooledsize  filteredsize  poolsize   wc  bc  wr  br   unpackthetatheta  filtersize  numfilters  pooledsize  outputsize   numimages  npsizefeature  0   actualvalue  npeyeoutputsizefeature    0    feature  npreshapefeature    1    2550   inputsize  inputsize  numimages      forward propagation   convolution  conv  getconvolutionfeature  wc  bc  numfilters  numimages  filteredsize    mean pool  pool  npreshapegetpooledconv  poolsize  pooledsize  filteredsize  numfilters  numimages    1  numimages     logistic regression  calcvalue  sigmoidpooledtransposedotwr   br     calculate cost  cost  npsumactualvalues  nplogcalcvalue    1  actualvalue   nplog1  calcvalue    numimages  cost   reg   npsumwc   2   npsumwr   2     2  numimages     back propagation   print wr   print br  outputerror   calcvalue  actualvalue   poolederror  wrdotoutputerrortranspose     poolederror  npreshapepoolederror   pooledsize  pooledsize  numfilters  numimages    converror  npemptyfilteredsize  filteredsize  numfilters  numimages    converrorfilter  nponespoolsize  poolsize     poolsize   2   for i in rangenumfilter    for j in rangenumimage    converror      i  j   npkronpoolederror      i  j   converrorfilter   converror   conv   1  conv     gradient  wrgrad  pooleddotoutputerror   brgrad  npsumoutputerror  0   wcgrad  npzerosfiltersize  filtersize  numfilters    bcgrad  npemptynumfilter   for i in rangenumfilter    bcgradi   npsumconverror      i     for j in rangenumimage    filterlayer  nprot90converror      i  j   2   wcgrad      i    convolve2dfeature      j   filterlayer   valid    wrgrad  numimages  wcgrad  numimages  brgrad  numimages  bcgrad  numimages  wrgrad   reg  wr  numimages  wcgrad   reg  wc  numimages  global costcache  costcache  cost  return  cost  npconcatenatewcgradflatten    bcgradflatten    wrgradflatten    brgradflatten     axis0    PRON implement a numerical gradient to verify if PRON be return the right value  and PRON be  however  PRON softmax regression seem to be predict the same value across the entire batch when PRON do sgd   however  when PRON use scipyminimize function  PRON give PRON different value for softmax regression  any help be greatly appreciate   
__label__pde __label__nonlinear-equations __label__time-integration __label__operator-splitting PRON have recently come across the strang splitting and have some question  for the differential equation of the form    dy  dt   l1  l2y  strang splitting implement the time splitting as     begineqnarray    tilde y1  amp    amp  el1 delta t2  y0    amp   amp  bar y1  el2 delta t  tilde y1    amp   amp  y1  el1 delta t2  bar y1   tilde y2  amp    amp  el1 delta t2  y1    amp   amp  bar y2  el2 delta t  tilde y2    amp   amp  y2  el1 delta t2  bar y2      tilde yn  amp    amp  el1 delta t2  yn1     amp   amp  bar yn  el2 delta t  tilde yn    amp   amp  yn  el1 delta t2  bar yn   endeqnarray       however  from the equation above  PRON be obviously that the half time step with  y1  and  tilde y2  can be combine into a single time step  so PRON be equivalent to      begineqnarray    tilde y1  amp    amp  el1 delta t2  y0    amp   amp  bar y1  el2 delta t  tilde y1   tilde y2  amp    amp  el1 delta t  bar y1    amp   amp  bar y2  el2 delta t  tilde y2      tilde yn  amp    amp  el1 delta t  bar yn1     amp   amp  bar yn  el2 delta t  tilde yn   yn  amp    amp  el1 delta t2  bar yn  endeqnarray       this look like that PRON be exactly the same as the first order time splitting scheme except the first and last half time step  and the computation be fast with the reduction  be PRON miss something here   also  what be the time splitting method with four order  how do PRON look like explicitly  PRON be look for some numerical algorithm for solve nonlinear schroedinger equation   PRON can combine those two step since both of PRON involve the same operator   l1     for the fourth  order method PRON recommend that PRON check out PRON answer to a similar question on stack overflow and also section 4 in the follow paper  see also the discussion below    yoshida  h  1990   construction of high order symplectic integrator  physics letters a  1505–7   262–268  httpdoiorg1010160375960190900923  let  a  l1  and  b  l2 the analysis of this type of scheme be do use the baker  campbell  hausdorff  bch  formula  which tell PRON that the expression for  c in  beginequation   expt a  expt b   expt c    endequation   be give in term of lie bracket involve  a and  b  recall that   a  b   ab  ba    beginequation   c  a  b   fract2  lefta  bright    fract212  left  lefta  lefta  brightright   leftb  leftb  arightright  right    fract324  lefta  leftb  leftb  arightrightright    dotsb  endequation   the bch formula for the particular case of the strang splitting be give by  beginequation   explefttfract2  aright  expleftt bright  explefttfract2  aright   expleftt c1  t3 c3  t5 c5  dotsb right    endequation   where  beginalign    c1  amp a  b    c3  amp frac124  lefta  lefta  brightright   frac112  leftb  leftb  arightright      ampdotsb  endalign    in general   c3  do not vanish and this imply that the scheme be second order  in order to construct high order method  one can choose a splitting such that the appropriate term in the bch formula vanish  the paper by yoshida cite above discuss this in depth    particular choice of  a and  b will yield symplectic method but from the discussion above PRON see that the order of the scheme do not necessarily have to do with PRON be symplectic   additional useful reference be   hairer  e  lubich  c   amp  wanner  g  2010   geometric numerical integration  vol  31   book  springer  heidelberg   and  leimkuhler  b   amp  reich  s  2004   simulate hamiltonian dynamic  vol  14   book  cambridge university press  cambridge    this look like that PRON be exactly the same as the first order time splitting scheme except the first and last half time step  and the computation be fast with the reduction  be PRON miss something here   PRON be exactly correct  as other have already say   also  what be the time splitting method with four order   there be many high  order splitting scheme  some develop very recently   this be the most comprehensive list PRON know of and include various method of order 4  
__label__machine-learning __label__dataset __label__text-mining __label__search PRON recently read similarity measures for short segments of text  metzler et al     PRON describe basic method for measure query similarity  and in the paper  the datum consist of query and PRON top result  result be list of page url  page title  and short page snippet   in the paper  the author collect 200 result per query   when use the public google api to retrieve result  PRON be only able to collect 4  10 result per query   there be a substantial difference between 10 and 200   hence  how much datum be commonly use in practice to measure query similarity  eg  how many result per query    reference be a plus    when use the public google api to retrieve result  PRON be only able to collect 4  10 result per query   here be how to get more than 10 result per query  httpssupportgooglecomcustomsearchanswer1361951hlen  google custom search and google site search return up to 10 result per query  if PRON want to display more than 10 result to the user  PRON can issue multiple request  use the start0  start11  parameter  and display the result on a single page  in this case  google will consider each request as a separate query  and if PRON be use google site search  each query will count towards PRON limit   there be other search engine api as well  eg  bing  
__label__classification __label__predictive-modeling __label__xgboost PRON be try to build a 0  1 classifier use xgboost r package  PRON question be how prediction be make  for example in random forest  tree  vote  against each option and the final prediction be base on majority  as regard xgboost  the regression case be simple since prediction on whole model be equal to sum of predcition for weak learner  boost tree   but what about classification   do xgboost classifier work the same as in the random forest  PRON do not think so  since PRON can return predictive probability  not class membership    the gradient boost algorithm create a set of decision tree   the prediction process use here use these step   for each tree  create a temporary  predict variable   apply the tree to the new datum set   use a formula to aggregate all these tree  depend on the model   bernoulli  11  expintercept  sumtemporary pr      poisson  gamma  expintercept  sumtemporary pr    adaboost  1 1  exp2intercept  sumtemporary pr      the temporary  predict variable  be a probability  have no sense by PRON own   the more tree PRON have  the more smooth be PRON prediction   as for each tree  only a finite set of value be spread across PRON observation   the r process be probably optimise  but PRON be enough to understand the concept   in the h2o implementation of the gradient boost  the output be a flag 01   PRON think the f1 score be use by default to convert probability into flag  PRON will do some search  test to confirm that   in that same implementation  one of the default output for a binary outcome be a confusion matrix  which be a great way to assess PRON model  and open a whole new bunch of interrogation    the intercept be  the initial predict value to which tree make adjustment   basically  just an initial adjustment   in addition  h2ogbm documentation 
__label__algorithms __label__convergence __label__monte-carlo PRON be struggle with convergence criterion when perform a monte carlo simulation on a uniform distribution  any help would be much appreciate   say PRON want to sample uniformly a 1d interval  for the sake of simplicity    PRON use a random number generator  in fortran  to draw x value between 0 and 1  then  how do i choose the number of point n such that PRON have a good sampling   PRON know the expect mean   05  and PRON can easily compute the average of the position of PRON mc point  ie μ   x1    xn   n PRON be think that PRON could define a simple criterion such that  μ   lt  1  for instance  in order to decide if n be large enough or not   please can anyone tell PRON if there be a good way to figure this out   thank a lot   PRON may not be able to determine the exact number of point require to obtain 1  error  but PRON can estimate the order of magnitude of point need to obtain this accuracy   monte carlo converge at a rate  ofrac1sqrtn  where  n be the sample size   this mean the absolute error be bound as  mumuapproxltfraccsqrtn  where c be some constant   roughly speak  this mean for every additional digit of accuracy  PRON will need 100 time more point   since PRON need two digit of accuracy to obtain 1  accuracy  ie because 1  be equivalent to 001   PRON will somewhere on the order of  100  10010  5  point   this estimate be not a guarantee that PRON will obtain 1  accuracy with exactly  10  5  point because PRON do not know the constant c  this constant depend on the property of the function PRON be sample and PRON order of magnitude may not be estimatable   PRON have not come across an  explicit  formulation which can define the optimum number of sample for mc simulation   typically  a good way would be to estimate the variance of simulation with each sample size   n  as  sigma2n then the standard error for  n sample would be  fracsigmansqrtn PRON could repeat the error estimate by increase PRON sample size and stop at that sample size for which the error hit a certain threshold  
__label__neural-network __label__r __label__predictive-modeling __label__regression __label__random-forest PRON be try to predict sale quantity base on attribute of the item  sale be aggregate by week wise and prediction be also do by week wise  PRON be have large number record with zero sale quantity than compare to positive salesfor 20 positive sale 250 zero sale record be therei want to increase PRON training datum by add small value to sale quantity and combine with total record   at present PRON be use random forest and neural network i be not get any good result  please correct PRON if anything be not considerable  PRON have try smote for categorical response variable scenario but never try on continuous response variable but PRON think that should not matter  PRON can run oversampling or undersampling base on the predictor variable  if PRON be use r  PRON can use dmwr library and use the smote function  in python PRON get to use imblearnoversamplingsmote  avoid neural network for weekly sale datum  there simply be not enough data point to make PRON work   the technique work great with image or video datum  PRON can recognize number  or cat video  but PRON need something in the neighborhood of 200000 data point to train PRON network   imagine the cost and complexity of get 200000 weekly data point  that be something between 3000 and 4000 year of collect   
__label__neural-networks __label__artificial-neuron __label__reference-request please if PRON know good research about limitation of neural network give PRON a link  there be a lot paper about expressive power of neural network but PRON need research about limitation  PRON know about the universal approximation theorem  but PRON need research that enplane  what function neural network can not learn computational efficient or can not learn by gradient algorithm   one of the important qualification of the universal approximation theorem be that the neural network approximation may be computationally infeasible    a feedforward network with a single layer be sufficient to represent any function  but the layer may be infeasibly large and may fail to learn and generalize correctly    ian goodfellow  dlb  PRON can not think of any function that PRON would definitively declare as unlearnable  but neural network have many problem  consider adversarial example and adversarial patch  which highlight the poor generalization go on under the hood of recent advance in computer vision   neural networks be also inherently limit by the innate prior bake into PRON architecture and the sample density of PRON training datum  check out this recent discussion at stanford s ai salon between yann lecun and christopher manning on innate prior if that be the kind of limitation PRON be talk about   this answer depend very much so on the type of neural network and algorithm use for training   if PRON be use gradient descent on a neural network of one input layer  one output layer  and no hidden layer there be many function that PRON can not learn  one simple one be the xor function  due to the fact that xor be not linearly separable  PRON can not be represent by a neural network with no hidden layer   if PRON be use neat to build recurrent neural network then all function     can be represent give enough time and datum  this be due in part to the fact that recurrent neural network be turing complete   one of the big cause for limitation when use neural network be base on the difficulty of interpretation as to what the network be do  the network be gradually build up an understanding of the function as PRON go from the input layer to the output layer  but PRON be very difficult for PRON to understand this building up process and interpret what the neural network be attempt to do  this make PRON very hard if not impossible to manually tweak PRON neural network in a meaningful way   another limitation be the need for training  in large amount  in order to have a meaningful representation of PRON datum  neural network have a tendency to need large amount of datum before converge to a meaningful hypothesis space  this have result in clever algorithm to generate training datum without need human interaction  such as generative adversarial networks  but the underlying problem remain     not all function can be compute by neural network  however all computable function can be  an example of an uncomputable function be the mapping of all program from the program to whether or not this program will halt  the halting problem    to answer the question  PRON want to go a small sidestep to the first neural network ever invent  the mcculloch  pitts neurons from 1943  from today s perspective PRON may be surprising  but these primitive neuron be tur complete  that mean PRON can compute any algorithm like a  neural turing machine  ntm    the reason be  because the mcculloch  pitts neuron be implementation of logicgate  and  or  not  which can be connect to any function  include a counter  a for loop and a prime  number  checker   neural network have no limitation from a computing perspective  because today s neural network be more powerful than the former mcculloch pitts neuron  the limitation be only the gradient descent algorithm for find the weight  the learn algorithm be not very efficient and there be no good alternative available  in the error map  the problem must be reduce to zero  error  that mean the neural network output the right value  gradient descent be a heuristic to follow a certain path in the maze in the hope  that the door out be at the end  in many case  gradient descent find only an impasse  perhaps an example   a neural network should print out  if the input number be a prime number or not  with gradient descent the error rate can reduce to the value  54   in theory  the neural network be able to find the prime  number algorithm  that mean to reduce the error rate down to 00  but the gradient descent algorithm do not know how to reduce the error  rate from  54  to  53  and further  PRON be iin a local minimum  the reason be  that the error map be under the fog  of  war  to uncover the information cpu  intensive trialamperror search have to be do   bypass the problem be easy  the programmer have to implement the problem  solve algorithm by hand in a normal programming language  and use a neural network only for find a parameter in PRON algorithm  
__label__r __label__pandas PRON often find PRON write code like the follow  oversimplfi example   df  readcsvcustomerdataexportcsv    df2  dfquerydate  gt   2017  01  10     datum  dffilteredgroupbytransactionidsum    plotdata  pivottabledata  columnsweekday   rowsnitem     etc etc  basically the problem be that while PRON be relatively easy to come up with semantic name for column  as random variable  PRON be hard for PRON to come up with meaningful name for each step of transform dataframe  additionally PRON prefer to have short name to make the code easier to type   work in jupyter notebook  the tab  completion be not the good    what be some good practice that people follow with this kind of thing   why not give PRON a name describe PRON purpose   dfcsv  readcsvcustomerdataexportcsv    dfdatefiltered  dfquerydate  gt   2017  01  10     dfgroupedbytransid  dfdatefilteredgroupbytransactionidsum     cleanup  rmdfcsv  dfdatefiltered   plotdata  pivottabledfgroupedbytransid  columnsweekday   rowsnitem   
__label__predictive-modeling __label__regression __label__bayesian-networks bayesian network deal with probability  so how do one use PRON for predict an quantitative result  there be couple of research paper that PRON come across that use tree augmented naive bayes  but could not understand how PRON function to forecast a quantitative outcome variable   
__label__numerical-analysis __label__numerical-modelling how to determine if a pde be structure  preserving  energy，mass  conserve   be there some standard in judge the preserve  structure   or rather  how to derive the formulation of energy  preserve form of a pde   PRON be new to this field  PRON be long for receive some reference  some book   representative paper or some expert  homepage  about these topic   a standard reference for basic topic in partial differential equation be the textbook by lawrence c evans  partial differential equations   PRON chapter 86 discuss noether s theorem in a very condensed manner   the question be very broad  so PRON be difficult to pinpoint a specific reference  the wikipedia article on noether s theorem be probably a good start  include the reference give there   if a pde be derive from a principle of least action and if PRON have invariance  then some quantity be conserve  for instance  in mechanic  invariance with respect to time imply conservation of a certain scalar quantity  call  energy    invariance with respect to the origin of the frame imply conservation of a certain vector quantity  call  momentum   and invariance with respect to the orientation of the frame imply conservation of another vector quantity   angular momentum    there be a general theorem due to emmy noether that study the structure of these symmetry and conserved quantity   about the example of point mechanic  landau s course  1  be crystal clear and very complete  about emmy noether s general theory   2  be very enlighting and reasonably easy to read without require too much background    1  landau and lifshitz  course on theoretical physics  volume 1  mechanic   2  dwight neuenschwander  emmy noether s wonderful theorem  PRON think that mass convervation do not belong to the same category of preserve quantity  for instance in fluid dynamic PRON be explicitly enforce by the continuity equation   as bruno levy point out  certain class of physical system have well  know structural property  for example  consider a hamiltonian system   dot z  jnabla hz  where  j be a symplectic matrix and  h be the hamiltonian  the hamiltonian  h PRON be preserve  as be any other function  f such that the poisson bracket  h  f   nabla htop jnabla f  0  for all  z symmetries of these kind of system give rise to conservation law  and there be software package use symbolic algebra that can automatically discover some symmetry of arbitrary ode system  see also this list   hamiltonian system have other useful property not directly relate to symmetry or conservation law  for example  every equilibrium point be either a saddle point or a center  ie there be no asymptotically stable equilibria  the volume of a region of phase space be always preserve under hamiltonian flow  and many other  however  PRON be arguably not feasible to algorithmically determine whether some totally arbitrary system be hamiltonian if PRON be not already sure to begin with  especially when PRON take into account non  canonical poisson structure   other kind of system have different structural property  for example  consider a gradient flow   dot z  nabla fz   then  df  dt le 0  along the trajectory of this system  the heat equation be the prototypical example of a gradient flow  gradient flow have equilibria at extrema of  f and the stability of an equilibrium point depend on the eigenvalue of the second derivative of  f unlike hamiltonian system  a gradient flow can have stable equilibria   both hamiltonian system and gradient flow have interesting and useful structure  but in general there be no algorithm to take an arbitrary differential equation and find what class of system PRON belong to  PRON can of course rule out some possiblitie  for example  if the system clearly have an asymptotically stable equilibrium  PRON can not be hamiltonian  likewise  if the linearization of the system be not a symmetric matrix  PRON can not be a gradient flow by clairaut s theorem   moreover  there be always weird example from real application that defy any classification scheme  for example  linear pde be often classify as be either elliptic  hyperbolic  or parabolic  but there be numerous example that do not fit neatly into these category  the euler  tricomi equation change type within the spatial domain  the pde that describe thin  film flow be a mixed elliptic  hyperbolic system  etc  
__label__machine-learning __label__python __label__deep-learning __label__time-series __label__anomaly-detection PRON work on multivariate time series datum  PRON have sensor datum generate by a machine every time PRON be operate  datum set consist of machineidmachine of same model   hour  operate  measurement from various sensor  the machine start to degrade after operate for certain hour  PRON would like to find the hour after which there be step change after which the performance start to degrade   PRON want to do this use machine learn approach preferably and would like to plot the graph mark the deviation  which ml technique could be use for this approach   PRON have perform exploratory datum analysis where PRON could find the point at which there be deviation occurrence  now  PRON want to confirm this by run a model to detect the occurrence of step change  in the figure above  the decline start somewhere at 100 and decline gradually  now  be there any way PRON could find this pint through model   PRON greatly appreciate any link or suggestion to deal with this   thank in advance    anomalydetection be an open  source r package to detect anomaly  which be robust  from a statistical standpoint  in the presence of  seasonality and an underlie trend   a blogpost that introduce the package can be find here and more formal paper can be find here   PRON could model the process as a weibull distribution which be common for survival analysis and reliability engineering  there have be work on monitor the  health  of system use PRON  example be here and here   PRON would try use google s causalimpact package   PRON use  case be not causal inference exactly  but causalimpact rely on bayesian structural time series model  use the bst package  and have some good default that keep PRON from need to dive into bst immediately   basically  PRON fit a model to the first part of PRON datum  then forecast the rest  PRON see where PRON model deviate from the forecast   use bayesian model mean PRON can get error bound  so PRON can have a degree of confidence in the deviation   in PRON case  PRON would set PRON  intervention  point to wherever timestamp PRON want to separate PRON modeling data from PRON forecasting datum  then compare the forecast to the actual datum  look up  nowcast     here be a tutorial to get PRON start  and here be an introductory video  
__label__finite-element __label__pde __label__space-time-galerkin PRON have read some paper about time  fractional pde solve by finite element method  the time  fractional derivative be the caputo derivative define by     fracpartialalphaupartial talphafrac1gamma1alphaint0tfracpartial upartial sfracdst  salpha0ltalphalt1      now  PRON consider the following time  fractional diffusion equation  beginalign    ampfracpartialalphaupartial talphadelta u  finomega   ampu0onpartialomega   ampu0u0   endalign   where  omega be a convex and bounded domain   1what do mean the singularity of the time  fractional derivative   2in what case  the singularity of the time  fractional derivative can lead to low smoothness of solution  u  3what do mean the regularity of a function be not smooth about time   could anyone give PRON some explanation   
__label__apache-spark __label__pyspark what happen when PRON do repartition on a pyspark dataframe base on the column  for example  dataframerepartitionid    do this move the datum with the similar  PRON would  to the same partition  how do the sparksqlshufflepartition value affect the repartition   
__label__pde __label__computational-geometry __label__hyperbolic-pde the interested equation be advection  diffusion equation  one of the canonical example be navier  stokes equation  however  PRON would like to let the coefficient of diffusion constant go to zero   epsilon rightarrow 0   while mesh size   delta x  delta dy rightarrow 0   and time stepsdelta t rightarrow 0   decrease towards zero  then  PRON can obtain inviscid compressible equation which can be an example for the euler equation of gas dynamic  the following equation be scalar advection  diffusion equation in 2d cartesian coordinate     fracdfdt   ufracdfdx   vfracdfdy   epsilon left  fracd2fdx2   fracd2fdy2  right  the main interest be to find exact formulation for the euler equation of gas dynamic in cylindrical coordinate   r  theta with artificial diffusion part   edit  the main equation that PRON be interested in the euler equation of gas dynamic in 2d the cartesian vector form can be state as    beginpmatrix   rho   rho u   rho v   e  endpmatrixt  beginpmatrix   rho u   rho u2  p   rho u v   ue  p   endpmatrixx  beginpmatrix   rho v  rho u v   rho v2  p   ve  p   endpmatrixy  textbf0  here  conservation of mass  momentum in x and y direction and energy equation can be see in a matrix form  however  PRON would like to add artificial viscosity with small constant factor as in navier  stokes equation to smoothen the possible oscillation that will occur during numerical simulation  for this reason  PRON need to know the vector laplacian form in cylindrical coordinate for navier  stokes equation  then PRON can apply the above theory that be state   have a look at chapter 3 of the bachelor thesis  modellierung und simulation von  dispersionen in turbulenten strömungen  by manuel baumann   PRON be in german but the equation and picture can be understand   in chapter 311  the coordinate transformation that in particular affect the convection be explain   in chapter 321  the transform rans equation be formulate  set the right term to zero and PRON get the euler equation  equation  19  on page 29    for the euler equation  the main work will be the convective term that in polar coordinate write as  note that in polar coordinate  the coordinate unit vector  etheta   er depend on the angular coordinate  theta this mean  eg      partial  theta  utheta cdot etheta    partial  theta  utheta  cdot etheta  utheta cdot er     what make the additional term appear in the last equality of the equation above   as a reference  PRON can recommend  mechanics of fluid flow  by longwell   disclaimer  PRON be supervise the thesis mention above  
__label__machine-learning __label__neural-network __label__reinforcement-learning __label__q-learning PRON have be work on a tic  tac  toe assignment for PRON robot learning class  PRON be ask to program a tic  tac  toe game and assign   1 if x win  1 if o win and 0 PRON the game result with a draw  in part 1  PRON be tell to use q table and in part 2 PRON be tell to replace the q table with a neural network as the functional approximator   PRON be PRON understanding that both method should be achieve the optimal policy  can PRON confirm or deny PRON understanding    PRON be PRON understanding that both method should be achieve the optimal policy  can PRON confirm or deny PRON understanding   yes  PRON would expect a neural network for q learning to find the optimal policy  provide PRON remain stable the value estimate may be slightly inaccurate  but the result policy should be completely optimal  that be because in tic tac toe  all the value estimate should be 1  0 or  1  and the data be cleanly separate   PRON should be able to get a neural network to learn the optimal q table from the first experiment use supervised learning  in fact that would be a good test of whether PRON nn have capacity to learn that table    neural network add naively to q  learn agent be often not stable  in fact that be so common a problem in scale up rl agent that PRON have a name   the deadly triad   this be generally not solve by elegant mathematical change to the agent  but by some engineering trick   experience replay  save observation  s  a  r  s   and sample from this memory table later to train in mini  batch   alternating network  use an old frozen copy of the neural network to calculate  textmaxa   qsa for the td target  r  gammatextmaxa   qsa 
__label__optimization __label__constraints PRON mean  in handle boxed constraint   in term of stability  and more importantly  the numerical performance   PRON have already write some well  optimize and well  test c  cuda  c code for several unconstrained optimzation method   and PRON use augment largrangian to handle constraint  however  PRON look like bounded bfgs be quite different than the standard bfgs  PRON can not do PRON over PRON exist bfgs routine  PRON require almost a complete re  write  PRON do not really want to do that unless there be significant gain there   have anyone  with practical experience  can tell PRON if the rewrite be worth do or PRON can stick to the let lagrangian to handle PRON   btw  PRON usually use bfgs to solve mle type optimzation problem  where the evaluation of the objective function be costly   
__label__r use the r package  deducer   PRON save a graphic  chart  as an ep file   PRON can open the eps file  PRON be just a bunch of text   how do PRON re  generate the graphic  chart  from the eps file  preferably use deducer or r    eps be  encapsulated postscript   PRON mean for embed like an image in document  or send to printer  PRON can view PRON with a postscript document viewer  and there be free postscript document viewer for linux  windows  and mac os  ghostview  evince  etc etc   so although PRON can view the graphic once PRON have get a postscript document viewer  PRON can not load PRON into r as if PRON have just plot PRON  
__label__optimization __label__python __label__signal-processing __label__cvxpy PRON want to compare several estimation  reconstruction algorithm for lasso  and as a reference PRON be use cvxpy  but for some reason PRON get a mirror solution across the mid point   the following provide pseudocode for PRON code   generate a random signal of length 128  fourier transform  undersample  formulate the lasso problem and give PRON to  cvxpy  here be the code     problem datum   x  nparray    02  05  06  08  1    0    128  5   dtypecomplex    x  x  nprandompermutation128   1   n  128  a  npzerosnn   dtypecomplex    aprm32    npfftfftnpeyenprm32    b  xr    construct the problem    x  cvx  variablen   gamma  1  obj  cvx  minimizecvxnormax  b  2   gammacvxnormx  1    prob  cvx  problemobj   probsolve    and this be an example for a solution PRON receive     thank PRON  
__label__bigdata __label__apache-hadoop __label__apache-spark tom white s  hadoop  the definitive guide  have become a popular guide to the entire hadoop ecosystem and earn a reputation as provide both a broad survey  as well as cover individual aspect of hadoop in decent depth  have anyone thus far attempt to provide the spark equivalent   learning spark  lightning fast big data analytics be a fairly comprehensive book cover the core concept as well as the high level component involve in the spark stack  this be the book recommend by databrick for PRON spark developer certification as well   if PRON be interested more about the use case build use spark  PRON suggest advanced analytics with spark 
__label__linear-regression __label__gradient-descent __label__matlab PRON be write an algorithm to fit a sine wave  PRON want to have 4 parameter  amplitude  frequency  phase  amp  centre position   when PRON try to program with all 4 parameter PRON be not able to find a good fit  so PRON try to first make PRON work with each parameter separately and make the other 3 constant hardcoded value  PRON manage to make this work for all parameter except for the frequency  so PRON think the problem be the derivative of the error function wrt the frequency parameter  as see below all the other parameter of the sine be hardcod value except the frequency parameter   this be iterate for a large number of time  for i  1lengthx    computing hypothesis and error  trialyi   4  sinw2end   xi   4  2   errorend   errorend    trialyi   yi2   end  errorend   errorend    2lengthx     dw2  0   derivative value   derivative of error function with respect to frequency  w2   for i  1  lengthx   dw2  dw2   4  sinw2end   xi   4  2  yi    4  xi   cosw2end   xi   4     hardcod derivative  end  dw2  2  dw2  lengthx    normalising error  w2end  1   0   w2end   w2end  1   alpha  dw2   alpha be the update constant  PRON seem like the parameter be not converge on the right value with whatev alpha and initial value PRON give PRON  while the plot of the error with time be decrease but converge on a value  0   the problem seem to be that in the case of frequency parameter  the error function be not convex  and have multiple local minima of value great than 0  the gradient descent be converge to one of these minimum  the error will reduce to 0 only if the start estimate lie in the right convex basin   for the amplitude and center value parameter  the error function be convex  and for phase parameter  there be multiple local minima all of value 0  
__label__data-mining __label__classification __label__clustering PRON have no experience in datum science so this will be one of those question   PRON have datum from  100k purchase make via a webshop regard a catalogue of around  100 item  the history of purchase flatten out look like  item1 item2  itemn sex state  5  0  0  m  ny  25  15  0  f  il  0  1  1    ny  by play around with the datum  PRON can deduce simple fact like  90  of all purchase include at least 3 item1    if there be at least 4 of item2  PRON be likely that item3 be 0  or  60  of all customer from ny be male  but only 40  of those from il be   give the amount of combination and datum there be  the most obvious question  how can PRON approach wring out more information from a data set like the above  PRON be mostly interested in how one item do or do not entail inclusion of another   frequent item  set mining be what PRON be look for  PRON can see the tree structure of PRON frequent itemset and the association rule afterwards   for PRON datum PRON would suggest to look at the whole for a while to get a sense on what PRON have in hand  play with concept like probability distributions  entropy  etc would be really helpful in case PRON can reduce the size of PRON feature   pca also give PRON the opportunity of project PRON datum into a low  dimenstional space and PRON can see also plot show first several pc in 2d or 3d and get an impression about PRON datum   before all above PRON strongly suggest to see if PRON have miss value and if yes try to cope with PRON  
__label__complexity __label__image-processing PRON have a question  PRON need to calculate the computational complexity of image segmentation algorithm  can anyone please help PRON   for example  PRON have a screen  size picture with white background contain k randomly  position black object with random size  between 90  90 to 110  110  in PRON  PRON be go to calculate how long PRON take for a rapid current computer to segment all black item in PRON image   for example  use connected component labeling to segment the component  httpenwikipediaorgwikiconnectedcomponentlabel  one  passimage    m  nsizeimage    connect  zerosm  n    mark  value   difference  increment   offset   1  m  1  m    index      noofobject  0   for i  1m   for j  1n   ifimagei  j1   noofobjects  noofobject  1   index     j1m  i     connectedindexmark   while isemptyindex   imageindex0   neighbors  bsxfunplus  index  offsets     neighbor  uniqueneighbor       index  neighborsfindimageneighbors      connectedindexmark   end  mark  mark  difference   end  end  end  if PRON have a detailed description of the algorithm  preferably pseudocode   then calculate the complexity be simply a matter of go through PRON and count operation   if PRON would like to provide some pseudocode  PRON could help PRON more   most introductory book on scientific computing include some example of count the number of operation in different algorithm   trefethen  amp  bau have some detailed example with picture  the application be different  but the process be the same    if PRON have a pseudocode for the algorithm that PRON be implement  one key to determine the asymptotic computational complexity be to observe the nest loop  for loop  while loop  etc     if PRON have nest for loop which contain only operation that be compute in constant time  or at least  bound by a constant   then the asymptotic complexity rest in the total number of iteration invoke by the loop   for example   for i1 to n  for j1 to n  do stuff   all constant time operation   endfor  endfor  this algorithm would run in  thetan2 time because the instruction  do stuff   occur  n2 time  if  on the other hand  PRON have while loop or if  then statement  then  PRON become more important to ask PRON whether PRON be interested in the good case  bad case  or average case scenario  since each may be different   PRON be kind of straight forward to calculate the complexity of the two  pass algorithm from the wikipedia page PRON link  first  one iterate from upper leave to lower right over all pixel and assign a preliminary label to each pixel while maintain a map of equivalence relation for the label  this need four check for every pixel  in the second pass one go over the label component and replace the preliminary label accord to the equivalence relation   complexity an image with  n pixel   m of which be  foreground     4n for the first pass and at most  m for the second pass  
__label__linear-algebra __label__eigenvalues PRON want to compute the derivative of a generalize eigenvalue  lambda which be solution of  a u  lambda bu   a  b  ulambda all depend on  t  in PRON case  a  b be know explicitly  and the eigenvalue  lambda and PRON correspond eigenvector  u can be immediately compute use eig    if PRON write formally the derivation  PRON arrive to the problem     alambda bu   alambda bulambda  bu     1  in the above equation PRON know  a  blambda  u  ab the unknown be  lambda and  u if  a  b be symmetric  PRON can get rid of  u by take the scalar product with  u in PRON case  a  b be not always symmetric  so PRON need to solve   1 with both unknown  u and  lambda  this have the form  xu   vlambda  w where  x be a know matrix  non  invertible  and  v  w be know vector   how can PRON compute  u and  lambda   the eigenproblem PRON have do not have a unique solution  any multiple  of  u solve PRON as well  so suppose PRON impose the condition   utu1 the two equation be    aulambda bu  qquad utu1     give derivative     alambda bu  lambda  bu   lambda bau  qquad 2ut u    0     this be nothing but a system of  n1  equation in  n1  unknown     beginpmatrix   alambda b  amp  b u ut  amp  0  endpmatrix   beginpmatrixu lambda  endpmatrix     beginpmatrix    lambda bau  0  endpmatrix      which should have a unique solution  provide this matrix be nonsingular  which should happen whenever  lambda have multiplicity 1  
__label__optimization __label__convergence PRON encounter an optimization problem  the simplified version be like follow   denote function  fxmathbfrnrightarrowmathbfr  where  fx be a smooth lower bounded convex function  ie  fxgt0    the hessian matrix of  fx be positive definite  however  fx be not a strongly convex function  namely PRON can not assume the hessian matrix  hxsucc m in  suppose for the sequence  mathbfxkk1infty  the gradient of the  fx converge to zero   beginequation   limkrightarrowinfty  mathbfgk2  0text   and   fxk1le fxk    k01    endequation   where  mathbfgkfracpartial fpartial x  x  xk do PRON have the follow result  beginequation   limkrightarrowinfty  fxk    inf   xinmathbfrn  fx    endequation   PRON expect  limkrightarrowinfty  fxk   infxinmathbfrn  fx however  PRON can not find proof for this result  there be some result about this problem  but unfortunately  either assume the optimum point of  fx be attainable  namely  there exist  xinmathbfrn  such that  fx    inf fx or there exist  xinmathbfrn  such that  limkrightarrowinfty  xk  x  could any one help PRON to solve this problem or give PRON some comment  thank for PRON help   what about    fx  y   fracx2y   fracepsilonx2   qquad nabla fx  y   big  frac2xyfrac2epsilonx3   fracx2y2  big   qquad x  ygt0  with the sequence   xk  yk    k  k2  here  epsilon be a small positive real number that make the hessian positive definite instead of semidefinite   then  fxk  yk   1epsilon k2 do not converge to the global infimum  inf fx  y   0   but the sequence of gradient   mathbfgk  bigfrac2kfrac2epsilonk3   frac1k2big converge to zero   informally speak  the usual proof of convergence base on the inequality  fygeq fx   nabla fxty  x would not apply here because while the gradient converge as  k1  not fast enough   the distance between  xk and the close minimum at  x0  diverge as  k 
__label__gofai __label__symbolic-computing in year past  gofai  good old fashioned ai  be heavily base on  rule  and symbolic computation base on rule   unfortunately  that approach run into stumbling block  and the world move heavily towards statistical  probabilistic approach lead to the current wave of interest in  machine learning    PRON seem though  that the symbolic  rule base approach probably still have application  so  could one  learn  rule use a probabilistic rule induction method  and then layer symbolic computation on top   if so  how could the whole process be make truly two  way  so that something  learn  from processing rule  can be feed back into how the system learn rule   
__label__python __label__sparse __label__data-handling __label__data-storage __label__r PRON have be work in r but sometimes switch to python  PRON would like a more inter  language portable way of store a large array than a csv file   the particular csv file PRON be deal with be about 10  6 row by 10  3 column  but only about 1  of the entry be non  zero   process a large csv file everytime PRON start up r or python take far too long   PRON have hear hdf5 be a great solution for this  but PRON have limit experience with PRON  be hdf5 appropriate for store non  hierarchical array datum  what about for a single sparse array  PRON also be unsure which hdf5 r package to use   assume PRON sparse array be 2dimensional  PRON can decompose PRON into three vector of column  index   row  index   and value fairly easily with a single traversal of the matrix   PRON can then store these vector in whatev file format PRON want  no need to switch to hdf5 for that reason alone   if PRON be interested in a  standardized  format  PRON should have a look at the matrix market exchange format  the  coordinate format   which be good for sparse matrix  simply add metadata to the format suggest by aron in PRON answer and specify how the datum have to be format   PRON happen to have this little python function kick around   def storesparseproblemspmat  rhs  filename     convert the sparse matrix to lil  spmat  spmattocoo     make a record array out of the above  rcv  recarray3  lenspmatrow    dtyperowlti8      collti8      valltf8       set the value  rcvrow    spmatrow  rcvcol    spmatcol  rcvval    spmatdata   open the output h5  h5  openfilefilename   w     save the matrix  h5createtable    m   rcv  titlethe matrix     add the rhs  ary  h5createarray    rhs   rhs  titlethe right hand side     close PRON out  h5close    PRON could make this faster by turn on table s magic blosc compression  and switch from a table to a carray  but this should at least get PRON start   the load function be really simple  but PRON can not seem to locate PRON   if someone have interest  PRON will code PRON up and post PRON  
__label__gpu __label__aws be there any cloud developer platform which provide free access to an nvidia gpu instance  maybe only for a limited trial period  PRON would like to work on PRON before commit to a pay option  PRON have try aws  google cloud developer platform and ibm bluemix  but none of PRON have this option   nvidia have PRON be own online lab where PRON can help PRON learn gpu base datum processing  may be worth check out for PRON if PRON want to learn and play around within the online lab environment   httpsdevelopernvidiacomdlionlinelab  all the good   try out floydhub  PRON give 100 free gpu hour  
__label__model-selection __label__binary this be more of a conceptual question  PRON be look for some guidance on select an appropriate model to predict a binary outcome   a business track PRON individual sale through a number of stage  if PRON go through all the stage and the sale be complete PRON have a positive outcome   otherwise the process may fail at any stage and then PRON have a negative outcome   PRON just need some hint on what direction to look into  PRON could look at simple binary classification  logistic etc  but PRON feel there should be more detail   thank  
__label__r __label__ab-test cross post this from cross validated   PRON have see this question ask before  but PRON have yet to come across a definitive source answer the specific question   what be the most appropriate statistical test to apply to a small a  b test   what be the r code and interpretation to analyze a small a  b test   PRON be run a small test to figure out which ad perform better  PRON have the following result   position 1   variation  impression  click  row175326  row3767 7  position 2   variation  impression  click  row175316  row3767 13  position 3   variation  impression  click  row17532  row3767 7  PRON think PRON be safe to say these number be small and likely to be not normally distribute  also  PRON be click datum so there be a binary outcome of click or not and the trial be independent   appropriate test  in analyze each position for significance  PRON think comparison with a binomial or poisson distribution make the most sense   accord to the openintro stat  and other source  book  a variable follow a poisson distribution   if the event be consider be rare  the population be large  and the event occur independently of each other    the same source classify a binomial variable approximately the same way add that the probability of success be the same and the number of trial be fix   PRON appreciate this be not an either  or decision and analysis can be do use both distribution   give a  b  split  testing be a science that have be practice for several year  PRON imagine that there be a canonical test  however  look around the internet  PRON mostly come across analysis that use the standard normal distribution  that just seem wrong   be there a canonical test to use for a  b test with small  s of click   interpretation and r code  PRON have use the follow r code to test significance for each position   position 1   binomtest7  767  p26753    exact binomial test  datum   7 and 767  number of success  7  number of trial  767  p  value  1077e05  alternative hypothesis  true probability of success be not equal to 003452855  95 percent confidence interval   0003676962 0018713125  sample estimate   probability of success  0009126467  PRON interpret this result to mean  the probability of success in the test group be indeed different than the control group with a 95  confidence interval that the success probability be between 368  and 187   ppois26  1753   lambda7767   lowertail  f    1  0009084947  PRON interpret this result to mean  give a poisson distribution with a click rate of 7 per 767 trial  there be a 09  chance of have a click rate of 26 or more per 753 trial in the same distribution  contextualiz in the ad example   there be a 1  chance that the control ad actually perform the same as the test ad   be the above interpretation correct  do the test and interpretation change with the different position  ie be the result of the poisson test more appropriate for position 3 give the small number    the approximation binomialk  n  p   poissonk  s   where s  np  can be show under the assumption   1  n   k  to say that nn  k    nk    2  p  ltlt1  to say that  1pn  k    1pn    PRON be up to PRON whether those be sufficiently satisfied  if the exact calculation can be do quickly  in PRON opinion  PRON be nice to stay with that   also since  if the probability of row 3 sample be different from the row 1 sample  PRON would almost certainly be on the low side  PRON would probably good for PRON to use  binomtest7  767  p26753   alternativeless    the final option indicate that the alternative to PRON null hypothesis be that the probability be less than 26753  not equal to  of course  that be simply just the sum of binomial probability from 0 to 7  PRON can check PRON   the interpretation be that this be the probability of have get at most 7 roll from random chance  if the probability truly be 26753   keep in mind the interpretation of that last sentence  these kind of test be generally use when PRON know what the inherent probability be that PRON be compare to  eg to see if the set of coin flip have a probability significantly different from 12 which be what PRON expect from a fair coin   in this case  PRON do not know what the probability be that PRON be compare to  PRON be just make the very crude guess that the 26753 outcome of row 1 reflect the true probability  PRON be good than a regular normal t  test in this case  but do not put too much stock in PRON unless PRON have a much high sample size for row 1   ok  so here be PRON data   dd  lt dataframepositionrep13  each2    variation  repc13   3    impression  repc753  767   3    click  c267161327    which be  position variation impression click  1  1  1  753  26  2  1  3  767  7  3  2  1  753  16  4  2  3  767  13  5  3  1  753  2  6  3  3  767  7  the two model assumption PRON be think about be binomial  modbin  lt glmcbindclick  impression  click   variation  position   family  binomial  datum  dd   where the dependent variable be construct to have the count of the event of interest in the first column  and the poisson  mdpois  lt glmclick  variation  position  offsetlogimpression     family  poisson  data  dd   where the logimpression  offset be necessary whenever the number of trial differ across observation   this mean coefficient be interpretable in term of change in rate not change in count  which be what PRON want   the first model generalise the binomt to a setting with covariate  which be what PRON have   that get PRON a more direct answer to PRON question  and good  if not perfect  measurement of the relevant uncertainty   note  both model assume no interaction between variation and position   independent effect     this may or may not be reasonable   PRON would want more replication to investigate that properly   swap the  for a  to do so   in this data summary confirm that the two model give rather similar result  so concern about poisson vs binomial do not seem to matter much   in the wild  count datum be usually overdispers  that is  more variable than PRON would expect from a poisson with a constant rate or a binomial with constant click probability  often due to unmodel determinant of click rate  probability   if that be the case then prediction interval from these model will be too narrow   the correct model be binomial  both poisson and normal be just approximation  the binomial pdf be define on the integer between zero and number of trial  the poisson be define on the integer between 0 and infinity  and normal be on all real variable between   infinity   in other word  for a poisson there be a  possibly small  but non zero probability of have more click than impression  for gaussian PRON can have even negative click  of course  the particular parameter determine how big an impact this have  probably worth plot the respective pdfs 
__label__machine-learning __label__neural-network __label__feature-selection in the literature  PRON have come across statement like people with high income and with long work hour be more likely to be diagnose with chronic disease such as stroke  the above  mention study  page8   explore the association between behavioral habits and chronic diseases use ann   as PRON be new to ml   PRON be unable to figure out how to make such conclusion with feature  study in neural network or other machine learning technique   be there a way to quantify the likelihood in ann similar to logistic  regression wherein regression coefficient give the change in the  log odd of the outcome for a one unit increase in the predictor  variable   currently use azure ml studio  as PRON be new to ml  PRON will try to explain in PRON simple way   1  PRON be unable to figure out how to make such conclusion with feature study in neural network or other machine learning technique   machine learning have many application  what PRON be talk about here come under the term inference  PRON mean to understand how PRON output be affect as PRON input change  PRON suggest PRON follow the book an introduction to statistical learning with applications in r on page 19 of this book  PRON be given  inference  PRON be often interested in understand the way that y be affect as x1  xp change     PRON instead want to understand the relationship between x and y   which predictor be associate with the response    what be the relationship between the response and each predictor    can the relationship between y and each predictor be adequately summarize use a linear equation  or be the relationship more complicated   PRON have not post the whole thing here  just some important point   so here  instead of prediction  PRON just analyze PRON model  after analyze PRON can make such conclusion   2  be there a way to quantify the likelihood in ann similar to logistic regression wherein regression coefficient give the change in the log odd of the outcome for a one unit increase in the predictor variable   as far as PRON understand  ann have multiple layer  PRON be not like logistic regression which just define one coefficient for each predictor  in ann  the coefficient be define for each layer separately  and in each layer  for each node   hope this help  
__label__python __label__scikit-learn __label__feature-extraction __label__image-classification PRON be try to extract feature from image use   def processimageimagefp    image   imreadimagefp   image   resizeimage    300  2003    image  equalizehistrgb2grayimage     edge  skimagefeatureblobdogimage   return edgesreshapeedgessizetolist    where imagefp be an image path   PRON be have a problem due to the different size of the return  in general  the reshape guarantee the same size in the other algorithm   be there a way to get always the same size   PRON only see a way  to truncate the list  the stupid way    please read through the scikit documentation that be find here and PRON be assume that PRON have go through the method through which PRON calculate blob in the image  if not the link be here  PRON return the 2d array of array with 3 value in each array  give coordinate and stddeviation of gaussian of the blob find  the length of the array return be the number of blob PRON have find in the image  so eliminate element from that array by resize PRON  be a wrong way of approach and may eliminate the feature which PRON may find important  
__label__efficiency PRON be wonder specifically in regard to a recursive function such as massive a game tree  PRON can not specifically say how big yet  but definitely push the limit of a give processor or processor array   be PRON correct to say that pass a variable to a function require an operation  certainly there must the the flipping of some bit  do this reduce efficiency   this be more of a stack exchange or programmer exchange question  see   here for c language or php  javascript or c  and  more generally   try to avoid global variable  PRON be good for maintain PRON code  global variable should be parameter  ie PRON value should not change during execution  about efficiency  PRON think PRON be a micro  optimization that be not very relevant  PRON be not an expert but PRON think that work with function argument rather than global variable allow more optimization from the compiler  something with the stack and the register    pass a variable to a function only require some pointer reference in the stack  assume PRON global variable be not a huge object   that be very fast and PRON be not even sure a profiler can detect such a thing   first of all  note that this forum be for computational science  not computer science  these be different field  with computational science be scientific computing  more like computational mathematic and scientific simulation  that say  even though the example in question be not relevant to this forum  there be thing that should be discuss here   the inefficiency be not in the passing of value to function  but in many other place  this be because a global variable can change at anytime  the program can not know if the value will change  and more importantly in a scripting language  PRON can change type  because the program do not know if the variable change  the program have to be ready to deal with whatev have happen  thus all compiler  interpreter optimization be turn off and any code around the variable have to go into  safe mode   example  a good example be to look at julia  julia be a recent programming language which  although PRON  look like a scripting language   be actually design to be efficiently compile at every step of the way  if PRON write PRON julia code correctly  type  stable  etc    then PRON code will compile to be the same speed as c  fortran  and PRON can use codellvm to see that the machine code be the same as what PRON would get from a c  fortran compiler   however  the language have problem with global variable  a large discussion on why be find on github  the summary be as follow   in a  just  in  time  compile language like julia  the compiler look at what PRON currently have and use a bunch of trick to optimize the code  however  if PRON do not necessarily know what type the variable be  PRON have to carry around a bunch of code for make PRON switch between integer  float  etc  if the variable be a global  PRON also have to deal with problem like if some other thread change the value  this mean that a lot of other compiler trick like inlin be also not able to be do  thus the result compile code have a lot more step to check every little detail since the global can do just about anything  this make deal with PRON really inefficient   as PRON crawl up the ladder to more dynamic typing  these inefficiency be more baked into the language  at the bottom  all language be still run compile code  so something like matlab  r  python always have this extra code around because PRON number can just change type all the time  javascript do too  though PRON be odd since  everything be a float  so PRON have to have part convert thing to integer at the right time   however  smart interpreter  like the javascript jits  will sometimes try to still do some optimization  but for the same reason as above  PRON also can not optimize too much when there be global variable since there be really no way to know in advance what a global variable will be  both in type and value   when all bet be off  PRON have to play PRON safe    for an example in the other direction  if PRON pass a variable in and PRON look like an integer  and all of the code look like PRON will not modify PRON  PRON can jit compile the function so that PRON actually be an integer  result in much lean and more efficient machine code  note that this require that the compiler can guarantee this variable be not change anywhere else  otherwise PRON will because breakage    conclusion  this be why the general rule for efficiency be to not use global variable  there be no way for the interpreter  compiler to know what to do with PRON  so PRON have to take the safe and least efficient route  also  since PRON can change anywhere at any time  PRON can because a lot of bug in program  and so global be generally frown upon for design reason  in the end  unless PRON really have to  do not use global   tldr  pass the variable into the function will because  a lot less problem than a global variable   from a style point of view  PRON be usually good to avoid global variable  as already point out    from a performance point of view  the answer depend on PRON programming language  for example  if PRON language do not support pass a multidimensional array by reference  then a global  non  local array can help  but PRON guess that all modern language use for computing do not have such shortcoming   whether PRON cost more to call a function when PRON have more argument be language and compiler  dependent  often  the extra cost will be zero or negligible  so that a global variable be more likely to hurt than benefit PRON  as explain in the other answer   
__label__feature-selection __label__scikit-learn PRON be work on the problem with too many feature and train PRON model take way too long  PRON implement forward selection algorithm to choose feature   however  PRON be wonder do scikit  learn have forward selection  stepwise regression algorithm   no  sklearn do not seem to have a forward selection algorithm  however  PRON do provide recursive feature elimination  which be a greedy feature elimination algorithm similar to sequential backward selection  see the documentation here   httpscikitlearnorgstablemodulesgeneratedsklearnfeatureselectionrfehtml  sklearn do have a forward selection algorithm  although PRON be not call that in scikit  learn   the feature selection method call fregression in scikit  learn will sequentially include feature that improve the model the most  until there be k feature in the model  k be an input    PRON start by regression the label on each feature individually  and then observe which feature improve the model the most use the f  statistic   then PRON incorporate the win feature into the model   then PRON iterate through the remain feature to find the next feature which improve the model the most  again use the f  statistic or f test   PRON do this until there be k feature in the model   notice that the remain feature that be correlate to feature incorporate into the model will probably not be select  since PRON do not correlate with the residual  although PRON may correlate well with the label    this help guard against multi  collinearity   scikit  learn indeed do not support stepwise regression  that be because what be commonly know as  stepwise regression  be an algorithm base on p  value of coefficient of linear regression  and scikit  learn deliberately avoid inferential approach to model learning  significance testing etc   moreover  pure ols be only one of numerous regression algorithm  and from the scikit  learn point of view PRON be neither very important  nor one of the good   there be  however  some piece of advice for those who still need a good way for feature selection with linear model   use inherently sparse model like elasticnet or lasso   normalize PRON feature with standardscaler  and then order PRON feature just by modelcoef for perfectly independent covariate PRON be equivalent to sort by p  value  the class sklearnfeatureselectionrfe will do PRON for PRON  and rfecv will even evaluate the optimal number of feature   use an implementation of forward selection by adjust  r2  that work with statsmodel   do brute  force forward or backward selection to maximize PRON favorite metric on cross  validation  PRON could take approximately quadratic time in number of covariate   a scikit  learn compatible mlxtend package support this approach for any estimator and any metric   if PRON still want vanilla stepwise regression  PRON be easy to base PRON on statsmodel  since this package calculate p  value for PRON  a basic forward  backward selection could look like this      from sklearndataset import loadboston  import panda as pd  import numpy as np  import statsmodelsapi as sm  datum  loadboston    x  pd  dataframedatadata  column  datafeaturename   y  datatarget  def stepwiseselectionx  y   initiallist     thresholdin001   thresholdout  005   verbose  true       perform a forward  backward feature selection  base on p  value from statsmodelsapiols  argument   x  pandas  dataframe with candidate feature  y  list  like with the target  initiallist  list of feature to start with  column name of x   thresholdin  include a feature if PRON p  value  lt  thresholdin  thresholdout  exclude a feature if PRON p  value  gt  thresholdout  verbose  whether to print the sequence of inclusion and exclusion  return  list of select feature  always set thresholdin  lt  thresholdout to avoid infinite looping   see httpsenwikipediaorgwikistepwiseregression for the detail      include  listinitiallist   while true   change  false   forward step  exclude  listsetxcolumnssetincluded    newpval  pd  seriesindex  exclude   for newcolumn in exclude   model  sm  olsy  smaddconstantpddataframexincludednewcolumnfit    newpvalnewcolumn   modelpvaluesnewcolumn   bestpval  newpvalmin    if bestpval  lt  thresholdin   bestfeature  newpvalargmin    includedappendbestfeature   change  true  if verbose   printadd    30  with p  value   6formatbestfeature  bestpval     backward step  model  sm  olsy  smaddconstantpddataframexincludedfit     use all coef except intercept  pvalue  modelpvaluesiloc1    worstpval  pvaluesmax    null if pvalue be empty  if worstpval  gt  thresholdout   change  true  worstfeature  pvaluesargmax    includedremoveworstfeature   if verbose   printdrop   30  with p  value   6formatworstfeature  worstpval    if not change   break  return include  result  stepwiseselectionx  y   printresulting feature     printresult   this example would print the follow output   add  lstat  with p  value 50811e88  add  rm  with p  value 347226e27  add  ptratio  with p  value 164466e14  add  dis  with p  value 166847e05  add  nox  with p  value 548815e08  add  chas  with p  value 0000265473  add  b  with p  value 0000771946  add  zn  with p  value 000465162  result feature     lstat    rm    ptratio    dis    nox    chas    b    zn   
__label__r __label__interpolation PRON question be ¿ how can PRON mask or crop the result  use r  of idw interpolation to only the area contain the original set of data point   in the example below  20 random point be use to interpolate a surface use the idw function in gstat  a convex hull be obtain from the point set and plot on the interpolate map  but PRON would like to crop the map so only area within the point cloud show the interpolation result   ¡ thank for any suggestion   víctor g f        idw in r    librarygstat     setseed1234   x  lt rnorm2010  y  lt rnorm2010  z  lt rnorm2010    xyz  lt dataframexyz   coordinatesxyz   lt xy  xr  lt rangex   yr  lt rangey   b  lt roundxr2xr11002     grd  lt expandgridxseqfromxr1toxr2byb    y  seqfrom  yr1to  yr2by  b      coordinatesgrd   lt  xy  griddedgrd   lt true    idwlt  idwformula  z1  location  xyz  newdata  grd     imageidw  col  terraincolors16    contouridw  add  t   pointsx  y  col4  pch19     libraryspatstat   convlt  convexhullxyx  y   plotconv  add  true     here PRON be answer PRON own question   to obtain an idw surface interpolation restrict to the area contain the original set of xyz data point   create a rectangular grid base on the xy point range use expandgrid  base   create a convex hull use the function convexhullxyspatstat   cut an irregular grid use the function inoutsplanc  on the rectangular grid mask by the convex hull  perform the interpolation use the function idwgstat  with the new grid   be sure to detach spatstat  because PRON have a different function for idw with the same name  example code        idw in r     load gstat   librarygstat      an example xyz datum set   setseed1234   x  lt rnorm201000  3000  y  lt rnorm201000  3000  z  lt rnorm2010  xyz  lt dataframexyz      calculate range and spacing   xr  lt roundrangex1xr  yr  lt roundrangey1yr  b  lt roundxr2xr1602b     create a grid   grd1  lt expandgridxseqfromxr1toxr2byb    y  seqfrom  yr1to  yr2by  b    plotgrd1      load spatstat and splanc  libraryspatstat   librarysplanc      extract convex hull  w2  lt convexhullxyxy      detach spatstat because PRON have a different  idw  function   detachpackage  spatstat   unload  true      create a new crop grid   grd2  lt grd1inoutgrd1w2bdry1       plotgrd2      convert grid and dataset to sp object   coordinatesgrd1   lt xy  griddedgrd1    lt true  coordinatesgrd2   lt xy  griddedgrd2    lt true  coordinatesxyz   lt xy     run the interpolation   idw1lt  idwformula  z1  location  xyz  newdata  grd1   idw2lt  idwformula  z1  location  xyz  newdata  grd2      plot the result   imageidw1  col  terraincolors16    contouridw1  add  t   pointsx  y  col4  pch19   imageidw2  col  terraincolors16    contouridw2  add  t   pointsx  y  col4  pch19     here be what PRON would do   idwoutput  lt asdataframeidw    13    add the idw raster   the first step to incorporate the idw result into the interactive map be to turn PRON into a spatial object  idwraster  lt rasterfromxyzidwoutput    PRON only want to show the interpolation result within the area of which PRON have data point   to do this  PRON will clip the raster to datum region by create convex hull  buffer  and mask  clip  raster   perform convex hull and buffer operation on project utm data  xyzhull  lt gconvexhullxyz   xyzbuff  lt gbufferxyzhull  width  25000   arbitrary 1 km buffer   mask idw raster to confine to region that PRON have information for  idwrastercrop  lt maskidwraster  xyz   spplotidwrastercrop    create contour of the idw  idwcontour  lt rastertocontouridwrastercrop  nlevel  15   when PRON use PRON real datum  PRON can use mapview  ie mapviewidwrastercrop  to see the interactive map   cite  httpsrstudiopubsstatics3amazonawscom212793f130ecc723da4ed98680d6d3d5c4aff9html 
__label__parallel-computing __label__vectorization PRON be able to find many definition and notion to this term include   implement the algorithm  or part of PRON  in hardware   instruction like fmad  fpgas  parallelize the algorithm at the hardware level   simd  gpu etc   so  what exactly be hardware acceleration   PRON would like to understand the meaning of the word to not sound ambiguous in technical discussion   hardware acceleration mean any code run on specialize hardware  as oppose to software run on general purpose cpu such as standard x86 processor on PRON pc   PRON suppose the term be inherently ambiguous  eg do sse count as hardware acceleration    and probably mean different specific thing in different industry  the wikipedia page be a good starting point  as a member of the scientific computing community  when PRON hear  hardware acceleration  PRON instantly think gpu acceleration of numerical algorithm  and PRON second thought be fpga numerical implementation  but for example  hardware implementation of video codec can also be consider hardware acceleration  
__label__regression __label__correlation PRON perform a regression analysis with two dataset  each of which have size 50  one dataset be call spatial  and the other min value  and PRON want to check whether the two be correlate  PRON do the analysis in spss and the result scatterplot be as follow   PRON be not that much experienced but PRON seem to PRON that a line be not the perfect fit for this scatterplot  would a power line fit better  or what else do PRON suggest   PRON be right  a basic linear regression be unlikely to fit this data   PRON need some form of non  linear regression   basic form of this  include exponential regression  mention by dawny33   can be find in most spreadsheet software  include excel  package like scikit learn and other will allow for more flexibility   yes linear regression be not a nice fit for this problem   non  linear regression  as jamesmf suggest  can be a nice option   but  this look like a nice fit for exponential regression   the graph of exponential regression look something like this   so  adjust the parameter to fit PRON datum should go good   the box  cox transformation can also be use for fit the plot   PRON have take a sample datum set  and fit a box  plot transformation  with relevant parameter for transform PRON to look somewhat like the plot of PRON datum   sorry for the noise  as PRON be a quick and dirty implementation   but  yeah a box cox transformation should also be a nice way to fit   r code for the above plot   lambda   96  plotboxcoxelec  lambda    elec be a sample datum set   PRON try to estimate a few of PRON data value from the scatter plot PRON provide   then PRON perform a power model regression and come up with   y   5777 cdot 1016   cdot 1404x  the estimate value PRON use be below     70  001    75  0012    80  0015    90  0025    95  0075    98  015    99  020    995  025    999  032  of course  PRON actual model will differ because PRON have the actual data set  PRON just eyeball a few point so PRON could test the power fit   PRON suspect that PRON  x value  as PRON be percentage  lie in   0100 regard  y value  PRON seem positive  but many be very close to  0 so PRON would first decide whether  ys below a threshold should be put aside first as outlier  as PRON will have a huge influence on the first basic fit  PRON can reintroduce PRON afterward with robust fitting procedure   an important question be  be  ys bound or not  the slope seem very steep  so PRON have to guess if the derivative be infinite at  x100   to help PRON choose model   PRON believe a first idea be to perform a change of variable on the  x axis  with  x   frac1100xalpha and try some  alpha  value  to see if clear pattern appear  
__label__linear-programming __label__scipy PRON need to get just a random vertex of a polytope  any will do  the only way PRON can do this now be to pick a random function  say 0s  to maximize with scipyoptimizelinprog  however  this be wasteful  because PRON really do not care which vertex PRON get  PRON see something in the doc about get the internal state of the solver with a callback function  but PRON do not know enough about the simplex algorithm to understand which paramater PRON need  how can PRON just extract the vertex scipy find during phase 1 and then abort the calculation   if PRON use a 0 objective function  then the solver should stop as soon as PRON encounter a feasible solution  because that solution will be optimal    if the solver be use a 2phase primal simplex method PRON will stop immediately after the first phase and the solution will be a vertex of the polyhedron   in practice  lp feasibility problem like this really be not much easy than optimization problem with a nonzero objective   PRON may be able to adjust the parameter of the solver so that PRON find a primal feasible solution more quickly  but that would depend on the detail of the solver that PRON be use  
__label__r PRON would install an r package twitterr in PRON rstudio for access twitter datum for a particular handle  but whenever PRON do try to access the twitter datum PRON be show PRON an unexpected message   in dorppapicall“search  tweet   n  param  param  retryonratelimit  retryonratelimit    100 tweet be request but the api can only return 0  any thought on that   PRON try different parameter  handle and functionality but still get the same issue   here be PRON code for the particular functionality  mytweet  lt searchtwitterjabhij   geocode20593684  7896288  2000 km   n  100   mytweetsdf  lt docallrbind   laaplymytweet  asdataframe    viewmytweetsdf   any thought on that will be very helpful   cheers    PRON be because of the whitespace in the parameter for geocode   PRON should be   mytweet  lt searchtwitterjabhij   geocode2059368478962882000 km   
__label__optimization __label__geometry __label__data-handling suppose PRON have a black and white image  compose of binary pixel value in a 2d cartesian array  that contain an irregular  nonconvex shape   let PRON further suppose that the shape be one connected region   instead of store each individual pixel location  which may be too costly for very large image   PRON want to represent the exact same image as a set of  space  fill  rectangle   in do so  each rectangle can be represent by PRON two antipodal corner point  thus  there be no need to store information about each point inside the rectangle  PRON only need to store the matrix coordinate   i  j of the two opposite corner point   there be many way that one can fill the space with rectangle   so  PRON question be   how can PRON fill the space with the fewest number of rectangle   small datum compression   how can PRON find this optimal set of space  fill rectangle in polynomial time   or be this problem np  hard    the boundary of the domain have exactly two type of corner  convex 90 degree corner and concave 90 degree corner  for fill the domain with non  overlap rectangle  only the concave corner be important   PRON do not know how to upload picture  sorry   at each concave corner  PRON must have a  cut  in x or y  direction  each  cut  add one rectangle  unless the  cut  happen to end in another concave corner   edit fix the propose solution base on the reference give in the other answer   therefore  PRON define a bipartite graph with the axis  parallel  cut  between concave corner as node  two node be connect by an edge  if the correspond  cut  intersect each other  or have a common corner  a maximum independent set in this graph  a set of node with no edge between PRON  would translate into a solution of the original problem  such a set can be derive from a maximum matching of the bipartite graph   this be probably related to the dulmage  mendelsohn decomposition    actually  PRON find the answer  here   thank for the help though   PRON have not think about PRON too closely  but PRON think the basic algorithm would work like this   find the number of unique edge in either the  x or  y direction   take the direction that have the small number of edge  without loss of generality  PRON assume  x here   for each such edge  connect   xminy1 to   xminy2  draw the large possible rectangle  extend to terminate at the line  x  xmax  repeat this process for all edge on the  other  side  eg  move rightward  that have not yet be cover with a rectangle   this should cover the whole domain  but with the possibility of overlap as one start from either the left or the right  however  the number of unique edge in either the  x or  y direction act as an upper bound on the number of rectangle  
__label__optimization __label__convergence in the non  negative matrix factorization  nmf   PRON basically compute an approximation of a give matrix  v in mathbbrn time m into matrix  w and  h such that  w in mathbbrn time r and  h in mathbbrr time m or in other word   v approx wh for the sparse nmf with sparsity parameter  lambdage 0  and sparsity function  g  PRON minimize a cost function of this form     fw  h   frac12v  bar w h2  lambdasumijglefthijright  qquadtextwhere  bar w  frac w  w  and  ldots can be any differentiable norm  for  ghij    hij  the step for update  h and  w be suggest here as follow  for   beginalignhij  longleftarrow hijampodotfracvit bar wjritbar wj  lambda  wj longleftarrow wjampodotfracsumi hijleftvi  leftritbar wjrightbar wjrightsumi hijleftri  leftritbar wjrightbar wjrightendalign   where  r  wh and the matrix division and the  odot multiplication  be do  element  wise   in this paper  PRON be suggest to accelerate the convergence as follow   to speed up the convergence  the multiplicative term can be exponentiat with an acceleration parameter  delta large than one  for each iteration the costfunction be evaluate and if the cost be small than in the previous iteration   delta be increase and if PRON be large  then  delta be decrease   this be do in the  h update step  follow the notation of the update above  this will be as follow     hij  longleftarrow hijodotleftfracvit bar wjritbar wj  lambdarightdelta  question   how can one guarantee that with this acceleration PRON will have few computational step until convergence than without  or how to ensure a minimum number of increase and decrease  of  delta until convergence   
__label__data-mining __label__nlp matlab be a great tool for some mathematical experiment  neural networks  image processing   PRON would like to know if there be such a comprehensive and strong tool for datum manipulation and nlp task  such as tokenization  pos tag  parse  training  testing   however PRON be new to nlp and PRON need a tool which let PRON experiment  get familiar and progress  please take a look at this paper   survey on various natural language processing toolkits  in this work several nlp tool be introduce  example be   carabao language toolkit  gate  stanford nlp toolkit  nltk  just google the title of the paper and PRON will find the pdf file   there be a nlp toolbox for matlab call matlabnlp  PRON include module for tokenization  preprocess  stop word removal  text cleaning  stem   and learn algorithm  linear regression  decision tree  support vector machine and a naïve bayes   a module for pos tag be come soon   PRON have just finish PRON phd and have use some nlp in PRON  PRON university do not offer any nlp course  so PRON end up teach PRON nlp  PRON use this book   which serve as a great introduction to nlp use nltk  natural language tool kit   PRON give a good introduction into programming with python  so handy if PRON have never program in python before too   PRON would highly suggest use nltk from nltkorg  sorry can not post more than two link   the book PRON use be now out of date as nltk be now on version 30  the book mention previously be for nltk 2x  but the authors be work on a new version of the book for nltk 3x  PRON can view the unfinished book here   PRON would highly suggest use nltk and if PRON be new to natural language processing  PRON would highly suggest PRON try and get PRON a copy of the follow book   foundations of statistical natural language processing by manning and shutze  even though PRON do not contain any code  PRON server a great introduction to natural language processing  
__label__algorithms __label__precision __label__numerical-limitations when apply the classical formula for the angle between two vector     alpha  arccos fracmathbfv1  cdot mathbfv2mathbfv1 mathbfv2  one find that  for very small  acute angle  there be a loss of precision and the result be not accurate  as explain in this stack overflow answer  one solution be to use the arctangent instead     alpha  arctan2 leftmathbfv1  time mathbfv2  mathbfv1  cdot mathbfv2  right     and this indeed give good result  however  PRON wonder if this would give bad result for angle very close to  pi  2 be PRON the case  if so  be there any formula to accurately compute angle without check for a tolerance inside an if branch    PRON have test this approach before  and PRON remember PRON work correctly  but PRON have not test PRON specifically for this question    as far as PRON can tell  both  mathbfv1time mathbfv2 and  mathbfv1cdot mathbfv2  can suffer from catastrophic cancellation if PRON be almost parallel  perpendicular — atan2 can not give PRON good accuracy if either input be off   start by reformulate the problem as find the angle of a triangle with side length  amathbfv1   bmathbfv2 and  cmathbfv1mathbfv2  these be all accurately compute in float point arithmetic   there be a well  know variant of heron s formula due to kahan  miscalculating area and angles of a needle  like triangle   which allow PRON to compute the area and angle  between  a and  b  of a triangle specify by PRON side length  and do so numerically stably  because the reduction to this subproblem be accurate as well  this approach should work for arbitrary input   quote from that paper  see p3   assume  ageq b     mu  begincas   ca  bamptextif  bgeq cgeq 0  ba  camptextif  cgtbgeq 0  textinvalid triangleamptextotherwise   endcases     mathrmangle   2arctanleft   sqrtfraca  bcmuabca  cb     right  all the parenthesis here be place carefully  and PRON matter  if PRON find PRON take the square root of a negative number  the input side length be not the side length of a triangle   there be an explanation of how this work  include example of value for which other formula fail  in kahan s paper  PRON first formula for  alpha be  c on page 4   the main reason PRON suggest kahan s heron s formula be because PRON make a very nice primitive — lot of potentially tricky planar geometry question can be reduce to find the area  angle of an arbitrary triangle  so if PRON can reduce PRON problem to that  there be a nice stable formula for PRON  and there be no need to come up with something on PRON own   edit follow stefano s comment  PRON make a plot of relative error for  v110   v2costheta  sintheta  code   the two line be the relative error for  thetaepsilon and  thetapi2epsilon   epsilon go along the horizontal axis  PRON seem that PRON work   the efficient answer to this question be  not too surprisingly  in another note by velvel kahan     alpha2arctanleftleftfracmathbf v1mathbf v1fracmathbf v2mathbf v2rightleftfracmathbf v1mathbf v1fracmathbf v2mathbf v2rightright  where PRON use  arctanx  y as the angle make by   x  y with the horizontal axis   PRON may have to flip the order of the argument in some language     PRON give a mathematica demonstration of kahan s formula here   
__label__algorithms suppose PRON have classifier c1  cn that be disjoint in the sense that no two will return true on the same input  eg the node in a decision tree    PRON want to build a new classifier that be the union of some subset of these  eg PRON want to decide on which leaf of a decision tree to give a positive classification    of course  in do so there will be a trade off between sensitivity and positive predictive value   so PRON would like to see a roc curve   in principle PRON could do this by enumerate all subset of the classifier and compute the result sensitivity and ppv   however  this be prohibitively expensive if n be more than 30 or so   on the other hand  there be almost certainly some combination that be not pareto optimal  so there may be some branch and bind strategy  or something  that avoid most of the computation in many case   PRON would like advice about whether this approach be likely to be fruitful and whether there be any work or if PRON have any idea about efficiently compute the roc curve in the situation above   PRON may suggest that PRON use a greedy method  give a classifier to start  PRON will include the classifier that make the ensemble get the good performance improvement  if no improvement could be get through include more classifier  then stop  PRON will start with every classifier  the complexity will be at most nn  PRON have one more question  what do PRON mean by  pareto optimal   especially in PRON context  PRON find from wiki this explanation  httpsenwikipediaorgwikiparetoefficiency  through reallocation  improvement can be make to at least one participant s well  being without reduce any other participant s well  being   the improvement for the pareto efficiency be for each participant  which may correspond to each classifier  how do PRON define the improvement over one classifier   if PRON understand the question correctly  PRON have train an algorithm that split PRON datum into  n disjoint cluster  now PRON want to assign prediction  1  to some subset of the cluster  and  0  to the rest of PRON  and amont those subset  PRON want to find the pareto  optimal one  ie those who maximize true positive rate give fix number of positive prediction  this be equivalent to fix ppv   be PRON correct   this sound very much like knapsack problem  cluster size be  weight  and number of positive sample in a cluster be  value   and PRON want to fill PRON knapsack of fix capacity with as much value as possible   the knapsack problem have several algorihm for find exact solution  eg by dynamic programming   but a useful greedy solution be to sort PRON cluster in decrease order of  fracvalueweight  that is  share of positive sample   and take the first  k if PRON take  k from  0  to  n  PRON can very cheaply sketch PRON roc curve   and if PRON assign  1  to the first  k1  cluster and to the random fraction  pin01 of sample in the  kth cluster  PRON get the upper bind to the knapsack problem  with this  PRON can draw the upper bind for PRON roc curve   here go a python example   import numpy as np  from itertool import combination  chain  import matplotlibpyplot as plt  nprandomseed1   nob  1000  n  10   generate cluster as index of tree leaf  from sklearntree import decisiontreeclassifier  from sklearndataset import makeclassification  from sklearnmodelselection import crossvalpredict  x  target  makeclassificationnsample  nob   rawcluster  decisiontreeclassifiermaxleafnode  nfitx  targetapplyx   recode   x  i for i  x in enumeratenpuniquerawcluster     cluster  nparrayrecodingx  for x in rawcluster    def powersetxs       get set of all subset     return chainfromiterablecombinationsxsn  for n in rangelenxs1    def subsettometricssubset  cluster  target       calculate tpr and fpr for a subset of cluster     prediction  npzerosnobs   predictionnpisincluster  subset    1  tpr  sumtargetprediction   sumtarget  if sumtarget   gt  0 else 1  fpr  sum1targetprediction   sum1target  if sum1target   gt  0 else 1  return fpr  tpr   evaluate all subset  alltpr     allfpr     for subset in powersetrangen     tpr  fpr  subsettometricssubset  cluster  target   alltprappendtpr   allfprappendfpr    evaluate only the upper bound  use knapsack greedy solution  ratio   targetclustersimean   for i in rangen    order  npargsortratios1   newtpr     newfpr     for i in rangen    subset  order0i1    tpr  fpr  subsettometricssubset  cluster  target   newtprappendtpr   newfprappendfpr   pltfigurefigsize55    pltscatteralltpr  allfpr  s3   pltplotnewtpr  newfpr  cr   lw1   pltxlabeltpr    pltylabelfpr    plttitleall and pareto  optimal subset    pltshow     this code will draw a nice picture for PRON   the blue dot be  fpr  tpr  tupl for all  210 subset  and the red line connect  fpr  tpr  for the pareto  optimal subset   and now the bit of salt  PRON do not have to bother about subset at all  what PRON do be sort tree leaf by the fraction of positive sample in each  but what PRON get be exactly the roc curve for the probabilistic prediction of the tree  this mean  PRON can not outperform the tree by hand  pick PRON leaf base on the target frequency in the training set   PRON can relax and keep use ordinary probabilistic prediction  
__label__deep-learning __label__nlp __label__tensorflow PRON be try to understand a loss  this loss be the mean negative log probability  but in a batch of  k sample  PRON use  k1  sample as negative sample  the loss have be present as     lx  ytheta   frac1ksumi1k    fxi  yi   log sumj1kefxi  yi  where PRON have a set of  x   x1    xk input and  y   y1    yk output  then every  yj where  i ne j be a negative example for  x  here be PRON implementation  just want to confirm that this be right and that tensorflow do not somehow have this build in   def  negativelogprobabilitylossx  y     calculate dot product between each pair of input and response    bs x bs   k  tfmatmulcontextchannel  utterancechannel  transposeb  true    get the diagonal which be the sxi  yi    this represent the similarity score between each input xi and output yi   out   bs x 1   s  tfdiagpartk   s  tfreshape   1  1     calculate the log sumexi  yj    here every row have only the negative example   in   bs x bs    out   bs x 1   k  tfreducelogsumexpk  axis1  keepdim  true    compute the mean loss between each x  y pair   and the log sum of each other  k1  x  y pair  return tfreducemeans  k   
__label__neural-networks __label__machine-learning __label__convolutional-neural-networks __label__image-recognition PRON use this project for exampleframework  caffe  arhitecture of net  mod of alexnet  400 image be use for training   PRON have this result   or this   solver   net   cdnet  models  trainprototxt   testiter  500  testinterval  500  baselr  0001  lrpolicy   step   gamma  01  stepsize  100000  display  50  maxiter  450000  momentum  09  weightdecay  00005  snapshot  10000  snapshotprefix   cdnet  models  training  cdnet   solvermode  gpu  model of net   can anybody explain such behavior of accuracy and loss of PRON net  what PRON be do wrong   author of tutorial have get this result   
__label__reinforcement-learning __label__actor-critic in order to have an actor critic rl model there be two thing to be satisfied   value approximation function should converge to a local minimum    sums dpis  suma pis  aqpis  a   fws  afracpartial fws  apartial w   0  the follow condition should be satisfied with the parameterization    fracpartial fws  apartial w   fracpartial pis  apartial theta  frac1   pis  a      so specifically how can PRON design a model to meet the second condition   update  here PRON want to highlight the value function approximation in actor  critic method  PRON need to optimize the critic also as PRON do for the q learning but follow the on policy which be take the td error accord to the actor  here PRON will put the final equation of actor critic   here simply PRON can take the critic neural net s output as the state value function or the f above  so how to satisfy the condition 2   there be many paper out there that deal with neural network and rl  this blog will give a very good insight on a policy gradient network  deep rl with pg  now for PRON question  PRON really need to be familiar on how PRON train a neural network  a simple one for classification  if PRON check the derivation and how the weight be get update thing will be very clear to PRON on how PRON can implement the above   PRON will describe PRON to PRON as simple as possible so PRON get the link  a neural network in a very broad sense consist of nest function  the function that contain all the other be the one at PRON output layer  in case of stochastic policy gradients this be PRON boltzmann function  so PRON output layer will take all the previous layer s output and will pass PRON through the boltzmann function  in the case of nn the parametrization come from all previous layer   the blog link PRON send PRON describe a very nice and simple example of a vanilla policy gradient with nn  reinforce algorithm   by use the code  plus PRON understanding on feedforward network  PRON will see that the gradient be multiply by the reward  PRON be a very good exercise   for actor  critic  PRON need in general a network perform pg  stochastic or deterministic   PRON actor  and a network that will give PRON the reward signal  like the simple case in the blog  however  for various reason  instead of the actual reward PRON use another network that estimate the reward by perform q  learning as in deep  q learning  minimize square error between estimate reward and true reward    hope this help  
__label__integration what method be available to integrate a sharply peaked function  position of peak know  on a finite interval  the interval include the peak    currently PRON be get underflow use some of gsl s adaptive algorithm  PRON suspect that gsl fail to find the position of the peak  and hence be think that the function be mostly zero  be there a method in gsl so that PRON can tell where the peak be locate  or maybe PRON can use an alternative routine  PRON do not have to be gsl    if PRON know where the peak be  then PRON can always split the interval  for example  if PRON know that the peak be at  a and have a  width   however PRON want to define that  of  sigma so that PRON can say that PRON be mostly confine within   asigma  asigma  then split the integral as     intlu fx    dx    intlasigma  fx    dx    intasigmaasigma  fx    dx    intasigmau fx    dx      each of these three integral should now be relatively well  behave on PRON own  and should be easy enough to integrate  
__label__mindstorms PRON have a lego mindstorms ev3 and PRON be wonder if there be any way PRON could start cod the bot in python rather than the default drag  and  drop system  be a mindstorm consider ai   be this possible   PRON goal be to write a basic walking program in python  the bot be the ev3rstorm  PRON search and find this  but do not understand PRON   PRON can use python  ev3 which can be use to program lego mindstorms ev3 use python on ev3dev   see  set up a python development environment with pycharm   be a mindstorm consider ai   this depend on what type of software PRON write in PRON  the algorithm PRON write could be see as ai   PRON can absolutely use python to progam PRON  or java or other language   check this link for a tutorial  
__label__pde __label__hyperbolic-pde __label__advection-diffusion __label__advection __label__singular-perturbation give the one dimensional equation    epsilonfracpartial2upartial x2   fracpartial upartial x   0   with  0leepsilon ll1  with boundary condition  u0   0  and  u1   2   PRON can not neglect the diffusive term because of the boundary condition  in which situation  with very small  epsilon could PRON  what be the mathematical theory behind that could explain PRON   also  if PRON have a time  dependent equation    fracpartial upartial t   epsilonfracpartial2upartial x2   fracpartial upartial x  how would the situation change  what if the convective term be nonlinear  such as in the burgers equation  PRON have not be able to find reference on this topic  PRON would appreciate any   the stationary equation PRON show transport information from the right to the left via the advection term  PRON also diffuse slightly  if PRON switch off the diffusion term altogether  then PRON only have transport from the right to the left  and PRON need to also drop the boundary condition at the left  because information be from the right to the left  nothing that happen at the left end of the domain have any effect on the solution   a similar argument can be make for the time dependent equation   in general  these equation be example of  singularly perturb problem   PRON will be able to find a lot of literature on the subject   in addition to ajk s recommendation  here be a couple more recommendation   perturbation method in fluid mechanic by milton van dyke  introduction to perturbation methods by mh holmes   2013    PRON recommend the second to get start on perturbation method  the first one can be read after go through the first 2  3 chapter of the second   to answer PRON question about when the diffusion term can be ignore  in the first equation   PRON can be ignore when  u0  approx u1 in this case   ux will be more or less constant  this can of course be quantify  in PRON case with  u00  and  u1   2   the diffusion term can not be ignore  there be a boundary layer near  x0 see a plot of the exact solution below for  epsilon  001    in the time  dependent nonlinear case  if PRON drop the diffusive term then PRON have a nonlinear hyperbolic problem   solutions will naturally generate singularity  discontinuity  in finite time   to extend the solution beyond that time  one must consider weak solution  and uniqueness be lose   to specify a unique  physically relevant solution one typically invoke an entropy condition  which be equivalent to enforce that the weak solution be the  vanish  viscosity limit  of a diffusive equation   the canonical reference in this area be whitham   PRON be partial to the two text by leveque  although PRON focus on numeric more than theory  
__label__matlab __label__fluid-dynamics PRON have be task with model this 2d incompressible viscous flow use these boundary condition and for these value of u₁ and u₂ on a rectangular square grid with 193 by 129 grid point  PRON have be ask to model the steady  state flow at re200   PRON be not sure how PRON should go about this  do anyone recommend a comprehensive guide or perhaps a similar example PRON would be able to read through and reverse engineer   
__label__machine-learning __label__python __label__neural-network __label__deep-learning __label__tensorflow PRON have not gpu support so PRON often happen that PRON model take hour to train  can PRON train PRON model in batch  for example if PRON want to have 100 epoch for PRON model  but due to power cut PRON training stopsat 50th epoch  but when PRON retrain PRON model PRON want to train PRON from where PRON be leave  from 50th epoch    PRON would be much appreciate if anyone can explain PRON by some example   with tensorflow  currently the most straightforward and easy way to get persistence for PRON model be to use a tftrainmonitoredtrainingsession  PRON just need to use PRON instead the normal tf  session   that be frequently use  this an illustrative python snippet   with tftrainmonitoredtrainingsessioncheckpointdirtmpmymodel    savesummariessecs600  as sess     sessruntrainop  feeddict      with this  PRON model be automagically save every 600 sec in tmp  mymodel and restore the next time PRON restart the program  
__label__finite-element __label__projection PRON have a question about galerkin method  PRON do not understand why the galerkin method weight the residual by the shape function and set PRON equal to zero  PRON want to know what be reason of this  why PRON must set weight residual function equal to zero   let PRON say PRON want to solve the laplace equation   delta u  f ideally  of course  PRON would like to find a function  u so that the residual be zero   ru   delta u  f  0 but  u be an infinite dimensional object which in general PRON can not represent on computer  so PRON have to find finite dimensional approximation  uh since  uh be not the exact solution  PRON can not expect that  ruh0 the question be which set of equation PRON want  uh to satisfy instead   the galerkin method choose  uhsumi ui varphii to be a linear combination of  n shape function  varphii and then determine the coefficient  ui by require the residual to satisfy the set of  n equation  leftltvarphii  ruhrightgt0  but there be other choice that be possible  for example  the petrov  galerkin method require that  leftltpsii  ruhrightgt0  where the test function  psii be a set of  n weighting function separate from the trial function  varphii  PRON have a bit more material on this issue in lecture 4 at httpwwwmathtamuedubangerthvideoshtml   remember that when PRON multiply the strong  form equation by the shape function  the shape function be arbitrary  therefore  by require that the residual be orthogonal to any such shape function  such a residual be in fact very close to zero   this be not the same as require that the residual be zero exactly  but rather a somewhat weakened requirement that the discrete solution can satisfy   when PRON study the finite element method in graduate school  this notion of multiply by a weight function be also very alien to PRON   eventually  PRON do find a nice  albeit non  rigorous  analogy that help PRON understand PRON   this analogy be base on 3d vector geometry and an understanding of projection and dot  product   3d geometry  imagine a 2d plane lie somewhere in 3d euclidean space   this plane can be though of as the span of two vector  v1  and  v2  thus  any vector  w in the plane can be write as a linear combination of these vector  ie  w  c1v1c2v2   now imagine a point  q in 3d space that be not on the plane  consider the question   of all the point on the plane  which point be close to  q   PRON be the one point  w  not show in the image above  that lie on the line pass through the point  q and perpendicular to the plane   the point  w be also know as the orthogonal projection of  q onto the plane   even though PRON do not know the coordinate of this point  w  PRON do know that the vector between  q and  w be perpendicular to all vector that define the plane  ie  v1  and  v2  perpendicular also mean that the dot product be zero   if PRON denote the vector between  q and  w as  vecqvecw  then force  vecqvecw to be perpendicular to the plane also imply     vecqvecwcdot v10  and     vecqvecwcdot v20  this result in a system of equation which PRON can solve also notice that to construct  vecqvecw  PRON must know the coordinate of  q  analogy for the galerkin method  let PRON assume that the solution  uh be a finite linear combination of function  n1  nk  thus   uh  c1n1  cknk  this linear combination act like the plane in the discussion above   now  let PRON assume that there exist some exact solution  u  which PRON do not know   this solution  u be like the point in 3d space which be not on the plane   in the galerkin method  PRON be look for the solution in a space  plane  that be close to the true solution  point not in the plane    in this sense  the  good solution  be the choice of  uh that the difference  u  uh be perpendicular to the space  uh  note that for function space  the  dot  product  be define by the integral of PRON product  ie   ltu  vgtintomega u v  so  perpedicular imply that the dot product between the  u  uh and all those basis function  n1      nk must be zero  ie    intomega  u  uhn1  0          intomega  u  uhnk  0  now  PRON may be say to PRON   this whole setup rest critically on the assumption that PRON know the exact solution  u ahead of time   but the truth be that PRON generally do not know  u a priori   in which case  how can PRON possibly compute  u  uh in all these equation    PRON be so glad PRON ask   the truth be that PRON can not and do not evaluate  u  uh directly  but PRON do know what  u and  uh be suppose to satisfy  ie the pde   suppose PRON original pde problem be    nablacdotleftkxnabla urightf  PRON could also rewrite this as    au  f  where the operator  a be define by the expression  aunablacdotleftkx  nabla u right  so instead of consider the absolute difference between the solution  u  uh  PRON instead consider the residual difference  au  auh in all of the  perpendicular  equation   that is  consider not what  u and  uh be  but rather what  u and  uh satisfy instead   by replace the absolute difference with the residual difference in the  perpendicular  equation above  PRON can write    intomega  au  auhn1  0          intomega  au  auhnk  0  again  PRON still do not know what  u be  so this may not seem very helpful   but in fact   PRON can replace  au with the know source term  f  since  au  f    thus  PRON obtain the equation    intomega  f  auhn1  0          intomega  f  auhnk  0  thus  enforce the residual to be orthogonal to the give space result in a system of equation that one can solve for the coefficient  c1  ck  summary  the explanation above be a rough  analogy    PRON have not really derive anything or give a reasonable proof that  u  uh can be replace by  au  auh and still produce a close approximation   PRON have also not explain anything about obtain a weak form of the pde or how to choose the space where  n1  nk lie in   but the whole idea behind galerkin method as a projection be that for all possible linear combination of function in a give  finite dimensional  space  span of  n1  nk   PRON be look for the one that be close to a solution  point  which generally lie outside of the give space   close mean that PRON be look for the orthogonal projection from the true solution to the give space   if PRON do not know what the exact solution  u be  then PRON can not use the absolute difference as a metric in PRON projection   thus  PRON be force to use the residual difference as the metric of PRON projection  in other word   the galerkin projection be not about what  u be  PRON be about what  u satisfie   boris grigoryevich want PRON not to be able to create residual with the same function PRON use to create the solution   while this question be old and have be answer by plenty of smart people  PRON just want to jot down the intuition PRON use to explain the galerkin method to people   the goal in PRON situation be to find as close of a solution as PRON can to some continuous residual equation     ru   0  let PRON define the  ith basis function as  phiix  define the approximate solution as  uh   sumin  ai  phiix  and define the residual as  ruhx  the galerkin projection end up be     intomega  ruhx   phiix  dx  0   forall i  this integral expression can be view as an inner product write as     leftlt   ruh    phiirightgt   0   forall i  from the perspective of this inner product  the galerkin projection force the residual error to be orthogonal to the choice of basis function  so while there may be true error associate with use a lower dimensional representation of the solution  the galerkin projection aim to minimize the error component associate with the choose basis  
__label__nlp __label__intelligent-agent PRON be look to build a conversational ai  the goal be to create an ai that be PRON friend  as oppose to personal assistant that be mean for carry out command like  send a text  play that song   etc    what be the underlie technique involve  be there any good open source project PRON can build on top of  what kind of training datum do PRON need  PRON look at jasper  do not look like a good fit base on a shallow evaluation   PRON be imagine that the user text need to be parse out  like for eg use google s syntaxnet   may be PRON query some knowledge graph and then use nlg  or PRON use some kind of seq  seq modelling like lstm  rnn to generate response   a useful intro to conversational qampa system like ibm s jeopardy watson should be available from the ibm deepqa research team   this be watson    much of which be available free   httpresearcherwatsonibmcomresearcherviewgrouppubsphpgrp2099  in may  june 2012  the ibm journal of rampd publish a special issue entirely about the tech behind the ibm watson qampa software that play jeopardy   httpieeexploreieeeorgxpltocresultjspisnumber6177717  afaik  the article be no longer freely available on  line  but similar work should form a useful basis for build the essential piece in a conversational agent  a knowledge base  query  amp  update  understand and generate text and speech  etc   before PRON commence PRON will recommend that PRON refer to similar question on the network ie httpsstackoverflowcomquestions9706769anytutorialsfordevelopingchatbots  httpsstackoverflowcomquestions55042howcaniprogramasimplechatbotai and httpsstackoverflowcomquestions1748887howtocreateasmartchatbot  in PRON answer PRON follow the approach lay out on two outstanding article  chatbot with seq2seq  suriyadeepan r 2016  and deep learning for chatbots part 1  amp  2  benny b 2016  which be available here httpsuriyadeepangithubio20160628easyseq2seq and here httpwwwwildmlcom201604deeplearningforchatbotspart1introduction respectively   chatbot be also know as conversational agent or dialog system  PRON goal be to build a friendly ai  model personality may appear straight forward  however incorporate such knowledge into a model be still a challenging problem  this be because the model be train on a lot of datum from different source and user  below be a link to a paper on build such a persona  base neural conversation model httpsarxivorgabs160306155  there be two important concept that one need to understand when learn about chatbot  that be the chatbot publish platform and the chatbot development platform  a chatbot publishing platform be a medium through which the chatbot can be access by the user ie fb messenger  line  telegram or whatsapp  on the other hand a chatbot development platform be a tool that can be use to create a chatbot  chatbot development platform ie beep boop  flow xo  botsify and chatfuel can be use to build chatbot without write any code  this be do by simply use a drag and drop interface  such development platform however offer only limited functionality and customizability   marutitech blog   a developer can choose to use a rule  retrieval base approach  here PRON simply write a pattern and a template  such a bot reply with one of the template when PRON encounter a similar pattern from the user  rule base model tend to perform poorly when PRON encounter completely new sentence   suriyadeepan ram 2016   a different approach involve utilize a generative model  generative model construct response word by word base on the query  because of this the generate response be prone to grammatical error  however once PRON be train  the generative model outperform the rule base approach especially in handle previously unseen query   in probability and statistic a generative model be a model that generate all value for a phenomenon  both those that can be observe in the world and  target  variable that can only be compute from those observe   for PRON example below  PRON will first preprocess the datum and then train the generative model use sequence 2 sequence  seq 2 seq be a general purpose encoder decoder framework for tensorflow which can be apply to machine translation  summarize text  image captioning and conversational modeling  the seq2seq network connect two rnn s to work together to transform one sequence to another  an encoder network condense an input sequence into a vector while a decoder network unfold that vector into a new sequence   before train a dataset on a generative model PRON need to perform a preprocess step call padding  here PRON work on convert the variable length sequence into fix length sequence  some of the dataset which can be use to train a chatbot use a generative model approach include  the cornell movie dialog corpus  which be a collection of dialog from movie script and the ubuntu dialog corpus which be base on chat log from the ubuntu irc public channel   the next step will be to implement the continuous bag of words  cbow  model  which be a form of word embedding  cbow be a model of simplify representation use in nlp  in this step text be represent as a multiset  bag  of word disregard grammer but retain the frequency of word   the final step in PRON model be implement the attention mechanism  this step be very important since PRON prevent the information loss that could have occur while compress all the necessary information of a source sentence into a single vector   for source code  PRON recommend implement siraj rawal s seq 2 seq chatbot because of PRON clear documentation  PRON available here httpsgithubcomllsourcelltensorflowchatbot 
__label__machine-learning __label__predictive-modeling __label__apache-spark __label__model-selection __label__ensemble-modeling let PRON say PRON be build an app like uber and PRON want to predict the user s most likely destination base on the user s past history  current latitude  longitude  and time  date   here be the propose architecture   let PRON say PRON have a pre  train model host as a service  the part PRON be struggle with be  how do PRON get the user feature from the database in realtime from the riderid to be use by the prediction service  xgboost model   PRON be guess a lookup in a sql database will take too long  consider PRON have 1m user and ride   thank in advance   
__label__azure-ml PRON be try to use azure ml for the first time  so pardon PRON if PRON be too naive a question  PRON will be try to answer more than a dozen or so question use ml  in azure  PRON can create a simple experiment that answer one question perfectly  for example  detect anomalous datum in the dataset  or categorize the datum   PRON be think if PRON be possible to combine the two experiment into one  so  if the web service input be 0  PRON give PRON a list of anomaly in the datum  if the web service input be 1  PRON give PRON a list of possible category   a real world example  say PRON have event log  then first question be  what be the event that do not fit the pattern    and second question be  how many system fail and generate error normally   PRON can very well  create two experiment  and have two web service  PRON be think if PRON be possible to do PRON with just one experiment   
__label__machine-learning __label__neural-network __label__reinforcement-learning __label__q-learning PRON need to use reinforcement learning to teach a neural net a policy for a board game  PRON choose q  learining as the specific alghoritm   PRON would like a neural net to have the follow structure   layer  row  col  1 neuron  input  value of consecutive field on the board  0 for empty  1 or 2 represent a player   action  natural number  in that state  layer      neuron  hide  layer  1 neuron  output  value of action in give state  float   PRON first idea be to begin with create a map of state  action and value  and then try to teach neural network  if the teaching process would not succeed PRON could increase the number of neuron and start over   however  PRON quickly run into performance problem  firstly  PRON need to switch from simple in  memory python dict to a database  not enough ram   now the database seem to be a bottleneck  simply speak there be so many possible state that the retrieval of action  value be take a noticeable amount of time   calculation would take week   PRON guess PRON would be possible to teach neural network on the fly  without the layer of a map in the middle  but how would PRON choose a right number of neuron on the hidden layer  how could PRON figure out that PRON be loose periously save  learn  data   PRON need to use some function approximation scheme  in addition  experience replay would be useful for two reason   1  PRON want to keep past memory  2  PRON need to decorrelate the way to teach PRON network   have a look at deepmind s dqn on atari game  what PRON be describe be basically what PRON have solve  the paper be in PRON website  httpdeepmindcomdqnhtml  mnih v  kavukcuoglu k  silver d  rusu aa  veness j  bellemare mg  et al  human  level control through deep reinforcement learning  nature  2015 feb 255187540529–33   with respect to the network architecture  PRON will definitely require some experimentation  for an alternative  PRON can also have a look at hyperneat  PRON evolve the network topology   hausknecht m  khandelwal p  miikkulainen r  stone p hyperneat  ggp  a hyperneat  base atari general game player  in  proceeding of the 14th annual conference on genetic and evolutionary computation p 217–24  available from  httpdlacmorgcitationcfmid2330195  for more strategic game  maybe PRON can have a look at  giraffe  use deep reinforcement learning to play chess  httparxivorgabs150901549  in order to train an agent to play a board game  the first important task be to create a reinforcement learning environment   4 essential aspect of an environment be   observation space  what be the input  state  as well as PRON shape and range   action space  what be the possible action  as well as PRON shape and range   reward function  what be the reward for a particular pair of state  action  will PRON be immediate or sparse reward  the reward function will affect the difficulty of the environment a lot   when will an episode  a game end   PRON highly suggest use the same api as openai gym because of PRON popularity and high  quality  then PRON can try to apply direct these algorithm before try on PRON own  PRON be state  of  the  art algorithm and the quality be guarantee   about the neural network architecture  PRON can exploit the nature of the game for good result   for example  in go  the position be symmetric and the action be simple and position  independent  which be very suitable to use convolutional neural network  alphago zero    on the other hand  in chess  because the action be asymmetric and position  dependent  google have to redesign the architecture to train an agent to play chess  
__label__c __label__floating-point __label__computer-arithmetic this may be the wrong stackexchange site for this question   math  se  cs  se  programmer  se  and of course stackoverflow be all possibility   PRON be hop to reach an audience that may actually use this function  to get some feedback on how PRON would like PRON to work   c s double nextafterdouble x  double y  function return the next representable value from x  in the direction of y   commonly  or  infinity  but can be anything    PRON be tidy up gnu libc  libm s implementation to compile to nice code    PRON currently extract each double to two 32bit integer   PRON be pretty clunky and use lot of branch  and do not take advantage of int64t     if PRON be go to change PRON at all  PRON may as well make as much improvement as possible  but use an fp compare  equal would change the result in daz mode   how should PRON behave when the denormal  be  zero andor flush  to  zero be active   on x86  there be two separate flag which sacrifice gradual underflow for performance   by default  neither be set   one or both can be set  PRON be often use together  but do not have to be    daz be an input filter  when an fp math  compare instruction read PRON input  denormal be consider  00   so a compare between two denormal find PRON be equal   arithmetic can easily produce denormal result  though   ftz be an output filter  denormal input work normally  with no effect on compare instruction   however  ordinary math instruction can not generate denormal   denormal result be flush to  00   dblmin2  dblmin15 should give the same result as without ftz  eg if those input denormal be load as constant    in both case  esp daz   one may argue that the next representable value below dblmin  the small normal number  be  00  and that the next number below  00 or 00 be dblmin   implementing this would require check the ftz  daz flag   one may argue instead that in daz mode  two different denormal should be consider equal  because PRON do compare equal   thus  PRON be correct to return y when x equal y  as require by posix  and by the c11 standard  712113 pg 256 and annex f1083 pg 529     the motivation for this specification be for nextafter00  00  to return 00  and vice versa    the 2nd behaviour allow an extremely fast  amp  compact implementation  no take branch and 3 not  take branch pollute the branch  predictor  and 9 clock cycle latency on intel haswell  for comparison  an fp multiply take 5 cycle  for the common case  xy  and x0  and the result do not overflowinfinity  or underflow  denormal     less than half the total code size compare to glibc s current version   the c11 standard say    even though underflow or overflow can occur  the return value be independent of the  current rounding direction mode    but daz be not a round direction mode   the boostmathulp function s documentation observe that  PRON experience be that stdnextafter often break depend which optimization and hardware flag be in effect    consistency with other implementation be desirable for glibc s nextafter function  but consistency with glibc s exist behaviour may take precedence   the current implementation be not influence at all by any fp setting  because PRON use only integer comparison on the ieee float bit pattern   consistency with nexttoward  and with nextafterffloat  float   nextafterllong double  long double  be also important   PRON have not look into the long double version yet   integer compare be clunkier there  because int64t can not hold 80 bit   use integer  only compare be only slightly slow  bulky to implement  use a 64bit integer compare  and the sign of x and y  instead of just an fp compare and the sign of x    ieee float be cleverly design so PRON bit can be compare as sign  magnitude integer  and thus as 2 s complement integer with a check for the both  negative case    how do nextafter behave on other system  os x  msvc  anything else    if people be interested in test PRON  PRON may write a little test program   how do PRON wish nextafter behave    nextafter be pretty much fully specify except for daz  ftz  so wish other than that may be better fulfil by different function    do PRON miss anything   PRON know the current implementation be  good enough  for such a rarely  use function   PRON be ugly  but PRON be use so rarely that PRON do not have much impact    actually  be PRON rarely use   have anyone ever see code that use PRON a lot    part of the motivation here be that PRON enjoy optimize thing in assembly language  and integer op on fp datum be interesting  especially on x86  64 where integer instruction can be use on the vector register that be also use for float and double  but not 80bit long double for nextafterl  or for the direction arg of nexttoward    
__label__numerical-analysis __label__ode __label__computational-physics __label__operator-splitting trotter expansion say     eab   limptoinfty  bigea2p  eb  p  ea2p  bigp    with  p  2   PRON become  with high accuracy     ea4  eb2  ea2  eb2  ea4    let PRON say PRON want to solve   fracdydt   cy  where  c  a  b  c can be time dependent  how can PRON solve this equation by splitting  c into  a and  b  before use any ode solve scheme  will PRON yield a gain in performance by do so  like in matrix exponential   background information   httpswwwquoracomwhatisthesignificanceofthetrotterproductformulainphysic  PRON equation  fracdydt   cty have the exact solution     yt   mathcal t expleftint0t cs  dsright  y0      where  mathcal t be the time  ordering operator which order operator at later time to the left  approximate the integral by a discrete sum   t  ndelta t      yt   mathcal t expleftsumi1n hti  delta tright  y0  and split the term        yt   mathcal t prodi1n explefthti  delta tright  y0  now PRON can trivially apply the time  ordering operator  lead to    yt   prodi1n explefthti  delta tright  y0  the latter equation say that one can repeatedly apply matrix exponential to obtain the solution  this be a fact on which almost any numerical integrator rely   now note the step mark by     here  the exponential of a sum of several operator be decompose into a product of these exponential  that is  the elementary splitting formula  exph1h2   exph1  exph2 be apply several time  the formula be  of course  only an approximation when the two operator do not commute    how do this help PRON in solve PRON equation  in no way  but PRON be good to know  for the practical part of PRON question  please read the first answer in PRON link question on quora  PRON basically say PRON all   what PRON be propose be widely refer to as strang splitting   there be a huge literature on similar method   there be also many question on this site on the same topic and a tag for PRON   PRON question seem to suggest that in PRON case the ode be linear  but possibly time dependent   and PRON do not assume any special structure of  c   in this case there be  generically  nothing to be gain by use a splitting method   splitting method be useful when PRON right  hand  side can be decompose into two additive part  each of which be much easy to integrate than the combination  
__label__neural-networks __label__nlp capsule networks seem to be a good solution for problem  which make up hierarchical complexity    eye  nose  ear   face    finger  nail  palm   hand     human   nlp domain be a very clear hierarchical complexity problem  because there be word  sentence  paragraph and chapter  whose meaning change base on the style of low level   be there any research paper or software tool on capsule networks and nlp  which PRON should be aware of  be there relate research paper  which have be investigate hierarchical complexity within the domain of nlp  which could be easily translate to capsule network   for PRON to answer this question  first PRON need to look at why capsule network outperform  convolution neural network by as much as 45  in recognize image that have be rotate  translate or be under a different pose  PRON can find geof hintons paper on capsule network here for reference httpsarxivorgpdf171009829v1pdf  in a cnn architecture  a convolution layer be usual follow by a max pooling layer  this be so that the low level can detect low level feature like edge while the high level layer can detect abstraction like eye  however max pool in cnn s loose a lot of important information regard the location and spatial relationship between certain feature   on the other hand this be where capsule network excel  the way PRON represent certain feature be locally invariant  this be why capsule network can recognize image under different lighting condition and deformation  PRON be likely to excel at application such as video and object tracking but not necessarily nlp   the current approach in nlp map word and phrase to vector  from there PRON exploit the concept of vector and distance between PRON  cosine  euclidean etc   to perform operation such as  find the similarity between word and even document  machine translation and natural language understanding  nlu    capsule network be unlikely to succeed in nlp  this be because algorithm which aim to find the hierarchical structure of natural language or approach that focus on grammer have meet little success  research by stanford university aim at find the hierarchical structure of natural language can be find here httpsnlpstanfordeduprojectsprojectinductionshtml  although conclusive research regard other application of capsule network have not yet be conduct  PRON be likely to excel at application such as video intelligence and object tracking but not necessarily nlp  
__label__c++ __label__data-visualization __label__data-sets for compute the gradient of a scalar field  one can use the weight least square method as describe in the paper revisit the least  square procedure for gradient  reconstruction on unstructured meshes by dimitri mavriplis  pg  23    PRON question be  how can PRON reconstruct a gradient of a vector field   in the paper least  square gradient calculation from multi  point observation of scalar and vector field  methodology and application with cluster in the plasmasphere by j de keyser  et al   PRON seem that one could consider the vector field component as scalar field and feed this to the gradient calculation  under some assumption   PRON be write currently a generic c method that be suppose to compute this  where the result rank be determine by outer product trait class define for all combination available  scalar vector  vector  tensor  tensor  vector  vector  vector  etc   any thought on how to approach this   no assumption be need to reduce the vector least square problem to scalar least square problem  but PRON can take advantage of the fact that the matrix be the same for all vector component  so PRON can solve a single least square problem with multiple right hand side  save factorization  
__label__pde __label__finite-difference __label__numerical-analysis __label__fluid-dynamics __label__parabolic-pde burger  equation be a fundamental pde use in various field such as number theory  gas dynamic  heat conduction  elasticity  etc  PRON be crucial especially for develop numerical model for turbulence since nonlinear viscous burgers  equation can be consider as an approach to the navier  stokes equation  PRON can be state as    fracpartial upartial t   fracpartialpartial x  leftfracu22right   nu fracpartial2 upartial x2 where  altxltb   tgt0  with appropriate initial and boundary condition give   PRON can be determine later   let assume that the finite difference numerical method be consider to obtain approximate solution of this viscous equation  in PRON perspective  there be two decision need to be make to have stable solution  what kind of finite difference method can be consider for inviscid and viscous part  or both of PRON  ie  lax  wendroff  lax ricthmyer  crank  nicholson  lax  friedrich s  roe s scheme   how one can decide the value of the artificial diffusion number   nu   should PRON consider PRON relation with reynolds number since PRON be relate with the navier  stokes equation   note  this viscous burgers  equation be not go to be consider as linearize  PRON will be consider in a nonlinear form   
__label__neural-network __label__deep-learning PRON start to learn programming quite recently and  although PRON ultimate goal be to get into machine learning  PRON have not get near PRON yet in PRON study  so PRON be go to be a purely hypothetical  noob question   PRON just read an article about a legal requirement for company release software to be able to explain how the machine make a certain decision  use the example of a self  drive car cause an accident   as far as PRON understand  PRON be virtually impossible track down a single decision within a deep neural network  so PRON make PRON wonder  if each connection between the neuron have a delay correspond to the distance between the neuron on the diagram  and if the algorithm be judge by the speed of complete PRON s task at the evolution stage  would the network naturally organize PRON into cluster in order to maximize PRON efficiency  in turn make PRON easy for PRON to conceptualize the more abstract process   PRON be surely be already research but PRON can not find PRON online  likely due to PRON ignorance of the right keyword  so PRON would be grateful if PRON could refer PRON to the relevant read  thank in advance   
__label__classification PRON be currently work on a project that would benefit from personalized prediction   give an input document  a set of output document  and a history of user behavior  PRON would like to predict which of the output document be click   in short  PRON be wonder what the typical approach to this kind of personalization problem be   be model train per user  or do a single global model take in summary statistic of past user behavior to help inform that decision   per user model will not be accurate until the user have be active for a while  while most global model have to take in a fix length feature vector  mean PRON more or less have to compress a stream of past event into a small number of summary statistic    the answer to this question be go to vary pretty wildly depend on the size and nature of PRON datum  at a high level  PRON could think of PRON as a special case of multilevel model  PRON have the option of estimate a model with complete pooling  ie  a universal model that do not distinguish between user   model with no pooling  a separate model for each user   and partially pool model  a mixture of the two   PRON should really read andrew gelman on this topic if PRON be interested   PRON can also think of this as a learning  to  rank problem that either try to produce point  wise estimate use a single function or instead try to optimize on some list  wise loss function  eg  ndcg    as with most machine learning problem  PRON all depend on what kind of datum PRON have  the quality of PRON  the sparseness of PRON  and what kind of feature PRON be able to extract from PRON  if PRON have reason to believe that each and every user be go to be pretty unique in PRON behavior  PRON may want to build a per  user model  but that be go to be unwieldy fast  and what do PRON do when PRON be face with a new user  
__label__machine-learning __label__data-mining __label__nlp __label__dataset __label__text-mining PRON be wonder if there be any public dataset of google news with various news category such as politic  entertainment  lifestyle  general news  sport etc   PRON want to use such dataset for topic detection of various sentence or paragraph  PRON be plan to train a classifier with such a dataset and use PRON for prediction  however  PRON could not find any  be there any such known dataset available   this dataset be include with scikit  learn  a popular ml library for python   httpscikitlearnorgstabledatasetstwentynewsgroupshtml  PRON be posting to usenet and categorize by the group  the group title be not exactly  category  like PRON would see on google news  but each newsgroup be suppose to be on a specific topic as indicate by the name  so the concept be similar  for example   altatheism   atheism  compgraphic   computer graphics    recauto  automobile  recmotorcycle  motorcycles  here be a massive dataset of news with category which PRON create for exactly such a reason   httpswwwkagglecomtherohkindiaheadlinesnewsdataset  include all headline publish by times of india from 2001  2017 with category   contains 23 million entry  
__label__machine-learning __label__data-mining __label__clustering __label__statistics __label__visualization PRON have a large dataset like this       mat  cust  qtyag   net vag   demand hits  ag   qty   demand hits     net v   month       y   a    200   7000   200  769   957    769     1    x   a    100   1000   10  833   667    333     1    y   c    50   1750   8  192   38    192     1    x   b    20   2000   5  167   333    667     1    y   b    10   350   1  38    05    38     1    y   a    600   21000   78  860   867    860     2    x   b    60   6000   56  06    505    06     2    x   c    10000   1000000   45  989   405    989     2    y   b    98   3430   12  140   133    140     2    x   a    50   5000   10  05    90    05     2      this data be about sale of material  mat  to customer  cust   for each mat  cust combination PRON have the qty PRON order  qty   net value of PRON order  net v  and the number of order  demand hits  and the month  month    PRON also have percentage of order  net value  and quantity  for example  in the line one  mat y and cust a  the qty be 769   this mean that 769  of the total quantity of material y be order by customer a  so if PRON sum the qty  of mat y  cust a and mat y  cust c and mat y  cust b for month 1 PRON will add up to a 100    the same have be do for the other percentage as well   PRON be look for a good way to visualize this data  that could run into million of record even for one month  and also to statistically analyse PRON to draw any conclusion   any idea for an exploratory visual and statistical analysis   here be some question PRON be think about    be there customer who make up a large portion of the sale of particular item   be there specific mat  cust combo that can be consider  critical  base on demand hit or net value   can PRON cluster mat  cust into different category and treat each category differently base on PRON unique characteristic   be customer buying pattern repeat across time period   
__label__algorithms __label__finite-difference __label__poisson PRON be work on generate a  complex  solenoidal vector field  mathbfa from a prescribed  complex  vector  mathbfa and the gradient of a scalar   b  such that    mathbfa   mathbfa   nabla b  the condition that PRON require be then    nablacdotmathbfa   nablacdotmathbfa   nabla2 b  0 qquad longrightarrow qquad nabla2 b   b  where  b  nablacdotmathbfa  to complicate this slightly  PRON want to decompose  mathbfamathbfa   b and  b into    mathbfa   hatmathbfa   expiomega t  mathbfkcdotmathbfx     hatmathbfa   expiphi  mathbfa   hatmathbfa   expiphi  b  hatb  expiphi  b  hatb  expiphi  with this decomposition   b   nablacdothatmathbfa    imathbfkcdothatmathbfa    expiphi  and the poisson equation above become     daggerqquadnabla2 b   nabla2 hatb   2imathbfkcdotnabla b  lvertmathbfkrvert2 b  expiphi   hatb  expiphi  this be then solve for  hatb iteratively use the following solver scheme  give for 1dimension  but PRON be easily extend to high dimension       ddaggerqquad hatbj  fracdelta x2 hatbj  hatbj1   hatbj1   ikdelta xhatbj1   hatbj12  k2 delta x2  where  j in 2ldot  n1 indicate the discrete grid location  the position  xj  j delta x  and  delta x be the grid space  the boundary  j1n be fix at zero   hatbj be evaluate as  hatbj   hataj1   hataj12delta x ikhataj  the numerical solution to  hatb produce satisfie   nabla2 hatb   2imathbfkcdotnabla b  lvertmathbfkrvert2 b  hatb very well  however if PRON construct    hataj  hataj  frachatbj1   hatbj12deltax    ikhatbj  and evaluate    nabla cdot mathbfa  simeq bigfrachataj1   hataj12delta x   ik hatajbig  expiphi  then this be not only non  zero but also large  put the expression for  hataj in term of  hata and  hatb  one find that the above equation read       qquad nabla cdot mathbfa  simeq big  frachataj1   hataj12delta x   ik hataj  frachatbj2   2hatbj   hatbj24delta x2   2ikfrachatbj1   hatbj12delta x   k2hatbj big  expiphi   big  hatb   nabla2  2hatb   2ik fracpartialhatbpartial x   4k2 hatb  big  expiphi  this be just the 1dimensional version of   dagger  but with the laplacian operator evaluate as    nabla2  2 hatb   frachatbj2   2hatbj  hatbj24delta x2 suggest that PRON be this difference which be responsible for  nabla cdot mathbfa  neq 0  to overcome this  PRON attempt to invert      rather than use   ddagger   this time with boundary condition  hatbj  0  for  j  1  2  n1  n  but the  iterative  solution to the poisson equation diverge   PRON be wonder what be responsible for this  could PRON set the two boundary cell to zero because such a problem  the solver scheme for point  j involve  j2  j1  j1  j2 give the non  solenoidal vector  mathbfa  be there a good way to find a scalar  b such that  nablacdotmathbfa   nabla b   0    thank PRON for any help or suggestion   
__label__neural-network __label__feature-selection __label__feature-extraction __label__matlab if i train a network use neural network classifier  how can i know which feature be most important for predict the target variable  PRON mean how to create a  feature ranking  among the feature  from high importance value to lowi have see some literature about decision tree  adaboost but i be typically interested in neural network especially for classification purpose  to make PRON more clear  an example be show in the figure   httpspasteboardcogkbzs47png   there be multiple possibility   the obvious one be to sum weight of all connection from input layer to the first hidden layer per input neuron  the neuron with high sum of connection weight should be the most important  this have several problem   weights could be negative so PRON should use absolute value of PRON   the weight depend on the scale of the input variable  PRON should normalize the input for example by subtract mean and divide by standard deviation of the input  z  normalization    another possibility be to selectively remove feature  retrain the network and observe removal of which feature cause the big decrease in model accuracy  these be potentially the most important feature   the problem of this approach be  that PRON change the architecture of the network  retrain PRON and do not report the relevance of feature enter the original network   PRON can find an elaborate discussion on the problem here ftpftpsascompubneuralimportancehtml  there be multiple other approach  such as garson s  use complex function to correct for some problem  PRON can find several of PRON implement in neuralnettools package in r  the fir solution sevo propose be not feasible because of a third problem that be not mention  the first layer only learn a first representation of the input  which be use in later layer  even if the absolute weight of  x1  may be very big  if the later layer have small weight connect to these neuron the importance go down  this be exactly why neural network be consider to be difficult to interpret  the rest of the answer be useful  PRON just want to add this  
__label__machine-learning __label__r __label__feature-selection __label__random-forest __label__feature-extraction PRON be currently use a random forest model for classification  however PRON be unsure how the feature selection technique  varimp  work on r PRON understand the context of variable importance  however when PRON implement PRON in r PRON do not seem to produce the result PRON expect   when remove the most important variable  of 31 feature   the model s accuracy do not decrease  PRON would expect PRON to as PRON should be contribute the most to the model s ability to classify   could someone please explain what this function be do   what the function be do for each variable   record the out  of  bag  oob  accuracy for each tree    shuffle  or permute the value of that variable   this mean PRON take all the value of that variable in the datum  and assign those value randomly back out to the observation  which be a way of introduce noise and get rid of the signal that that variable provide   now PRON find the oob accuracy again  but this time the value for that variable be incorrect since PRON permute PRON   by introduce noise where PRON model expect signal  PRON should see a decrease in performance   compare the original accuracy in  1  to the accuracy in  3  for each variable   if the model performance decrease a lot for a variable in step  3  compare to  1   then PRON be deem to have great importance   why do remove the most important variable not have a negative affect on accuracy   PRON guess   probably because that important variable be correlate with some other variables  PRON have   PRON model can capture the information contain in that miss important variable by use a few other variable to make up for PRON   when PRON drop the important variable  which other variable see a notable gain   the varimp work on the principle how PRON variable be help  datum to split with minimum error and order PRON by PRON efficiency   as per PRON say even after remove the important variable PRON be still get same accuracy may be PRON be not affect PRON dependent variable to large extent or PRON do not perform need transformation   there be other way to find feature importance like recursive feature elimination  here be description rfe in caret 
__label__machine-learning __label__predictive-modeling __label__statistics __label__algorithms __label__regression consider one dependent variable  y  and 10 independent variable or features x1  x2  x3   x10   PRON want to create a non  linear polynomial regression model such that  y  a1x1b1  a2x2b2    a10x10b10  PRON be wonder be there any algorithm that will determine good possible value for power of independent variable that be value of b1  b2   b10 from datum   if all PRON care about be the quality of prediction  as oppose to explanatory power   skip linear model altogether and use gradient boost tree instead  gradient boosting can generally learn polynomial spline with ease  and PRON do not have to manually make a bunch of polynomial predictor PRON   by the way  gradient boosting be implement in python s scikit  learn library  r s caret library  and java  scala s weka library  
__label__neural-network __label__terminology spike neural networks be say to be the nn s third generation  feed  forward nn be the first  what be the second generation  stack nn   no   kumar  satish  neural network  a classroom approach  tata mcgraw  hill education  2004    the first generation of neural network model employ mcculloch  pitts tln type neuron  PRON be essentially switch network  the second generation of neural network model employ a smooth sigmoid  the sigmoidal function be motivate by the increase in firing rate of a neuron with the net input  which be saturate at a certain maximum frequency  the third generation of neural network model will use spiking neuron which encode information in spike train  
__label__machine-learning __label__unsupervised-learning __label__javascript PRON be work on a js library which focus on error handling  a part of the lib be a stack parser which PRON would like to work in most of the environment   the hard part that there be no standard way to represent the stack  so every environment have PRON own stack string format  the variable part be message  type and frame  a frame usually consist of call function  file  line  column   in some of the environment there be additional variable region on the string  in other some of the variable be not present  PRON can run automate test only in the 5 most common environment  but there be a lot more environment PRON would like the parser to work in   PRON goal be to write an adaptive parser  which learn the stack string format of the actual environment on the fly  and after that PRON can parse the stack of any exception of that environment   PRON already have a plan how to solve this in the traditional way  but PRON be curious  be there any machine learn tool  probably in the topic of unsupervised learning  PRON could use to solve this problem   accord to the comment PRON need to clarify the term  stack string format  and  stack parser   PRON think PRON be good to write 2 example from different environment   a   example stack string   statement on line 44  type mismatch  usually a non  object value use where an object be require   backtrace   line 44 of link script filelocalhost  gj  stacktracejs  thisundef     line 31 of link script filelocalhost  gj  stacktracejs  ex  ex  thiscreateexception     line 18 of link script filelocalhost  gj  stacktracejs  var p  new printstacktraceimplementation    result  prunex    line 4 of inline1 script in filelocalhost  gj  test  functional  testcase1html  printtraceprintstacktrace      line 7 of inline1 script in filelocalhost  gj  test  functional  testcase1html  barn  1    line 11 of inline1 script in filelocalhost  gj  test  functional  testcase1html  bar2    line 15 of inline1 script in filelocalhost  gj  test  functional  testcase1html  foo     stack string format  template    statement on line  frames0locationline    message   backtrace    foreach frame as frame   line  framelocationline  of  frameunknown0    framelocationpath    framecalledfunction    foreach   extract information  json      message   type mismatch  usually a non  object value use where an object be require     frame      calledfunction   thisundef       location    path   filelocalhost  gj  stacktracejs    line  44     unknown    link script         calledfunction   ex  ex  thiscreateexception       location    path   filelocalhost  gj  stacktracejs    line  31     unknown    inline1 script in             b   example stack string   referenceerror  x be not define  at repl15  at replserverselfeval  repljs11021   at repljs24920  at replserverselfeval  repljs1227   at interfaceltanonymousgt   repljs23912   at interface  eventemitteremit  eventsjs9517   at interfaceonline  readlinejs20210   at interfaceline  readlinejs5318   at interfacettywrite  readlinejs76014   at readstreamonkeypress  readlinejs9910   stack string format  template     type    message    foreach frame as frame    if framecalledfunction be undefined   at  framelocationpathframelocationlineframelocationcolumn    else   at  framecalledfunction    framelocationpathframelocationlineframelocationcolumn     if    foreach   extract information  json      message   x be not define    type   referenceerror    frame      location    path   repl    line  1   column  5         calledfunction   replserverselfeval    location    path   repljs    line  110   column  21             the parser should process the stack string and return the extract information  the stack string format and the variable be environment dependent  the library should figure out on the fly how to parse the stack string of the actual environment   PRON can probe the actual environment by throw exception with well know stack and check the difference of the stack string  for example if PRON add a whitespace indentation to the line that throw the exception  then the column and probably the call function variable will change  if PRON detect a number change somewhere  then PRON can be sure that PRON be talk about the column variable  PRON can add line break too  which will because line number change and so on   PRON can probe for every important variable  but PRON can not be sure that the actual string do not contain additional unknown variable and PRON can not be sure that all of the know variable will be add to PRON  for example the frame string of the  a  example contain an unknown variable and do not contain the column variable  while the frame string of the  b  example do not always contain the call function variable   
__label__caffe PRON be confused about the group parameter in caffe convolution layer   image PRON have an input of 20x3x227x227 and filter size be 96x3x11x11 with stride4 and padding0  if PRON set the group2  then the setting become   group1  20x1x227x227  filter1  96x1x11x11  group2  20x2x227x227  filter2  96x2x11x11  be that correct  if that be correct  how can PRON concatenate PRON to the output 20x96x55x55   
__label__software __label__matlab __label__languages __label__octave PRON want to learn matlab programming so that PRON can conduct some researh  analysis on PRON own and also  so that PRON can study  modify some matlab script that PRON have find online etc   however  the problem be that PRON can not afford matlab  gnu octave  from what PRON have hear  be quite matlab compatable  the challenge with octave though be that the documentation be very sparse   so matlab on one hand  be prohibitively expensive  ve   but have lot of good documentation  tutorial and script online     while octave be free  as in beer   even though  understandably  PRON lack comprehensive documentation  etc   PRON be think of download and instal octave  but use matlab documentation  and script  to help PRON get go  PRON be aware that there be some difference between octave and matlab  what PRON be try to work out be whether the difference be large enough to thwart PRON effort to  effectively   learn matlab by use octave    any constructive feedback welcome   for generally learn m  language programming and how matlab work  yes  octave be just fine  if PRON need a particular toolbox as part of PRON work  though  and no one have implement a free version of PRON  then PRON be out of luck   a student edition of matlab be not that expensive  if PRON be at a university  PRON be even possible that PRON have a site license  PRON should look into both possibility   practical example   PRON use matlab at work  and PRON take stanford s free online machine learning class last semester   PRON do all the homework in octave   PRON only notice 2 difference  PRON be only use core functionality    octave be a little more flexible about what sort of expression can be index  so if PRON take advantage of that  PRON code will not be portable   in PRON installation the pcolor   and image   plot do not work for reasonably sized array  like 1000x1000  small one  like 50x50  work fine   gnu octave be  mostly compatible with matlab   certain subtlety mean not all script be portable from matlab to octave   PRON be worth read the documentation for the language andor compatibility note in the faq or on wikibook  there be also port note   package similar to matlab toolbox exist  but PRON will need to check PRON out to work out how similar PRON be  also there be attempt at conversion script that take care of the difference between the two language  but to PRON knowledge none be perfect   there be some subtle difference in octave s interpretation of the matlab s programming language  octave use  endif  and  endwhile  to close if and while statement respectively  octave also allow PRON to declare function on the command line  PRON should  however  parse anything that matlab pars too  so if PRON stick to matlab s programming language  PRON should be fine   be aware that the big difference between the two program be in the build  in function that do most of the numerical heavy  lifting  eg  quad    ode15s  and such  all of these function should  however  be well document when use the  help  command  there be also mail list for octave should PRON have more specific question that be not cover by the online help   yes PRON can learn matlab via octave   but octave syntax be less restrictive and more in line with modern scripting language  matlab seem behind in this respect   see this wiki link matlab programming  difference between octave and matlab  another major difference to PRON be the availability of certain library for matlab but not for octave   disclaimer  PRON sometimes get annoyed when somebody try to tell PRON what PRON think PRON ought to do rather than answer the question PRON ask   but PRON be go to take a risk and suggest an alternative to PRON   PRON would suggest look at python s scientific computing package  numpy  matplotlib  and scipy   together  PRON provide PRON most of the core functionality of matlab  in some case PRON give PRON more than matlab    PRON be free and open source  and nowadays may have a wide user base than octave   much of the scientific computing infrastructure in python be support by enthought inc  and PRON would recommend that PRON install PRON free enthought python distribution  which include all three package PRON mention and more   of course  there be feature of matlab  particularly in the toolbox  but octave do not have those  that python lack   but PRON use python in combination with low  level language for most of PRON work  and PRON allow PRON to program in a language almost as convenient as matlab  interface with a host of useful package that matlab do not have  and to run the same easy  to  read code on a supercomputer   edit  PRON may also want to try the anaconda python distribution from continuum analytics   there be a free version that include all the above package and much more   PRON have use octave and matlab interchangeably on the same piece of code in the past  PRON have have no problem with compatibility   the main difference as far as PRON can tell be   matlab can be byte  compile  and be slightly fast   octave can use either  or  for comment  matlab only use   octave produce much good plot   regard PRON question   PRON would say yes  simply because the paradigms and principle be the same  the difference in the syntax be neglectable and can be summarize in a cheat sheet  in case PRON be work mainly with matlab one day    another suggestion for an alternative   as a matlab alternative PRON be use scilab  PRON be free as octave  have a decent community provide help and support and PRON be documentation be pretty good  the build  in documentation and help for function be similar to matlab s one   what PRON like most be the ability to execute the example with one click   the syntax be similar to matlab as well   regard the similarity and difference between scilab and matlab  this document may be of interest for PRON  an introduction to scilab from a matlab user s point of view  eike rietsch  may 2010  pdf    another plus for scilab over matlab  start  up and gui be a lot faster  PRON have not try octave yet   PRON be indeed possible  especially if PRON use a gui  such as guioctave   another language that PRON may find useful because many package have be write for PRON be r there be also a gui for the r language  call rstudio  and a translation document between r and matlab   yes PRON absolutely can  PRON do  much of the power out of matlab however do come with some of PRON hugely easy to use toolbox and builtin which may or may not have equivalent in octave   also be aware that while the base languag PRON be compatible  except for the newish oo feature in matlab   octave have a few  addition  to the syntax  like be able to use     for neq  and PRON also have some alternate default behavior like how PRON parse inline function  these be minor thing that will because octave code not to run in matlab right off the bat unless PRON have take care to avoid rely on that behavior   also matlab be a full computing environment  and not just a language that interfac with optimize linear algebra routine  so what PRON learn out of octave will only go so far  PRON will be functional in matlab if PRON be proficient with octave  but PRON will not be as productive with matlab as PRON could be   yes  PRON can learn matlab use octave   of course there be some limitation   octave and matlab share a much of PRON syntax   the two be interchangeable in that respect   the experience in matlab be more rich and user friendly  particularly when work with graphic  though octave have a new graphical user interface  gui  that be in beta   the two system will likely converge further as the graphical environment for octave improve   there be a lot of free course on coursera that give PRON a free temporary matlab student license for the duration of the course   search for machine learn course   the good free language which have matlab  like syntax be julia  PRON be also faster and have a more extensive package system  among other reason why PRON be good    but the linear algebra syntax be almost exactly the same  many algorithm PRON can translate to matlab by change ai  for indexing to ai    PRON believe PRON be the good language to learn right now  and PRON will  accidentally know matlab  just by use PRON  
__label__neural-networks __label__deep-learning __label__convolutional-neural-networks __label__deep-network as PRON may know  capsule networks have be recently introduce by hinton  there also have be several head up within PRON talk   as expect  the paper elaborate on the idea way theoretically  however  as a fan of occam s razor  PRON be wonder if anybody can simplify the idea behind the capsule networks or capsenets   thank  capsule network have two key idea   the first idea be how to represent multi  dimensional entity  capsule networks do this by group these property of a feature together   capsule     the second be that PRON activate high  level feature by agreement between low  level feature   rout by agreement     first  capsule networks partition the image into region subset   for each of these region  PRON assume that there be at most one instance of a single feature  call a capsule   a capsule be able to represent an instance of a feature  but only one  and be able to represent all the different property of that feature  eg  PRON  x  y  coordinate  PRON colour  PRON movement etc   the difference from convolutional neural networks  cnns  be that the capsules bundle the neuron into group with multi  dimensional property  whereas in cnn the neuron represent single  unrelated scalar property   this structured capsule representation allow PRON to do  rout by agreement    to understand this  let look at the example of a face detector  here  PRON could have capsule represent  mouth    eye    nose  etc   since the capsules be multi  dimensional PRON also train PRON to predict the parameter for the entire face   now  if the  mouth    nose  and  eye  capsule agree about the parameter of the face PRON have a very strong signal that this be a good prediction since accidental agreement in a high  dimensional space like a neural network be very unlikely   PRON use this to stack the capsules into deep network where the activation of high  level capsules be condition on agreement between the low  level capsules  eg the face capsule be activate by agreement on the face position between the nose  mouth  eye capsules in the early  low  level layer    in contrast to regular feed  forward net this require a bit of iteration in the forward pass through the network  but PRON can still use back  propagation train PRON   PRON be an interesting way to add a bit of structure to the datum  so far  PRON look like PRON be able to provide good generalization from limited training datum   one of the major advantage of convolutional neural network be PRON invariance to translation  however this invariance come with a price and that is  PRON do not consider how different feature be relate to each other  for example  if PRON have a picture of a face cnn will have difficulty distinguish relationship between mouth feature and nose feature  max pooling layer be the main reason for this effect  because when PRON use max pooling layer  PRON lose the precise location of the mouth and noise and PRON can not say how PRON be relate to each other   capsules try to keep the advantage of cnn and fix this drawback in two way   invariance  quote from this paper  when the capsule be work properly  the probability of the visual  entity be present be locally invariant – PRON do not change as the  entity move over the manifold of possible appearance within the  limited domain cover by the capsule   in other word  capsule take into account the existence of the the specific feature that PRON be look for like mouth or nose  this property make sure that capsule be translation invariant the same that cnn be   equivariance  instead of make the feature translation invariance  capsule will make PRON translation  equivariant or viewpoint  equivariant  in other word  as the feature move and change PRON position in the image  feature vector representation will also change in the same way which make PRON equivariant  this property of capsule try to solve the drawback of max pool layer that PRON mention at the beginning  
__label__python __label__convnet __label__keras PRON be currently work on design a certain number of cnn for extract feature from image   the image be spectogram  and each have a shape be  276x3    x be here the number of column  which incidently also the length of the feature vector that should  be create   so the cnn somehow have to use a kernel that have to be 276 row and 1 column wide  but be PRON possible in kera to make a 2d kernel and perform 1d convolution  the most important factor be the 1d convolution and the shape of the 2d kernel  as PRON be be use to alter the importance of some of the entry in the spectogram    designing cnn that do one column convolution across the x  axis  so the cnn somehow have to use a kernel that have to be 276 row and 1 column wide  but be PRON possible in kera to make a 2d kernel and perform 1d convolution   PRON be not sure PRON understand PRON question correctly  but if PRON have input image that be 276 high  x500 wide and have 3 color channel  then the follow perform column  wise convolution across the x  axis   row  col  chan  276  500  3    modeladdconvolution2d64  row  1  inputshaperow  col  chan       the output shape of this layer will be  1  500  64    PRON insert  64  as the number of filter  PRON can set that to whatev PRON need  whatev work best   note  shape be specify in tensorflow mode  ie the order be  row  col  channel    depend on PRON configuration PRON may have to change those shape to theano mode   channel  row  col    so the cnn somehow have to use a kernel that have to be 276 row and 1 column wide   PRON actually have to use a kernel that be 276 row  1 column and 3 channel wide since PRON input image have 3 channel  PRON would be 276 row and 1 column wide if PRON use grayscale image  but that should not matter  
__label__machine-learning __label__neural-network __label__deep-learning PRON be wonder about the benefit of advanced activation layer such as leakyrelu  parametric relu  and exponential linear unit  elu   what be the difference between PRON and how do PRON benefit training   relu  simply rectify the input  mean positive input be retain but negative give an output of zero   hahnloser et al  2010      fx   max0x      pros   eliminates the vanishing  explode gradient problem   true for all follow as well   sparse activation   true for all follow as well   noise  robust deactivation state  ie do not attempt to encode the degree of absence    con   die relu problem  many neuron end up in a state where PRON be inactive for most or all input    not differentiable   true for all follow as well   no negative value mean mean unit activation be often far from zero  this slow down learn   leaky relus  add a small coefficient    lt1   for negative value   maas  hannun   amp  ng 2013      fx   begincas   x  amp  textif  x geq 0   01 x  amp  textotherwise   endcases      pro   alleviates die relu problem   true for all follow   negative activation push mean unit activation closer to zero and thus speed up learn   true for all follow   con   deactivation state be not noise  robust  ie noise in deactivation result in different level of absence    prelu  just like leaky relus but with a learnable coefficient   note that in the below equation a different  a can be learn for different channel    PRON et al  2015      fx   begincas   x  amp  textif  x geq 0   a x  amp  textotherwise   endcases      pro   improve performance  low error rate on benchmark task  compare to leaky relus   con   deactivation state be not noise  robust  ie noise in deactivation result in different level of absence    elu     fx   begincas   x  amp  textif  x geq 0   aexpx1  x  amp  textotherwise   endcases      replace the small linear gradient of leaky relus and prelu with a vanish gradient   clevert  unterthiner  hochreiter 2016   pro   improve performance  low error and faster learn  compare to relus   deactivation state be noise  robust  
__label__machine-learning __label__neural-network __label__reinforcement-learning __label__q-learning PRON have be play with an algorithm that learn how to play tictactoe  the basic pseudocode be   repeat many thousand time   repeat until game be over   ifboard layout be unknown or explore    move randomly   else   move in location which historically give high reward      for each step in the game   determine board layout for current step  ifboard layout be unknown    add board layout to memory    update reward for board layout base on game outcome      now play a human and win   exploration  in the beginning the algorithm explore aggresively  and this reduce linearly  after say a thousand game PRON only explore in 10  of the move  all other move be base on exploitation of previous reward   reward  if the game result in a win  then award 10 point  if the game result in a draw  0 point  otherwise 5 point   actually  these reward can be  tune   so that if the game be short and PRON be win  then award more point or if PRON be longer award less point   this way the algorithm prefer win quickly  that mean that PRON learn to win as soon as possible  rather than aim to win later on   that be important so that PRON do not miss win immediately  if PRON miss such a move the opponent would likely a  move there to avoid let the ai win next time  and b  think the algorithm be stupid because PRON miss an  obvious  win   this algorithm do indeed learn  so PRON can class PRON as a mach learn algorithm   PRON think  but PRON be not sure  that PRON be a reinforce learning algorithm  however  accord to httpswwwcseunsweduaucs9417mlrl1tdlearninghtml PRON be not temporal difference learning  because PRON do not estimate the reward until the end  and PRON should be estimate the reward as PRON go along  that may mean that PRON be not reinforce learning   question 1  can PRON successfully argue that PRON be estimate the reward base on history  and still claim the algorithm be reinforce learning or even q  learning   question 2  if PRON replace the reward lookup which be base on the board layout  with a neural network  where the board layout be the input and the reward be the output  could the algorithm be regard as deep reinforcement learning   question 3  PRON would do not think that PRON have either a learning rate or a discount factor  be that important   PRON notice that the algorithm be pretty useless unless PRON train PRON with at least every move which an opponent try  so in a way PRON feel like PRON be use brute force rather than really  learn   this make PRON question whether or not machine learning tictactoe be really learn   PRON agree that use a neural network to learn image recognition can be class as learn because when PRON see an unknown image PRON be able to state PRON classification  but that be quite useless for game like tictactoe where similar look board layout be totally unrelated  one may lead to a win  the other may lead to a loss   so   question 4  can tictactoe algorithm be class as real learning rather than simply brute force   kudo for figure out a work tic  tac  toe playing algorithm from scratch   question 1  can PRON successfully argue that PRON be estimate the reward base on history  and still claim the algorithm be reinforce learning or even q  learning   first thing first  this be definitely not q  learning   however  PRON do think PRON classify as reinforcement learning  PRON have implement these key component of rl   a state  the current board   use as input on each step   an action  desire next board arrangement   use as output  when the action be effectively to choose the next state directly  this be sometimes call the afterstate representation  PRON be commonly use in rl for deterministic game   reward generate by the environment  where the agent s goal be to maximise expect reward   an algorithm that can take datum about state  action and reward  and learn to optimise expect reward through gain experience within the environment   PRON algorithm be close imo to monte carlo control  which be a standard rl approach   one of the big advantage of q learning be that PRON will learn an optimal policy even whilst explore  this be know as off  policy learning  whilst PRON algorithm be on  policy  ie PRON learn about the value of how PRON be currently behave  this be why PRON have to reduce the exploration rate over time  and that can be a problem because the exploration rate schedule be a hyper  parameter of PRON learning algorithm that may need careful tuning   question 2  if PRON replace the reward lookup which be base on the board layout  with a neural network  where the board layout be the input and the reward be the output  could the algorithm be regard as deep reinforcement learning   yes  PRON suppose PRON would be technically  however  PRON be unlikely to scale well to more complex problem just from add a neural network to estimate action value  unless PRON add in some of the more sophisticated element  such as use temporal  difference learning or policy gradient   question 3  PRON would do not think that PRON have either a learning rate or a discount factor  be that important   a discount factor be not important for episodic problem  PRON be only necessary for continuous problem  where PRON need to have some kind of time horizon otherwise the predict reward would be infinite  although PRON could also replace the discount mechanism with an average reward approach in practice    the learning rate be an important omission  PRON do not explain what PRON have in PRON stead  PRON have put update reward for board layout base on game outcome  that update step typically have the learning rate in PRON  however  for tic  tac  toe and q  learning  PRON can actually set the learning rate to 10  which PRON guess be the same as PRON approach  and PRON work  PRON have write example code that do exactly that  see this line which set learn rate to 10  however  more complex scenario  especially in non  deterministic environment  would learn badly with such a high learning rate   question 4  can tictactoe algorithm be class as real learning rather than simply brute force   PRON algorithm be definitely learn something from experience  albeit inefficiently compare to a human  a lot of the more basic rl algorithm have similar issue though  and often need to see each possible state of a system multiple time before PRON will converge on an answer   PRON would say that an exhaustive tree search from the current position during play be  brute force   in a simple game like tictactoe  this be probably more efficient than rl  however  as game get more and more sophisticated  the machine learn approach get competitive with search  often both rl and some form of search be use together  
__label__statistics __label__linear-regression __label__parameter PRON have in mind that r  squared be the explained variance of the response by the predictor  but PRON would like to know how the adjust value be compute  and if the concept have any change from the original   a google search for r  squared adjust yield several easy to follow explanation   PRON be go to paste a few directly from such result   meaning of adjusted r2  both r2 and the adjust r2 give PRON an idea of how many datum point fall within the line of the regression equation  however  there be one main difference between r2 and the adjusted r2  r2 assume that every single variable explain the variation in the dependent variable  the adjusted r2 tell PRON the percentage of variation explain by only the independent variable that actually affect the dependent variable   what be the adjusted r  squared   the adjusted r  squared compare the explanatory power of regression model that contain different number of predictor   suppose PRON compare a five  predictor model with a high r  squared to a one  predictor model  do the five predictor model have a high r  squared because PRON ’ good  or be the r  squared high because PRON have more predictor  simply compare the adjusted r  squared value to find out   the adjusted r  squared be a modify version of r  squared that have be adjust for the number of predictor in the model  the adjusted r  squared increase only if the new term improve the model more than would be expect by chance  PRON decrease when a predictor improve the model by less than expect by chance  the adjusted r  squared can be negative  but PRON ’ usually not    PRON be always low than the r  squared  
__label__classification __label__svm __label__graphs PRON have a collection of 80k graph  half of PRON be label 1  PRON represent the city of france  and the other half be label 0  represent city in the usa   PRON goal be to create a classifier that can predict whether a graph  represent a city   use a kernel on graph   since node in graph represent city be generally of degree  ltlt  number of node  PRON decide to use the all connect 345 graphlet kernel present in this article  article about all connect graphlet kernel   what this kernel do be simple  PRON represent each graph g by a vector fg  which be the distribution of the all connected 3  4 and 5 graphlet in the original graph  there be 29 different graphlet of size 3  4 and 5 so every graph be now represent by a vector of length 29  the article then define the kernel between two graph g and g  by the inner product between the two vector   kg  g     ltfgfggt   this mean that this kernel do not behave like other kernel PRON be use to work with  PRON specifically know the feature map and the hilbert space associate to the kernel  the hilbert space be r29 and the feature map be f  then in order to create the classifier PRON use svm   but here be the problem  if PRON use the feature map f as PRON be  then compute the gram matrix and apply svm  all prediction on the test datum be 0  that be pretty sad   what PRON have be notice be that 3 of the 29 graphlet be much more present than the other in the graph  so for a typical city  95  of the distribution be concentrate on theses 3 graph  which mean that when PRON compute the kernel  the inner product   the gram matrix have really similar value  the standard deviation of the matrix be 00003  naturally  PRON though  PRON want to transform f so that the difference between graph be clear   so PRON compute the mean distribution m of 345 graphlet in PRON database  PRON derive the inverse mean distribution m   PRON just invert every element of m   and instead of use f  i use f  which take a graph g  compute fg   and then fg  by multiplying point by point fg  and m   that way  the difference between graph when compute the kernel be more easy to see  note that the kernel be still valid since PRON be still an inner product in r29   PRON have no idea how mathematically accurate be this intuition  but when PRON apply svm with this other feature map f   PRON get a 82  accuracy  and the auc be 088  the roc curve be beautiful too  so PRON must have do something right   PRON actual question be why do svm like the feature map f  more than f  PRON mean PRON do not add any information use the new feature map  PRON just modify the old one use PRON intuition   
__label__neural-network PRON have a big list of spare part with several parameter  material  weight  size  manufacture complexity     for some part in this list  a price have either not be set or have to be adjust in order to be in line with other part  there be a few obvious and simple correlation in this dataset  for example   if material and complexity be the same  big part be more expensive   if size and material be the same  more complex part be more expensive   for equal size and complexity  more expensive material lead to high price   try to figure out all these rule by hand and stick PRON together seem to be an endless endeavour  so PRON think about train a neural network with the price part  parameter  input  and price  output  and let PRON figure out the price for the part which do not have a price yet  the decision of the nn could be supervise by an expert who know the part and could manually figure out a price   question 1  be this a good idea in general   question 2  if yes  what type of nn would be most suit for such a problem   yes PRON can use a neural network  or any other regression  like algorithm for this task  there be lot of algorithm that PRON can use  a simple feed  forward network should suffice  although  if PRON be not a machine learn expert  PRON be assume that PRON be not base on PRON question   PRON would suggest that PRON stick with something a little less exciting like ols  PRON be easy to implement   then PRON just need to spend some time do a lot of feature engineering  so that PRON can feed not just raw variable but meaningful feature to the model  for example the combination of feature like size  complexity  material would give a lot of model to work with   question 1  be this a good idea in general   solve this problem as a supervised learning regression problem be a fantastic idea and be the type of solution that will greatly benefit PRON company since PRON will translate to other similar and dissimilar problem much more easily than deterministic method   however use a neural network to solve this supervised learning regression problem be probably a very bad idea  neural network can add great value to certain problem  be among the very good algorithm for complex problem where compute power be not an issue  and have fascinating implication for the future of machine learning and artificial intelligence  but  PRON can be very tricky to train  require a lot of compute power  require a lot of datum  and perform poorly in many case   PRON suggest PRON scope the problem use linear regression and then try a support vector regressor  svm  svr  or naive baye regressor  the analogue method in svr work very well with limited datum and provide surprisingly accurate result   question 2  if yes  what type of nn would be most suit for such a problem   if PRON must use a neural network then try play with the problem   start with a feed forward neural network  note that PRON will likely underperform an svr   then think about move toward a convolution neural network  again  this be probably a very bad way to go  try linear regression follow by other method first   move forward  the documentation for either scikit  learn in python  h20 in java  or weka provide very shallow learning curve to jump on the merry  go  round and take a spin or two  please make sure that PRON thoroughly understand cross validation and scoring metric as this be essential to continued  adequate progress   hope this help  
__label__machine-learning PRON have a data set of tweet regard vaccine   PRON have be collect from an api because PRON have keyword like  flu  measl  mmr  vaccine  etc   PRON need to find tweet specifically about measle and the outbreak that occur in california this past february   PRON be not enough to search the datum set for word like  california  and  measles  because tweet like  mmr vaccination rate in palo alto on the rise  be about measle and california  but will not be capture by a naive search   be there any unsupervised algorithm that could help PRON out   a couple idea   if PRON have a large datum set of tweet  PRON could try use latent semantic indexing to find out which term be semantically relate base on PRON usage and co  occurrence  then after transformation  PRON can apply some document similarity metric  eg cosine similarity  to find tweet most relevant to PRON query  eg  california measl    use knowledge or lexical database like dbpedia and wordnet to calculate semantic relatedness between PRON query and the text of PRON tweet  or to identify tweet with related concept  
__label__eigenvalues __label__arpack PRON be use arpack to compute the eigenvalue of the problem  lambda mx  ax with reverse shift method with complex shift   a and  m be real   m be symmetric  then  PRON use znaupd e zneupd  PRON use superlu to solve the system  everything seem to be ok  PRON write a function that take the eigenvalue and the eigenvector compute with arpack and put PRON in the original problem   lambda mx ax  textresidual the norm of the residual be suppose to be almost zero or very small  anyway  PRON obtain an error of  103 that be quite high  do PRON have any idea where the problem can come from  PRON hope PRON be quite clear   thank   nora  
__label__performance __label__io __label__data-handling PRON plasma dynamic simulation often produce too much information  during the simulation PRON record various physical property on a grid  x  y  z  t  that be as large as  8192x1024x1024x1500   for at least 10 property  this information be process after the simulation be complete  with PRON PRON  make movie of property   perform a fourier analysis   calculate average property   this simple dumping of as much information as possible work fine when PRON study small system  this give PRON the flexibility to interact with the result and decide later what PRON want to do with PRON  PRON also allow PRON to allot PRON computational resource  cpu time  to simply run the simulation   PRON have begin the process of do the fourier analysis on the fly  and filter for only a select range of length scale  for numerical reason  PRON sometimes need to resolve length scale that be small than PRON be actually interested in  so in those case  this filter help greatly  PRON be also explore various parallel io library  eg parallel i  o option  in particular parallel hdf5   what strategy be available to maximize the efficiency of datum processing   be there any benefit to perform all analysis  not include post processing  eg movie and plot  on the fly   PRON can imagine this issue come up in other area of research  for example  PRON may have a molecular dynamic simulation that need to evolve for a long time  but PRON be interested in the brief moment when something interesting be happen  or in cfd  the early time development may be slow  but once turbulence set in  PRON may need a high time resolution to monitor dynamic   be there freely available example of sophisticated result collection from simulation   peter lepage be pretty famous in lattice  qcd circle for suggest a method whereby unfeasibly  large lattice grid could be reduce by find and apply good short range analytic solution   this be roughly equivalent to noticing that a set of well choose spline can allow accurate integration with few knot than the trapezoid method  except that as in PRON case PRON get to take advantage of PRON over four dimension at once    the result be that PRON trade raw size of the data  set for more computation per node  step  but come out ahead in the end because of the high dimensionality of PRON problem   PRON be not a subject PRON know well enough to give any decent hint on  but PRON have work in some field in the past   PRON think the current master of this art be the large particle physics experiment  PRON be most familiar with cdf and d0 because PRON be old and work at the university of chicago   PRON have hardware trigger that discard petabyte  or more  a year  however  this be the whole subject of quantization  discretization  or  throw away only what PRON do not need   PRON be not sure PRON can give a sensible answer in general  PRON would be good to narrow down the problem to something like   PRON have a pde simulation discretiz in the following way and would like to efficiently downsample    PRON think PRON may have to split PRON output to match PRON target   for the movie of property  PRON probably do not need the full spatial resolution and all variable  carefully choose what PRON want to show and think about the final resolution of the movie PRON will be show  PRON probably will not have 8 billion pixel   for the fourier analysis  or thing like pod   if PRON be temporal  PRON can probably just sample a few hundred point wisely choose in PRON domain  if PRON be spatial  PRON probably only need a few snapshot and not 1500  and again  not of all property   for time averaging  PRON can just keep add to the same field and do not have to worry about the time dimension right  spatial averaging be painful though  especially if PRON want to look at PRON evolution over time  but more online processing before dump the datum could reduce the size of PRON   this mean quite a bit of work to have dedicate output instead of a big generic one but this should help keep the cost and size down  hope this help   just one more thing PRON want to add  in general  the full resolution of the datum be only need for restart file  ie file to restart PRON simulation  PRON do not need that many of these for a give simulation  let PRON say 100  so that if something happen between 2 restart PRON lose at most 1  of PRON computation   whereas PRON probably want to crank up the frequency of output for PRON movie  and PRON can do that at just 164th of the resolution for example  1 every 4 point in each direction    the question be a bit broad  so PRON will provide a correspondingly vague answer that suggest possible technique in such case   1  on  the  fly processing  which PRON be already work on  one way to do on  the  fly processing and yet decouple PRON from the data  generate step be to generate a cyclic output file that always contain the last n step  and have the analysis run in a separate process  obviously PRON must synchronize the two to prevent a race condition   2  choose the store datum more carefully  this be highly situation  specific  unfortunately   3  compress PRON datum before store PRON  or use a storage library with integrated compression option  such as hdf5   4  store regular checkpoint instead of full output  if PRON store a full checkpoint every n step  ie enough datum to restart the simulation from there  PRON can reconstruct the miss datum in a highly parallel fashion if and when necessary  note that in the case of monte  carlo method  the checkpoint must include the state of the random number generator  PRON can actually consider this a highly application  specific compression technique  
__label__neural-networks __label__genetic-algorithms __label__genetic-programming __label__neat in the neat paper PRON say   the entire population be then  replace by the offspring of the remain organism in each specie   but how do PRON take place   PRON mean like be PRON pair and then mat   cause this would lead to fast extinction would not PRON   or be PRON pair each with each  this would lead to overpopulation very fast   how be PRON pair   neat have the constant number of organism in PRON population  which prevent overpopulation from happen   the process of mating include the following  firstly  the  bad network from every specie be remove  secondly  all specie receive a number of offspring that PRON can have  this be calculate by an adjusted neural network fitness   thirdly  offspring for specie be divide among neural network in those specie  fitter neural network have more offspring  finally  network from same specie be combine and create an offspring   speciation prevent fast extinction  
__label__finite-element __label__numerical-analysis __label__convergence __label__advection-diffusion when PRON have fine  scale feature  eg boundary layer  in the solution  PRON fem approximation on coarse mesh converge at strange apparent rate  look at cea s lemma  be this behaviour because of   the good approximation error behave strangely   or the constant change with h   or should PRON actually look at strang s first lemma   error in numerical integration of very fine  scale datum   PRON be because PRON typically neglect high order term in error estimate  for example  PRON can show that     e le cu  h2   cal oh3       the point be that when  h be small  the cubic term be small and can be neglect  in fact  when  h be small  PRON can observe quadratic convergence  but whenever  h be not small  where  small  be relative to the feature of the solution   the cubic and possibly even high order term can be large compare to  ch2  and will lead to behavior that be not as easily predictable  
__label__neural-networks __label__backpropagation PRON build this nn in c  PRON review PRON since 3 day  PRON check every line 100 time  but PRON can not find PRON error   if someone can please help PRON find the bug   1  the output be garbage  2  the weight go from 2e79 down to 18e80 after approximatly 400 iteration   mat flipmat m    mat outmncol  mnrow    for  int i  0  i  lt  mnrow    i   for  int j  0  j  lt  mncol    j   outj  i   mi  j    return out     layerlayerint node    randenginetime0      y  mat  node  1    net  matnode  1    e  matnode  1      layerlayerint node  int nextnode    layernodes     thisgtnextl  nextl   auto random  binduniformrealdistributionltdoublegt1  1   randengine    w  matnextnode  node    for  int i  0  i  lt  wnrow    i    for  int j  0  j  lt  wncol    j    wi  j   random           layerlayerint node  layer  nextl    layernode  nextlgtynrow     thisgtnextl  nextl     void layerfeedforward      nextlgtnet  wy   for  int i  0  i  lt  nextlgtynrowsi   nextlgtyi   signextlgtneti       void layerbackprop      for  double d  w   cout  ltlt  d  ltlt   t    e  flipwnextlgte   for  int i  0  i  lt  enrow    i    ei    neti    1  neti     cout  ltlt  ei   ltlt   t      w   lratenextlgteflipy       void layerbackproplastmat t     for  int i  0  i  lt  enrow    i    ei   neti    1  netiti   yi     cout  ltlt  ei   ltlt   t        void layerfeedforwardlayer  nextl     thisgtnextl  nextl   feedforward       double layersigdouble x     return 1   1  expx       networknetworkvectorltintgt  top    toptop     network  new layertopsize      networktopsize    1   new layertopback      for  int i  topsize2  i  gt  1  i   networki   new layertopi   networki  1       network  network      delete   network     void networkforward      forint i  0  i  lt  topfronti   network0gtyi   inputi    for  int i  0  i  lt  topsize    1    i   networkigtfeedforward       void networkforwardvectorltdoublegt  input     setinputinput    forward       void networkbackprop      networktopsize    1gtbackproplasttval    for  int i  topsize    2  i  gt  1  i    networkigtbackprop         void networkbackpropvectorltdoublegt  tvals     settvalstval    backprop       PRON know PRON a bunch of code but PRON be really desprate since PRON can not find what s wrong  PRON test PRON with a simple xor   edit   heres PRON main code    include  networkh    include  ltiomanipgt   use namespace std   vectorltvectorltdoublegtgt  input    00011110     vectorltvectorltdoublegtgt  trueval    0101     int main     ifstream fouttxt   fstreamout    fclear     cout  ltlt  fix   cout  ltlt  setprecision5    network net251     vectorltdoublegt  in  t  out   auto buf  coutrdbuf     for  int i  0  i  lt  1000    i    coutrdbuffrdbuf      in  inputi  4    netforwardin    out  netgetoutput     t  truevalsi  4    netbackpropt    cout  ltlt   n    coutrdbufbuf    if   i  101continue   cout  ltlt   PRON    ltlt  i  ltlt   n    cout  ltlt   int    for  double d  in   cout  ltlt  d  ltlt      cout  ltlt   n    cout  ltlt   outt    for  double d  out   cout  ltlt  d  ltlt      cout  ltlt   n    cout  ltlt   truet    for  double d  t   cout  ltlt  d  ltlt      cout  ltlt   n    double err  netgeterror     cout  ltlterrtltlt  err  ltlt   n   ltlt   n      coutrdbufnull    fclose     return systempause       a little search on google answer PRON question   xor input space be not linearly separable  PRON mean that PRON can not separate the input point in a 2d space into 1 area and 0 area by simply draw a line between PRON  PRON require at least 2 line to separate the xor input space and consequently 2 output node  use as classifier rather than regression   PRON can easily find PRON detail in google  search  xor problem in neural net    PRON can manually implement the desire neural network with two output node act as classifier as follow   where a  amp  b be two output node which act as classifier  by form aa  and bb  decision line respectively during training by backpropagation   the interpretation of the outputs of the node be give in the table where net column represent the overall output to be interpret   PRON show the above manual implementation just to give PRON the idea of how classification be do behind the scene   here be the actual automatic implementation   here  all the task be perform by the neural net behind the scene and PRON get the desire output from the output node in the topmost layer 
__label__convnet __label__torch documentaion for spatial convolution define PRON as  module  nn  spatialconvolutionninputplane  noutputplane  kw  kh   dw    dh    padw    padh    ninputplane  the number of expect input plane in the image give into forward     noutputplane  the number of output plane the convolution layer will produce   PRON do not have any experience with torch but i guess i have use a similar function in keras  convolution2d64  3  3  bordermodesame   inputshape3  256  256    which take as input the shape of the image that be 256  256 in rgb   PRON have read usage of spatial convolution in torch as below but unable to figure out what do the ninputplane and noutputplane paramter correspond to   local convlayer  nn  spatialconvolutionmm384  384  1  1  1  1  0  0   in the code above what do these 384384 represent   the ninputplane be the depth or the number of the layer of the input image  in case of rgb image  this should be 3 which correspond to the first number in the inputshape3  256  256    the noutputplane be the the number of the layer of the volume that the convolution step will produce which be also the number of filter  kernel apply to the input  by convention  there be an output layer for each filter  this correspond to the first argument of the convolution2d function  
__label__image-recognition __label__learning PRON be currently work on a prototype of an application that should be able to interact with user interface   now every user interface have some common element  like button  scrollbar  input field etc   PRON would like to use machine learning to  interpret  such user interface in a way  in which PRON later can input a user interface as an image for example  and let the prototype then  try out  the interface  meaning  click on button  use scrollbar inputt some text into input field etc   PRON know that this would have to be do use image recognition  since there be many different ui   PRON be specifically interested in websites  adobe reader with an open pdf  that in turn can be a form etc    and word with an open document  again this can contain form etc     now PRON main question be if there be already some research go on in this field that PRON can use  or even an exist tool for part of the process   any help be appreciate   PRON would try experiment with recurrent neural network  httpkarpathygithubio20150521rnn  effectiveness recurrent neural network can output sequence of variable length give input of variable length  in PRON case a recurrent neural network may output a sequence like the follow when give a user interface  click a button  select a field  type some text  hit enter  for another interface  the network may output only  click one button  click another button  and that be PRON  this would be useful for PRON because the sequence and length of action from interface to interface may change a lot   PRON could also experiment with reinforcement learning and build an algorithm that have an objective  reach some final page in as few action as possible   the algorithm would start by do random thing  like click the same button a bunch of time   and then gradually learn over time to take appropriate action  if PRON go that route PRON could use deep learning and monte carlo tree search  mcts  like what alpha go do   in either case PRON be go to need a framework that can train an algorithm quickly because PRON be likely to have to go through a lot of iteration  tensorflow  httpswwwtensorfloworg  be one option  PRON have start use PRON recently  and PRON like PRON a lot because of PRON easy of use   tensorflow be capable of build both recurrent neural net and deep neural net  
__label__finite-element __label__boundary-conditions PRON be test a finite elements code  to do this  i have create a simple one element quadrilateral mesh to verify the solution   this be a plane stress elasticity problem  and the local stiffness be compute through the follow weak formulation   the problem data be the follow   the resultant stiffness matrix and load vector   after apply a penalty boundary condition  multiply key boundary point in the matrix by a very large number  the final stiffness matrix be   and after solve the linear system above  the solution be   now  PRON be try to solve the same problem in a rotate mesh  45 degree counter clockwise   and PRON be face some problem to impose the dirichlet boundary condition  ie to impose the null displacement in the specified node   here be the new node coordnate   the new mesh be   the rotate stiffness matrix be the follow   and the rotate load vector be   the solution must be the same in both mesh  but PRON be face a problem to impose the new boundary condition  how can i impose the natural boundary condition in the rotate mesh   any help will be appreciate  thank PRON   the large  number approach PRON be use to impose constraint be generally refer to as a  penalty method  one upside of penalty method be that PRON be generally easy to apply  a downside  be that add large number to the equation make PRON ill  condition and can because error in the  solution   one way to look at penalty method be to write the total potential energy of PRON system     etp   frac12utku  utf     where  u be the vector of nodal displacment   k be the global stiffness matrix  and  f be the load vector  constraint on the  value of  u can be prescribe by include an additional term in the potential energy     etp   frac12utku  utf  fracr2sumi1nc  giu2     where the  giu be constaint equation that be function of  u   r be a large number  and  nc be the number of constraint   the equation   gi  equal zero when the constraint be satisfied   in PRON un  rotate case  the  gi be simply the component of  ui PRON want to constrain   in the rotate case  PRON want to impose constraint on the displacement in the rotate coordinate system  at a particular node  j these can  be write as     leftbeginarraycuprimej  vprimejendarrayright    leftbeginarraycccostheta  amp  sintheta  sintheta  amp  costhetaendarrayright   leftbeginarraycuj  vjendarrayright      so a typical  gi may be  costheta uj  sintheta vj if PRON want to constrain  uprimej to zero   by minimize the potential energy with respect to the  u component  just like PRON  do to find the stiffness matrix and load vector  there will also be a set of  2 time 2  constraint matrix  one for each displacement constraint  PRON want to apply  these be add to the global stiffness matrix just like the large diagonal term PRON add in the un  rotate problem   PRON can find more information on apply constraint in fem in these set of note from carlos felippa  multifreedom constraints PRON  multifreedom constraints ii  PRON refer to these type of constraint as  multifreedom  because PRON be a linear combination of more than a single degree  of  freedom in the fe model  PRON also discuss the alternate  lagrange multipli approach which be a good approach  numerically  to apply these type of constraint   if u impose a boundary condition u1v10  v20  u40  the single rectangular mesh should not rotate at all because node one be lock into place  node two can stretch horizontally while node 4 can stretch vertically leave node three free to mode   apply a stretch force of 6000lbs in the horizontal direction at node two and at node three will deform the element from a rectangular shape to some unknown shape but not rotate the rectangular mesh   when the rectangular mesh be rotate  the u and v boundary condition have change even though PRON have rotate the input force  the stiffness matrix should remain the same only the boundary condition have change    kjintegral  btdbdv where  fkx    rotate  x  whould change the integral by a factor equal to the jacobian matrix but not the stiffness matrix    fkx    equ    und    ebdwhere q be the differential operator  
__label__python __label__scikit-learn PRON be work on a multilabel text classification problem with 10 label  the dataset be small    7000 item and  7500 label in total  PRON be use python sci  kit learn and something strange come up in the result  as a baseline PRON start out with use the countvectorizer and be actually plan on use the tfidf vectorizer which PRON think would work better  but PRON do not  with the countvectorizer PRON get a performance of a 01 high f1score   076 vs 065   PRON can not wrap PRON head around why this could be the case  there be 10 category and one be call miscellaneous  especially this one get a much low performance with tfidf   do anyone know when or why tfidf could perform bad than count   PRON need to formalize this for PRON thesis   there be a few possibility  first  there be some variability in performance  PRON could have be only by chance that countvectorizer perform better than tf  idf  do PRON use cross validation  with how many fold   be the superior performance of the countvectorizer reliable  PRON would compare performance across fold to make sure countvectorizer consistently perform better   second  if PRON find that countvectorizer reliably outperform tf  idf on PRON dataset  then PRON would dig deep into the word that be drive this effect  PRON may be that common word  word which will appear in multiple document  be helpful in distinguish between class  there be substantial research that show that use of some function word  eg first person singular pronoun   PRON   change depend on someone ’s psychological state  function word like pronoun be very common and would be down weight in tf  idf  but give equal weight to rare word in countvectorizer  PRON be not suggest that first person singular pronoun in particular be drive PRON result  but PRON ’ worth look at what word be drive the effect  PRON would examine which word be important in both type of model  countvectorizer and tf  idf  and then think about whether the word that be most important for the countvectorizer make sense in the context of PRON text document and label  also  be PRON remove stop word  PRON could also see how the model perform with and without stop word  which would be another way to test whether frequent word be actually help PRON to distinguish between class   PRON be curious  which algorithm do PRON use  on sklearn PRON be say that naive bayes algorithm perform on word occurrence  PRON add that multinomialnb for instance may work with ratio figure like tdidf feature but PRON seem PRON not guarantee   PRON be currently work on a similar problem than PRON and use onevsrestclassfiier wrap classifier like sgdclassifier or linearsvc  multinomialnb give a predicition score  of 0  with tfidf feature  much bad than the other classifier  
__label__r __label__error-handling PRON be attempt to compile code use knitr in r  PRON code below be return the follow error  and cause error in the rest of the document   misslt  samplesensorglucoseisnasamplesensorglucose     error     warning  isna   apply to nonlist or vector  of type  null    strmiss    int  1103  213 113 46 268 186 196 187 153 43 175   do anyone know how to remedy this problem   thank in advance   PRON agree with ssdecontrol that a minimal reproducible example would be the most helpful  however  look at PRON code  pay attention to the sequence error   warning     PRON believe that the issue PRON be experience be due to an inappropriate setting of r s global warn option  PRON appear that PRON current setting be likely 2  which refer to convert warning to error  whereas  PRON  most likely want the setting 1  which be to treat warning as such  without convert PRON to error  if that be the case  PRON just need to set the option appropriately   optionswarn1    print warning as PRON occur  optionswarn2    treat warning as error  note for moderator  administrator  this question seem not to be a data science question  but purely an r question  therefore  PRON think PRON should be move to stackoverflow  where PRON belong  
__label__machine-learning __label__ai-design __label__training PRON be develop an ai tool to find know equipment  error and find new pattern of failure  this log file be time base and have know  message  information and errorim use a javascript library event drop to show the datum in a soft way  but PRON real job and doubt be how to train the ai to find the know pattern and find new possible pattern  PRON have some requirement   1  the tool have to do not depend of extra environment installation or the less possible  the perfect scenario be run the tool entire on the browser in standalone mode    2  possibility to make the pattern analyzer fragment  a kind of modularity  one module per error   which be the recommend kind of algorithm to do this  neural network  genetic algorithm  etc   exist something to work use javascript  if not what be the good language to make this ai    give how general the question be  PRON will just provide a general  approach that could be adapt or modify into achieve something  more specific   by log file analysis  PRON be go to assume that this be in reference to web log and tracking  and since PRON do not know what type of analysis PRON wish to achieve  PRON will also assume that PRON be attempt a classification problem  or something similar to PRON   which be the recommend kind of algorithm to do this  neural network  genetic algorithm  etc    the most general approach be to select a supervised learning  classification algorithm and have PRON learn the function or type of analysis PRON wish to achieve by train PRON with know example of datum pattern  by this  PRON mean  select an algorithm capable of map or classify unknown web log or track datum pattern to a predefined set of category  these category may be time spend  scroll  visit per page  visit per user  and so forth   those category be probably the basic  and PRON could easily be do manually  so for a more in depth process PRON could attempt analyse any correlation in the datum by utilise an approach similar to bagging   bagging or bootstrap aggregating be a type of algorithm use in ensemble learning  ensemble learning refer to the approach of apply or use multiple algorithm to achieve good predictive  classification performance   bagging rely on the process of obtain replica of the training datum set  web log  tracking datum  etc   and train the classifier on these replica   so essentially   different training datum subset be obtain randomly from the entire  training dataset   each training datum subset be use to train a different classifier   individual classifier be then combine by take a simple majority  vote of PRON decision  where the choose majority vote  result in  the ensemble decision   with try to identify any correlation between page visit  take a sufficiently large population of size n  large enough to ensure that any form of sample and resampl from the population do not repeatedly result in any substantial overlap   from the population  sample a subset of PRON  train a classifier on this subset sample  resample the population by replace the log visit in the previous subset with new log visit from the population  and train again  repeat this process for all the classifier in the ensemble with different subset sample  eventually  each classifier should determine PRON own correlation or measurable trait regard the log vist in the population   the ensemble algorithm then determine what this correlation or measurable trait be by consider the majority classification determine by the classifier compose the ensemble   hint  so simply   identify what PRON be try to measure or analyse   generate datum for training in which that measurable trait can be  discover   specify the measurable trait as a parameter   select a number of sufficiently similar supervised learning  algorithm   train each of these algorithm on the training datum   if successful  identify or define more parameter and just repeat  the above   what be the good language to make this ai   just have a look at python library for log file analysis  and also scala for pattern match inline with recognition  functional programming and strong for static type system   note PRON think the key step be define or redefine this problem as a supervised learning problem and go from there   also have a look at this log 
__label__pde __label__linear-algebra __label__petsc this may be a petsc newbie question  but   PRON be use petsc to solve a large sparse linear system  the initial creation of the matrix be fairly slow  which PRON be give to understand be largely due to memory allocation  that be tolerable for the moment  PRON problem be that as the simulation progress  the structure of PRON domain  mesh change such that the size of the result matrix will need to be increase  do PRON really need to create a brand new matrix for this each time  or be there a way to just resize an exist matrix   ie in order to avoid the cost of re  allocate the entire matrix from scratch    PRON have try call matsetsiz  but PRON give PRON an error   can not change  reset row size to      be PRON do something wrong  or be this a fundamental limitation   the general advice give on the mailing list be usually about efficient assembly   the short answer be  no  this be a fundamental limitation  the long answer be that  for one  there be no  end  all  be  all  sparse matrix implementation  in general  PRON will be fast to   loop over PRON mesh  domain  count how many entry PRON will need in PRON assemble matrix  create  allocate say matrix  assemble matrix  most of the time will be spend in the linear  nonlinear solver  so the matrix assembly be small  well  at least that be the goal  compare to compute the solution   PRON may want to check out create a vecscatter  or even better  try to use petsc s dmda  for structured  or sieve  experimental  for unstructured and mostly fem domain  to manage PRON mesh  these petsc mesh object will maintain much of the nitty  gritty partitioning  matrix creation  and vec scatter for PRON  so PRON do not have to write the mpi code PRON   the short answer be no   once a matrix have be assemble  PRON can not resize PRON  matsetsiz   be use for set the size before assembly   PRON would look into make the matrix creation faster  if PRON preallocate the matrix appropriately  see for example this question in the petsc faq  
__label__machine-learning __label__preprocessing __label__normalization for the distribution show below  PRON want to convert the exponential distribution to a normal distribution  PRON want to do this be as part of datum pre  processing so that the classifier can better interpret the feature  name ipc here    the regular log transformation do not work here because of the  x  axis  spread   how can PRON transform this datum to a normal distribution   a related answer have be point out in the comment but PRON be look for some python code excerpt as well   thank  the follow code work   import scipy  import numpy as np  ey  nprandomexponentialsize100   cdfy  scipystatsexponcdfnpsortey    invcdf  scipystatsnormppfcdfy   a normal distribution  hope this help 
__label__machine-learning __label__time-series __label__anomaly-detection PRON want to find outlier in power consumption in real  time  at hourly rate  ie  at the end of the hour  PRON should say whether power consumption in current hour be outlier  anomalous or not   approach   till now  PRON be do with follow step  say PRON want to find whether power usage between 9 am to 10 am be anomalous  for this  PRON first find the usage of past n day during the same time interval  then PRON find the mean  median of all the previous usage  now  PRON have usage of the current day and the mean  median usage of previous n day  which statistical measure should PRON use to declare whether current day usage be anomalous or not   use above approach  for 24 hour of a specific  test  day and use past 10 day consumption  PRON have obtain result as   figure interpretation  black line represent usage of current hour of current day  red and blue line represent mean and median of past 10 day for same time interval  from the visual inspection  PRON can say that the usage between 0710   0800  and between 2210  2300 be anomalous as there be big difference between actual and previous mean  median usage  PRON do not know which statistical measure should PRON use to point out such anomalous instance automatically  use the discuss approach   PRON have be use one heuristic to detect outlier value that be very simple  start by calculate an interval center on the mean and with three standard deviation in radius  next  calculate a second interval use tukey s box and whiskers measure  place the interval limit at the whisker  limit  finally  calculate the union of both interval and use this new interval to detect PRON outlier  any observation outside this interval be a potential outlier  please note that PRON may have to tweak both interval to calibrate for PRON desire precision  also  PRON may have to take into account possible trend and seasonalitie in these interval  depend on how the historic datum behaf  
__label__algorithms __label__numerical-limitations PRON be read algorithm design manual by skiena  which say in chapter 8  section 814 when talk about the calculation of binomial coefficient   intermediate calculation can easily because arithmetic overflow  even when the final coefficient fit comfortably within an integer   PRON can not think of a reason why  should not whatev calculation be do in between always be less than the final result   the problem with binomial coefficient be that PRON be a quotient of factorial  which may be rather large and in particular much large than the final binomial coefficent     binomnk   fracnkn  k  for example   binom105252  but  103628800   the numerator   in another example  binom100575287520  and thus can be represent as a regular integer  while  100  approx 10158 can not be represent as an integer   thus  when calculate the binomial coefficient  PRON ’ usually unwise to just calculate  n and  kn  k and divide the two  because PRON computer may not be able to handle  n instead  PRON need to consider way that do not contain large number as intermediate result such as     binomnk   prodlimitsi1k fracn1ii  here  each factor be a comparibly small integer and no intermediate result be large than the final result   even then  binomial coefficient may become very large easily  and even be beyond the limit of float  and may only be intermediate result PRON  PRON may thus need to resort to calculate everything in the logarithmic domain and make use of an approximation of the factorial such as stirling ’s  most math library provide a loggamma function for such purpose  which return  loggamman logn1  on the other hand  many math library also provide a reasonable implementation of binomial coefficient    intermediate result may overflow even if the final result do not  there be way one can re  arrange an equation so that this do not happen but PRON be up to the programmer  maybe an optimize compiler that be also a theorem solver can do PRON too  but PRON have never see one  and the cpu will not re  order a sequence of arithmetic instruction on PRON own   instead of try to prove PRON in theory PRON will merely provide a trivial example of at least one instance of PRON happen   assume an 8 bit machine for simplicity   int8 a  10  int8 b  30  int8 c  c   a  b   2  the end result be 150  which fit in an 8 bit register  but the intermediate result   a  b  be 30 x 10 which be 300 which do not fit in an 8 bit register and thus overflow   due to the overflow  if one be to run the above calculation on an 8 bit machine  the result would be a surprising 22 instead of 150  300 mod 256 be 44 and 442  22   
__label__python __label__r __label__spss PRON have the problem that PRON get a huge source datum file which be show text for all variable value instead of numerical id  so for example  PRON would like to have the variable gender cod as 1 and 2 instead of  female  and  male  write out  and equally the same for 200 other variable of which some have up to hundred of distinct variable value   therefore  do this manually be not really an option here   could anybody please point PRON to a solution or hint within r  spss or python how PRON can assign numerical id to each distinct variable value   PRON think this would be a problem other people face more commonly as well  but PRON have find nothing of this kind at all   thank PRON for any help   PRON can use the python sklearn preprocess labelencoder  here be some example code from this page with PRON comment    make a label encoder instance  le  preprocess  labelencoder     show PRON the datum PRON have to encode  so PRON column  lefitparis    paris    tokyo    amsterdam      get a order list of all class PRON find  listleclass     transform a column  list  letransformtokyo    tokyo    paris      transform encode back to original  listleinversetransform2  2  1     spss have an autorecode command which will do the whole job with one command  for example   autorecode vr1 to vr100 into kvr1 to kvr100print   this will take text variable vr1 to vr100 and recode PRON into new numerical variable kvr1 to kvr100 in which each textual category in the old variable be now automatically number in the new variable  with the textual category now use as a value label   the print sub  command will show PRON in the output window a list of all the number code choose for text category in each variable   please note  use the to convention  as in  vr1 to vr100   only work when the variable be consecutively order in the file  if PRON be not  PRON have to name PRON separately   in r PRON turn PRON categorical value into a factor   dfrid  asnumericfactordfrmycolumn   
__label__machine-learning __label__r __label__classification PRON be new to data science and be work on a dataset have roughly 213000 row and 31 column  the 31st column be PRON response variable have value 0 s and 1 s  PRON a classification problem and the datum set be unbalanced  as after PRON apply logistic regression to PRON  PRON get a model accuracy of 9979   however  by just count the total number of 0 s and 1 be PRON would still show an accuracy of 99 as PRON correctly classify max no of 0 s   also the confusion matrix be of no help too   PRON do some digging up and learn to use precision recall in such scenario  PRON question be should PRON even use logistic regression on this dataset  and then use precision recall   if anyone can shed any light on what approach should PRON take  that would really help PRON move forward   find a solution  for an unbalanced dataset  first use smote and then apply any model to use check auc  PRON may use logistic regression if PRON dataset follow some clear polynomial curve  which PRON can verify by plot the datum and look how PRON be distribute  in that case  even if the datum be skew PRON may get a good classifier   the fact that PRON achieve high accuracy  99  as PRON mention  by always predict one of the value be the reason why classifier accuracy be not use to evaluate a logistic regression model  instead precision and recall give PRON much good insight into the quality of the classifier because PRON measure both how many of the example PRON classify as positive be actually positive and how many of the positive example in the training set PRON classify correctly   the tradeoff between both metric help PRON find a classifier which be both precise but which can generalize   when evaluate PRON algorithm  especially when PRON dataset be unbalanced  PRON should use more metric than just accuracy  the accuracy be how many example PRON have correctly identify in total  as PRON have see if PRON have an unbalanced dataset where 05  of PRON instance be 1 be then this will result in 995  accuracy if PRON blindly set all PRON output as zero  this be obviously wrong albeit the high accuracy  the accuracy be calculate as   accuracy  fracsumtp   sumtnsumtp   sumtn   sumfp   sumfn  where tp be true positive  tn be true negative  fp be false positive and fn be false negative   if PRON want to capture the performance of PRON unbalanced dataset PRON should look into the percentage of fp and fn PRON be calculate  PRON can do this use the sensitivity and the specificity  calculate the sensitivity as   sensitivity  fracsumtp    sumtp    sumfn     and the specificity as   specificity  fracsumtn    sumtn    sumfp      an ideal classifier should have the accuracy  specificity and sensitivity all be 1  this would mean every sample be correctly classify  in PRON case where PRON be get very high false negative  PRON will see that PRON sensitivity will be very low  this be a measure with which PRON can state that PRON algorithm be perform poorly  PRON be good form to always include these metric in any statistical study PRON be do  accuracy alone be not sufficient to prove that PRON be obtain good result   moreover  there be the receiver  operator curve  roc   this will tell PRON PRON false positive rate for any true positive rate  PRON can then calculate the area under this curve  auc  to get a comparable metric of performance   all of these should be use together when exclaim the performance of PRON algorithm  the roc and auc can be omit however leave out the sensitivity and specificity of PRON algorithm be unwise  
__label__optimization __label__algorithms __label__nonlinear-programming __label__constrained-optimization __label__nonconvex PRON be look at a problem of constrained minimization  where the function to be minimize contain the heaviside function  and as such be not twice continuously differentiable   PRON question be what effect would the use of a twice continuously differentiable approximation to the heaviside function have on the accuracy and efficiency of the optimization   the problem take the form   min sumin  kileft   hleft  xi  right  tright      st  sumin  kileft   xi   rright  le 001  where  hleftx right   be the heaviside function  and  x  k  t  rinmathbbr  would the use of the approximate heaviside   hleft  x right  approx frac12frac12tanhkxfrac11e2kx  help with the minimization and give a sufficiently accurate result  or would a legendre polynomial expansion  similar to httpwwwphysufledufry6346legendresteppdf  be more successful   this approach will need to be extend into multiple dimension before implementation  for example in 3 dimension the minimization become   sumisumjsumlk1ik2jk3lleft  hleft  xijkright  t right     and the constraint   sumisumjsuml  k1i  k2jk3lleft  xijk   r right  le 001  the minimization be cover by the question constrain minimization in n dimension  this question be regard the effect of use an approximation to the step function in the algorithm  and ultimately the choice of algorithm to use   if the number of variable PRON have be modest  then PRON can just subdivide the domain into the half  line  quadrant  octant  etc of the space  in each of PRON  the heaviside function be constant  for example  for a single optimization variable  PRON original problem be     minx khxt    textst    kx  r  le c     PRON can split this into two optimization problem  first  solve     x1   arg minx kt   qquad textst    kx  r  le c quad textand  quad xle 0      then solve     x2   arg minx k1t    qquad textst    kx  r  le c quad textand  quad xge 0      note that both of these be linear optimization problem and be  consequently rather simple to solve  PRON be possible that one of these problem have no solution because the feasible set be empty   if only one of the problem have a solution  then PRON be do  if both have a solution  PRON simply have to compare the value of the objective function at these point to find which one be good   if PRON have more than one variable  xi  then the problem become slightly more complicated  for example  if PRON have two  then PRON need to consider four case of sign of the variable  xi for general   ndimensional problem  the number of case to compare be  2n this will quickly become impractical  but for modest number PRON should be able to deal with PRON  in particular  PRON can use the usual trick of integer programming  eg branch and bind  if PRON have already find a solution  xast from one of the sub  problem  then this provide PRON with a value of the objective function that PRON need to get below to find a good solution  PRON believe PRON can use this in the dual formulation of linear program to rule out that a give one of the other subproblem have a good solution  without even solve PRON  in effect  PRON may get away with solve many few than  2n linear program   if PRON be optimize with respect to the  ki  j   ki  j be not an argument to the heaviside function  for each  i  j   and the  xi and  t be know parameter  PRON problem be continuous and  twice continuously  differentiable with respect to  k PRON problem still look to be nonconvex at first glance  so deterministic global optimization would require a branch  and  bind method with convex relaxation   if the  xi be decision variable  with or without the  ki  j   then PRON problem be not just non  differentiable  PRON be discontinuous  consequently  again  PRON be nonconvex and require a branch  and  bind method  generally speak  as wolfgang point out  if PRON know location of discontinuity  PRON can subdivide PRON problem into a number of small subproblem on which PRON may be able to use more efficient method  read  not branch  and  bind  preferable some second  order optimization algorithm   the number of subproblem may be unacceptably large  as wolfgang point out  in this case   2n  where  n be the number of decision variable that be argument to heaviside function    if PRON replace the heaviside function with the hyperbolic tangent approximation  PRON gain twice  differentiability  but give that the hyperbolic tangent be nonconvex over any subinterval contain 0  PRON may also introduce nonconvexity into PRON problem  as a result  PRON do not think PRON gain much by introduce that approximation  and PRON would only use PRON in the event that PRON be impractical to divide the problem into many subproblem over which all function be convex  continuous  and twice  differentiable   if the problem be just about nondifferentiability and not about discontinuity  PRON would say PRON could try look at subgradient method or bundle method  these be slow than method that require some degree of differentability  PRON be worth note that even derivative  free method require some degree of differentiability  PRON merely assume that derivative information be unavailable  
__label__algorithms __label__compiling suppose a program be write in two distinct language  let PRON be language x and language y  if PRON compiler generate the same byte code  why PRON should use language x instead of the language y  what would make a language better than the other to resolve a problem  what define than one language be fast than other   about the language speed  PRON ask that because often PRON see people say thing like   c be the fast language  ats be a language fast as c   PRON be seek to understand the  fast  definition for programming language   the question PRON ask  in general  be quite difficult to answer because there be just too many language for so many thing  each language have be build either to satisfy some necessity or for some intend audience  also  even for general  purpose language PRON think these be build in a way PRON can serve good to a particular subgroup   anyway  PRON do not know what be PRON expectation about use a programming language in general  so PRON will try to answer PRON question in a intuitive  maybe even wrong   and bias  way  first  let talk about execution speed  speed and size be not directly relate to the semantic of a language in particular  but on how PRON associated compiler translate PRON source code into processor instruction  in the era of 256 kb ram mainframe  code efficiency be a must  thus most language try to stay as close to assembler code  faster  processor  dependent programming language  as possible  two example of this era be fortran vi and kampr c nowadays  the gear be shift toward  great level of abstraction  java  c  f2003  etc   and high performance  f2008  upc  c  etc  language  which intertwine new language semantic with system architecture  PRON think that PRON will get into a point in which make one single processor faster would become very expensive due to the appearance of quantum effect  so the future may be go towards massively parallel system  mainframe  again   in order to be fast   about which language be good  PRON depend  depend on what field be PRON  on the exist  previous code PRON have to work on  and personal taste of course  for example  people on mathematic andor work on fem would tend to work more in language that let have high level of abstraction  ie c  pascal   whereas people work on the physics andor engineering science tend to work more with  procedural   not quite true  but   language like fortran  upc  or c why the difference  to be honest PRON do not know  but some people tend to be really opinionat about this  of the five language PRON have mention  PRON can do whatev the mathematician  physicist  engineer do  with relative ease  how do PRON know  well  PRON decide to do a parallel 2d navier stokes solver in fem both in c and fortran and  from the programming perspective  the difficulty be relatively the same  on the c code PRON get 130 line more of code  but that ’ a different story    advice  try to pick a language that PRON know be widely use in the specific realm PRON be go to be work  learn all the available programming paradigms  also parallel  in that language  by corollary  move to another language would be just a matter of learn the  new word   there be lot of thing include clarity of syntax  expressibility  eg  array slicing  support for whole array operation etc    availability of of api  libs etc  eg  if PRON want to use openmp then PRON choice of language be rather limited  same argument hold for thing like mpi  numerical libs etc   in an ideal world PRON should be able to use any language that would eventually generate the same machine code  PRON may be true a few year  decade down the road especially with thing like llvm    suppose a program be write in two distinct language  let PRON be language x and language y  if PRON compiler generate the same byte code  why PRON should use language x instead of the language y   programmer productivity be important  if PRON be easy to get the job do with language x  which could be for various reason  like syntax  library  tool  than language y  that be a net win  if PRON compiler generate the same byte code  as far as PRON know  PRON will execute in roughly the same amount of time under comparable condition  same hardware  same load  etc     what would make a language better than the other to resolve a problem   performance be usually a limit consideration  if performance matter  PRON be almost certainly go to look at a compile language and exclude interpreted language  after that  PRON be usually some combination of personal preference and programmer productivity to maximize useful work do before deadline   for an example of a productivity versus performance argument  interpret language like python  matlab  and julia typically lead to short program accomplish a given task  here be one example of an empirical ranking  but there be other  such as in code complete  this phenomenon typically have a number of consequence  like   PRON be usually fast to write such a program  PRON be usually fast to debug such a program  PRON usually mean there be large standard library available to accomplish common task  however  the cost of this expressiveness be usually performance  which PRON can see in benchmark  so the typical pure python program be usually 2  3 order of magnitude slower than a typical c program  for julia  PRON be suppose to be more like a factor of 2ish  which be why PRON be popular   also  PRON have to know  and be good at  these language to extract performance from PRON  and for matlab and python  if a pure interpreted language implementation be not fast enough  PRON have to start replace part with compile language implementation anyway  once PRON start delve into multi  language program  productivity go down  and PRON be hard to debug  similarly  if PRON have to learn one of these language  that take time  and when PRON be not a language expert  PRON program more slowly than if PRON be a language expert  so there be a number of tradeoff  
__label__machine-learning __label__statistics PRON have a data set that be pivot in to the following format    key   PRON would   0   1   5   10   15   60   120   180     365   so key could be   product   1000   15000   4000   etc  where product be the category of item be review and key be the identifier for the product  the only field  0  1   180    365   be individual daily sample identify how many of  x  product be log as either sell  in  stock etc   what PRON need to do be perform some kind of analysis on an entire slew of product and PRON inventory level  ie each import of datum PRON need to make sure the incoming datum be accurate or predictably accurate and that some human do not typo a stock level  the problem be  use a simple average or rolling average can introduce significant variance and smooth out the average render PRON analysis less reliable  ideally this analysis would trigger an alarm that someone would have to investigate   be there a good and more accurate way of perform this analysis   thank   the first thing PRON should do be to identify how large an error PRON analysis can handle  that will make PRON job much easy because PRON will not have to find everything   a standard way of identify  suspicious  data be be benford s law  which predict the distribution of the first digit of each number  PRON can also be generalize for for other digit  httpenwikipediaorgwikibenfordslaw  as for find outlier  PRON would probably use boxplot  particularly because PRON can achieve high datum density with PRON  reduce the time to manually skim PRON   one thing that may be useful be to compare the ratio of one variable to another in PRON company PRON use this method all the time  
__label__machine-learning __label__pandas PRON be try to reproduce an algorithm design in a paper  and everything be go well except one thing   PRON say PRON consider the length zero  mean accelerometer vector and create a feature for the mean and standard deviation of this value  and PRON do not understand what be PRON zero  mean vector   example dataset   06946377  12680544  050395286  5012288  11264028  095342433  4903325  10882658  008172209  061291564 18496431  30237172  11849703  12108489  7205164  13756552  24925237 6510526  061291564 1056939  5706926  050395286 13947236  70553403  can any body help PRON   PRON find only this information httpswwwquoracomwhatdoesitmeanwhenavectoriszeromean but PRON be not sure about PRON   thank PRON    zero  meaned  mean the vector have be transform so that PRON mean be 0   typically  PRON would do this by subtract the mean of each column from that column   this be for dimensional as well as algorithmic reason  PRON do not want to subtract a person s weight from PRON height    PRON sound like here PRON be actually talk about the row mean  that is    06946377  12680544   050395286 would be transform to   4857924  85172577  365933344  41632863  740047  where the first three be the original feature minus the row mean  the fourth be the row mean  and the fifth be the standard deviation of the original feature   this would make sense if the three have the same unit  if PRON be all acceleration at the same scale  this work   and so PRON want a separate measure of how much PRON be be accelerate at all and how much PRON be be accelerate in a particular direction   mean center be one of many related technique to preprocess datum for downstream analysis in multivariate method   PRON may sound odd at first  but PRON mean exactly what PRON say  the vector have a mean of zero  in pseudocode   sumvector   lenvector     0   in multivariate datum  this typically be apply along each column in a dataset so each column can be more easily compare to another within a similar range of datum  after mean center  each row only include how PRON differ from the average sample from that variable in the original datum  typically  sample be also scale to have unit variance as well  allow PRON to more readily compare the datum across continuous variable with different range   for example  if PRON have a dataset of patient with variable height  weight  age  householdincome  despite each variable be a continuous value each of these variable will be in different range  height may be between 60  75 inch  weight between 100  300 lb  and so on   why do all this   remove the mean and standardize the variance will help downstream method not  learn  the mean and variance of PRON datum  make PRON easy to find relationship between variable  many assume that PRON data be center  scale  normalize in some way and will behave poorly if PRON do not do so  
__label__linear-algebra __label__matrices __label__linear-solver __label__reference-request PRON be look to solve a matrix equation and not sure where to start look for resource   the equation be    ax  xb  c  where  ainmathbbrntime n   binmathbbrmtime m   cinmathbbrntimes m and  xinmathbbrntime m   alternatively  ax  xa  c with  n  m would be useful    PRON can see that PRON could change PRON into a system    amathbfx    mathbfb  with  a  in mathbbrnmtime nm    mathbfxin mathbbrnm    mathbfbin mathbbrnm  but be wonder if there be any shortcut PRON could take   this be a sylvester equation  although normally the matrix would all be square  even for rectangular matrix  the bartels  stewart method apply  also a hessenberg  schur method for the problem  axxb  c  golub  nash  van loan    the idea be to use the schur decomposition    a  urutop  qquad btop  vsvtop     so that  axxb  c be transform into    ryystop  f  qquad y  utop xv  f  utop cv     since  r be upper  triangular and  stop low  triangular  this can be use to solve the system directly  if  y   y1  ldot  ym be the column of  y  and  fk be the column of  f  then the equation for  yk depend only on  yk1ldot  ym  and can be solve with back  substitution      rskkiyk  fk  sumjgtk  skjyj     when use the real schur decomposition  this would sometimes be a couple linear equation in  yk  yk1 instead   this also show PRON do not matter if  mneq n to convert to the usual sylvester equation with  m  n one could also just pad the matrix with zero to obtain an equivalent problem  
__label__python PRON have the follow code in python   import numpy as np  from scipysparse import csrmatrix  m  csrmatrixnpones2  2dtype  npint32    printm   printmdatashape   for i in rangenpshapem0     for j in rangenpshapem1     if ij   mi  j   0  printm   printmdatashape   the output of the first 2 print be    0  0   1   0  1   1   1  0   1   1  1   1   4    the code be change the value of the same index  ij  and set the value to zero   after execute the loop then the output of the last 2 print be    0  0   0   0  1   1   1  0   1   1  1   0   4    if PRON understand the concept of sparse matrix correctly  PRON should not be the case  PRON should not show PRON the zero value and the output of last 2 print should be like this    0  1   1   1  0   1   2    do anyone have explanation for this  be PRON do something wrong   sparse matrix be a matrix that have most of PRON element as zero  matrix on the other side of PRON  be dense  csrmatrix create an all zero element matrix and if PRON be update the element as  mi  j   0   well  PRON be not really do anything as element be already zero  for example  import numpy as np  from scipysparse import csrmatrix  mat  csrmatrix22   dtype  npint16toarray    print matn   for i in rangenpshapemat0     for j in rangenpshapemat1     mati  j   ij  print mat  give    0 0    0 0      0 1    1 2    which be true  the first matrix  an all zero matrix create and the second  be the update one   edit   after substitute when  ij  the resultant matrix still have four element but this time with   00 and   11 index be  0 so the result be   0  0   0   0  1   1   1  0   1   1  1   0   4    but explain PRON expect result   0  1   1   1  0   1   2    the result have basically two element with   01 and   10 index be  1  and  1  and shape be just   2 when flatten which do not make sense as matrix be still of size   4  so python output be basically give all the value in the matrix with index  if PRON be rightly understand what PRON be assume  since the matrix be sparse  PRON be expect PRON to give only the value where the element be non  zero  well this do not do that   hope PRON help   PRON appear that csr do not remove the zero by default  PRON will first have to call eliminatezero   on PRON object  once PRON do this PRON will see that the data contain only the non zero element of PRON matrix   after run the loop in PRON code   printm   give   0  0   0   0  1   1   1  0   1   1  1   0  let PRON inspect the data  printmdata   PRON see that the zero element be store    0 1 1 0   let PRON remove the zero element to make PRON matrix sparse  meliminatezero    and inspect whether the matrix be sparse   printmdata   this confirm that the zero element be remove and hence the matrix be sparse    1 1   PRON hope this help  
__label__ranking suppose PRON want  the machine  to learn the    function  and PRON training set be a collection of pair of number   n1  n2 with the output true if  n1  gt  n2  and false otherwise  which method be the most likely to work  actually  PRON secret goal be a form of the netflix challenge  PRON have a set of  ntuple of real and boolean value  and PRON want to rank PRON base on some training rank datum   this have surely be study a lot   the classical solution be to try a one  class svm apply only to the element rank above the other  PRON can read some paper about PRON look by svm rank or svm rank that will show PRON why that work  
__label__python __label__numpy PRON have an  n by  m numpy array represent a rectangular lattice  l  where each site contain a one or a zero  represent two different material  PRON be model heat flow across this lattice  the idea be that the conductivity between two lattice site depend on the material in both site   to solve the heat equation PRON need to create from this a sparse  nmtime nm matrix  c  where each of the  nm row and column correspond to a cell in PRON original matrix  and where the   i  jtextth element represent the thermal conductance between site  i and  j on the lattice   PRON question be whether there be an efficient way to implement this in numpy  currently PRON have nest python loop that iterate over every element of  l and add correspond entry to  c  but for large lattice this obviously get rather slow  be there a way to do this use numpy primitive rather than python loop   for clarity  PRON current code look like this   a  splilmatrixwhwh    def coordstoindexx  y    return xhy  def setlinkp1x  p1y  p2x  p2y    i  coordstoindexp1x  p1y   j  coordstoindexp2x  p2y   ci  gridp1x  p1y   cj  gridp2x  p2y   if cicj   d  1  else   d  1e3  conductance be much small between different material  ai  j   d  return d  for x in rangew    for y in rangeh    d  0   if xgt0   d   setlinkx  y  x1y   else   d   1  if xltw1   d   setlinkx  y  x1y   else   d   1  if ygt0   d   setlinkx  y  x  y1   else   d   1  if ylth1   d   setlinkx  y  x  y1   else   d   1  i  coordstoindexx  y   ai  i   d  a  atocsr    
__label__r __label__graphs PRON be do PRON academic project  PRON be have the base paper for reference  the paper be ieee paper  effective and efficient clustering method for correlate probabilistic graph   i wish to do this in r tool  in this paper two algorithm be implement  i like to implement the peedr algorithm in the paper  how can i give the input for that algorithm   suggest the packgage in r tool  the paper can be find here  httpieeexploreieeeorgxplsabsalljsparnumber6570474  søren højsgaard have many quality resource for graphical model in r PRON have a tutorial  graphical models with r  and a list of cran package   additionally  mclust be one of the good clustering package in r 
__label__calculus __label__java PRON be make a java function to calculate  int0pi2x  sinx  dx use simpsons rule  when the function equal  0  or  0sin0 should PRON just add  1   limit  or  1010  limit   PRON professor say to use  1010 why would this be   public static double simpsonsrulefunction1double valuen  double valuea  double valueb  double valuedx    double e  00   double simpsonsrule  00   double valueholder  00   valuen  2   valuea  0   valueb   math  pi2   forint i  1  iltvaluen1  i    valuedx   valueb  valueavaluen   e  valuea    i1valuedx    if  i1     limit as x gt  0  simpsonsrule   mathpow1010      else if   i  2   0   ampamp   i  gt  1   ampamp   i  lt  valuen1      simpsonsrule   4emathsine         else if   i  2   0   ampamp   i  gt  1   ampamp   i  lt  valuen1     simpsonsrule   2emathsine         else if  i   valuen1    simpsonsrule    emathsine           simpsonsrule  simpsonsrule    valuedx3    systemoutprintlnnsimpsonsrule   simpsonsrule    whilemathabsvaluehold  simpsonsrule   gt  mathpow106     systemoutprintlnnvalueholder   valueholder    valueholder  simpsonsrule   valuen   2   valuedx   valueb  valueavaluen   simpsonsrule  0   forint i  1  iltvaluen  1  i    e  valuea    i1valuedx    if  i1     limit as x gt  0  simpsonsrule   mathpow1010      else if  i  2   0    simpsonsrule   4emathsine         else if   i  2   0   ampamp   i  gt  1   ampamp   i  lt  valuen  1     simpsonsrule   2emathsine         else if  i   valuen  1    simpsonsrule    emathsine           simpsonsrule  simpsonsrule    valuedx3      return valuen     PRON can simplify PRON code by remove redundant check in if statement  PRON can also pull out the valuedx which be constant in the loop   double e  00   double simpsonsrule  00   double valueholder  00   valuen  2   valuea  0   valueb   math  pi2   valuedx   valueb  valueavaluen   simpsonsrule  10  math  pi2   forint i  2  iltvaluen  i    e  valuea    i1valuedx    if  i  2   0     even  simpsonsrule   4emathsine         else  odd  simpsonsrule   2emathsine           simpsonsrule  simpsonsrule    valuedx3    also PRON look like PRON repeat this same loop twice in PRON code for some reason  PRON havnt run this so there may still be a few bug especially in the index etc but this be homework so PRON will not do PRON all for PRON   PRON can look at a python or c variant which do what PRON want on wikipedia  the python implementation be particularly close to PRON  to answer PRON question though  PRON be evaluate between 0 and  pi2 thus the line   simpsonsrule  10   math  pi2  correspond to add  f0   fpi2 where  fxfracxsinx in the limit  f0gt1  while  fpi2pi2 
__label__python __label__bayesian __label__monte-carlo PRON have a variable which be pareto  ly distribute  x   with unknown alpha and m  PRON want to find out the distribution of PRON mean  so PRON use the following model   with pymc3model   as model   alpha  pymc3uniformalpha   0  02   m  pymc3uniformm   0  2   x  pymc3paretox   alpha  alpha  m  m  observed  obs   meanx  pymc3deterministicmeanx   ttmeanx    step  pymc3metropoli    trace  pymc3sample20000  step  step   burnedtrace  trace1000    but PRON only get a single value for  meanx    alpha  and  m  every time PRON sample  and both  alpha  and  m  converge to the mean of PRON respective uniform distributionalpha  converge to 01 and  m  to 1  what be the correct way to sample the mean of a pareto  ly distribute random variable   thank  
__label__data-mining __label__dataset __label__visualization __label__pandas __label__data-cleaning below be an excel sheet with crop and country   1  mean that the crop be grow in that country whil  0  mean PRON be not grow in that country   PRON want to find where there be an intersection  as in PRON want to find similar crop that be grow in most of the country or at least most of the country   how do PRON do this   suppose that PRON want to know the name of country with more than ten ginger    read PRON datum use pdreadcsv and specify appropriate input base on PRON excel file in a typical variable name csvfile and transpose PRON  csvfile  csvfilecsvfileging    gt  10   also if PRON want to see the country with zero ginger   csvfile  csvfilecsvfileging     0  
__label__deep-learning __label__tensorflow __label__keras PRON have create a deep neural network that solve the spiral dataset classification problem  however  when measure the performance  the accuracy go up and down but always stay at around 50   which be of course very bad   the image below show loss and accuracy of 100 epoch of training   how can PRON fix this  PRON have do research and PRON do not see where the error be in PRON code  be the error in the architecture of PRON network   PRON code    make sure PRON have the require library load  librarykera   librarytensorflow   libraryggplot2    load the datum  spiraldata  readtablespiraldata   header  true    visualize the datum  qplotx  y  datum  spiraldata  colour  label    store the datum in feature and label   xlt  cspiraldatax   ylt  cspiraldatay   feature  lt matrixcx  ynrow  lengthx    label  lt matrixspiraldatalabel    create model   model  lt kerasmodelsequential     add layer and compile the model    PRON model consist of 4 hidden layer  each with 6 neuron   model   gt  layerdenseunit  6  activation   tanh   inputshape  c2     gt  layerdenseunit  6  activation   tanh     gt  layerdenseunit  6  activation   tanh     gt  layerdenseunit  6  activation   tanh     gt  layerdenseunit  1  activation   sigmoid     gt  compile   optimizer   rmsprop    loss   binarycrossentropy    metric  caccuracy       train the model  iterate on the datum in batch of 32 sample    also  visualize the training process   model   gt fitfeature  label  epochs100  batchsize32    evalute the model  score  model   gt evaluatefeature  label  batchsize32   printscore   PRON  spiraldata  dataset   x y label  1 0 1  1 0 0  0971354 0209317 1  0971354 0209317 0  0906112 0406602 1  0906112 0406602 0  0807485 0584507 1  0807485 0584507 0  0679909 0736572 1  0679909 0736572 0  0528858 0857455 1  0528858 0857455 0  0360603 0943128 1  0360603 0943128 0  0181957 0991002 1  0181957 0991002 0  307692e06 1 1  307692e06 1 0  0178211 0970568 1  0178211 0970568 0  0345891 090463 1  0345891 090463 0  0496812 0805483 1  0496812 0805483 0  0625522 067764 1  0625522 067764 0  0727538 052663 1  0727538 052663 0  0799514 035876 1  0799514 035876 0  0839328 0180858 1  0839328 0180858 0  0846154 666667e06 1  0846154 666667e06 0  0820463 0176808 1  0820463 0176808 0  0763975 0342827 1  0763975 0342827 0  0679563 0491918 1  0679563 0491918 0  057112 0618723 1  057112 0618723 0  0443382 071888 1  0443382 071888 0  0301723 078915 1  0301723 078915 0  0151937 082754 1  0151937 082754 0  923077e06 0833333 1  923077e06 0833333 0  0148202 0807103 1  0148202 0807103 0  0287022 0750648 1  0287022 0750648 0  0411343 0666902 1  0411343 0666902 0  0516738 0559785 1  0516738 0559785 0  0599623 043403 1  0599623 043403 0  065738 0294975 1  065738 0294975 0  0688438 014834 1  0688438 014834 0  0692308 116667e05 1  0692308 116667e05 0  0669572 0144297 1  0669572 0144297 0  0621838 027905 1  0621838 027905 0  0551642 0399325 1  0551642 0399325 0  0462331 0500875 1  0462331 0500875 0  0357906 0580303 1  0357906 0580303 0  0242846 0635172 1  0242846 0635172 0  012192 0664075 1  012192 0664075 0  107692e05 0666667 1  107692e05 0666667 0  0118191 0643638 1  0118191 0643638 0  0228149 0596667 1  0228149 0596667 0  0325872 0528323 1  0325872 0528323 0  0407954 0441933 1  0407954 0441933 0  0471706 0341433 1  0471706 0341433 0  0515245 0231193 1  0515245 0231193 0  0537548 0115822 1  0537548 0115822 0  0538462 133333e05 1  0538462 133333e05 0  0518682 0111783 1  0518682 0111783 0  0479702 0215272 1  0479702 0215272 0  0423723 0306732 1  0423723 0306732 0  0353545 0383025 1  0353545 0383025 0  0272434 0441725 1  0272434 0441725 0  0183971 0481192 1  0183971 0481192 0  00919062 0500612 1  00919062 0500612 0  123077e05 05 1  123077e05 05 0  00881769 0480173 1  00881769 0480173 0  0169275 0442687 1  0169275 0442687 0  02404 0389745 1  02404 0389745 0  0299169 0324082 1  0299169 0324082 0  0343788 0248838 1  0343788 0248838 0  0373109 0167412 1  0373109 0167412 0  0386658 00833083 1  0386658 00833083 0  0384615 116667e05 1  0384615 116667e05 0  0367792 00792667 1  0367792 00792667 0  0337568 015149 1  0337568 015149 0  0295805 0214137 1  0295805 0214137 0  024476 0265173 1  024476 0265173 0  0186962 0303147 1  0186962 0303147 0  0125098 0327212 1  0125098 0327212 0  00618938 0337147 1  00618938 0337147 0  107692e05 0333333 1  107692e05 0333333 0  00581615 031671 1  00581615 031671 0  0110398 0288708 1  0110398 0288708 0  0154926 0251167 1  0154926 0251167 0  0190382 0206232 1  0190382 0206232 0  0215868 0156247 1  0215868 0156247 0  0230974 0103635 1  0230974 0103635 0  0235768 0050795 1  0235768 0050795 0  0230769 1e05 1  0230769 1e05 0  0216903 00467483 1  0216903 00467483 0  0195432 00877067 1  0195432 00877067 0  0167889 0121538 1  0167889 0121538 0  0135977 014732 1  0135977 014732 0  0101492 0164567 1  0101492 0164567 0  00662277 017323 1  00662277 017323 0  00318831 0173682 1  00318831 0173682 0  615385e06 0166667 1  615385e06 0166667 0  00281431 0153247 1  00281431 0153247 0  005152 013473 1  005152 013473 0  00694508 0112592 1  00694508 0112592 0  00815923 0088385 1  00815923 0088385 0  00879462 0063655 1  00879462 0063655 0  00888369 00398583 1  00888369 00398583 0  00848769 0018285 1  00848769 0018285 0  00769231 333333e06 1  00769231 333333e06 0  visualized  the dataset look like this   PRON network be actually work  PRON just take a lot of epoch to learn the spiral  in fact PRON can see from PRON learning curve that learning be still occur  just not much per epoch   try 60000 epoch    when PRON try PRON model  in python  but still same datum and model  use 60000 epoch PRON get loss under 00001 and accuracy of 100  reliably   there be a few factor involve in why PRON need this amount of iteration   the data set size be small  which mean PRON get less update to weight per epoch  PRON need to compensate by increase the number of iteration   PRON have a  starve  network topology that can just about learn the spiral  but need to be quite precisely optimise before PRON start perform well  PRON could increase the number of neuron per hidden layer slightly  or maybe add another hidden layer   if PRON add one more hidden layer  size 6  tanh activation  the network learn 100  accuracy in under 20000 epoch   if instead  PRON increase the original four hidden layer to size 16  the network learn 100  accuracy in under 10000 epoch   tanh be not optimal for a deep network  because the gradient diminish in the deep part of the model  rmsprop will compensate for that  but still change to relu will improve convergence speed   if PRON use the four  hide  layer model  with layer size 16 and relu activation  the network converge to 100  accuracy on the training datum set in around 2000 epoch   import panda as pd  import numpy as np  nprandomseed4375689   from kerasmodel import sequential  from keraslayer import dense  from kerasoptimizer import rmsprop  traindata  pdreadcsvspiralscsvvalue  trainx  traindata02   trainy  traindata2   model  sequential    modeladddense16  activationrelu   inputshape2      modeladddense16  activationrelu     modeladddense16  activationrelu     modeladddense16  activationrelu     modeladddense1  activationsigmoid     modelcompilelossbinarycrossentropy   optimizer  rmsprop     metricsaccuracy     history  modelfittrainx  trainy  batchsize32  epochs2000  verbose0   score  modelevaluatetrainx  trainy  verbose0   printscore  
__label__machine-learning __label__python __label__classification __label__scikit-learn __label__churn PRON be busy with a supervised machine learning problem where PRON be predict contract cancellation  although a lengthy question  PRON do hope someone will take the time as PRON be convinced PRON will help other out there  PRON have just be unable to find any solution that have help PRON   PRON have the follow two dataset   1   model dataset   contains about 400k contract  row  with 300 feature and a single label  0   not cancel   1   cancel     each row represent a single contract  and each contract be only represent once in the datum  there be 350k  not cancel  and 50k  cancel  case   feature be all extract as at a specific date for each contract  this date be refer to as the  effective date   for  cancel  contract  the  effective date  be the date of cancellation  for  not cancel  contract  the  effective date  be a date say 6 month ago  this will be explain in a moment   2   live dataset   contains 300k contract  row  with the same list of 300 feature  all these contract be  not cancel  of course  as PRON want to predict which of PRON will cancel  these contract be follow for a period of 2 month  and PRON then add a label to this datum to indicate whether PRON actually end up cancel in those two month  0   not cancel   1   cancel   the problem   PRON get amazing result on the  modelling dataset   random train  test split   eg precision 95   auc 098   but as soon as that model be apply to the  live dataset   PRON perform poorly  can not predict well which contract end up cancel   eg precision 50   auc 07    on the modelling dataset  the result be great  almost irrespective of model or datum preparation  PRON test a number of model  eg sklearn random forest  keras neural network  microsoft gbmlight  sklearn recursive feature elimination   even with default setting  the model generally perform well  PRON have standardize feature  PRON have bin feature to attempt improve how well PRON will generalize  nothing have help PRON generalize to the  live dataset   PRON suspicion   in PRON mind  this be not an over  training issue because PRON have get a test set within the  modelling dataset  and those result be great on the test set  PRON be not a modelling or even a hyper  parameter optimization issue  as the result be already great   PRON have also investigate whether there be significant difference in the profile of the feature between the two dataset by look at histogram  feature  by  feature  nothing be worryingly different   PRON suspect the issue lie therein that the same contract that be mark as  not cancel  in the  modelling dataset   which the model train to recognize  not cancel  of course  be basically the exact same contract in the  live dataset   except that 6 month have now pass   PRON suspect that the feature for the  not cancel  case have not change enough to now make the model recognize some of PRON as about to be  cancel   in other word  the contract have not move enough in the feature space   PRON question   firstly  do PRON suspicion sound correct   secondly  if PRON have state the problem to be solve incorrectly  how would PRON then set up the problem  statement if the purpose be to predict cancellation of something like contract  when the datum on which PRON train will almost certainly contain the data one which PRON want to predict    for the record  the problem  statement PRON have use here be similar to the way other have do this  and PRON report great result  but PRON be not sure that the model be ever test in real live  in other case  the problem to be solve be lightly different  eg hotel booking cancellation  which be different because there a stream of new incoming booking and book duration be relatively short  so no booking in common between the modelling and live dataset  contract on the other hand have long duration and can cancel at any time  and sometimes never   PRON hard to answer without a good look at the datum   but if PRON have to guess  PRON point seem valid   consider there be no problem with cross validation method or leak   if PRON be  measure  the contract feature at different point in time  there may be a high bias that the feature of the cancel contract  measure at the point in time in which PRON be cancel  may be very different from the  initial  feature of those same contract   hence  PRON modelling wld be learn how to predict a contract be be cancel at the give date PRON be cancel  and not prior to PRON  that s why PRON would not be work properly on PRON  real world datum    if PRON can  try use the datum from the moment the contract be set  initialized   to build PRON model   if PRON model make a prediction 6 month into the future  then PRON do not make sense to judge PRON performance before 6 month  if only 2 month have pass  then possibly 23 of the true positive have yet to reveal PRON true nature and PRON be arrive a premature conclusion   to test this theory  PRON would train a new model to predict 2 month out and use that to get an approximation of live accuracy while PRON wait 4 more month for the first model  of course  there could be other problem  but this be what PRON would try first  
__label__r __label__databases PRON would like to retrieve datum from an azure documentdb to be able to process PRON with r  PRON be not able to find a package to do PRON  have someone an idea on how to proceed   
__label__deep-learning __label__deep-network __label__training PRON want to implement dsd  dense  sparse  dense training for deep neural network by han et al  in short  the paper suggest the follow training scheme to improve the network accuracy   train as usual till convergence   prune the network and train with sparsity constraint   remove the sparsity constraint and let the prune connection to recover   PRON question be about step 2  train with sparsity constraint  the paper mention training with a binary mask specify the prune weight to keep  untouched  so the sparsity constraint be satisfied  however that mean implement a dedicated layer that take the binary mask as an additional blob and handle PRON accordingly   PRON wonder a simple approach will give the same result  what if after the prune step PRON keep the location of the prune weight and then use dense training  but after each iteration to zero the originally prune weight   the forward path be the same  take zero weight for the prune weight anyway   but would PRON negatively affect the backward path  since the constraint be not there  or be PRON equivalent to the formal training scheme   thank   PRON approach will give a similar result   the difference with use the constraint be that the constraint allow the network to essentially confirm that PRON do not need the connection before PRON reallocate PRON  ie if the weight be lessen by the constraint and PRON result in something not work  then the weight will start increase   
__label__statistics PRON perform an a  b test with the goal of increase the average order value   PRON get the following result   high order value be some time more important for PRON that conversion   PRON know how to calculate the significance of the conversion rate  but how do PRON calculate the significance of the average order value  how many conversion do PRON need to trust the 10  increase in average order value   
__label__machine-learning __label__reference-request __label__software-development PRON be look for resource that talk about good practice or simply some example  at specific company or in general  on how tech company that heavily use machine learn like twitter  facebook etc  set up PRON datum science workflow to make PRON easy to take model from exploratory to production level   in 2012 twitter publish a paper on what PRON analytic stack look like  PRON heavily rely on pig  since this be already a few year old  PRON quesiton be  be there a more recent version or something similar to that available for one of the big tech company use datum science   a recent blog entry from slack describe PRON architecture  but PRON say little about how PRON be use more specifically for machine learning purpose   PRON try to look for some resource of PRON interest and come up with these online course available at edx  data science professional project  big data analysis with apache spark  data science and engineering with apache ® spark ™  these be just the few resource  that may get PRON attention and may be useful for PRON  in addition to that  PRON can explore edx  ds dashboard for more such course  go to github there be lot of systematic data science stuff from scratch show how thing be do at professional end  setup thing up  etc   hope PRON help  
__label__nlp PRON be interested in the field of name entity disambiguation and want to learn more about PRON  PRON have hear that there be contest organise by various association on these kind of research topic  these contest be very helpful as PRON give a practical experience in these field   PRON find one such contest organise by microsoft research here though the date have already pass  can anyone point PRON to any other such contest  also  be there a site which catalogue these contest so that one can just go there and know about all upcoming contest   thank in advance   some of the grec share task challenge include a name entity recognition  amp  coreference resolution component  ie  disambiguation   but PRON do not think PRON have run grec since 2010    httpssitesgooglecomsitegenchalrepositoryregincontextgrecner 
__label__untagged have there be any attempt to deploy ai with blockchain technology   be there any decentralized example of ai network with no central point of control with ai node act independently  but accord to a codified set of rule  create  validate and store the same share decentralized database in many location around the world   swarm intelligence be the term for system where relatively simple agent work together to solve a complicated problem in a decentralized fashion   in general  distribute computing method be very important for deal with problem at scale  and many of PRON embrace decentralization in a deep way   give the reality of hardware failure and the massive size of modern dataset relative to individual node  the less work be pass through a central bottleneck  the good   while there be people interested in do computation on the blockchain  PRON seem to PRON like PRON be unlikely to be competitive with computation in dedicated cluster  like aws    PRON think the good example of ai be deploy on the blockchain be singularitynet  PRON just have a successful token sell where PRON sell out of PRON agi token which will be able to be use to essentially  pay  for ai task to be do for PRON  various ai will be put on the network and able to interact and communicate with each other to get various task do  there be some great video online where dr ben goertzel explain this further  and here be a link to PRON whitepaper  
__label__optimization __label__sparse __label__constrained-optimization __label__discrete-optimization PRON be look at an optimization problem that look like this      textminimize  mathbfxtqmathbf x      mathbf x in mathbb rn   xi in lbrace 0  1 rbrace  textsubject to  mathbf x1  m  lt  n     in word  the goal be to select a fix number  m of element from  mathbf x that minimize the quadratic term    q be a correlation matrix  ie a  n time n symmetric matrix with  qji  in  1  1  PRON be look for an efficient algorithm for this problem  by approximation if necessary  and be wonder if this problem belong to any know class of optimization problem  PRON be certainly similar to the quadratic knapsack problem  but in this case the constraint here be not an upper limit on a weighted sum  but simply the number of non  zero element   the number of subset of size  m of a set of size  n be  binomnm  which be bound by   leftfracnmrightm le binomnm  le leftfracenmrightm  so in the limit of  m ll n  PRON can come up with an  onm algorithm by try all subset of size  m  and in the limit of  m approx n  PRON can try complement of all subset of size  n  m for  m on the order of  n2   however  the brute force approach have a runtime on the order of  2n and PRON do not think there be any get around this  ie the problem be np  hard in general   there be several heuristic on the wiki article about the quadratic knapsack problem  many of PRON be either design for the 0  1 problem or can be adapt from the weighted  the article also have a comprehensive list of reference  many of which discuss both the weighted and 0  1 case  PRON could also try search github for example code  this one come up right away but PRON be for the linear knapsack problem    a simple heuristic be to pick an initial candidate set  x of size  m  and improve PRON by swap element  let  ei be the standard basis vector in the  ith direction  to implement this heuristic  pick an index  i such that  xi  1   and evaluate the objective function at  x  ei  ej for all  j such that  xj  0 if there be any  j such that the exchange  i leftrightarrow j reduce the objective function  pick the one that give the small value of the objective  and then repeat for some other value of  i if run all the way to optimality  this could take  o2n swap  but PRON may give PRON an acceptable solution in less time  additionally  the inner loop over  j can be parallelize   PRON do not know if this exactly work for PRON  but will give the relaxed version a shot   preliminary   correlation matrix can be see as the covariance matrix of the standardized random variable  xisigma  xi and  any correlation matrix can be convert to a covariance matrix as     qtextcorrx    lefttextdiagsigma  rightfrac  12sigma lefttextdiagsigma  rightfrac  12  if standard deviation  sigmai be know  then use  d  sqrtdiagsigma      sigma  dqd      formulation   use the definition above PRON can equivalently write  q  d1sigma d1 if PRON use this definition of  q and set  b  d1  then      beginalign   xt q x  amp xt bt sigma b x    amp xt bt vt lambda v b x    amp xt bt vt sqrtlambdatsqrtlambda  v b x   endalign       sigma  vt lambda v follow from the eigen  decomposition and nicely exist for the covariance matrix  then  substitute  m  bt vt sqrtlambdat  one can show that      xt q x  xt mt m x  lvert m x rvert2      the problem then turn to      argminx lvert m x rvert2  lambda  x0m      which be generally tackle via the  l1  variant of the form      argminx lvert m x rvert2  lambda x1      now instead of think about  x as a binary vector  PRON could relax this constraint and seek for the sparse solution under real  x this be a non  quadratic  l1regulariz least square problem and one can solve PRON by standard technique  such as  PRON guess  lasso  maybe PRON be then possible to look at the solution and determine  m this give local optimality but should work well in practice  for hard constraint lagrange multiplier can be use   PRON may have to double check the formulation  as PRON do not have much time now  PRON will do PRON later  but the method above should give PRON the idea  
__label__machine-learning __label__pandas __label__dataframe PRON be implement machine learn model and use training dataset from mysql table and all this be build upon django  so basically all the calculation be do by convert entire datum from mysql table to dataframe   df  pdreadsqlselect  from naivebayesplayerconnection   however  PRON be face problem in compare dataframe column value with a string   so PRON have a column name classification in mysql table which have 2 fix value  rs  or  nrs  store in varchar10  format  since PRON have convert an entire table into dataframe whenever PRON calculate the count of  rs  value in classification column in dataframe PRON always return 0  but actually  there be 63 entry of  rs    totalrs  dfclassificationdfclassificationrscount    in above line of code PRON be try to find out all record where classification be  rs  which should be 63 but PRON be get 0  what be PRON do wrong   PRON have try above code when read datum from csv instead of mysql table and everything work fine   PRON do not see anything glaringly wrong with PRON code  but PRON can try this instead   dfclassificationvaluecountsr    if PRON omit the indexing at the end there  the call to valuecount   will concretely show PRON what value appear with what frequency in the classification column  which may help PRON debug what be go on if this still do not work  
__label__matlab __label__numerical-analysis __label__interpolation so the problem PRON be work on be as such  give the x  y  z datum of a periodic object over time  from the origin in 3d space   need not be uniform   calculate the period of the object  if the datum give be periodic and the calculation be possible    so the good idea PRON have be able to come up with so far  be to use some form of interpolation to get a function for the datum  but PRON be not sure how PRON would go about use the function to calculate the period  also  if interpolation be the good way forward  would use cubic spline interpolation work   finally  since the datum give be 3d  and so far all the experience PRON have be work with 2d datum for interpolation  if PRON be to use cubic spline to interpolate  how would the actual implementation work   PRON actually have to do this for very large data set on occasion  since PRON datum be usually somewhat uniformly distribute  PRON have find the follow procedure simple and sufficient   define   p equiv datat0   deltat  equiv lvert datat   p rvert2   then find the inflection point of  deltat and measure the  delta t between PRON  if  deltat at the inflection point be sufficiently small  and the variation of  delta t between multiple neighboring inflection point  then PRON can be confident PRON have a good measurement of the period   if PRON have a very non  uniform distribution  where the 2norm do not seem like a good metric  PRON may interpolate the datum as PRON mention and then follow a similar procedure  something like a spectral representation  maybe spherical harmonic  though what work best will depend on the object PRON  what be PRON     for a spline interpolation PRON could use a build  in matlab call  then uniformly sample that interpolation and use the method above   for a spectral representation PRON may simply use the normalized basis coefficient in the procedure above  
__label__clustering __label__similarity suppose PRON have five set PRON would like to cluster  PRON understand that the simhashing technique describe here   httpsmoultanowordpresscom20100121simplesimhashing3kbzhsxyg44676  could yield three cluster   a    b  c  d  and  e    for instance  if PRON result be   a gt  h01  b gt  h02  c gt  h02  d gt  h02  e gt  h03  similarly  the minhashing technique describe in the chapter 3 of the mmds book   httpinfolabstanfordeduullmanmmdsch3pdf  could also yield the same three cluster if PRON result be   a gt  h01  h02  h03  b gt  h04  h05  h06    c gt  h04  h07  h08    d gt  h09  h10  h08  e gt  h11  h12  h13   each set correspond to a mh signature compose of three  band   and two set be group if at least one of PRON signature band be match  more band would mean more matching chance    however PRON have several question relate to these    1  can sh be understand as a single band version of mh    2  do mh necessarily imply the use of a data structure like union  find to build the cluster    3  be PRON right in think that the cluster  in both technique  be actually  pre  cluster   in the sense that PRON be just set of  candidate pair     4  if  3  be true  do PRON imply that PRON still have to do an  on2 search inside each  pre  cluster   to partition PRON further into  real  cluster   which may be reasonable if PRON have a lot of small and fairly balanced pre  cluster  not so much otherwise   as correctly point out above minhash and simhash both belong to locality sensitive hashing   reference  httpsenwikipediaorgwikilocalitysensitivehashing  the main difference between the two be the way collision be handle   simhash  use cosine similarity  minhash  use jaccard index   answer to PRON question   no  PRON use different collision handling technique to validate similarity  also there be a variant on single hash function for min hash but PRON work differently  for more detail  check the follow reference out  httpsenwikipediaorgwikiminhash  variant with a single hash function   yes  httpsgithubcomchrisjmccormickminhashblobmasterrunminhashexamplepy  PRON think the complexity can be reduce to  on log n with modify form of binary search while cluster  
__label__fourier-analysis PRON be try to understand why the amplitude of the fft  compute with numpy  of a gaussian differ from PRON analytic solution   the  mathcalfepi t2   epi f2 however if PRON calculate PRON with  the fft function in numpy the result gaussian s amplitude be not 1   PRON have already do the following   PRON do divide the fft result by the number of sample  normalize    PRON have even try shift the gaussian so that PRON first sample be PRON  height  this correct the sinusoidal behaviour  and remove the maginary  part  but do not improve the amplitude result  same as absolute value of  unshifted case   PRON will however be take the absolute value in any case   here be the code   def testgauss1dself  a  fc  deltaf    deltat  1020fc   n  intnpceil1deltafdeltat1  if  n  2    0   n  n  1  t  nplinspacen12deltatn12deltatn   f  nplinspace12deltat12deltatn   xt  npexpnppit2    xt  nprollxt1n12    xt  1npsqrt2nppianpexpt22a2     print  std    14a2nppi2    print  ampl    npsqrt2nppia  pltplottxt   pltshow     xf  npfftfftxtn   print  n    n  xf  npfftfftxtn  xf  nprollxf1n12   pltplotfnpabsolutef    pltshow    here be the image   as PRON can see the amplitude be incorrect in the second image  should be one    why be this   PRON think PRON have use an incorrect normalisation  the factor of 1n be the result of apply both the transform and the inverse transform  for just the forward normalisation PRON therefore want 1sqrtn    when PRON run PRON code with this normalisation  PRON see a peak of sqrt2   so the correct normalisation be therefore 1sqrt2n     PRON suspect that the final factor of two come from the exact definition of n  and whether or not positive and negative frequency should count twice  in any case  PRON can not seem to find PRON on the numpy documentation page  httpdocsscipyorgdocnumpyreferenceroutinesffthtml  the normalization need to include the sample frequency   multiple PRON xf by deltat and PRON should be all set  
__label__machine-learning __label__apache-spark __label__naive-bayes-classifier PRON be use spark naive baye model to train on label sample  PRON be use string indexer to encode all of PRON feature  vector assembler to convert individual feature to a feature vector and normalizer to normalize PRON feature vector  PRON have train PRON model and i want to predict on new unlabled datum  PRON question be   1  this may be silly  should i use string indexer to encode PRON unlabled sample feature  even if i do will PRON encode the same way PRON have before training for eg  before train cat have be encode to 1 and on unlabeled sample will not PRON be encode to some other value   2  PRON have encode PRON class label as use string indexer as well after PRON model have predict on new unlabeled sample PRON return PRON an index  i try indextostr to convert index back to string PRON give PRON an class cast exception   dataframe pr  nbmodeltransformuk    predshow10    indextostr converter  new indextostring    setinputcolprediction    setoutputcolorgpred     dataframe convert  convertertransformpr    convertedshow10    
__label__python __label__classification __label__neural-network __label__scikit-learn __label__image-classification PRON be work on an animal classification problem  with the datum extract from a video feed  the recording be make in a pen  so the problem be quite challenging with a dark background and many shadow   initially PRON try scikit  image  but then someone help PRON with an advanced tool call crf  rnn  httpcrfasrnntorrvision  that do a great job segmenting and labelling object in an image  PRON do the following   import caffe  net  caffe  segmentermodelfile  pretrained   imagefile   0045crop2png   inputimage  caffeioloadimageimagefile   from pil import image as pilimage  image  pilimagefromarraynpuint8inputimage    image  nparrayimage   meanvec   npmeanimageval   for val in rangeimageshape2     PRON be  image        1   PRON be  PRON be  reshapedmeanvec  curh  curw  curc  imshape  padh  750  curh  padw  750  curw  printpadh  padw   999    PRON be  nppadim  padwidth0  maxpadh0     0  maxpadw0     0  0    mode   constant   constantvalu  255   segmentation  netpredictim    segmentation2  segmentation0curh  0curw   the result image segmentation be rather poor  although two cow be recognize correctly    PRON use a train crf  rnn  modelfile  pretrained   which work well for other problem  but this one be hard  PRON would appreciate any suggestion on how to pre  process this sort of image to extract the shape of most cow   PRON would be appreciate if PRON could explain precisely what PRON goal be   PRON want to identify what animal be in PRON picture   PRON want to count the number of animal   PRON want to get the position of each animal in the picture   in any case  PRON know that PRON can get some already train neural net from google or anywhere else  this neural net can be use with caffe as PRON be the case in this google deepdream stuff on github  look at   httpsgithubcomgoogledeepdreamblobmasterdreamipynb  then  if PRON want to highlight or identify the position of PRON animal  PRON will find this article inspire   httpwwwmatthewzeilercompubscvpr2010cvpr2010pdf  PRON explain how to reverse convolutional network to identify what part of PRON image help to recognize what be inside  the find projection get PRON something similar to PRON second picture  call a mask   but depend on the neural net PRON use  PRON can get good result  
__label__matrices __label__petsc PRON be try to do something fairly simple somehow PRON have make PRON hard   be there an example of how to send a 3d array to a binary file   PRON depend on the object representation of PRON 3d array  if PRON be a petsc mat  PRON want to look at matview  if PRON be a vec  PRON want to look at vecview  the link be to petsc documentation  and each of the link have a list of relevant example  PRON may not be exactly what PRON be look for  but PRON should be close  in general  PRON be look for something like a  view  type command  if these suggestion do not help  PRON suggest e  mail the petsc user list  
__label__matlab __label__algorithms __label__complexity __label__dense-matrix everyone  PRON have a question about computational cost for a algorithm  that is   PRON have two vector  un vnin mathbbcn  a matrix  ain mathbbcntim n  can be both sparse and dense  and scalar coefficient  alphan  than PRON need to compute  run by matlab     1   yn  un  alphanvn  PRON computational cost  mainly cpu time  be refer to as  mathcalk     2  matrix  vector product   zn  aun  PRON computational cost  mainly cpu time  be refer to as  mathcall    so PRON want to know that PRON have   1   5mathcalk    gtmathcall    2   5mathcalk    ltmathcall    be the assumption of 12  rational  PRON want to receive some comment about this problem   let the average number of sparse entry in a row for a sparse matrix be  ns then the complexity of the matrix  vector product   zn  aun  be approximately  onns  the complexity of  yn  un  alphanvn be  o2n for the second to hold true   5mathcalk    ltmathcall  PRON need   52n  lt   nns  so if  ns  gt  10   ie   the second condition hold true  
__label__optimization __label__random-number-generation PRON be look to find the most efficient way to change integer from a random number generator to a different inclusive number range   PRON know of 2 way so far   change the number into a decimal in the range of  01  and multiply PRON by the difference between the minimum and maximum value  in the new range   find the remainder of the number divide by the difference between the minimum and maximum number  in the new range    the difference will have to be increment by 1 to get the correct inclusive range on the result  there be however  a problem with both of these method   the decimal method involve a lot of float point calculation  which be slow  the remainder method will favor low number in the number range  to illustrate  2 above  consider get a random value of a unsigned byte   PRON get a random number in the range of 0  255   suppose PRON want a number in the range of 1  255  PRON may use the follow formula   number  random    255  1   any number from 0  254 will simply be increase by 1  give PRON a range of 1  255   255  however will also grant PRON a 1  give 1 double the probability as the rest of the number   this illustrate the follow   probabilty of number in range  newmin  newmin  oldmax   newmax  newmin   be  oldmax  oldmin    newmax  newmin  round up  probability of number in range  newmin  oldmax   newmax  newmin   newmax  be  oldmax  oldmin    newmax  newmin  round down  in PRON situation  PRON be get an 8 byte value  so the effect of this flaw in the remainder method would require an insanely large sample of number before the flaw effect the result noticeably   so if these be the only 2 method available  PRON would disregard this distribution flaw to increase performance   be there a method that have good performance than method  1 but good result than  2   the usual approach to get an integer random number in the range   0ldot  n  a half open range  be a piece of code of the form  unsigned int rnd      unsigned int k   do   k  rand     while  k  gt randmax  nn   return k  n     this work because rand   produce random number uniformly in the range  0randmax   then  the do  while loop produce random number uniformly in the range  0randmax  nn  where the upper bound be the large multiple of randmax less than or equal to randmax  and consequently k  n be a uniformly distribute random number in the range  0n    if PRON want random number in an interval  a  b   use the function above on the interval  0n  b  a  and just add a to every number PRON get  
__label__experiments PRON notice PRON have a biased sample in PRON a  b test and be wonder if difference  in  difference would help PRON make valid conclusion about the datum  or if there be another way to proceed   PRON run an new experiment on PRON site  where PRON offer 50  of PRON user a new feature  PRON assign user with odd id into the experiment group and user with even id into the control group and then run the experiment  however  PRON see that even prior to run the experiment  there be a statistically difference between the two group  PRON think this be because PRON run many experiment where PRON segment base on odd  even of the PRON would  so people in the experiment group have see many treatment   so lesson learn  PRON will flip a coin next time instead of use the PRON would  however  PRON would like to see if PRON can still make inference from the current experiment  PRON have hear of something call difference  in  difference  would this work in this case  or be there a different approach that would work better  ideally  PRON do not want to scrap the test and start over since 50  of PRON user have already see the new feature   at this point in the experiment  nothing PRON can do can make the two group equal   what PRON can do be make assumption about how the two group will behave   if PRON can safely assume that previous treatment that PRON have apply use the even  odd sampling method will not interact with the new a  b test  then a difference  in  difference method would be appropriate   as a simple example of when this assumption  and a difference  in  difference  be inappropriate   let PRON say in PRON first test PRON send group a  the odd  a book title  the importance of eat healthy    in the second test  PRON send group a  the odd  dietary supplement   group a have be compromise by prior treatment that could impact the effect on the second test  
__label__pde __label__finite-element __label__discontinuous-galerkin PRON be deal with a couple system of three transient  non  linear convection  diffusion equation  let PRON just say to simplify the problem that PRON take the follow form      nablacdotd1u2u3nabla u1    nablacdotmathbfchiu2u3          fracpartial u2partial t   nablacdot f1u1u2u3    nablacdotd2u2u3u2    0        fracpartial u3partial t   nablacdot f2u1u2u3    nablacdotd3u2u3u3    0     assume a general set of boundary condition for each equation      ui   gi  d   textongammadi          dinabla ui   gi  n   textongammani       PRON spatial discretization be via dgfem and the time discretization be just backward euler  anyway  PRON will use the superscript   k to denote the solution at the  kth time step  this be how PRON be decouple the equation   1  solve for  u1k1 use information from the previous time step for  u2 and  u3 this effectively linearize the equation      nablacdotd1u2ku3knabla u1k1     nablacdotchiu2ku3k        2  use the new  u1k1 and information from the previous time step for  u3 to solve for  u2k1      fracpartial u2partial t   nablacdot f1u1k1u2k1u3k     nablacdotd2u2k1u3knabla u2k1     0     3  solve for  u3k1      fracpartial u3partial t   nablacdot f2u1k1u2k1u3k1     nablacdotd3u2k1u3k1nabla u3k1     0     so the first equation be linear and the last two be non  linear  PRON be use newton s method to handle the non  linearity  however  there be some bug in PRON code and PRON be a bit stuck try to interpret the result   PRON know this be long but PRON be try to be as detailed as possible here  just a couple of question   1  PRON test the simple case where  u1   u2   u3   5t and PRON get about machine precision error use linear basis function  newton s method be also converge quadratically  PRON be use a timestep approximately equal to  hp1 PRON can also increase PRON to  100hp1 which surprisingly do not seem to affect the convergence much  use this time step  PRON have     sample newton iteration            current time  7812500          u2 iteration  1  error  4155186e01  u2 iteration  2  error  2480219e02  u2 iteration  3  error  9169256e05  u2 iteration  4  error  1250692e09  u2 iteration  5  error  8042476e16  iterations for u2 convergence  5  u3 iteration  1  error  4155186e01  u3 iteration  2  error  2480219e02  u3 iteration  3  error  9169256e05  u3 iteration  4  error  1250692e09  u3 iteration  5  error  7664044e16  iterations for u3 convergence  5    error result               l2 error             u1 error in l2norm   453162e12  464396e12  615316e12  107398e11  u2 error in l2norm   412483e13  730247e13  139962e12  276142e12  u3 error in l2norm   409102e13  729944e13  139935e12  276121e12             h1 error             u1 error in h1norm   105844e11  203320e11  384458e11  747941e11  u2 error in h1norm   640033e13  133164e12  271254e12  547814e12  u3 error in h1norm   634670e13  133089e12  271211e12  547862e12  now the bizarre part be that if PRON take  u15t   u26t  and  u37t  PRON result be awful  PRON be think maybe the first case be the exception rather than the norm since PRON be the only example PRON have test so far that seem to work  anyway  the weird part about this one be that PRON be get about machine precision error for  u2 while the other be flatlin around 1e0  PRON be not sure how this could possible be since the solution to  u2 depend on the solution to  u1 and  u3  which be incorrect if PRON code be to be believe  PRON observe quadratic convergence for  u2  but obviously not the other   the time step be around  hp1 here  but PRON observe similar result for small time step           current time  0074219          u2 iteration  1  error  3908065e03  u2 iteration  2  error  2219552e06  u2 iteration  3  error  2889154e12  iterations for u2 convergence  3  u3 iteration  1  error  1373826e01  u3 iteration  2  error  1564705e02  u3 iteration  3  error  2492157e03  u3 iteration  4  error  7872758e04  u3 iteration  5  error  1311116e04  u3 iteration  6  error  2997904e05  u3 iteration  7  error  9158730e06  u3 iteration  8  error  1519301e06  u3 iteration  9  error  3555316e07  u3 iteration  10  error  1067623e07  u3 iteration  11  error  1755777e08  u3 iteration  12  error  4195674e09  u3 iteration  13  error  1242190e09  u3 iteration  14  error  2024999e10  u3 iteration  15  error  4936768e11  u3 iteration  16  error  1443518e11  u3 iteration  17  error  2333785e12  iterations for u3 convergence  17             l2 error             u1 error in l2norm   302951e00  543305e00  596871e00  425056e00  u2 error in l2norm   145749e13  271039e13  369646e13  495591e13  u3 error in l2norm   629385e01  830180e01  111972e00  113907e00             h1 error             u1 error in h1norm   786736e00  181562e01  367674e01  397038e01  u2 error in h1norm   233954e13  200202e12  495659e12  103835e11  u3 error in h1norm   200438e00  391391e00  624097e00  706979e00  PRON think maybe the most telling part be the convergence of the newton iteration  namely  what do PRON mean if PRON be not observe quadratic convergence  PRON initial guess be just the solution at the previous time step  and since the time step be quite small  PRON be pretty sure the initial guess be not the issue  PRON know that non  quadratic convergence can imply that the jacobian be not be compute properly  but the first example be so similar to the second that PRON can not imagine that be the case  anyone have any insight here   2  PRON be a bit iffy on the spatial discretizaton of the convection term      intomeganablacdot f1u1u2u3z  intomeganabla zcdot f1u1u2u3    intgammazf1cdotmathbfn        via green s identity   the dg discretization be     sumeinmathcalmhintenabla zcdot f1   sumeingammatextintinteleftleftf1cdotmathbfnrightrightz    sumeingammatextoutintef1cdotmathbfnz  sumeingammatextinintef1cdotmathbfnz     where     be the average and     be the jump  in all of the literature PRON have come across  the inflow integral be move to the right  hand side  so if PRON be consider the second equation  solve for  u2   be this correct      sumeingammatextinintef1u1g2du3cdotmathbfnz     so when compute the jacobian for the second equation  do this inflow term not contribute to the jacobian at all since  u2d be know   PRON be able to solve a single transient  non  linear convection diffusion equation use newton s method  so PRON be not entirely sure what PRON be that PRON be miss   
__label__regression __label__feature-selection __label__random-forest __label__algorithms sorry for the bad title  PRON can not find a good one  so PRON will try to explain what PRON be look for   PRON be do sale forecasting with a regression forest   spark  scala for the technology   PRON have work on some test datum and PRON do PRON forecast use training datum   but some of the feature which PRON have use can not be employ to forecast the future as PRON would not be know to PRON at any give time  for example the number of customer of a day  PRON category  what kind of advantage PRON have etc   do PRON have to find other feature that will be as useful as these one or do PRON need to perform prediction on these feature before PRON sale forecasting and use the prediction   be there any another solution  also  what kind of algorithm should PRON use for the  feature forecast    the question be poorly phrase  PRON have try to edit PRON to the good of PRON ability  however here be the problem PRON have state   some of the feature which be be use now can not be use later because PRON may not be know  will PRON affect the model   if PRON can be use  what type of algorithm can be choose   the answer to the first problem  PRON have to check the accuracy first before make choose any new feature if the rest of PRON feature give a good enough accuracy then there be no need to choose new feature   the second problem  to predict those value of the feature PRON be use now if PRON be discrete in nature try classification algorithm whichev fit the model good  else try something along the line of regression if the input value be continuous  and then use these predict value along with PRON exist model and check how the accuracy vary  
__label__neural-network PRON know that a neural  network architecture be mostly base on the problem PRON and the type of input  output  but still  there be always a  square one  when start to build one  so PRON question be  give a input dataset of mxn  m  be the number of record  n be the number of feature  and a c possible output class  be there a thumb  rule to how many layer  unit should PRON start with   this question have be answer in detail on crossvalidated  how to choose the number of hidden layer and node in a feedforward neural network   however  let PRON add PRON own two cent   there be no magic rule for choose the good neural network architecture  but if PRON can find an architecture someone have use to solve a similar problem this be often an excellent starting point   the good place to look be official or unofficial example use popular neural network library  such as keras  pytorch  or tensorflow  and architecture describe in academic literature  kera  example on github be a great resource   these architecture be likely choose after lot of trial and error  so most of the work will have be do for PRON   follow imran s answer  PRON find this paper in one of in the comment of the crossvalidated post PRON link to  besides an attempt to find the right architecture use genetic model  instead of use a rule  of  thumb   section 21 give some theoretical bound to how many hide unit should be in a one  two  hide  layer system  
__label__python __label__random-forest __label__scikit-learn PRON have a pandas dataframe x20346  4116   all independent column have binary variable as 0 or 1  whereas dependent column have continuous variable  when PRON execute below code use the scikit  learn library  PRON take for ever and PRON be not get any result   xtrain  xt  ytrain  yt  traintestsplitx13  y  trainsize08   rf  randomforestregressornestimators100   maxfeaturesauto   oobscore  true   njobs1  verbose0   rffitxtrain  ytrain   PRON be use this for feature selection  in case of less number of row PRON can get result  but when number of row go up then this problem start happen  do PRON have any idea how to make PRON work   have PRON try increase  njobs or set oobscore to false  PRON be not sure what the logic be in sklearn to default maxfeature to auto   nfeature   use sqrt be common in random forest and should gain efficiency   some of the setting be likely too aggressive for PRON machine   as other have point out  njobs may help by take advantage of openmp thread that be likely available on a multicore processor   but let also look at some other parameter from the documentation   randomforestregressornestimators10  criterionmse   maxdepth  none  minsamplessplit2  minsamplesleaf1  minweightfractionleaf00  maxfeaturesauto   maxleafnod  none  bootstrap  true  oobscore  false  njobs1  randomstate  none  verbose0  warmstart  false   why do not PRON set nestimator to 10 and then slowly ramp PRON up   maxdepth should probably be set both so PRON tree be not too computationally intensive and so PRON do not overfit PRON model  be PRON cross validate    PRON have set maxfeature to auto which specify that PRON use nfeature  but PRON have not set nfeature   PRON could consider turn this down to some percentage of PRON full feature set   in general PRON need to cross validate PRON model and tinker with the parameter in order to find something that run on PRON box in finite time use the available memory and perform adequately   PRON could also consider tune and cross validate a simple decision tree and then use that insight to tweak the parameter of the random forest   sample the datum   big dataset often require lot of compute power   PRON sometimes drastically reduce the size of PRON training datum while PRON be zero in on the good model and then time the scaling as PRON increase the dataset   this will give PRON a good idea of how much time PRON should expect for the full problem   other model   finally  maybe PRON have a problem for which  bootstrap aggregation  be a bad fit  maybe another type of aggregation would work better like  boost  in which case a  random jungle  or  ada  boost  algo may work better   or perhaps PRON may want to move to a computationally less intense method that do not require aggregation like a  support vector regressor    scikit  learn have a great benefit in that PRON be very simple to swap out different model and try different pipeline   cloud computing   PRON be not sure what box or laptop PRON be run this on  but PRON may just be easy to spin up an aws  google  ms  ibm  instance and run this on a well  power box in the cloud   the aws spot market be crazy affordable over the weekend or in the middle of the night  so PRON can work on some very powerful 8 core machine for  50 per hour  so a 24 hour job be only  12   hope this help  
__label__databases __label__parsing update  see bottom of this question   not sure if PRON be at the right channel here but PRON have good hope someone here may be able to help PRON   PRON be try to process and analyse datum from a footscan system that be export from the footscan 9 gait essentials software to a rsdb database  this database be a standard sqlite database with a different file extension   PRON have no problem access the datum via python but the most interesting datum  the raw data from the sensor  be store as a  blob   see sqlite documentation for more detail    export this datum from sqlitepro to csv show that the datum in the blob look like this  although much long     lt9c4e79cb cbd09de5 c380fc78 a74e819f  ab2172c7 d9cf311a 357aebacgt   PRON question be  what would be PRON first step to try to discover how this data be encode  and do anyone maybe recognize this type of encoding   extra hint  the data represent all force of a force plate with 64x64 force sensor  sample at 300hz  so PRON be most probably a multidimensional array   update  PRON be make some progress  as comment below  the value be most probably hexadecimal value  but PRON be not sure if the value be base 16  most value seem to be  10  9   furthermore  PRON find a table in the database call  contact  which link to entry in the blob table which contain the most hexadecimal value   40000   PRON guess be that these be the entry which be recognize as actual step  PRON extract the datum from one of these step here  contact PRON would 2  blob PRON would 24   this blob contain 50792 hexadecimal value   this be the entry for this step in  contact    PRON would   35d9a73e608d4ba3  9823de0540f11c93   timestamp  2  delete  null  framewidth  64  frames  24  frameoffset  12204  framecount  214  frameheight  64  framestorewithcontactscontacs   5d10546e5ef3  4469b898d7b9bd58458f   originalid   32ae0f73  8022  4151  92e7  510c0e86d09d   orphan  false  full export of the blob table can be download here   the full rsdb can be find here  PRON may have to rename to sqlite    the table  foot  and  region  in the full database may contain useful datum too  but PRON be not yet sure how   
__label__machine-learning __label__scikit-learn PRON be new to the field of machine learning  but have do PRON share of signal processing  please let PRON know if this question have be mislabel   PRON have two dimensional datum which be define by at least three variable  with a highly non  linear model way too complicated to simulate   PRON have have vary level of success at extract the two main component from the datum use method like pca and ica  from the python library scikit  learn   but PRON seem these method  or at least  these implementation of the method  be limit to extract as many component as there be dimension in the datum  for example  2 component from a 2d point cloud   when plot the datum  PRON be clear to the train eye that there be three different linear trend  the three color line show the direction   when use pca  the main component be align to one of the color line  and the other be at 90 °  as expect  when use ica  the first component be align with the blue line  and the second be somewhere in between the red and green one  PRON be look for a tool which could reproduce all three component in PRON signal   edit  additional info  PRON be here work in a small subset of a big phase plane  in this small subset  each input variable produce a linear change on the plane  but the direction and amplitude of this change be non  linear and depend on where exactly on the big plane PRON be work  at some place  two of the variable can be degenerate  PRON produce change in the same direction  for example  say the model depend on x  y  and z a change in the variable x will produce a variation along the blue line  y cause a variation along the green line  z  along the red one   the short answer be yes   essentially PRON will be perform some sort of feature engineering  this mean construct a series of function of PRON datum  often    phijx   mathbb rp rightarrow mathbb r      j1  k  which  string together  define a transform data vector  boldsymbol phix of length  k  there be a number of way  good and bad  of do this  PRON may want to look up term like   spline and generalise additive model   the kernel trick  how to make a model where  krightarrow infty    feature engineering  of the manual variety  eg add an  x2  column to PRON datum    deep learning  representation learn  as PRON may guess from such a varied bag of technique  this be a large area  PRON go without say really but care have to be take to avoid overfitt   this paper representation learning  a review and new perspectives deal with some of the issue around what make a particular set of feature  good   from a deep learning perspective   PRON guess u be look for feature which extract out new feature  a feature which good representthe dataset  if that be the case then PRON call such method  feature extraction   
__label__machine-learning PRON would like to find frequent pattern in datum that have be create by an accelerometer of a smart watch   the algorithm should return the part of the datum that occur after a pattern   in the good case  the algorithm find the movement automatically that show how PRON open door  grab PRON phone etc   unfortunately  until now PRON only be able to find algorithm that find frequent item set in datum   do anyone know a solution on how to find those pattern in continuous datum   thank PRON very much    
__label__finite-element __label__numerical-analysis when apply galerkin method  PRON have two convention  ie multiply the test function v at left  right   v  uu  v    both way will not matter for simple problem like poisson s equation  since the stiffness matrix k and mass matrix m be symmetric  and the transpose do not change   however  if there be also first order derivative in the differential equation  then the correspond assemble matrix be anti  symmetric for  v  dudu  v    each one be the transpose of the other  then PRON be a non  unique problem   PRON know PRON get wrong at some point  anyone could help   PRON usually write linear algebra equation as  au  b  which can be rewrite to  beginequation   vt a u  vt b qquad forall vin mathbb rn   endequation   from this  PRON see that the convention in linear algebra be the test function on the left  while in pde  PRON have PRON on the right  thus  for a nonsymmetric bilinear form  the safe bet for integrate the matrix entry be  beginequation   aij   auj  vi    endequation   rule of thumb  the row in the finite element matrix correspond to the test function that be be multiplied to the pde  while the column in the finite element matrix correspond to the weak solution  trial function    long answer  say PRON consider the convection term in the diffusion  convection  reaction equation in a bounded open domain  omega  with a maybe not divergence free convection vector  mathbfb      nablacdotanabla u   underbracemathbfbcdot nabla utextconvection    cu  f      with zero dirichlet boundary condition so that the trial function space for  u coincide with the test function space for  v  PRON do not matter PRON multiply the test function  v on the left or right      intomega  v  mathbfbcdot nabla u   quad textcompare with     intomega   mathbfbcdot nabla uvtag1      the correspond matrix entry will be the same if  v be test function  if PRON perform integration by part  the convection term will be different      intomega  nabla cdotmathbfbv  u  quad textor     intomega  u nabla cdotmathbfbvtag2      if PRON let  u  sumj1n ujphij  where  phij be the basis function in PRON finite element space  plug  u s expression into  1   and choose  v  phii as the test function to get the  ith row the linear equation system      intomega  anablabig  sumj1n ujphijbig  cdot nabla phii   intomega  left  mathbfbcdot nabla big  sumj1n ujphijbigrightphii   intomega  c big  sumj1n ujphijbigphii intomega  fphii      the matrix  b correspond to the second convection term have entry      bij   intomega  phii  mathbfbcdot nabla phij   intomega   mathbfbcdot nabla phijphii      PRON do not matter PRON multiply the test function on the left or on the right  the index in the  u correspond to the column  while the index in  v correspond to the row   however  PRON do matter the differentiation be perform on which function  the trial function  u or the test function  v if PRON use  2  as PRON convection term in the bilinear form  the matrix  b will be different   PRON seem to PRON that PRON confuse this difference with the  multiply the test function on left or right   because in 1d  use the notation in PRON comment   n nx be matrix generate by  2   for this be perform the differentiation on the test function  v  nx n be the matrix generate by  1   this be perform the differentiation on the trial function  u  or PRON may confuse  multiply the test function on left or right  with the formalism of a non  symmtric bilinear form  in case the bilinear form for convection term be already define as      bu  v    intomega  v  mathbfbcdot nabla u       notice if PRON switch  u and  v  PRON be     bv  u    intomega  u  mathbfbcdot nabla v      intomega  nablacdotmathbfbu  v  intpartial omega  vmathbfbucdot mathbfnds     intomega   nablacdotmathbfb   u vintomega   mathbfbcdotnabla u  v   intpartial omega  vmathbfbucdot mathbfnds     bu  vintomega   nablacdotmathbfb   u v   intpartial omega  vmathbfbucdot mathbfnds     which be totally different  even after integration by part   this be not about the position where PRON multiply this test function  v like PRON say in the comment  if use  leave  correspond to column   right  correspond to row  to assemble the finite element matrix  PRON will get nowhere near correct matrix if divergence of  mathbfb be not zero  also boundary term do not vanish if there exist neumann boundary   maybe the minus sign in  2  make PRON think this matrix be anti  symmtric  however PRON be not  because the entry on PRON diagonal be not zero in general      bii   intomega  phii  mathbfbcdot nabla phii  neq 0      a matrix with non  zero diagonal can not be anti  symmetric   PRON teach PRON student to always multiply from the left  for exactly the reason PRON mention  more detail here   httpwwwmathtamuedubangerthvideos6764html  httpdealiiorg800doxygendealiistep3html 
__label__deep-learning __label__convolutional-neural-networks __label__nlp __label__keras train a conv2conv autoencoder for the purpose of text clustering  with parameter fix as follow   numfilter  128  drop  03  numtoken  2000  maxseqlen  64  the model take very long to barely converge  and PRON be not sure where about to start for tweak PRON  afraid PRON have do something wrong on the way  what do PRON think   input  inputshapemaxseqlen  numtoken    x  conv1dnumfilter   5   activationrelu   paddingsameinput   x  maxpooling1d2   paddingsamex   x  conv1dnumfilter   2   1    3   activationrelu   paddingsamex   x  maxpooling1d2   paddingsamex   x  conv1dnumfilter   2   2    3   activationrelu   paddingsamex   x  maxpooling1d2   paddingsamex   x  conv1dnumfilter   2   3    3   activationrelu   paddingsamex   x  maxpooling1d2   paddingsamex   x  conv1dnumfilter   2   4    3   activationrelu   paddingsamex   x  maxpooling1d2   paddingsamex   x  conv1dnumfilter   2   5    3   activationrelu   paddingsamex   encode  maxpooling1d2   paddingsamex   flatten  flattenencod     decode use conv1d and upsampl 1d  x  upsampling1d2encoded   x  conv1dnumfilter   2   5    3   activationrelu   paddingsamex   x  upsampling1d2x   x  conv1dnumfilter   2   4    3   activationrelu   paddingsamex   x  upsampling1d2x   x  conv1dnumfilter   2   3    3   activationrelu   paddingsamex   x  upsampling1d2x   x  conv1dnumfilter   2   2    3   activationrelu   paddingsamex   x  upsampling1d2x   x  conv1dnumfilter   2   1    3   activationrelu   paddingsamex   x  upsampling1d2x   x  conv1dnumfilter   5   activationrelu   paddingsamex   pred  densenumtoken  activationsoftmaxx   autoencoder  modelinput  pred                                                                    layer  type   output shape  param                                                                    input7  inputlayer    none  64  2000   0                                                                   conv1d73  conv1d    none  64  128   1280128                                                                   maxpooling1d37  maxpooling  none  32  128   0                                                                   conv1d74  conv1d    none  32  256   98560                                                                   maxpooling1d38  maxpooling  none  16  256   0                                                                   conv1d75  conv1d    none  16  512   393728                                                                   maxpooling1d39  maxpooling  none  8  512   0                                                                   conv1d76  conv1d    none  8  1024   1573888                                                                   maxpooling1d40  maxpooling  none  4  1024   0                                                                   conv1d77  conv1d    none  4  2048   6293504                                                                   maxpooling1d41  maxpooling  none  2  2048   0                                                                   conv1d78  conv1d    none  2  4096   25169920                                                                   maxpooling1d42  maxpooling  none  1  4096   0                                                                   upsampling1d37  upsampling  none  2  4096   0                                                                   conv1d79  conv1d    none  2  4096   50335744                                                                   upsampling1d38  upsampling  none  4  4096   0                                                                   conv1d80  conv1d    none  4  2048   25167872                                                                   upsampling1d39  upsampling  none  8  2048   0                                                                   conv1d81  conv1d    none  8  1024   6292480                                                                   upsampling1d40  upsampling  none  16  1024   0                                                                   conv1d82  conv1d    none  16  512   1573376                                                                   upsampling1d41  upsampling  none  32  512   0                                                                   conv1d83  conv1d    none  32  256   393472                                                                   upsampling1d42  upsampling  none  64  256   0                                                                   conv1d84  conv1d    none  64  128   163968                                                                   dense7  dense    none  64  2000   258000                                                                   total param  118994640 trainable param  118994640 non  trainable  param  0  note that PRON encoding be give from the dense layer in the middle   model2  modelinput  flatten   ypr  model2predictencoderinputdata   which be now of dimension 4096  ideally PRON would be small  100400   
__label__finite-difference __label__discretization standard finite difference formula be usable to numerically compute a derivative under the expectation that PRON have function value  fxk at evenly spaced point  so that  h equiv xk1   xk be a constant  what if PRON have unevenly spaced point  so that  h now vary from one pair of adjacent point to the next  obviously PRON can still compute a first derivative as  f39x  approx frac1hkfxk1    fxk  but be there numerical differentiation formula at high order and accuracy that can adapt to variation in the grid size   jm s comment be right  PRON can find an interpolate polynomial and differentiate PRON   there be other way of derive such formula  typically  PRON all lead to solve a van der monde system for the coefficient   this approach be problematic when the finite difference stencil include a large number of point  because the vandermonde matrix become ill  condition   a more numerically stable approach be devise by fornberg  and be explain more clearly and generally in a second paper of PRON   here be a simple matlab script that implement fornberg s method to compute the coefficient of a finite difference approximation for any order derivative with any set of point   for a nice explanation  see chapter 1 of leveque s text on finite difference method   a bit more on fd formula   suppose PRON have a 1d grid   if PRON use the whole set of grid point to determine a set of fd formula  the result method be equivalent to find an interpolate polynomial through the whole grid and differentiate that   this approach be refer to as spectral collocation   alternatively  for each grid point PRON could determine a fd formula use just a few neighboring point   this be what be do in traditional finite difference method   as mention in the comment below  use finite difference of very high order can lead to oscillation  the runge phenomenon  if the point be not choose carefully   the simple method be to use finite difference approximation   a simple two  point estimation be to compute the slope of a nearby secant line through the point  x  fx   and  xh  fxh1  choose a small number h  h represent a small change in x  and PRON can be either positive or negative  the slope of this line be    fxhfxover h  this expression be newton s difference quotient   the slope of this secant line differ from the slope of the tangent line by an amount that be approximately proportional to h as h approach zero  the slope of the secant line approach the slope of the tangent line  therefore  the true derivative of f at x be the limit of the value of the difference quotient as the secant line get close and closer to be a tangent line  httpmathformeremortalswordpresscom20130112anumericalsecondderivativefromthreepoints  this address PRON question and show the formula PRON be look for  for the second derivative   high  order derivative follow the same pattern   the above answer be great in term of give PRON a code to use  but be not as good in term of theory  if PRON want to delve deeper into interpolate polynomial  take a look at this theoretical treatment with a few concrete example   singh  ashok k  and b s bhadauria   finite difference formulae for  unequal sub  interval use lagrange ’s interpolation formula    international journal of mathematics and analysis 317  2009    815  827    link to pdf   the author use lagrangian interpolation  see the wikipedia article  to calculate 3point  4point and 5point interpolate polynomial  as well as PRON first  second and third derivative  PRON have expression for the truncation error as well  which be important to consider when use any finite difference scheme  PRON also have the generic formula for calculate interpolate polynomial use n point   lagrangian interpolate polynomial be useful because PRON and PRON derivative can be very accurate in the domain PRON be interpolate  and PRON do not assume an even grid space  due to the nature of lagrangian interpolate polynomial  PRON can never have more order of derivative than PRON have grid point   PRON think this answer PRON question well because the paper PRON cite have formulae for arbitrarily high  order finite difference scheme  which by nature be for uneven grid and be limit only by the number of grid point PRON include in PRON stencil  the paper also have a generic formula for the truncation error  which will help PRON evaluate the lagrangian interpolate polynomial scheme against other scheme PRON may be consider  the author s paper should give the same result as fornberg s method  PRON contribution be really just tally a few example and give an estimate of the error  which PRON may find useful   PRON find both the paper PRON cite and fornberg s work to be useful for PRON own research   give a 2d irregular spaced datum  PRON would like to know how to find derivative by interpolation  do lagrange 2d interpolation work at irregular spaced datum 
__label__efficiency __label__javascript PRON have implement an interactive visualization with d3js  javascript to explore the frequency and various combination of co  occur item set  PRON want to complement the interactive exploration with some automate option   do someone know an efficient javascript implementation of the association rule mine   PRON typical scenario will have just up to 30 different item   there be some good web site with implementation of frequent item set mining  improvement from the initial apriori algorithm   httpwwwborgeltnetapriorihtml  any help be greatly appreciate   PRON do not know how efficient PRON be  but PRON do find some implementation   httpsgithubcomdmargesapriori  httpsgithubcomseratchapriorijs 
__label__algorithms __label__time-integration __label__numerical PRON want to understand how the rattle algorithm work   can somebody give PRON an example  in pseudocode or use any programming language like python or matlab  of how would PRON implement a numerical integrator for the simple pendulum problem   the follow snippet of code be an implementation of rattle on a system with the constraint  gx  y   k x2  y2  1  0   constraint   double gconst double2amp  r    return k  rx  rx  ry  ry  10      gradient of constraint   double2 gconst double2amp  r    return double220  k  rx  20  ry      void rattledouble2amp  q  double2amp  p  doubleamp  lambda  double h     declare auxiliary constant   const double2 gqprev  gq     deal with constraint on the configuration manifold   q   h  p   double lambdar  00    solve use newton s method   for  sizet k  1  k  lt maxiter  k    if  k   maxiter   abort     const double2 r  q  lambdar  gqprev   const double phi  gr    const double dphidl  dotgr   gqprev    const double update  phi  dphidl   if  fabsphi   lt  tol  ampamp  fabsupdate   lt  tol   break   lambdar  update     q  lambdar  gqprev   p  lambdar  h  gqprev    deal with constraint on the tangent space   const double2 gq  gq    double lambdav  dotgq  p   dotgq  gq    p  lambdav  gq   lambda   lambdar  lambdav   20   assertfabsgq    lt  tol  ampamp  fabsdotgq   p    lt  tol      PRON use the notation from chapter 7 in the book simulating hamiltonian dynamics   for PRON convenience  PRON have upload a standalone program that PRON can compile   run  and play with at httpsgistgithubcomjmbr9857614filerattlecpp  the program produce output that can be use to plot the graph show below  an example trajectory and PRON correspond plot of total energy as a function of time   
__label__conjugate-gradient let  ain mathbbrntime n  symmetric and positive definite   suppose PRON take  m unit of work to multiply a vector by  a   PRON be well know that perform the cg algorithm on  a with condition number  kappa require  mathcalo   msqrtkappa  unit of work   now  of course  be a  mathcalo statement this be an upper  bind   and the cg algorithm can always terminate in zero step with a lucky initial guess   do PRON know if there exist a rhs and an initial  unlucky  guess that will require  mathcalthetasqrtkappa step   put another way  be bad  case work  complexity of cg really  theta  m sqrtkappa   this question arise when PRON try to determine if the benefit of a preconditioner  low  kappa  outweigh PRON cost  high  m    right now  PRON be work with toy problem and would like to have a good idea before PRON implement anything in a compile language   the answer be a resounding yes  the convergence rate bind of   sqrtkappa1    sqrtkappa1 be sharp over the set of symmetric positive definite matrix with condition number  kappa in other word  know nothing more about  a than PRON condition number  cg really can take  simsqrtkappa iteration to converge  loosely speak  the upper  bind be attain if the eigenvalue of  a be uniformly distribute  ie  pepper   within an interval of condition number  kappa  here be a more rigorous statement  deterministic version be more involved but work use the same principle   theorem  worst  case choice of  a   pick any random orthogonal matrix  u  let  lambda1ldotslambdan be  n real number uniformly sample from the real interval   1kappa  and let  bb1ldotsbn be  n real number sample iid  from the standard gaussian  define   a  umathrmdiaglambda1ldotslambdanut then in the limit  ntoinfty  conjugate gradient will convergence with probability one to an  epsilon accurate solution of  ax  b in no less than  omegasqrtkappalogepsilon1 iteration   proof  the standard proof be base on optimal chebyshev polynomial approximation  use technique find in a number of place  such as greenbaum s book or saad s book   take this as PRON original question  do PRON know if there exist a rhs and an initial  unlucky  guess that will require  thetasqrtkappa  step   the answer to the question be  no    the idea of this answer come from the comment from guido kanschat   claim   for any give condition number  k  there exist a matrix  a  with that condition number for which the cg algorithm will terminate in at most two step  for any give rhs and initial guess    consider  ain mathbbrntime n   where  amathrmdiag1kappakappaldot  kappa  then the condition number of  a be  kappa  let  bin mathbbrn be the rhs  and denote the eigenvalue of  a as  lambdai where    lambdai  leftbeginarrayll1  amp  i1  kappa  amp  inot 1  endarray  right       PRON first consider the case where  x0   in mathbbrn  the initial guess  be zero   denote  x2in mathbbrn as the second estimate of  a1b from the cg algorithm   PRON show that  x2    a1b by show  langle x2a1b  ax2a1brangle  0  indeed  PRON have  beginalign    langle x2a1b  ax2a1brangle  amp  left x2a1b righta2    ampminpin mathrmpoly1   left  paa1   b righta2   ampminpin mathrmpoly1   sumi1n  plambdai   lambdai12 lambdai bi2    ample sumi1n  widehatplambdai   lambdai12 lambdai bi2   0  endalign    where PRON use the first order polynomial  widehatp define as  widehatpx  1kappa  xkappa  so PRON prove the case for  x0 0  if  x0   not  0   then  x2 overlinex2 x0 where  overlinex2     be the second estimate of the cg algorithm with  b replace with  overlineb   b  a x0  so PRON have reduce this case to the previous one  
__label__finite-difference __label__stability __label__hyperbolic-pde __label__method-of-lines when solve a general pde such as    frac  partial 2 epartial t 2   frac  partial 2 epartial z 2   frac  partial epartial z     this equation can be solve by the method of line  the second order time  derivative can also be transform into a first  order differential equation  but now the spatial derivative have to approximate  some method such as backward and central finite difference approximation exist  for vary order of accuracy   but be there any other possibility to good approximation these spatial derivative   also  on a relate note  when solve the above equation on a domain of  t   01000  and  z   01000   the step can be make such that use a 1000x1000 grid   delta z sim 1  and  delta t sim 1 but if one want to consider an arbitrary precision such as  delta z sim 1014  this result in very large oscillation  depend on the initial condition   in the above equation  be there any particular limit for how small  delta z can be make  if PRON be to instead change the above into a system of first order differential equation in  z and discretiz  t  would the stability condition be analogous to reverse the courant condition  ie become  c delta t  gt  z    
__label__machine-learning PRON have a hobby project which PRON be contemplate commit to as a way of increase PRON so far limited experience of machine learning  PRON have take and complete the coursera mooc on the topic  PRON question be with regard to the feasibility of the project   the task be the following   neighboring cat be from time to time visit PRON garden  which PRON dislike since PRON tend to defecate on PRON lawn  PRON would like to have a warning system that alert PRON when there be a cat present so that PRON may go chase PRON off use PRON super soaker  for simplicity s sake  say that PRON only care about a cat with black and white coloring   PRON have setup a raspberry pi with camera module that can capture video andor picture of a part of the garden   sample image   PRON first idea be to train a classifier to identify cat or cat  like object  but after realize that PRON will be unable to obtain a large enough number of positive sample  PRON have abandon that in favor of anomaly detection   PRON estimate that if PRON capture a photo every second of the day  PRON would end up with maybe five photo contain cat  out of about 60000 with sunlight  per day   be this feasible use anomaly detection  if so  what feature would PRON suggest  PRON idea so far would be to simply count the number of pixel with that have certain color  do some kind of blob detection  image segmenting  which PRON do not know how do to  and would thus like to avoid  and perform the same color analysis on PRON   this be an interesting and also quite ambitious project   PRON be not sure anomaly detection  at least in the sense describe in the course PRON follow  would be a very fitting algorithm in this case   PRON would consider a more viable approach to be what have be discuss at the end of that course where a photo ocr workflow be demonstrate   the approach would consist of segment PRON image in small  block   and go through PRON one  by  one use a supervised learning algorithm and try to classify each block accord to whether PRON contain a cat or not  if one block contain a cat  the alarm go off  as a bonus  PRON get the position of the cat as well  so that PRON may think of incorporate some  automatic  response as a future step to PRON project   the benefit here be that PRON will not have to train PRON algorithm use a dataset specific to PRON garden  which  as PRON mention be difficult to create   but PRON can use image of cat take off the net  eg perhaps PRON can search for  cat on grass  or something   and perhaps patch of photo from PRON  or other  garden  therefore PRON dont have to spend PRON time collect photo from PRON camera  and PRON avoid the risk of have a very small  comparable  sample of positive  ie cat    now  of course how easy PRON be to build an accurate cat detector be another topic   PRON could simplify PRON problem significantly by use a motion  change detection approach  for example  PRON could compare each image  frame with one from an early time  eg  a minute earlier   then only consider pixel that have change since the early time  PRON could then extract the rectangular region of change and use that as the basis for PRON classification or anomaly detection   take this type of approach can significantly simplify PRON classifier and reduce PRON false target rate because PRON can ignore anything that be not roughly the size of a cat  eg  a person or bird   PRON would then use the extract change region that be not filter out to form the training set for PRON classifier  or anomaly detector    just be sure to get PRON false target rate sufficiently low before mount a laser turret to PRON feline intrusion detection system   the strategy of motion  change detection be certainly adequate  but PRON would add an extra operation  PRON would detect those region that be more likely to be change  for instance  the ladder seem a place where human can be  also cat  and grass where dog  cat or human can be   PRON would capture a map with size of the object and trajectory and with this PRON would create a cluster with the aim of detect an object  with specific size within the image in term of pixel  that move with a certain speed and trajectory   PRON can achieve this by use r or PRON would suggest opencv in order to detect movement and follow different object   be PRON a bird  be PRON a cat  PRON have black  and  white cat  sized  magpie here  so that would fail   first thing would be to exclude all area that be green  cat be seldom green   then compare the rest to a reference image to remove static thing like stone and stair   detect object of a minimum size should be possible  but for a classification the resolution be too low  could be also PRON neighbor test PRON new remote control drone   with two camera PRON could do a 3d mapping of the object and eliminate flying object   opencv s background subtraction will find object move about PRON harden  after that PRON could use a classifier or shape analysis to differentiate between cat  people  tree and etc  
__label__machine-learning __label__deep-learning say PRON have a dataset with million of row and the attribute plain text  key  and output ciphertext  could deep learning  theoretically  be use to find pattern in the output that help decipher the ciphertext  be there any other potential approach   probably not  modern encryption system be design around cryptographic random number generator  PRON output be design to be statistically indistinguishable from true randomness  machine learning be generally base on discover statistical pattern in the datum  and with truly random datum there be none  even for flawed crypto where there be some small pattern to be find  the large amount of randomness in the input will overwhelm any direct attempt to decrypt the ciphertext   in addition  there be no heuristic PRON can use to tell if PRON be get close to a correct decryption  a single bit out in a guess at a key for example will completely scramble the output  blame hollywood for when PRON show decryption on screen like some crossword puzzle where the correct letter drop into place   that all  or  nothingness rule out discover algorithm via a machine learning process  even when PRON have the encryption key  the good PRON can do be brute  force all know algorithm  if PRON do not have the key  then PRON have to brute  force all possible key too   PRON could explore how difficult the problem be by attempt to guess the seed value use for a random number generator  use the mersenne twister rng  the standard one use in eg python   then the input could be the bit pattern for 624 32bit unsigned integer  and the output could be the 32 bit of the seed use to generate that series  the reason PRON suggest those specific number be because PRON be in fact possible to crack mersenne twister with that much datum  however  PRON still think that ml approach would be entirely the wrong tool to do so   another simple variant would be to see if PRON can teach a network to either produce or reverse a cryptographic hash  PRON could start with a know break one such as md5  input and output could be 80 bit  which simplify the architecture and pipeline enough that PRON could put together this test in a few hour  even though md5 be know to be compromise  PRON think there be zero chance PRON could teach a neural network to find any pattern   one important detail  if PRON want to  crack  an encryption  PRON will not be able to use the key as a know value  chance be though  that even if PRON provide the key to an ml process  PRON will be unable to learn how to decrypt  
__label__algorithms __label__computational-physics __label__fourier-transform __label__integration __label__fftw PRON have a 3d integral that be almost a radial convolution of the form    int d3khmathbfkgmathbfk  k     and PRON be look for a fast and efficient algorithm  eg fft  to solve PRON numerically  in the follow PRON give PRON a brief description of the function PRON be use and the problem PRON encounter   the function and the radial case  consider the fourier transform of a spherical top hat function     tildewik  riri1frac1viint wireimathbfkcdot rd3rfrac4pivikintriri1sinkrrdrequivtildewik  where  ri be some radius  if  fk be a general function  PRON come from an interpolation  so there be no analytical formula  the convolution integral    hijkint d3kfmathbfktildewileftmathbfkmathbfkrighttildewjleftmathbfkmathbfkright  be radial  thus PRON can be solve with a one  dimensional fft algorithm   the  kderivative and the convolution  dij2mathbfk  consider now the  kderivative of one of the two window function  some term in the integral be no longer radial     int d3kfmathbfktildewileftmathbfkmathbfkrightfracpartialtildewjleftmathbfkmathbfkrightpartial k     int d3kfkfractildewileftmathbfkmathbfkrightmathbfkmathbfkfracpartialtildewjleftmathbfkmathbfkrightpartialmathbfkmathbfkleftkfracmathbfkcdotmathbfkkright    equiv kdij1kfracmathbfkkcdot dij2mathbfk  particularly the integral  dij2  be still a convolution but PRON be not radial     dij2mathbfkint d3kfmathbfkmathbfkfractildewileftmathbfkmathbfkrightmathbfkmathbfkfracpartialtildewjleftmathbfkmathbfkrightpartialmathbfkmathbfk  problem  now  solve the integral by use common integration procedure be not really doable  because the function  tildewik oscillate really rapidly so the numerical integral take a lot of time and the result be never really reliable and PRON depend a lot on the choose precision   on the other hand a three dimensional fft algorithm be really memory consuming  in fact  tildewik be a rapidly oscillating function and PRON need a lot of point to be well sample  around 50007000 point for each dimension  thus in three dimension the matrix become really big  but for example PRON do not know anything about possible method to parallelize the process   PRON also check other possibility like hankel transform  bessel function for the angular part or legendre polynomial  but without any success  if PRON be possible to rewrite PRON in the form of a 2dimensional convolution  perhaps PRON would also be enough   
__label__petsc PRON have be try to use a petscbag to read in a number of option PRON want to pass to a program PRON be write  before PRON come across the documentation for petscbag  PRON be plan on use a struct and a function that would take the struct and use petscoptionsgetsometype   to set all the value in the struct   PRON figure that petscbag would save some code  but see as each variable in the petscbag have to use petscoptionsregistersometype    PRON look to be just as much code to use the petscbag as not to use PRON   on the other hand  PRON do not know that much about how the struct PRON be go to use would work when run with mpi   what should a petscbag be use for  what advantage do PRON have over vanilla struct   item in a petscbag be self  document though the associated help string  see when PRON run with help and with petscbagview    can be overridden at run  time through the option database  prefixfieldname 42e7   and be serializable in ascii and portable binary  with a normal struct  PRON have to write code to take input  supply help  and serialize  include deal with backward compatibility as PRON add new field    PRON be a reasonable thing to use for a collection of problem  specific datum  eg specification of a physical model configuration  PRON be not a good place to put  problem  sized datum  such as an unstructured mesh or high  dimensional observational datum  
__label__classification __label__dataset PRON be look at datum from the london data store base on social characteristic between london borough   since there be only about 30 london borough  the data set PRON be look at be naturally very small  for example  PRON may be fit regression  correlation to a plot of about 30 point   what be appropriate way to conduct classification on such small datum set  and why   why  be important   PRON be think of something like svm  or naive bayes  or regression if the datum be continuous   what be very inappropriate way to conduct classification here   PRON do not think PRON need some classification algorithm  PRON can use PRON basic understanding on data business knowledge to do the classification  as the number of datum point be too low  the model can not give PRON good  generalise result   even if PRON try apply some complex algorithm like svm  nn  PRON be of no use as the data be too low   if PRON still want to apply some machine learn algorithm and then PRON can apply naive bayes  decision tree as these be the basic algorithm  can do the job  
__label__machine-learning __label__classification __label__svm PRON be train a support vector machine classifier  svm  on 100000 observation  PRON would like to try different parameter combination  include kernel type  use cross  validation  however  PRON be computationally expensive to optimize over these different parameter value  therefore  PRON would like to test PRON on a subset of 5000 observation on a validation set and then base on those result choose the parameter for the final model that be train on 100000 observation   PRON question be then whether there be something intrinsic in these parameter that will make this approach problematic  more specifically  be there a parameter  where training on the full datum set will require a different value for optimal performance   PRON be use the follow kernel with PRON parameter in parenthesis   rbf  gamma  c   poly  degree  c   sigmoid  gamma  c   linear  gamma    in theory  if PRON have a large enough random sample of PRON data set  PRON should be representative of the characteristic in the large population of datum that will affect the relationship between parameter value and performance  PRON seem to PRON that a  5 sample of PRON datum may be too small for what PRON be try to do  when PRON be perform cross  validation on a test  dev datum set  PRON usually set aside somewhere in the neighborhood of  10 for the hold  out test  evaluation datum  and use  90 for PRON development work  but  as PRON point out  this may be too big give the computational constraint PRON be up against  depend on the type of application PRON be work on  PRON could look at feature distribution in PRON sampled datum and compare PRON to that in the large population  if the two set be comparable  then PRON may be ok to do PRON dev  work on the small datum set  PRON say PRON depend on the type of application PRON be develop  because many statistician would consider this cheating  in that PRON be generally not good model evaluation practice to look at hold  out datum at all  here be the procedure PRON would recommend   separate out  10 of PRON datum for hold  out evaluation   divide the remain  90 into  510 parameter optimization  and the remainder for final parameter eval   compare the feature distribution of the parameter optimization and final parameter eval data set  if PRON be not comparable  redraw the sample  within the context of the  90 development sample   when PRON have a good subsample  run PRON parameter optimization experiment on the small parameter optimization datum set   with PRON final parameter setting  evaluate with cross  validation on the the  90 combine parameter optimization and final parameter eval data set   perform final model analysis by training on the  90 datum set and classify on the  10 hold  out evaluation set   PRON may also be worth look into optimize PRON analytic pipeline  for example  be file PRON  o  feature generation and feature extraction part of the the workflow PRON be use  or have PRON do all of that off  line and be just concerned with the svm evaluation part  
__label__symbolic-computation be there any library available for solve problem at the level of college physics   in particular  PRON be interested in library for general purpose programming language that can calculate result both numerically and symbolically   library which could be use for implement something like this be sympy  python   ginac  c   or symbolicc  c    PRON be ask this question because PRON have start cook up a c  library for symbolic computation and have use PRON to solve a few projectile motion problem  but would like to review other work out there   here be an example of a program which solve such a problem   here be what the program output when run   any pointer or suggestion welcome  thank   
__label__parallel-computing __label__performance __label__mpi __label__intel-mkl __label__multicore preface  PRON seem to lack a fundamental understanding of good practise recommendation give by intels mkl user guide for use mkl in threaded application  so let PRON clarify PRON together   wording and the question  there be especially two different way to optimize numerical code  either openmp or the message passing interface  mpi   and combination of both  PRON seem to PRON mkl settle with openmp internally out of the box  intel recommend to not combine openmp parallelization with a manual parallelization  eg  a domain decomposition at high level  with the aid of mpi  see here    PRON be confuse now about what to do in multi  processor environment  read cluster  with many physical computation unit  PRON first approach towards the topic be   use mpi  do a domain decomposition of the problem  distribute the small chunk to all cpu in the cluster and use sequential mkl   mkl without openmp  there   PRON especially ask PRON how PRON could be do differently when rely only on multi  threaded mkl   mkl with openmp  in the cluster  what if one matrix in a matrix vector product be to large to fit into memory of one compute node  will PRON be automatically spread over all compute node by mkl   nothing stop PRON from decompose the problem up PRON and feed the relevant partition datum into mkl sequentially  or even in parallel  PRON will work as long as PRON avoid data race  but PRON may experience performance penalty unless PRON be very careful about how PRON do PRON   the reason PRON be discourage to combine openmp code with mkl be that openmp be generally not equip to handle the parallel scheduling challenge of nest parallelism  this be slowly start to change   thus the overhead of this limitation result in decrease performance   there be no issue in do this with mpi however  but PRON will have to be very careful with how PRON set process affinity in the mpi rank and thread affinity in openmp  or else PRON will have again very degraded performance  PRON should look at the mpi documentation as well as the openmp documentation for how to coordinate the two correctly  PRON be not possible to give general advice here because there be no standardized way yet to handle this  PRON have to read the documentation of PRON openmp and mpi   alternatively  intel also release a version of mkl that be thread with the threading building blocks  tbb  library  this library can very efficiently handle nest parallelism  but then PRON will have to use tbb to achieve this  while PRON be possible to mix openmp and tbb  PRON be not recommend   this mean PRON can call mkl within tbb parallelize code and experience less performance penalty than if PRON have do the same within openmp   edit  PRON should add that mkl be strictly focused on single node performance  thus any extra parallelism PRON add to PRON in the form of openmp or mpi must be manage PRON  PRON will not distribute matrix for PRON nor perform block linear algebra  PRON will have to block the matrix PRON and perform the correct sequence of blas or lapack call to achieve the result PRON want  
__label__visualization __label__sql PRON be work on develop a data dashboard  app for a microsoft sql database  currently  PRON be develop the dashboard use shiny and r the app be mostly for exploratory analysis to allow people to filter out some subset of datum  build some plot  and export datum  plot   PRON question be what be some other option for create this type of dashboard  PRON be aware of tableau   PRON could consider powerbi  PRON offer a couple of useful licensing scenario  bundle with office 365 etc   and also have connector to source datum that already work well with sql server   PRON do not say if sql server be run on premise or in the cloud  but powerbi should work in both case  
__label__artificial-neuron on the wikipedia page PRON can read the basic structure of an artificial neuron  a model of biological neuron  which consist   dendrites  act as the input vector   soma  act as the summation function   axon  get PRON signal from the summation behavior which occur inside the soma   PRON have check deep learning wiki page  but PRON could not find any reference to dendrite  soma or axon   so PRON question be  which type of artificial neural network implement or can mimic such model most closely   ann research do not try to model biological neuron  as the aim be to achieve good performance at prediction task  however  there be a body of literature in neuroscience that look at computational model of neuron  neuron be complicated cell and PRON understanding of neuron be still not complete   ann approximate biological neuronal network   the approximation begin with extreme simplicity in the early perceptron design   spiking network be example of more accurate approximation   more accurate still  be complex simulation of neuron behavior that therefore necessitate significant computing resource   if PRON be interested in a mathematical overview on analysis of biological neuron model PRON can recommend dynamical systems in neuroscience by eugene izhikevich   only a small portion of the habituation  sensitization  and classical conditioning behavior of neuron have be primitively simulate in ann system  simulation of actin cytoskeletal machinery1 and other agent of neural plasticity  central to learn new domain  be in PRON beginnings2   as of this writing  the complexity of neuron activation dwarf the model be use in work commercial ann system  but the research continue along multiple front   the neuroscience of learning3   parallel hardware approach that good support ann simulation accuracy4  5  and  dynamic frameworks6  this list and the example reference in the superscript  with link below  represent a tiny sample of the information available and the work in progress   reference   1  molecular cell biology  4th edition   lodish h  berk a  zipursky sl  et al   new york  w h freeman  2000   section 181 the actin cytoskeleton   2  neuron software  yale u   3  molecular cell biology  4th edition   lodish h  berk a  zipursky sl  et al   new york  w h freeman  2000   section 217 learning and memory   4  artificial neural networks on massively parallel computer hardware  udo seiffert  university of magdeburg  germany   5  neurogrid  stanford u   6  explanation of dynamic computational graph framework 
__label__machine-learning __label__regression PRON be try to evaluate what be the right number of cluster need for clusterize some datum   PRON know that this be possible use davies – bouldin index  dbi    to use dbi PRON have to compute PRON for any number of cluster and the one that minimize the dbi correspond to the right number of cluster need   the question be   how to know if 2 cluster be good than 1 cluster use dbi  so  how can PRON compute dbi when PRON have just 1 cluster   
__label__optimization __label__python __label__constrained-optimization PRON believe this would be an interesting problem   PRON have a blackbox function which can take 2  60 input variable   x1x2  xn which be to be optimize  PRON be call this objective function as a blackbox function because PRON be  parameter consist of the input variable   x1x2  xn and variable from a simulation output   y1y2  ynthese simulation output variable   y1y2  yn take the input variable   x1x2  xn as input for PRON simulation  each variable in   y1y2  yn can be represent as a function of all  xi s   ie  yi  nx1x2  xn  where n represent a network be simulate   both  xi s and  yi s have variable bound   PRON optimization problem can be formulate as   objective   minimize  sumi0n  aixibixiyi  constraint    sumi0n xi  const   ximinltxiltximax   0ltyiltyimax   0ltxiyiltxiyimax   ai s and  bi s be constant   further information   each simulation may take 3  20 sec depend of the complexity of the network and number of input variable   xi  the objective function be nonlinear for sure but other  characteristic be unknown  uni  modal or multi  modal  smooth or  non  smooth  convex or non convex  noisy or non  noisy   one good news be that PRON have an initial feasible point  x0x1x2  xn  PRON can also treat the  xi s as integer  this also mean that the constant on the equality constraint   x1x2  xn  const can also be treat as an integer by round the decimal to the near integeri do not know if PRON help   whereas  ai s and  bi s be positive real number   PRON need   1choice of optimization algorithm   2a suitable open source solver in python that have strong computational power   so far what PRON have come across   with PRON search for a solution to PRON need  PRON have most often come across surrogate modelling  bayesian approach  derivative free method  evolution algorithms  solvers for surrogate modelling pysot  rbfopt from coin  or   solvers for bayesian approach bayesopt  hyperoptgpyopt  fmfn  solvers for derivative free blackbox optimization method yabox  most of the above method do not accept equality constraint which be PRON main concern   PRON question   PRON question be basically to satisfy PRON need   be there a suitable algorithm to solve PRON problem  if yes  suggest  some free python solver   also welcome   any new idea or suggestion to solve such a problem be also welcome  various suggestion on how PRON could tailor PRON problem to place PRON in a suitable class of problem and then apply a suitable solver be also welcome   PRON really appreciate PRON time  thank   
__label__svm __label__performance __label__multiclass-classification PRON be try to build a model for classify mnist dataset use svm  with raw feature PRON be get accuracy of around 94   use linear kernel    when PRON try PRON with pca  with different number of component  3550250500  PRON be get accuracy around 11    what can possibly the reason for this   firstly  please read up on what exactly be a pca algorithm and when can PRON be use and what purpose do PRON solve   just throw PRON on a problem and expect PRON to improve PRON result would work most of the time   pca reduce the feature in the dataset into dimension  and PRON be for the user to select the number of dimension PRON  PRON want to use in the model as feature   so  choose the right number of feature to use or eliminate from the model would dictate PRON accuracy   there be a few thing that PRON may be do  but worth mention   do PRON  whiten  PRON datum  mean center and normalize by the standard deviation  this step may be implementation  dependent   httpdeeplearningstanfordeduwikiindexphppca  do PRON check to see how much variablility be explain by the number of choose parameter  for image  80  be common   make sure PRON understand the implementation of the pca PRON be use  PRON always confuse PRON by the terminology different implementation use  eg score  rotation  etc  PRON suggest follow a tutorial and match PRON output with PRON implementation to get a deep understanding   httpsebastianraschkacomarticles2015pcain3stepshtml  PRON model be now build on transform datum  to predict make sure PRON be use these same transformation on the new input datum  
__label__data-mining __label__text-mining PRON be try to build a classifier which predict the class of a document base on the important word in the document  PRON first calculate the tf  idf weight for the document relative to the corpus  and then train a naive bayesian classifier with the high weighted word  determine by some arbitrary threshold   here be PRON code so far  write in golang but hopefully readable     trainclassifier take a document and   train the give classifier with the   term for the give class   func trainclassifierc  pb  classifier  word   string  class string    if  stringinsliceclass  c  class    c  class  appendc  class  class     if   ok   c  observationsclass    ok   c  observationsclass    amppb  observation   total  0   word  makemapstringint64        observation   c  observationsclass   for   word   range word   if   ok   observation  wordsword    ok   observation  wordsword   0    observation  total  observation  wordsword    c  trained    as a brief explanation  PRON be count occurrence of each word for each class which PRON can then use to do the bayes trick to get pcx1  x2  x3     PRON question be how can PRON use the tf  idf weight in this situation  that is  rather than use an arbitrary threshold to pick some of the word  PRON want to be able to use all of the word but somehow weight by PRON tf  idf weight   
__label__floating-point __label__computer-arithmetic PRON have a program that have a nest loop  together with PRON parent run at  on2 complexity perform float point arithmetic   PRON see that the performance of the code when compile with icc be comparable to that of g ffast  math   do this mean that icc be switch on an equivalent of the ffast  math implicitly   PRON can find out by ask icc and g to print out all the optimization switch that be actually enable  look at the verbosity option for each   to first order approximation  icc do use ffast  math by default   if PRON run icc help  PRON will show PRON PRON option  one of the section be float point  which begin like this   float point    fp  model  ltnamegt   enable  ltnamegt  float point model variation   noexcept  enable  disable float point semantic  fast12    enable more aggressive float point optimization  precise   allow value  safe optimization  source   enable intermediate in source precision  strict   enable fp  model precise fp  model except  disable  contraction and enable pragma stdc fenvaccess  double   round intermediate in 53bit  double  precision  extend   round intermediate in 64bit  extended  precision  PRON do not say PRON here  but in icc documentation here  PRON say that fast1 be the default   the other option similar to ffast  math be   noprec  sqrt  determine if certain square root optimization be enable  noprec  div  improve precision of fp divide  some speed impact   nofast  transcendental  generate a fast version of the transcendental function    noftz  enable  disable flush denormal result to zero  nofma  enable  disable the combining of float point multiplie and  add  subtract operation  eg  fuse multiply  add be on by default  and precise square root be off by default   but the correspondence between ffast  math and fp  model fast1 be probably not very precise  PRON could well be that some other specific option be include in fast  math  which can be see here  but exclude in fp  model fast1 or vice versa  PRON look to PRON like among other thing ffast  math would disable non  finite float  point arithmetic  inf  nan   which icc do not do by default  
__label__machine-learning __label__random-forest __label__ensemble-modeling as the title say   where do the random in random forests come from   for each tree PRON randomly select from the variable that PRON can use to split tree node  generally PRON randomly select 13 of the variable per tree  
__label__neural-network __label__deep-learning in classical neural net  PRON have that each layer be connect only with the follow layer   what if PRON relax this constraint and allow PRON to be connect to any or all subsequent layer   have this architecture be explore   PRON seem to be backprop would still work on this   at the very least  this type of network could be emulate by artificially create identity neuron at each layer that bring in the value of every early node  where the incoming weight to those layer be fix at 1   what PRON describe have be explore in deep residual neural networks   a residual block will combine two or more block from a standard architecture like a cnn with a skip connection that add the input to the first block to the output of the last block   the intuition be that deep network have a hard and hard time learn the identity function between layer  which have be prove to be useful especially in image recognition task  residual connection also mitigate the problem of vanish gradient   residual connection help solve the  degradation  problem  where deep architecture lead to reduced accuracy  for example googlenet win ilsvrc in 2014 with a 22layer cnn  but in 2015 microsoft resnet win with a 152layer res net  
__label__machine-learning __label__scikit-learn __label__pandas __label__k-means unsupdf have only one column that be review   PRON want to form 2  positive and negative  cluster of the review   from sklearncluster import kmeans  tfidfvectorizer  tfidfvectorizer    tfidfmatrix  tfidfvectorizerfittransformunsupdf   numcluster  2  km  kmeansncluster  numcluster   kmfittfidfmatrix   cluster  kmlabelstolist    the above piece of code be throw an error  valueerror  nsamples1 should be   nclusters2  on the line kmfittfidfmatrix   PRON unsupdf must be in the wrong shape  otherwise  PRON should work   here be how to fit k  means to single dimensional text datum in pandas   import panda as pd  from sklearncluster import kmeans  from sklearnfeatureextractiontext import tfidfvectorizer  df  pd  dataframecorpus     PRON be sam  sam  i  be     that sam  i  be  that sam  i  be  PRON do not like that sam  i  be     do PRON like green egg and ham      PRON do not like PRON  sam  i  be  PRON do not like green egg and ham      x  tfidfvectorizerfittransformdfcorpus   km  kmeansnclusters2fitx   kmlabelstolist    result in a list similar to this   0  0  1  1  
__label__prediction __label__feature-scaling PRON have a support vector machine in scikit  learn  python  that get train once in a while when enough new datum have accumulate  user help train the model by submit new datum    PRON store the model in pkl format for persistence  however  the svm need scale datum and PRON be wonder what would be a good way to persist the scaler  since PRON be fit on the train datum that change over time  as user submit more datum    be there a common solution for this  or be there a value that PRON can store with the model that work as some sort of seed to recreate the scaler   edit  add implementation    PRON have a database with a dataset  user periodically add data point to this dataset  PRON also have an estimator database which keep track of all the train model and PRON pkl file location  when enough new datum point be add the system will start to train a new estimator  PRON extract all the feature and scale the datum  then train a model  after that PRON compare the exist good model with the newly train one and if the new one be good  PRON set the new one to  active   there be only one active model a a time and that be the one that will be use for prediction on new datum   PRON hope this help   so after look some more PRON find this question  save scaler model in sklearn  which say that one can picle the scaler aswell  which work  
__label__machine-learning __label__neural-network __label__data-cleaning __label__forecast PRON have a real world datum set of credit borrower  50000 record   the set contain category such as married  single  divorced  etc  as well as continuous datum such as income  age  etc  some record be incomplete or contain outlier  the dependent variable be defaulted  good  01   PRON be try to train a neural network to predict default base on the training datum  PRON have experience with neural network and use PRON for sample datum with great result  however  PRON never have to normalize noisy real world datum   any thought what PRON should keep in mind in respect to    how to normalize the category  can PRON assign an indexed number  should PRON try to stratify PRON    how to deal with miss datum  assign 0    how to deal with the fact that default be only about 5  of the datum set  what transfer function would be useful to predict these low probability    basically any other real world data advice be very much appreciate   thank in advance   PRON bring up a number of good question here ans  PRON will do PRON good to cover each of PRON in turn   PRON be not an exhaustive treatment but hopefully PRON help   1  how to normalize the category   first  assess whether PRON categorical variable can be consider zero variance  eg all record possess one category only  or near zero variance  vast majority of record belong to very few category   create a basic frequency distribution to identify this   while PRON do not matter as much in neural context per se  PRON be a good idea to consider filter low variance variable from PRON model  just be careful as eliminate near zero variance variable may have PRON throw out the baby with the bath water   PRON have work with neural nets before so PRON know that PRON need to convert category to numeric value   a good question to ask be whether a give categorical value be ordinal in nature  eg on a likert scale of 1 to 5  and whether PRON want to preserve ordinality  this push PRON into an area such as that describe by pinto da costa and cardoso  httpswwwresearchgatenetpublication221112186classificationofordinaldatausingneuralnetwork    2  how to deal with miss datum   assume that PRON be talk about miss continuous value  PRON will want to impute these numeric value base collectively on the value that be present across the entire attribute   there be a number of approach to use here but something important to keep in mind be dispersion   in a basic sense  if PRON attribute be skew by outlier  PRON will want to steer clear of a mean  base calculation and go with a median  base approach   3  how to deal with the fact that  default  be 5  of the datum   this be not an immediate concern give that PRON have 2500 example of what denote a  default   in a 50000 dataset   a sane approach here would be to ensure that PRON be use a k  fold cross validation scheme  say with 10 fold  to ensure that PRON be truly randomize training vs test   this will help protect PRON against overfitt   again  this be pretty high level guidance  but PRON be prudent   extra credit   PRON have not get into other standard tactic such as normalization of PRON continuous attribute  but PRON may want to get up to speed there to better generalize PRON model  while also develop a deep understanding of the dynamic at play in the datum   this could point PRON to try out other algo  etc  
__label__linear-algebra __label__reference-request PRON be seek application in the industry for the moore  penrose generalize inverse  adagger of a matrix  a   the moore  penrose inverse of  ain mathbbcmtime n  denote  by  adagger  be the unique matrix  x satisfy the follow four  penrose equation   begineqnarray    i  axa  a   ii xax  x    iii  ax   ax  iv   xa    xa  endeqnarray   where  a denote the conjugate transpose of a metrix   associated with  adagger  PRON can define orthogonal projection operator  aadagger and  aadagger  any reference would be welcome   some us of the pseudoinverse be in httpwwwsiamorgsearchtype1amptermspseudoinverseampsearchx0ampsearchy0   but one can use the singular  value decomposition  svd  whenever one can use the pseudoinverse  and the svd be much more flexible to apply  and therefore much more used  moreover  the pseudoinverse be numerically unstable and must be regularize in practice  and this be usually do via the svd   the svd be ubiquitous in application  too numerous to even give a sensible partial list   the standard reference for learn about generalize inverse  not just moore  penrose generalize inverse  be generalized inverses  theory and application by ben  israel and greville and generalized inverses of linear transformations by campbell and meyer   one typically use a qr decomposition  such as the  xy representation of projector in  on the numerical analysis of oblique projectors   g w stewart  simax  2011  or an svd to calculate a pair of generalize inverse in practice  since the method suggest in the textbook above  normally some variant of lu decomposition  be  as professor neumaier point out  numerically unstable  in contrast  the qr and svd  base method be far more stable   although not typically identify as such  generalize inverse play a role in any method identify as  petrov  galerkin   or  galerkin    the orthogonality relation between the two relevant subspace  these end up be the range and nullspace of a projector  be induce by a bilinear form  the two orthogonal subspace determine a  nonunique  pair of generalize inverse  such method play a role in the numerical solution of partial differential equation  numerical sparse linear algebra  which PRON can see in the popular textbook iterative methods for sparse linear systems by yousef saad  and model  order  reduction method   ben  israel and greville discuss application of generalize inverse such as   integral solution of linear equation  linear programmm  least  square solution of inconsistent linear system  tikohonov regularization  difference equation  and other   another application of the pseudoinverse be in precondition saddle point problem  give a saddle point matrix    a  beginpmatrix  f  amp  bt  b  amp  0 endpmatrix  arise in incompressible flow  elman s  bfbt  preconditioner approximate the inverse of the schur complement    s  b f1  bt  with    sbfbt1    bbt1  b f bt  bbt1    this be essentially application of the moore  penrose pseudoinverse consider the identity  when  bbt be nonsingular     beginalign  bdagger  amp bt  bbt1    btdagger  amp  bbt1  b endalign  this preconditioner be subsequently improve by move the least square argument from the discrete to the continuous setting  result in new diagonal scaling term that significantly improve performance  see elman et al  2006   block preconditioner base on approximate commutator  
__label__optimization __label__nonlinear-programming __label__constrained-optimization what PRON be try to solve be the following  rayleigh  quotient  like minimization   begineqnarray   beginsplit    p0quadminx fracleft  ax  brighttop left  ax  brightxtop  x  st  quadquad ax  b geq 0  endsplit   endeqnarray   where  ain rmtime n   mgtn  be a full rank matrix   PRON be well know that the solution of the rayleigh  quotient  minimization be the eigen vector correpond to the minimum eigen  value  PRON seem that PRON can not be solve use the same approach as the rayleigh  quotient  minimization   thank very much   PRON think the comparison PRON be try to draw be to the rayleigh quotient minimization  beginalign   minx  ampfracleft  ax righttop left  ax rightxtop  x  endalign   to try to find the eigenvalue of  atopa  the chief difference PRON see between those problem be that if  x be an optimal solution to the rayleigh quotient minimization  then so be  cx for all scalar  c neq 0   which make sense   x would be an eigenvector of  atopa  so  cx be also an eigenvector for  c neq 0     there be no such scale  invariance for solution of the constrained optimization problem PRON have show  add constraint and affine term to the numerator of the objective destroy the property exploit by rayleigh quotient iteration  consequently  PRON would guess that PRON be correct  PRON probably can not solve this problem use the same method as rayleigh quotient iteration   the problem PRON have formulate look to PRON like PRON be a nonlinear  probably nonconvex  program  PRON may be worth reformulate PRON so that PRON look more like  beginalign   minx  f   ampf   textrmst    amp   ax  btopax  b   f xtopx  0    amp  ax  b geq 0   endalign   this sort of reformulation work great for linear fractional programming  but PRON do not know that PRON will help PRON much for PRON problem  other than to avoid issue occur if the objective function be evaluate at  x  0 the  f term be a new variable introduce to stand for the fractional objective function  the term  f xtopx be now a potential source of nonconvexity  along with the nonlinear equality constraint   let  fxdfracax  b2x2 PRON assume that  b be not in the image of  a  otherwise there be nothing to do   the first thing to do be to minimize  fx under the  free   constraint  ax  bgt0  dfx  hrightarrow dfrac2ax  btahx2ax  b2xthx4  dfx0  iff  for every  h    x2ax  btaax  b2xth0 that be equivalent to   x2ax  btaax  b2xt that imply   ax  btaxax  b2  and then   ax  btb0 remark that if  bgt0  or  blt0   then there be no solution   in a second time  PRON must add constraint in the form  gixeitax  b0 use lagrange method  one have the condition  dfx2sumilambdaidgix0   that is  x2ax  btaax  b2xtx4sumilambdaieita0 
__label__finite-difference __label__eigenvalues __label__eigensystem __label__helmholtz-equation PRON be try to simulate a 1d parallel plate waveguide  this be the eigenvalue problem     leftfracd2dx2   omega2mu0epsilon0righte  beta2e  as a matrix problem  if PRON 1d domain be divide into  n point  this become     lnomega2mu0epsilon0ie  beta2e where  ln be the  ntimes n discrete laplacian operator  scale by the spatial point space   delta x2  for a parallel plate waveguide  this problem have know analytical solution of cosine  if the center of the waveguide be at  x0   or sine  if one of the plate of the waveguide be at  x0     however  PRON only get these result when  and PRON have determine this experimentally   omega2mu0epsilon0  gt  2delta x2 so  when this criterion be meet  an eigenvector look like this   and when not meet  PRON look a lot like   the rapid oscillation  and the criterion under which PRON occur  lead PRON to think that this be almost certainly an issue of numerical stability  however  PRON have not be able to find any resource explain PRON  andor how to rectify PRON   for reference  PRON be use a system where length be in micron and  muo  epsilon0  c  1 PRON may be that this scaling be not self  consistent  but PRON be not sure how to go about correct that either   any help  or pointer to any resource  greatly appreciate   
__label__algorithms __label__software __label__interpolation __label__performance PRON have this data plot on a graph in which all point have the same value on the y  axis  eg a constant integer  c   while the x  axis be the time in second   so  for a c  25 on the y  axis  there be point on the graph correspond to value on the x  axis equal to  let PRON say   32  59  78  142  249  286  301  310  398  etc   PRON goal be to find a sequence  arithmetic progression  of the type   an  an1   t   where  where  an be the time in the x  axis and  t the ratio of the ap   PRON do not know the period  t nor the first element of the sequence  and therefore when the sequence begin  and there be white noise  probably most of the point that be in the datum be not part of the sequence    so  in short  PRON would like to discover a sequence with period  t in the middle of a bunch of  random  number  give these number   PRON be think first about a brute force algorithm but that would have a running time of  on2 but PRON believe there may be way to improve or even algorithm that may suit this kind of job  beforehand  PRON try fft  fast fourier transform   but PRON do not work on this case because the data be not uniform  any suggestion on algorithm or idea to tackle this problem be more than welcome   here be the representation of the datum on a graph  image  this be essentially a linear fit of the datum   anan−1t1  this be obvious once PRON realize that an ap form a straight line when plot in 2dimension  any linear regression tool  even excel  should suffice for this exercise    t be give by the slope of the fit  
__label__scraping PRON have just recently start create crawler to scrape datum  as PRON be create a script yesterday  a question pop in PRON head that PRON have still not be able to find online   be there a difference in use different web browserschrome vs firefox vs opera  etc   when scrap datum  or automate a task  for instance  automate click through a set of link that ultimately lead to download a file    again  PRON be fairly new to scraping and automation so PRON just want to get a sense of the good way to go about do such thing   not sure if this matter  but PRON be utilize python   any resource  reading material would also be greatly appreciate   if PRON be use a scrap tool that manage a web session for PRON  PRON should be able to get most thing do use a scraper  PRON should be able to log in to site and service and download the content that PRON be interested in   the big difference between scrap directly in python use beautifulsoup or something similar be that the basic python web scraper will not execute active content on a page  that is  if a page use javascript to load some dynamic content and then display PRON  this content will not be available to PRON from PRON scraper   this may not matter for the site that PRON be work with  but more and more site be rely on javascript to function  so PRON may find that PRON eventually need a scraper that can run javascript   if PRON do need to execute javascript from PRON scraper  PRON will want use full headless browser to do PRON scraping  some example be   phantomjs  slimerjs  headless chromium  in term of actual difference between browser  there should be minimal difference at this point unless PRON be deal with fairly cut edge feature  real time communication  live streaming  etc   
__label__machine-learning PRON be try to implement a temporal difference algorithm that learn the maximum revenue over a period of time use price as the action  inventory as the state  and revenue realize as the reward  the problem PRON be have be that PRON can not seem to get PRON to converge on an optimal policy  PRON seem like PRON get stick at a revenue somewhere around 60  of optimization and then will not budge anymore  be there some common pitfall that may be cause this  PRON have try play with the rate of exploration a little bit  but that have not seem to help PRON much   edit   ok  so PRON go back through everything  and PRON seem like the problem be that  when PRON stop explore  PRON just keep increase the q  value of state that PRON be already deem the good in the past  so for example  PRON visit the price 5 at a certain state and time and get a reward  then  in the next several episode  PRON continue to visit 5  and continue get a reward  add that reward to the q  value until PRON be pretty high  at that point  even if PRON be to explore at the same state and time  the reward PRON get be not enough to overcome the inflated q  value  so PRON just go right back to 5 in the next episode  here be the step PRON be try to follow   sound like an interesting application  to debug rl application  PRON like to perform rollout  first identify a state where the current policy appear to be clearly wrong   maybe sample some state and look at PRON individually   then switch off the learning and run a sample from that state for both the current rl policy and the action PRON think be correct  this should give PRON the true action value  hopefully  that will give PRON a hint as to what be go wrong  if PRON be an exploration problem  then the current policy will not be try any action similar to the action PRON think be correct  if the form of the value function be wrong  then PRON will not fit the value come from PRON sample  
__label__machine-learning __label__decision-trees __label__matlab PRON be use fitctree function from matlab to fit a cart tree to PRON dataset  10000 data point  20 feature    first  how can PRON exactly prune PRON and to what level should the tree be prune  PRON think PRON have to use the prune method but PRON be confuse by the  mergeleaf  and  prune  flag   second  what value of  minleafsize    minparentsize  and  maxnumsplit  be reasonable to try   
__label__neural-networks __label__machine-learning __label__deep-learning __label__research __label__programming-languages PRON have hear before from computer scientist and from researcher in the area of ai that that lisp be a good language for research and development in artificial intelligence  do this still apply  with the proliferation of neural network and deep learning  what be PRON reasoning for this  what language be current deep  learning system currently build in   first  PRON guess that PRON mean common lisp  which be a standard language specification  see PRON hyperspec    then  common lisp be great for symbolic ai  however  many recent machine learning library be cod in more mainstream language  for example tensorflow be cod in c  amp  python  deep learning library be mostly cod in c or python or c  and sometimes use opencl or cuda for gpu compute part    common lisp be great for symbolic artificial intelligence because   PRON have very good implementation  eg sbcl  which compile to machine code every expression give to the repl   PRON be homoiconic  so PRON be easy to deal with program as datum  in particular PRON be easy to generate  subprogram  that be use meta  programming technique   PRON have a read  eval  print loop to ease interactive programming  PRON provide a very powerful macro machinery  essentially  PRON define PRON own domain specific sublanguage for PRON problem   much more powerful than in other language like c  PRON mandate a garbage collector  even code can be garbage collect   PRON provide many container abstract datum type  and can easily handle symbol   PRON can code both high  level  dynamically type  and low  level  more or less startically type  code  thru appropriate annotation   however most machine learning  amp  neural network library be not cod in cl  notice that neither neural network nor deep learning be in the symbolic artificial intelligence field  see also this question   several symbolic ai system like eurisko or cyc have be develop in cl  actually  in some dsl build above cl    notice that the programming language may not be very important  in the artificial general intelligence research topic  some people work on the idea of a ai system which would generate all PRON own code  so be design PRON with a bootstrapping approach   then  the code which be generate by such a system can even be generate in low level programming language like c see jpitrat s blog  david nolen  contributor to clojure and clojurescript  creator of core logic a port of minikanren  in a talk call lisp as too powerful state that back in PRON day lisp be decade ahead of other programming language  there be number of reason why the language be not able to maintain PRON be name   this article highlight som key point why lisp be good for ai  easy to define a new language and manipulate complex information   full flexibility in define and manipulate program as well as datum   fast  as program be concise along with low level detail   good programming environment  debug  incremental compiler  editor    most of PRON friend into this field usually use matlab for artificial neural networks and machine learning  PRON hide the low level detail though  if PRON be only look for result and not how PRON get there  then matlab will be good  but if PRON want to learn even low level detailed stuff  then PRON will suggest PRON go through lisp at  least once   language may not be that important if PRON have the understanding of various ai algorithm and technique  PRON will suggest PRON to read  artificial intelligence  a modern approach  by stuard j russell and peter norvig   PRON be currently read this book  and PRON be a very good book   ai be a wide field that go beyond machine learning  deep learning  neural network  etc  in some of these field  the programming language do not matter at all  except for speed issue   so lisp would certainly not be a topic there   in search or ai planning  for instance  standard language like c and java be often the first choice  because PRON be fast  in particular c  and because many software project like planning system be open source  so use a standard language be important  or at least wise in case one appreciate feedback or extension   PRON be only aware of one single planner that be write in lisp  just to give some impression about the role of the choice of the programming language in this field of ai  PRON will give a list of some of the best  know and therefore most  important planner   fast  downward   description  the probably best  know classical planning system  url  httpwwwfastdownwardorg  language  c  part  preprocess  be in python  ff   description  together with fast  downward the classical planning system everyone know  url  httpsfaicsunisaarlanddehoffmannffhtml  language  c  vhpop   description  one of the best  know partial  order causal link  pocl  planning system  url  httpwwwtempasticorgvhpop  language  c  shop and shop2   description  the best  know htn  hierarchical  planning system  url  httpswwwcsumdeduprojectsshop  language  there be two version of shop and shop2  the original version have be write in lisp  new version  call jshop and jshop2  have be write in java  pyshop be a further shop variant write in python   panda   description  another well  know htn  and hybrid  planning system  url  httpwwwuniulmdeeninkiresearchsoftwarepandapandaplanningsystem  language  there be different version of the planner  panda1 and panda2 be write in java  panda3 be write in scala  these be just some of the best  know planning system that come to PRON mind  more recent one can be retrieve from the international planning competition  ipc  httpwwwicapsconferenceorgindexphpmaincompetition   which take place every two year  the compete planner  code be publish open source  for a few year   
__label__classification __label__text-mining __label__random-forest PRON be work on a text classification problem use random forest as classifier  and a bag  of  word approach   PRON be use the basic implementation of random forests  the one present in scikit   that create a binary condition on a single variable at each split  give this  be there a difference between use simple tf  term frequency  feature  where each word have an associated weight that represent the number of occurrence in the document  or tf  idf  term frequency  inverse document frequency   where the term frequency be also multiply by a value that represent the ratio between the total number of document and the number of document contain the word    in PRON opinion  there should not be any difference between these two approach  because the only difference be a scale factor on each feature  but since the split be do at the level of single feature this should not make a difference   be PRON right in PRON reasoning   decision tree  and hence random forests  be insensitive to monotone transformation of input feature   since multiply by the same factor be a monotone transformation  PRON would assume that for random forests there indeed be no difference   however  PRON eventually may consider use other classifier that do not have this property  so PRON may still make sense to use the entire tf  idf  
__label__random-forest PRON be relatively new to the random forest game  and PRON be hop for a little guidance in this problem PRON be have  PRON be try to predict future result of client of PRON company use the last 5 year of datum  the problem that PRON be run into  and maybe PRON be not even a problem  be that many client be not that old  right now  for new client  PRON have zero in all the variable up until PRON become active  PRON feel that these zero may be throw off the decision tree process as there be nothing differentiate these zero from active client actually have zero in these variable  however  random forest can not handle miss independent variable so PRON be not sure what the good way to go about this be   any help  guidance would be greatly appreciate   thank   
__label__computational-geometry __label__geometry __label__computational-physics PRON can calculate the electric potential over every point in a define space by solve laplace s equation  to do this in a computer program PRON set up an 2d array matrix and loop the internal point apply the follow formula     phix  y fracphixi1yjphixi1yjphixi  yj1phixij14  mean that the electric potential at  phix  y be just the average of the 4 neighbouring point  include in the array be area of constant potential ie the algorithm do not alter PRON  these be the boundary condition  the outer point of the grid matrix  set to 0  and the two plate  at a user define location  set to 1 and 1   now with the grid set up as PRON be currently  evenly space in both x and y direction  only a parallel plate capacitor can be model  if i try to put the plate at an incline PRON will just look like a stair case   PRON question be  how can i set this up in term of a computer program in order to calculate the electric potential over all point with one plate rotate at an arbitrary angle  theta   thank   what PRON be describe be a very specific way to solve the laplace equation  use the jacobi iteration to solve the five  point stencil when use finite difference  but there be much more general method that would be difficult to derive from PRON starting point  a more promising approach be to start with the poisson equation and ask how PRON can be approximate with finitely many point  one way would be to apply the finite element method use an unstructured grid that allow PRON not only to consider inclined plate but  in fact  any kind of geometry  this then give PRON a finite dimensional linear system that PRON can  if PRON want  invert use the jacobi iteration  but again  if PRON start with the linear system PRON will realize that there be many other method that allow PRON to solve PRON  direct solver such as gaussian elimination or lu decomposition  or if PRON want to exploit the fact that the system be sparse  solver like conjugate gradients or multigrid  
__label__sampling __label__geospatial let assume PRON have an image which only have white background and black point  all same size  PRON need to randomly sample crop with a hardcoded size  the condition be that all the crop need to contain at least  n point inside  also  ideally the crop can be rotate any number of degree  in the following sample image PRON have manually draw what PRON would be consider as valid crop  green  and non  valid crop  red   the condition in this example be that a valid crop must have at least 5 point inside   note that PRON have make the image with paint so the crop be not exactly same size  PRON should   an also there be only two rotation  there should be any type of rotation   PRON be look for an efficient algorithm for do this and PRON idea be to implement PRON in python  any suggestion   thank  
__label__matlab __label__sparse __label__convex-optimization __label__inverse-problem __label__semidefinite-programming in sdp base phase retrieval PRON have intensity measurement  absfft2x2  of the form  axo   ak  x2  b2  phase retrieval be then find x that obey axob  the quadratic measurement can be lift and interpret as linear about rank 1 matrix x  xx   ak  x2  b2  trax   with x  xx  and PRON want to recover the phase  x  signal to recover    in the standard approach name as phase lift PRON imagine lift of dimension x  xx  and PRON look to minimize rank of trx   subject to axb  x0  and PRON need roughly 4n measurement for n length x signal if PRON understand PRON correctly    x dimmension be nxn   this be fine for 1d problem  when PRON have 2d pahse retrieval go to 4d in x do not look good approach  what be the trick for phase recover in the case of 2d phase retrieval problem   just few reference link   PRON would like to ask how to construct phase retrieval  with matrix completion  of test picture 2d  let say phantom  phantommodified shepp  logan50  in matlab  with semidefinite program like phaselift   to retrieve a phase from diffraction pattern where only the square of amplitude be record one need to rely on phase retrieval with projection algorithm like saxton  gerchberg or solve the problem cast PRON into matrix completion problem  where the sdp come to minimize rank of high dimension matrix  which be consider np hard    httpwebstanfordedumahdisolphaseretrievalcdppdf phase lift method  example in 1d online   httpwebstanfordedumahdisolphaseretrievalcdphtml  via semidefinite method like phaselift  httpwebstanfordedumahdisolprcodehtml code  or httparxivorgpdf11116323v3pdf  httpusersisyliuseenrtohlssoncodehtml code  thank PRON   
__label__computational-physics term within in the integral how to simplify in case of 2d and axisymmetric case  formulation of cst  quad and axisymmetric problem  how the term within integral be take   how the potential energy be formulate for cst  quad and asisymmetric problem  PRON be much interested in formulation part  how the formulation of strain energy  body force and traction force be do  PRON mean how the potential energy pi be simplify  once after simplification of pi i can do the derivation part like obtain stiffness matrix and get work potential   for cst and axisymmetric problem how the traction force be take in to consideration   for axisymmetric problem how the strain energy be formulate   simplified pi for cst and axisymmetric problems be my interest  
__label__machine-learning __label__classification __label__data-mining __label__evaluation PRON be look for an example or tutorial of a system predict numeric value by the use of various classifier like svm  decision tree  ann or knn which optimise PRON choice of algorithm and parameter setting at runtime  assume there be unlimited input datum and the system can train PRON nonstop  the system should be able to adapt to different input datum and choose the most accurate prediction set as possible  example in python  c or java would be prefer  thank PRON in advance   PRON do not have much hand  on experience in this  but nevertheless PRON think this kind of processing of streamingdata be call  online learning   and find optimal hyperparameter for any type of learning  online or batch  automatically such that the system be self  tuning be an area of active research   prof  george hinton talk about PRON generally in this video  lecture 16c  bayesian optimization of neural network hyperparameter  in this playlist  httpswwwyoutubecomplaylistlistplipvv5tnogxkkwvkb1rkwkq2hm7zvphz0   13min   if PRON like audio  a listener ask a similar question in this  episode of the talking machines podcast    about 8 minute in   clever switching of the learn algorithm also have be explore  see  bag  or  boost  on wikipedia  or ensemble learn  for a relatively easy  to  use java  base gui tool PRON may try  moa massive online analysis 
__label__reference-request PRON be an engineer and PRON be plan to get a big toolbox than excel to solve difficult problem  PRON start learn python  as that seem the script language to go for math intense job  and run in the background of many application  and plan to take a deep look at sage and ascend  as those seem to be two good free package that do different job  ascend especially be write with process engineering in mind   right now PRON pursue this in PRON spare time   now  PRON look for problem that be not too far from PRON background   process  amp  environmental engnieering  possible task  that PRON think lend PRON selve to numerical solution    compressible flow through channel  flow through network of pipe  gas  heating   mass  flow through complex system with lot of interconnection and chemical  biological reaction  thermodynamic cycle  sizing heat exchanger  PRON think in most professional environment this would be do in aspen or aquasim  the company PRON be at do not  need to  work like that  note that PRON be not look at cfd or finite elements   most of these PRON could get from PRON work  but without a correct solution to benchmark PRON calculation  ideally  PRON would have a problem and a correct solution  where the problem need some engineering or physics understanding and the ability to translate this into some code  depend on the platform use to solve   or a project euler for engineer   where can PRON find work example with result to sharpen PRON tooth on   PRON be not sure what PRON have to say here qualifie as an answer  but what PRON want to say will not fit into a comment  either  so PRON have decide to take a chance   an autodidact such as PRON will know PRON better than PRON can infer from read one question  but in PRON view  cast about for computational problem to tackle seem bit like put the cart before the horse  PRON also seem like a quick way to make the study of computational method dry and tedious   PRON think a good approach be to search for specific application that interest PRON  at first  do not dwell much on the computational requirement  but once PRON find an area that strike PRON interest  dig a little deep and see if the extant computational method look interesting  if so  PRON may be worth reproduce an algorithm here or there  use existing tool as PRON reference   but even good  make a judgement about whether or not those exist tool have room for improvement  do PRON think there be a good way  perhaps an approach draw from another application that may map over well  maybe PRON read something in a paper  but the author seem better at write a journal article than publish useful code  or perhaps there be a commercial package whose functionality could be duplicate in open source  and PRON be just the guy to set the algorithm free   nothing will motivate learn new computational skill  master python or sage  learn the in an out of an open  source library  etc   master a new mathematical technique  like the need to use PRON to solve a particular problem that be eat at PRON  that be where PRON would put PRON stake in the ground and start work on code   sure  PRON may not learn to code as elegantly as someone who sit down and plod through a formal text and work out all of the exercise  PRON be possible that some of the code PRON come up with would make a seasoned expert scoff  but there be time to learn the formality and convention as PRON go  PRON will be able to tell when PRON be time to take a break from the creative exploration and sit down with a reference manual for a few hour  
__label__data __label__programming PRON be learn js  html and css  but PRON doubt js be very good at data analysis  so  what would PRON guy recommend PRON learn to start PRON  career  in data science  what be the good programming language for processing datum   ps PRON love statistic and programming so PRON think this will be fun   this be no doubt a duplicate  but here be how PRON would weigh in on the major language   r   fantastic support for package and specialise stat analysis  community  PRON can find a package to do just about anything PRON need  and PRON will be relatively easy to use   be good for throw together prototype and perform exploratory analysis   be free and open source   slower than python  basically do not loop over anything  PRON be an odd language for a programmer to use  come from a software dev  background   clearly design by mathematician   relatively little choice of good ide  python   fast   also very good as a general purpose language so have  broad  package support  free and open source   easy to use for big data application   not as streamline for analysis as r  syntax can be difficult to read  no surround brace to make PRON obvious where functions if statement end    can be particularly tedious work with dataframes compare to r  matlab   generally slow   have very impressive package for signal processing  image recognition and all the cool stuff   be very readable and easy to comprehend generally   be not free  student license be available  be quite complicated for PRON to get PRON hand on one though   have very good support for mathematical analysis similar to r  but much good matrix function   personal recommendation  python  kill two bird with one stone  learn good general to advanced programming concept and datum science at the same time   good article  httpswwwlinkedincompulservspythonmatlaboctavejuliawhowinnersivaprasadkatru  python be a great choice if u be from programming developer  this article will be good if u want more detail  httpwwwkdnuggetscom201505rvspythondatasciencehtml  also uve mention to recommend u to  start PRON career in datum science  PRON think kagglecom will be the good choice for the beginner  PRON will help u to know whether this field suit u or not   and for blog i think kdnuggetscomwill be a good choice   hope this answer be helpful  
__label__classification __label__unbalanced-classes PRON have a dataset that PRON want to classify as fraud  not fraud and PRON have many weak learner  PRON concern be that there be much more fraud than not fraud  so PRON weak learner perform good than average  but none perform good than 50  accuracy in the complete set   PRON question be whether PRON should set up testing and training set that be half fraud and half not fraud or if PRON should just use a representative sample   training set must represent the dataset PRON application  algorithm be actually go to face  PRON suggest PRON to take a representative sample instead of divide the training and test set with exactly half fraud an half non  fraud  but please make sure that the training set contain both positive and negative example for fraud for PRON classifier to perform good   in situation where a particular class be really a minority  PRON suggest use rare category detection  in this particular case of fraud  non  fraud  fraud be a rare category  PRON an active field of research  refere to rare category detection  be PRON possible that add generate datum to PRON data set will decrease the fraud  non fraud ration and make PRON dataset more representative  usable   at genielog  PRON be produce test datum for designing and testing fraud detection tool  PRON PRON generator gedis studio PRON can define regular profile and fraudster profile  instantiate each category to a customizable ratio  for ex  2  of customer will have fraudulent usage of generate event    PRON do PRON successfully for telecom cdr  httpwwwgedisstudiocomonlinecalldetailrecordscdrgeneratorhtml  and credit card usage  there be a freely available access to the online generator on httpwwwdatageneratorcom  PRON be pretty sure that even if the tool be not match PRON need at least the approach can be valuable  otherwise PRON would be interested to read any objection   regard  PRON think that PRON depend on PRON data set  there be many way to handle unbalanced data set  just search  for example httpswwwquoracominclassificationhowdoyouhandleanunbalancedtrainingset  PRON think that the simple way be to use the same distribution of class in the train and test set   if PRON have really small amount of minority class PRON can try one  class classification  
__label__tensorflow __label__object-recognition in the config file there be some parameter which PRON do not understand properly  PRON will mention PRON here   firststagefeaturesstride  be this the ration of input  output   heightstride  in the firststageanchorgenerator   what be this   
__label__finite-difference __label__convergence __label__error-estimation PRON have use an explicit finite difference scheme to model the 1d time dependent temperature distribution in a friction weld  PRON want to now verify the consistency and convergence of PRON algorithm   PRON have no exact solution and assume PRON must use an approximation produce at a very refined mesh space to calculate the relative error for the coarser mesh  PRON plan to calculate the  l2  and  linfty error norm at a number of time interval  during both heating and cool   PRON be under the impression that the  l2  norm will provide the good overall description of the error  whereas the  linfty will allow PRON to bound PRON error   so  be this be a logical and robust method by which to estimate PRON error and confirm the consistency of PRON difference scheme   both the  l2  and the  linfty norm may be viable measure of the error  and if PRON problem be well natured  PRON may even behave the same way as PRON mesh spacing tend to zero  but if the solution to PRON problem be not smooth  think of a step function  which can not be approximate by smooth function in  linfty   or PRON method have bad convergence in  linfty  PRON may converge with different order  if at all  if PRON observe that  PRON will have to look at the mathematic behind PRON method to see if PRON implementation be consistent   by the way  PRON can simplify PRON error estimation  if PRON compute solution on a sequence of mesh with space for instance  hk  2k  and the difference of two consecutive solution behaf like  hkalpha with positive  alpha  PRON can use the gemoetric series to deduce that PRON method be of order  halpha comparinng solution on consecutive mesh be usually easy  
__label__r __label__bayesian PRON have recently purchase and read the excellent book  introduction to empirical bayes by david robinson  as someone who be not a train statistician  but have an interest in data science  PRON find this book to be one of the most straightforward to follow  PRON make sense to PRON   however  PRON question be around how to extend the empirical bayes estimation output   david s example deal with how to find the good baseball batter  however  PRON always look at PRON total at bat and hit figure  what PRON would like to do be segment the estimation for more discreet scenario and then combine into a single final estimate   for example  if individual estimate be conduct for   total all  time batting record  batting record at today s ballpark  batting record against today s pitcher  batting record at this time of year  maybe some batter be better early or late season   etc  PRON very simple  non statistician  brain say to find the mean of all individual estimate  however  PRON gut say this be not the right way to go about PRON  unfortunately  PRON do not know what the right way be   PRON be reasonably competent with r  so any example use that language would also be most appreciate   
__label__social-network-analysis  PRON be conduct a network text analysis as part of a systematic literature review of social network analysis in computer  support learning  PRON node be term in the include study  and the tie represent how close these term be to each other in the text  window size of 10    this be PRON first time conduct any sort of network analysis and PRON be unsure about interpret PRON result   PRON run a weighted clique percolation  k5  on PRON datum in the hope of find subcommunitie of term that could give PRON a quantitative overview of the content of PRON pool of study  instead of find overlap subcommunitie  PRON find one large subcommunity subsume small one  k5   what can PRON make out of this datum then  be PRON a result still work note   
__label__optimization __label__convex-optimization __label__constrained-optimization PRON be go through an article lately and there be one point which be very confusing  so  PRON have the follow original constrain binary quadratic problem as the following  the pre  assumption of certain parameter be  qin znn   ain znn  and  bin zn1   textmin  xtqx    st  axleq b   textand  xin 01n let PRON call this optimization problem the problem  p  now  by the classical lagrangian reduction  this can be relaxed  as   textmin  xtqxlambdatax  b    st  xin 01n and  lambda  gt0 let PRON call this optimization problem the problem  llambda  since the relaxation serve as a low bind  PRON ideally plan to make PRON as maximum as possible  so  let  dlambda   xtqx  lambdatax  b  PRON have the other optimization    textmax  dlambda   st  lambda geq 0  let PRON call this optimization problem the problem  l  so far so good  now  here be the confusing point   the article mention that the problem  l can be rewrite as the follow    max  mu   st  mu leq xtqx  lambdatax  b     for any  xin 01n and  lambda geq 0  let PRON call the last optimization problem the problem  w  PRON do not quite understand why this be a rewrite  so  suppose  mu0 be the optimal solution of problem  w  then PRON follow that  mu0leq xtqxlambdatax  b for any  xin 01n and  lambda geq 0 while on the other hand  for the problem  l  what PRON understand PRON be like this  for each give  lambda  there be one optimization problem  llambda  and one optimal solution  which PRON take a minimum by the definition of  llambda   then  finally  among all the optimum solution  one maximum value be choose  somehow  PRON can not see that in the formulation of the optimization problem  w  or do PRON overthink  any comment would be greatly appreciate   PRON have not define  dlambda  and  therefore  problem  l  correctly  PRON should be     dlambda   minx in 01n  leftxtqxlambdatax  bright       now note that any problem of the form     beginalign   textmaximize   ampminzfz  textsubject to   amp  mathcalc    endalign      where  z be the  vector  optimization variable and  mathcalc be a set of constraint  can be equivalently write as     beginalign   textmaximize   ampt  textsubject to   amp  tleq fz  text  for all  z     amp  mathcalc  endalign      where the optimaziation variable be the vector  z and the scalar  t 
__label__machine-learning __label__clustering __label__scikit-learn __label__dbscan PRON want to perform dbscan on PRON datapoint  but PRON do not have access to the datum  PRON just have the pairwise distance of datapoint  additionally  PRON have no idea about the number of cluster but PRON do want that each cluster contain at least 40 datum point  do dbscan work with these condition  for instance  can PRON have something like this  or be more information need  PRON want to emphasize that PRON have compute the pairwise distance and this be not the result of euclidean or some other method   from sklearncluster import dbscan  db  dbscanminsamples40  metricprecomput    ydb  dbfitpredictmypairwisedistancematrix   PRON be not sure what be ep parameter of dbscan    how should PRON set that   dbscan do not guarantee a minimum cluster size  there be know situation  cf  wikipedia  where a cluster can have few than  minpt  point  furthermore  PRON have the concept of noise  point that do not have enough neighbor   for epsilon  also see the wikipedia article  as PRON do not specify the number of cluster  this parameter be what mostly control how many cluster PRON get  set PRON to 0  and everything will be noise  set PRON to the maximum distance  and everything will be in one cluster   really read the article  PRON be about density  not about cluster size  
__label__bigdata __label__apache-hadoop PRON be try to set up hadoop in the pseudodistribut form  so PRON edit the suggest xml file  in the file yarnsitexml  PRON have to insert    ltnamegtyarnnodemanageraux  servicesltnamegt    ltvaluegtmapreduceshuffleltvaluegt   what be the  mapreduceshuffle  option   the mapreduceshuffle in this config file be part of plugable shuffle and sort   shuffle and sort be what connect the mapper to the reducer  a nice graphical representation of this be the following  shuffle be call  copy  in this figure    the hadoop arcitecture alllow these step to be customize   the pluggable shuffle and pluggable sort capability allow replace the build in shuffle and sort logic with alternate implementation  example use case for this be  use a different application protocol other than http such as rdma for shuffle datum from the map nod to the reducer node  or replace the sort logic with custom algorithm that enable hash aggregation and limit  n query   the mapreduceshuffle parameter be mention in the hadoop docs  and be the default value for this configuration  
__label__python __label__keras __label__feature-engineering PRON want to know how to implement PRON handmade feature in PRON keras model   below PRON have some code where the embeddingmatrix be the stanford glove nlp dataset   def bidlstmmaxlen  maxfeature  embedsize  embeddingmatrix    inp  inputshapemaxlen     x  embeddingmaxfeature  embedsize  weightsembeddingmatrix    trainable  falseinp   x  bidirectionallstm300  returnsequenc  true  dropout025   recurrentdropout025x   x  attentionmaxlenx   x  dense256  activationrelux   x  dropout025x   x  dense6  activationsigmoidx   model  modelinput  inp  output  x   PRON want to implement some extra feature  preferably numpy array of feature  into PRON keras model   any suggestion   very interesting  question  PRON be  wonder if there  be nough skilled  data science  entusiast  that  could  answer this question  because PRON be  interested in simmilar stuff as well   congratulations  for this  question  man 
__label__r libraryggplot2   libraryreadr   librarystringr   libraryneuralnet   train  lt readdeliminputtraintsv    test  lt readdeliminputtesttsv    category  lt strsplitfixedtraincategoryname3   categoryt  lt strsplitfixedtestcategoryname3   traincategorya  lt category1   testcategorya  lt categorytest1   traincategoryb  lt category2   testcategoryb  lt categorytest2   traincategoryc  lt category3   testcategoryc  lt categorytest3   train  trainnamestrain   in ccategoryname      train  trainnamestrain   in citemdescription      train  trainnamestrain   in cname      test  testnamestest   in ccategoryname      test  testnamestest   in citemdescription      test  testnamestest   in cname      trainbrandname  lt asnumerictrainbrandname   traincategorya  lt asnumericfactortraincategorya    traincategoryb  lt asnumericfactortraincategoryb    traincategoryc  lt asnumericfactortraincategoryc    testbrandname  lt asnumerictestbrandname   testcategorya  lt asnumericfactortestcategorya    testcategoryb  lt asnumericfactortestcategoryb    testcategoryc  lt asnumericfactortestcategoryc    max  applytrain  2  max   min  applytrain  2  min   maxt  applyt  2  max   mint  applyt  2  min   scale  asdataframescaletrain  center  min  scale  max  min    scaledt  asdataframescalet  center  mint  scale  maxtest  mint    setseed2   result  neuralnetprice  itemconditionid  shipping  categorya  categoryb  scale  hide  3linearoutput  t    result  lt lmitemconditionid  price  shipping  categorya  categoryb  datum  train   pre  lt computeresult  scaledtest   solution  lt dataframetestid  testtestid  price  pre   writecsvsolution  file   samplesubmissioncsv   rownam  f   
__label__curve-fitting be there a general approach to find self  similar solution  ie collapse several function onto a single function by some transformation   PRON have datum from some experiment  and the function generate clearly appear to have some kind of self  similar solution  PRON be the result of vary a small number  4  of dimensionless parameter  and PRON would like to find how  or if  PRON can collapse these onto a single function   PRON have always rely on physical insight and find an appropriate nondimensionalization  if PRON data arise from a physical process where PRON know the equation or at least the geometry of the problem PRON may have some luck via the buckingham pi theorem follow by trial and error  see also this paper   price  james f  dimensional analysis of models and data sets  similarity solutions and scaling analysis    2006   pdf  four be a non  trivial number of parameter when the functionality be unknown  in general PRON will need some additional insight to determine the  expect  functionality of the datum and then PRON can perform some curve fitting    with four parameter PRON can fit an elephant  and with five PRON can make PRON wiggle PRON trunk    john von neumann 
__label__convex-optimization PRON work on a method base on this paper   gradient method for minimize composite function   but unfortunately  every time PRON code PRON do not work  even PRON investigate inequality of the paper numerically  but PRON could not figure out what be the problem  be there any matlab implementation of the paper available  PRON search the internet but PRON do not find any thing  in PRON work  PRON do not need to find lipschitz constant as PRON do in paper   
__label__python __label__wavelet PRON be try to calculate the three  term connection coefficient     λl  md1d2d3   ∫∞∞ φd1x  φd2lx  φd3mx  dx     for daubechie wavelet numerically use python  this report  p 20  give a nice overview of how to calculate PRON   PRON write a  well document  implementation in python and for the two  term connection coefficient have no problem but when PRON compare PRON 3term result with the one of resnikoff and wells  1998  PRON find lot of error  only the  l  m  coefficient appear to be correct    PRON be star at the code for week now  but PRON can not find any error  PRON would be very pleased if somebody would give PRON a hint where the error may be   
__label__numerical-analysis __label__fluid-dynamics __label__simulation __label__computational-physics __label__numerical-modelling PRON would like to incorporate a cfd topic in a project PRON currently have  the deadline be a little less than a month from now  basically  what PRON would like to do be solve some equation and simulate fluid behavior  the programming language PRON will use to accomplish this in be c  however PRON intend to rely on matlab for later visualization and good flexibility  after get PRON solution for the pde from PRON c code   PRON be currently a beginner in cfd and would like to boost PRON understanding by get PRON involved in such an interesting project  at least from PRON perspective   be there any interesting yet simple example out there to help PRON accomplish this  PRON so far have not one idea in mind  especially since most of the topic circulate the web be kind of  advanced  for someone in PRON situation as of yet  any reference  tutorial  idea or the like would be greatly appreciate    there be many simple case that PRON could look at  eg  2d  flow over a backward  face step   lid  drive cavity flow    inviscid  wave resistance of thin ship or travel pressure distribution   PRON should be easy to do in a month  there be also hide depth  eg 3d  turbulence effect  non  linearity  etc  to the problem that could give year of interest  fascination and frustration   PRON can go through the follow mooc   httpopenedxseasgwueducoursesgwmae62862014fallabout  go through the lesson and cod the example and exercise will help PRON understand and build the tool PRON require  with that PRON can try out more substantial problem   in PRON opinion  the simple way to start be with some 1d compressible flow  eg sod s shock tube problem  then maybe extend that to 2d for visual  incompressible flow be  tricky  than compressible flow  
__label__machine-learning __label__statistics parametric method make explicit assumption about the functional form of  f  and supervise learning be pretty the same right  PRON be a way to build a statistical model for predict  or estimate  an output base on one or more input   if PRON be the same  why would PRON have to create different name  be there a reason behind this   parametric method in simple term follow a particular distribution  the most common example would be that of normal distribution  where 64 percent of the datum be situate around  1 standard deviation from the mean  the essence of this distribution be the arrangement of value with respect to PRON mean  similarly  other method such poisson distribution etc have PRON own unique modeling technique   parametric estimation may have lay the foundation to some of the most vital part of machine learning but PRON be an absolute mistake to think supervised learning be the same thing   supervised learning may include approach to fit the aforementioned parametric model but this may not always be the case  more often  the datum scatter be quite spread out  PRON may not just be fit one parametric model but a hybrid of more than one  supervised learning also take into account the error which most parametric model do not consider unless incorporate manually  PRON could say that supervised learning be an evolved and more robust version of parametric method which be highly flexible  
__label__lapack PRON be tryng to learn to use a lapack subroutine but PRON get stick  PRON hope this be the right forum  in this fortran program PRON would like as a test to find the shur form of the matrix   0110   use cgee  but the matrix  m  after the call in which the shur form should be write  be make only of zero  other subrotune like dgemm work and PRON do not get error so PRON do not think be a problem with the linking   do PRON know where PRON do wrong     program example  implicit none  integer   d  sdim  info  realkind8allocatable   rwork     logical  external   sel  complexkind8   allocatable   mwvswork     logical  allocatable   bwork     d2    allocatemd  d    allocatewd    allocatevsd  d    allocatework2d    allocaterworkd    allocatebworkd      m110d00d0   m121d00d0   m211d00d0   m220d00d0     printmbeforem    printwbeforew  call cgeesvssel  d  m  d  sdim  w  vs  d  work1rwork  bwork  info   printmm  printww  printvsvs  printinfo  end program example  logical function selx   complexkind8   x  if  dimagxgt1d0  then  sel  true   else  sel  false   end if  return  end    read this answer real eigenvalue find  PRON understand that the problem be that cgee work with single precision arithmetic and not double as PRON think    this solve the problem    in general what PRON understand be that lapack subroutine be name so that   subroutine beggin with  d   like dgemm  dgetri  dgees    deal with real double precision number   subroutine beggin with  c   like cgemm  cgetri  cgees    deal with complex single precision number   subroutine beggin with  z   like zgemm  zgetri  zgees   deal with complex double precision number   thx a lot  
__label__pde __label__nonlinear-equations __label__time-integration __label__newton-method PRON be have difficulty understand how to apply newton iteration to nonlinear pde and then use a fully implicit scheme to time step  for example  PRON want to solve burgers equation    ut   u ux   uxx   0  so discretis time use a backward euler    ut   fracun1   unh  PRON find that  beginalign    ampfracun1   unh   un1   un1x    un1xx   0    ampun1   h  un1xx   h un1   un1x   un     ampi  h d2   un1   h un1  d un1   un     ampi  h d2   un1   nun1    un      1   endalign   where  n represent PRON nonlinear term  note that the nonlinear term be write implicitly   now  PRON want to apply newton iteration to this nonlinear ode  but here be where PRON get stuck   do PRON just apply newton iteration to the lhs of   1  ignore the  un term  ie  solve   i  h d2   un1   nun1    0   or be PRON suppose to include the  un term   just a reminder  PRON want to time step use a fully implicit scheme after use newton iteration  so PRON believe PRON just want to solve the lhs  0    what be PRON then suppose to do with the information from the initial guess and result of PRON newton iteration  how do PRON use this information in PRON time step   as PRON be sure be painfully obvious  PRON be quite confused as to how to approach this problem  if someone could give a detailed description of how to apply newton iteration and time step to nonlinear pde  not elliptic pde though   or could help PRON with the problem at hand  PRON would be very grateful  thank in advance   PRON be a bit easy to see if PRON write PRON equation in the a semi  discretis system of the form  uprimet   fut and with the application of the  thetamethod and approximate  uprimet  approx  wn1   wntau this give     wn1   wn    1theta  tau fwn   thetatau fwn1    0  with unknown vector  wn1 and time step  tau here  uprimet be the non discretis time partial derivative and  fut represent PRON discretised spatial derivative evaluate at  ut at time  t use the  thetamethod give PRON a bit of flexibility as PRON can change the method between fully implicit to fully explicit  and anywhere in between    this equation can be solve use newton iteration     nuk1   nuk    i  thetatau an1  left  nuk   wn    1theta  tau fwn   thetatau fwn1   right  where  k be the iteration index   kgeq0   and  an be the jacobian matrix of  fwn PRON use the symbol  nuk for iteration variable such that PRON be distinguish from solution of the equation at a real time point  un as PRON say the iteration need an initial value  PRON be perfectly valid to choose  nu0  w0  or for a good estimate PRON can precompute one iteration  nu0  w0  tau fw0 strictly speak this be the so  call modify newton iteration because the jacobian be not update during the iteration which be know to work well for stiff pde  
__label__constrained-optimization as PRON know  the multi  objective optimization method be develop so fast  ieepsilon constraint  PRON have a problem that PRON want to apply the most recent moop method for PRON   please tell PRON the recent method for solve multi  objective optimisationmoop    thank PRON   the one that PRON know be   the physical programming  pp  method be suggest by messac  1996    the normal boundary intersection  nbi  method be develop by das and dennis  1998  and das  1999    the normal constraint  nc  method by messac et al   2003  and messac and mattson  2004  represent a modification of the nbi approach  enhanced normalized normal constriant method be also devolve  2008   utyuzhnikov et al   2005  2009  suggest modification to the pp method to make PRON possibly simple and more flexible for practical application 
__label__pde __label__fourier-analysis PRON need to solve system of two couple partial differential equation numerically   beginalign   fracpartial x1partial t   amp c1nabla 2 x1  f1x1x2  fracpartial x2partial t   amp c2nabla 2 x2  kfracpartial x1partial t   endalign   the domain of system be a square region   boundary condition   beginalign   x  amp textconstant  implie fracpartial x1partial x   fracpartial x2partial x   0  y  amp textconstant  implie fracpartial x1partial y   fracpartial x2partial y   0  endalign   PRON try to solve this system with fourier transform  solution become unstable after few iteration  PRON have solve this system earlier with finite difference scheme and PRON work well so PRON know that constant of system be perfectly fine   PRON question be can fourier transform be use to solve these equation   PRON read somewhere that PRON because of neumann boundary condition one can not apply fourier transform   be this correct   if yes what be alternativei have read that cosine transform should be use but want to confirm    the fft can be use for periodic boundary condition  because von neumann boundary condition be effectively  mirror  boundary condition  PRON have to do a  mirror continuation   before PRON can apply an fft  one drawback of this approach be that PRON will blow up the datum volume by a factor 4  which be not important if PRON be just interested in experiment a bit   the use of the cosine transform implicitly do the  mirror continuation  and avoid the factor 4 overhead   note that depend on where the grid point near the boundary be locate  there be two different way to do a  discrete mirror continuation   hence PRON will find that library like fftw offer different variant of the cosine transform  correspond to these different  discrete mirrored continuation    
__label__python __label__scikit-learn __label__pandas __label__feature-engineering PRON be try to predict the winner of a tennis match from the player participate and PRON respective rank   label of PRON datum   winner  loser  winner rank  loser rank  the problem be that the column hold the player name in PRON datum be label  winner  and  loser   what be the good course of action to render this dataset usable for machine learning   could PRON just assign the  winner  column  vector as PRON target variable  and construct two new column   player1    player2   and populate PRON with random choice from  winner  and  loser    PRON think PRON need to formalize PRON problem a little bit   if PRON be to predict the winner of a tennis match  PRON would do something like this  PRON will be predict the probability of player 1 winning  or player 2  without loss of generality   then  this be a simple classification problem   then  for the feature  PRON will organize PRON in a way that use player 1 or player 2 to represent the feature   for example  if the variable be  player  current  ranking and face  to  face  win  rate  then the feature vector will be something like   player1currentrank  10  player12facetofacewinrate  07  player2currentrank  20  player21facetofacewinrate  03  PRON can never have target variable in PRON feature  otherwise PRON will not make any sense predict PRON  
__label__apache-spark do anyone know if there be functionality similar to stopwordsremover but intend to clean out html syntax  eg get the text without any html tag after transformation   write simple class  if someone will be interested   import orgapachesparkmltransformer   import orgapachesparkmlparamparam   import orgapachesparkmlparamparammap   import orgapachesparksqlcolumn   import orgapachesparksqldataset   import orgapachesparksqlrow   import orgapachesparksqlfunction   import orgapachesparksqltypesdatatyp   import orgapachesparksqltypesstructtype   import javautiluuid   public class htmlstripper extend transformer   private static final string htmlstripper   htmlstripper    private string inputcolumn   private string outputcolumn   public htmlstripper  string inputcolumn  string outputcolumn    thisinputcolumn  inputcolumn   thisoutputcolumn  outputcolumn     override  public string uid     return uuidfromstringhtmlstrippertostring       override  public structtype transformschemastructtype schema    return schemaaddoutputcolumn  datatypes  stringtype  true      override  public datasetltrowgt  transformdatasetltgt  dataset    datasetsqlcontextudfregisterhtmlstripper   string str  gt  strreplaceallltgtgt        datatypes  stringtype    column col  datasetcolinputcolumn    col  functionscalludfhtmlstripper  col    return datasetwithcolumnoutputcolumn  col      override  public transformer copyparammap extra    return new htmlstripperinputcolumn  outputcolumn       
__label__machine-learning __label__python __label__neural-network __label__classification __label__nlp be PRON possible for an algorithm to predict a new class that have never be before in training  for example  in PRON training datum PRON have   PRON need a flight to boston gt  flight query  what be two plus two gt  math query  who be the president of the united states gt  donald trump  how can PRON make a model that can predict a new class for   how be PRON today gt      should be something like conversation query   PRON be almost look for a way to combine supervised learning and unsupervised learning  use supervised learning for a model to predict new class  unsupervised learning   do PRON mean a class label that the algorithm have never see before  then no  PRON be not possible  if PRON do not have label datum for all PRON sample PRON can run a lda  to get some topic and then assign label base on the topic obtain  even this approach would not be really great  since PRON problem be classification  PRON would recommend restrict PRON domain and use only sample for which the label be know  for good classification accuracy PRON can look into cnn   identify new class sound like unsupervised learning to PRON  not supervise learning  supervised learning imply a fix set of label  and appropriate training datum  that PRON as a human provide  which be how PRON teach the machine   teach the machine to identify new class be a different matter altogether  PRON be essentially ask the machine to intelligently identify something PRON have no knowledge of  because no designated training datum   knowledge  exist in the model  this be  as far as PRON know  not possible  
__label__classification __label__time-series __label__pca PRON have a set of datum compose of time series  8 point  with about 40 dimension  so each time series be 8 by 40   the correspond ouput  the possible outcome for the category  be eitheir 0 or 1   what would be the good approach to design a classifier for time series with multiple dimension   PRON initial strategy be to extract feature from those time series  mean  std  maximum variation for each dimension  PRON obtain a dataset which PRON use to train a randomtreefor  be aware of the total naivety of this  and after obtain poor result  PRON be now look for a more improved model   PRON lead be the following   classify the series for each dimension  use knn algorithm and dwt   reduce the dimensionality with pca and use a final classifier along the multidimension category   be relatively new to ml  PRON do not know if PRON be totally wrong   PRON do agree with jan van der vegt  standardization  eg   1  1   or normalization n0  1  combine with the activation function can be very important with neural network  PRON would check the dissertation of pichaid varoonchotikul   flood forecasting use artificial neural network  for the in and out of ann  PRON have very interesting caveat  anyway  PRON be use to try first without  but when result be unsatisfactory  PRON be use to make trial with either both   not sure PRON will help but PRON would check the r package tsclust and related doc  the author be very kindly and PRON will help PRON to find specific model to do so  PRON be expert on time series analysis   good luck   if PRON be in python  there be a couple of package that can automatically extract hundred or thousand of feature from PRON timeserie  correlate PRON with PRON label  choose the most significant  and train model for PRON   httpsgithubcomblueyondertsfresh  httpsgithubcomrtavenartslearn  PRON be on the right track  look at calculate a few more feature  both in time and frequency domain  as long as number of sample   number of feature  PRON be not likely to overfit  be there any literature on a similar problem  if so  that always provide a great starting point   try a boost tree classifier  like xgboost or lightgbm  PRON tend to be easy to tune hyperparameter  and provide good result with default parameter  both random forest and boost tree classifier can return feature importance  so PRON can see which feature be relevant to the problem  PRON can also try remove feature to check for any covariance   most importantly though  if PRON result be unexpectedly poor  ensure PRON problem be properly define  manually check through PRON result to make sure there be not any bug in PRON pipeline   PRON can add more feature to PRON dataset as below   PRON can try nold package if PRON data be from a highly non linear process   max  min  mean  skew  kurtosis  and if possible some rolling stat   PRON be work on something similar  and PRON ask a relate question  
__label__machine-learning __label__classification PRON be try to predict tag for stackoverflow question and PRON be not able to decide which machine learning algorithm will be a correct approach for this   input  as a dataset PRON have mine stackoverflow question  PRON have tokeniz the datum set and remove stopword and punctuation from this datum   thing i have try   tf  idf  train naive bayes on the dataset and then give user define input to predict tag  but PRON not work correctly  linear svm  which ml algorithm PRON should use supervised or unsupervised   if possible please  suggest a correct ml approach from the scratch   ps  PRON have the list of all tag present on stackoverflow so  will this help in anyway   thank  one interesting algorithm that PRON have once test be call topmine  httpwebengrillinoiseduelkishk2  under code and datasets   PRON be able to extract bi  gram that could be use as key word and PRON can also assign PRON into topic   if PRON have the tag of each of the question that PRON mine  then supervise method make sense   PRON may use the tf  idf representation of a give question  feed PRON to an svm or neural net  and use that to predict 01 for each tag in PRON target set   if there be too many possible class  tag   PRON may be tricky to balance PRON datum   a simple approach may be to use the tf  idf vector to compute the k  nearest  neighbor of a question  then PRON can use the tag of the most similar document  by whatev distance metric do good  to predict the likelihood that question have each tag   if PRON do not have the tag for the question PRON mine  PRON should consider unsupervised method   lda  for example  could identify topic within the question and important word within those topic  this exact problem be a kaggle competition sponsor by facebook  the particular forum thread of interest for PRON be the one where many of the top competitor explain PRON methodology  this should provide PRON with more information than PRON be probably look for  httpswwwkagglecomcfacebookrecruitingiiikeywordextractionforumst6650shareyourapproach  in general  PRON appear that most people treat the problem as a supervised one  PRON primary feature be a tf  idf  or unweighted bow  representation of the text and PRON ensembl 1000 of single  tag model  owen  the winner of the competition  note that the title text be a more powerful feature than the content of the body of the post  
__label__linear-algebra __label__matlab __label__basis-set give an arbitrary set of  numerical  square complex matrix  mathcalaa1a2cdot  am  PRON be interested in compute the real matrix lie algebra generate by  mathcala  call PRON  mathcallmathcala that is  PRON would like a basis for     mathcallmathcala    mathbbspanrb  bincupk1inftymathcalck      where  mathcalck be define recursively as  mathcalc1mathcala  and  mathcalck1x  yx  yincupj1kmathcalcj for  kgeq 1  this calculation come up in  quantum  control theory   currently PRON be use a method find here which search only through repeat lie bracket  ie one of the form   aj1aj2aj3cdotsajn1ajncdots   and be guarantee to terminate  however PRON be interested to know if there be any other  fast  method  perhaps use p hall base  perhaps a recursive algorithm  PRON default language at the moment be matlab   this link describe how to do this use p hall base   on an only somewhat related note  if PRON be implement this PRON would worry about the numerical instability of test linear dependency  make sure to use a method for test independence of new matrix that allow for numerical inaccuracy  maybe compare the norm of  a  pa to the norm of  a  where  p be the projection onto the space of matrix PRON have find before  
__label__rootfinding PRON need an efficient way to numerically find the first  n positive root  lambdan of the transcendental equation    dfracj0  lambdan r  y1  lambdan   j1  lambdan  y0  lambdan rj0  lambdan r  y0  lambdan   j0  lambdan  y0  lambdan r    dfracdeltalambdan  where  delta be a constant   PRON have try newtzero and chebfun in matlab  PRON be both take longer than PRON would like  the root be need for the solution of a pde  transient heat transfer in one spatial dimension  with time vary boundary condition use eigenfunction expansion  if anyone know of a technique to determine the root even a little faster  PRON would be much appreciate  thank PRON   
__label__r __label__lu suppose PRON have the follow 3 by 3 matrix   plt3  xlt  matrix1p  p  p   pmb x be just a  p by  p matrix where every entry be  1p  now PRON want to solve the system    pmb apmb xpmb 1p  where  pmb 1p be a vector of 1 of length  p  PRON know  pmb 1p be the solution to such a system   but when PRON do  in r   solvex  rep1p    PRON get  error in solvedefaultx  rep1  p     lapack routine dgesv  system be exactly singular  u22   0  instead of  pmb 1p PRON question be why  PRON matrix do not have an inverse  which be why dgesv return an error  
__label__machine-learning __label__classification __label__nlp __label__text-mining __label__sentiment-analysis PRON be do sentiment analysis on twitter and PRON come to doc2vec to try another set of feature  compare to bow and other  to feed a classifier   however  PRON come to 0  in recall and precision for the negative class  and really low score for the neutral class   in fact  since PRON begin to work on this dataset  negative class have always be a problem  notably due to class imbalance with the positive class  PRON can see PRON in the support part of the two picture   however  when combine bow with custom feature PRON achieve quite high score compare to doc2vec   doc2vec  bow  custom  the class be organize from negative to positive   negative  1696 tweet  neutral  4339 tweet  positive  5657 tweet  the classifier use be in both case logistic regression   PRON question be   how to explain such low score for negative and neutral class for doc2vec model   not enough training datum  too strong imbalance    how to increase the recall on a disadvantaged class in a general way  notably for PRON bow approach    thank in advance   
__label__matlab __label__ode __label__nonlinear-equations how can PRON solve a system of nonlinear differential equation use matlab  PRON know PRON need to use the shooting method but how should PRON do PRON  PRON know PRON have to control the value of f  so that PRON satisfy the equation  a good first attempt for  f0 should be  0664   c  g23    p005     cfff0      c  pgfg0    fpartial derivative of f in respect to n  boundary condition    n0f  f0  and  g0   g06   stop condition    ngtinf   f1  and  g1   
__label__machine-learning __label__clustering __label__time-series PRON be ask here to get some advice  PRON goal be to detect similar pattern in an n  dimensional datum  for example  extreme simple  PRON have two ax  x  axis be time  in second  and y  axis be power  in watt   so PRON get time  series with a specific pattern  PRON goal be to select a subset of the time  series and calculate the similarity of other input datum to the select subset   in the end  PRON want to define multiple pattern and the output predict which pattern come most closely to the give input datum   be PRON suitable to use machine learning eq  pattern recognition for that problem or be PRON a pure overkill   PRON get the advice to use ml  also under the condition that PRON have an generic n  dimensional datum   if ml be suitable for this problem  can PRON give PRON some advice where PRON could start  PRON do not ask for a solution in any way  PRON just want to have some suggestion in which direction PRON should look  because ml be a very new topic for PRON and PRON do not have much experience in practice   thank PRON   so let PRON start   first of all please have a look at the edit PRON make to PRON original question  PRON be not just an edit but imply important conceptual thing so PRON need to start with PRON   graphs  graph be a reserved term in mathematic for network  like object  PRON better use the term plot for sake of clarification  so the question be absolutely not about graph   time  series datum  the example PRON make ie a temporal sequence of datum point be call time  series  a n  dimensional datum be not necessarily a time  series and however both seem to inhibit the same structure  PRON be conceptually different  but not necessarily irrelevant  type of datum and different set of algorithm and problem be define for either  so please comment here if the data be time  series or not   curve plot  the last but not the least be that a plot be use to just illustrate the datum and all data analysis algorithm and approach use that datum for inference  so PRON never infer anything from the plot PRON but from the datum that PRON depict  if PRON have any plot  like datum which PRON do not know the datum behind PRON need to use image processing which PRON be pretty sure be not the topic of this question but want to mention as PRON explicitly ask how to do the analysis with plot PRON   after these point PRON get back to PRON question  yes  what PRON be look for be pattern recognition and PRON can use many machine learning approach to solve PRON problem  if PRON already know what be different class of pattern  PRON would be call classification but now  as PRON understand  PRON do not know the class and PRON want to find similar pattern  find pattern of similarity in datum be call clustering   as PRON do not know if PRON data be a time  series  eg amount of rain vs time  or just a structured n  dimensional datum  eg amount of rain vs humidity  PRON try both   time  series  when PRON have different segment of time  series and PRON need PRON similarity PRON can use correlation analysis or dynamic time warping  if the time series be high frequency eg speech datum or eeg  then PRON better convert datum to frequency  or time  frequency  domain and then extract feature from those segment and then use those feature to determine similarity  please note that the term similarity be pretty wide and PRON be define accord to the nature of datum and problem  PRON may search for time  series clustering for more approach   non  temporal data  similarity between the parameter of a regression model fit to those subset of datum may help  another approach be to apply cluster algorithm to those segment  these algorithm can be apply to the original datum subset or some feature extract from those  dimensionality reduction algorithm like pca can be use for feature extraction and may be helpful for a good clustering   PRON write PRON answer in rush so PRON would appreciate any comment or further question   good luck  
__label__computer-arithmetic PRON recently encounter a case where PRON need an integer division operation on a chip that lack one  arm cortex  a8   while try to research why that must be  PRON find out that in general division take many more cycle than addition  subtraction or multiplication on pretty much any integer  or fix  point  architecture  why be this the case  be PRON not representable with a two  layer and  or logic like everything else   division be an iterative algorithm where the result from the quotient must be shift to the remainder use a euclidean measure  see 2  whereas  multiplication can be reduce to a  fix  series of bit manipulation trick   while all current cpu s appear to use an iterative approach as aterrel suggest  there have be some work do on non  iterative approach  variable precision floating point division and square root talk about a non  iterative implementation of float point division and square root in an fpga  use lookup table and taylor series expansion   PRON suspect that the same technique may make PRON possible to get these operation down to a single cycle  throughput  if not latency   but PRON be likely to need huge lookup table  and thus infeasibly large area of silicon real  estate to do PRON   why would PRON not be feasible   in design cpu s there be many trade  off to make  functionality  complexity  number of transistor   speed and power consumption be all interrelated and the decision make during design can make a huge impact on performance   a modern processor probably could have a main float point unit which dedicate enough transistor on the silicon to perform a float point division in a single cycle  but PRON would be unlikely to be an efficient use of those transistor   the float point multiply make this transition from iterative to non  iterative a decade ago  these day  single cycle multiply and even multiply  accumulate be commonplace  even in mobile processor   before PRON become an efficient use of transistor budget  multiply  like division  be often perform by an iterative method  back then  dedicated dsp processor may dedicate most of PRON silicon to a single fast multiply accumulate  mac  unit  a core2duo cpu have a float point multiply latency of 3  the value come out of the pipeline 3 cycle after PRON go in   but can have 3 multiplie in flight at once  result in a single  cycle throughput  meanwhile PRON be sse2 unit can pump out multiple fp multiplie in a single cycle   instead of dedicate huge area of silicon to a single  cycle divide unit  modern cpu s have multiple unit  each of which can perform operation in parallel  but be optimise for PRON own specific situation  in fact  once PRON take into account simd instruction such as sse or the cpu integrate graphic of the sandy bridge or later cpu s  there may be many such float  point divide unit on PRON cpu   if generic float point division be more important to modern cpu s then PRON may make sense to dedicate enough silicon area to make PRON single cycle  however most chip maker have obviously decide that PRON can make good use of that silicon by use those gate for other thing  thus one operation be slow  but overall  for typical usage scenario  the cpu be fast andor consume less power  
__label__machine-learning __label__recommender-system PRON be try to build a recommendation engine for an e  commerce site  by use the common recommendation approach  PRON be assume that each product PRON recommend have the same value  so all PRON need to do be optimize the conversion rate probably use a common recommendation algorithm  but when the product s price vary a lot  what PRON really need to optimize be the follow formula for each user   value of recomendation   probability to convert    product price   the big problem than choose the right algorithm and approach be choose the right metric  so PRON could compare the different algorithm  for example  if PRON would like to only optimize the conversion rate  PRON would use the precision and recall or false  positive metric   what metric and approach  algorithm be recommend in this case   thank  this be actually slightly similar to the problem that insurance company face except that PRON seem like PRON loss cost be know   insurer have some probability of loss and then  give loss  the magnitude of the loss follow some distribution   the cost to the insurer be dependent on both and PRON tend to be inversely relate  low loss be more likely than high one    in PRON case  the value be know so PRON do not need to predict PRON the way insurer need to predict loss so PRON could simply   model the probability  phat   multiply the predict probability by the know value  score  phat  value   recommend base on the result score  insurance company typically do the same thing in calculate premium except that PRON also need to model value   PRON sometimes model the two component jointly but typically PRON have separate model for frequency and severity and then just multiply PRON together to determine how much premium PRON should charge somebody  
__label__algorithms __label__parallel-computing __label__graph-theory __label__combinatorics PRON be try to generate all graph with n or few vertex that can be embed in some lattice  eg square  triangular  kagome  do there exist algorithm to both enumerate and draw these graph  what PRON be look for be a technique to   start with a listing of graph  probably in adjacency matrix format  of size k  from this list generate  draw all graph of size k1  eliminate those graph which be disconnected  isomorphic to another graph  keep go until PRON get graph with n vertex  which be either already parallel or reasonably easy to parallelize  PRON have see suggestion to use a canonical labelling scheme to cut down on the number of graph that need to be eliminate through isomorphism at the end  all the graph PRON would like to draw be planar and no disconnected graph be allow    edit  PRON be also wonder whether PRON would be good to use an adjacency matrix or list to store PRON graphs  since there will be many hundred of thousand of PRON  PRON would like a compact storage method   PRON turn out PRON original question be somewhat orthogonal to what PRON actually need to do  since start with a list  e  v  and map PRON to a particular lattice be  hard   PRON opt to instead start off with the lattice type assume  in this case  PRON specify graph use coordinate on the lattice  so a 4cycle on the square lattice would be   00    01    10    11   use this representation  PRON be easy to add new site to a graph  there be four possible position  check if a site be already there   this also prevent PRON from generate disconnected graph  PRON define two vertex as connect by an edge if PRON be lattice near  neighbour  since PRON have several different meaning for  distinct  graph  depend on the hamiltonian PRON be use  PRON could be distinct under the dihedral group  distinct under x  reflection but not y  reflection  different adjacency list  etc   PRON implement several function to check for different kind of isomorphism  depend on which hamiltonian PRON be use  PRON canonise distinct graph use a simple lexicographic ordering scheme  which allow PRON to eliminate a huge number of similar graph  to check if one graph be a subgraph of another  PRON check if PRON lattice coordinate can all be shift by a constant vector so that the result list s element can all be find in the lattice coordinate of the large graph  although PRON have not implement any parallelization yet  PRON seem likely that check each graph for subgraph be parallelizable  since a graph with n  1 vertex can only have graph of n vertex or below for subgraph  if all the graph be non  isomorphic   as be attempt to add site in all possible position for a graph and canonise the distinct graph  
__label__algorithms __label__graph-theory __label__combinatorics PRON be try to solve this problem use genetic algorithm and be have difficulty choose the fitness function  PRON problem be a little different than the original traveling salesman problem  since the population and maybe also the win unit do not necessarily contain all the city   so  PRON have 2 value for each unit  the amount of city PRON visit  the total time and the order PRON visit the city  PRON try 2  3 fitness function but PRON do not give good solution   PRON need an idea of a good fitness function which take into account the amount of city PRON visit and also the total time   edit  more accurate description  the objective of the ptsp be to visit the maximum number of waypoint of the map in the minimum number of time step  the map take the form of a two  dimensional board  where ten waypoint be scatter around and multiple obstacle be present  the follow image be an example of a map with obstacle and waypoint  thank   PRON have a multi  objective optimization problem  PRON want to maximize the number of city but also minimize the amount of time  this be  by PRON  not a well pose problem since PRON need to say which of two solution   n1t1 and   n2t2 be good if two path visit  ni city and take  ti time unit  what PRON need to do  then  be to first define a partial ordering of such tuple that describe which of the two PRON consider better  once PRON have that  come up with an objective function will become mostly obvious  
__label__convnet __label__feature-extraction this be a bit of a confusion PRON have and PRON never see any explanation of PRON  so PRON have to ask the question  whenever PRON see the discussion on how a convnns extract feature  PRON see something like this   and the discussion that follow be that feature be increasingly  hierarchically  more complicated and meaningful  as become obvious from the example here  then  PRON extract some layer from a convnn  crf as rnn    there be 64 feature map in conv11  256 in conv31 and 512 in conv51   so PRON first problem be that PRON can not relate the result PRON have to the example above  PRON fail to see how the feature become more complicated  all PRON see be that the image become increasingly less distinct  the final output  pixelwise prediction  be very good  so PRON must work  but PRON totally do not understand how  all PRON see be that visible feature be increasingly hard to distinguish as feature map size reduce from 1280x768 to 75x50   this be the original image   conv11  conv31  conv51  
__label__python __label__keras can someone please post a straightforward example of keras use a callback to save a model after every epoch  PRON can find example of save weight  but PRON want to be able to save a completely function model after every training epoch   set  saveweightsonly  to false in the keras callback  modelcheckpoint  will save the full model  this example take from the link above will save a full model every epoch  regardless of performance   kerascallbacksmodelcheckpointfilepath  monitorvalloss   verbose0  savebestonly  false  saveweightsonly  false  modeauto   period1   some more example be find here  include save only improve model and load the save model  
__label__data-leakage PRON have the impression the former be use in ml whereas the latter be use in econometric  PRON both carry the idea that information from the target be  leak  in explanatory variable   be there any difference between those two notion   
__label__python __label__neural-network __label__accuracy PRON have modify the blocks tutorial  to train an mlp neural net with a dataset provide for an assignment in PRON ml course   PRON would like to evaluate the accuracy of the network with vary parameterization  but PRON be not sure how to obtain the accuracy in first place   inspect the mainloop object useing dir    amp  var    PRON be not come across anything other than the testcostwithregularization   PRON be possible to record mean of different theano variable via the monitor class  so perhaps the answer lie there within   final output    training have be finish     training status   batchinterruptreceiv  false  epochinterruptreceiv  false  epochstart  false  epochsdone  1  iterationsdone  16  receivedfirstbatch  true  resumedfrom  none  trainingstart  true  log record from the iteration 16   testcostwithregularization  23032071347  trainingfinishrequested  true  trainingfinish  true  
__label__machine-learning __label__python __label__predictive-modeling __label__model-selection __label__ranking so PRON be have an xgboost model  xgbm  which for a set of feature  give PRON a prediction between  0  1   xgbmf1   fn    0  1   the model work fine and PRON measure PRON success with roc auc   now if PRON would want to check between different xgbm model  let say xgbm1 vs xgbm2 the result PRON would get from the roc auc would be sufficient   PRON problem be that for the same algorithm  xgbm PRON would like to check different datum set   PRON objective be to find the good constaintx that will generate trainx  testx  which PRON will verify with PRON model   which mean that with constaint1 PRON will get a train1test1 set for the xgbm and with constaint2 PRON will get a train2test2 set for the xgbm   but  alas  PRON can not compare between the two because the roc auc be not good to compare between 2 different enviorment  dataset  but between different algorithm on the same datum set   could anyone recommend a measurement that will help PRON measure which training  testing set  or which constaint  will produce the good result   a little about the problem   PRON a ranking problem that PRON address as a  binary  logistic   the target be 0 for fail and 1 for success  the ratio on the event  dataset  between 1 and 0s be 008   please comment anything PRON need for PRON to clarify the problem for PRON   PRON will elaborate  in PRON system PRON have user who generate datum  row   a lot of PRON generate garbage datum which be random at good  when build the training and testing set from the datum PRON provide PRON  PRON can filter some of PRON out  if the constraint be forgiving  PRON will have lot of garbage datum and the result of PRON model should be mediocre  if PRON constraint be too harsh PRON will lose a lot of datum but the result may be good  or mediocre as well   nevertheless PRON need to find a fair constraint that will yield a training set and test set so that PRON can evaluate the model performance on the test set and compare PRON to other model  same algorithm  generate by different data set  because of that constraint PRON try to find    
__label__machine-learning __label__neural-network __label__missing-data PRON have  million  of item each have n binary feature  when a feature be  0  PRON could be that the information be simply miss  so  give the datum with the currently observe 1 s  PRON would like to have a probability of the  0  feature be  1    PRON be think this can be a neural network with all feature as input and same as output  but then PRON do not know how the training would work  PRON do not have grind truth   PRON would like some help express PRON problem and hopefully not reinvent the wheel  be this be a classical problem in ml  and what approach can be apply   a simple approach could be the follow  suppose  i in 01d be the vector PRON want to predict which of the  0  entry could be  1  and  j in j the rest of the feature vector  take the  k near neighbor  under some suitable distance  jaccard  hamming  manhattan distance   for each  0  entry the probability could be the percentage of the  k near neighbor that have  1  in the correspond entry   this problem have be extensively study in the collaborative filtering community  the best known example be the netflix prize  this blog post provide a nice explanation of this approarch for binary datum   another  more involved  approach be matrix completion  in particular check this reference  if PRON be into deep learning check this  
__label__petsc PRON be look at the documentation for dmdavecgetarray and be surprised that PRON could create a plain ordinary c array whose index somehow range from  say  istart to istart  size  1  rather than from 0 to size  1  or a multidimensional array  also an ordinary c array  whose index range from istart to istart  isize  1  jstart to jstart  jsize  1  etc  PRON be not sure how that work  so PRON look into the code and find that dmdavecgetarray use the function vecgetarray1d  vecgetarray2d  etc  behind the scene to create these oddly indexed array  the code for vecgetarray1d look like this   petscerrorcode  vecgetarray1dvec x  petscint m  petscint mstart  petscscalar  a       petscint  n   vecgetlocalsizexampn    if  m   n  seterrq2petsccommself  petscerrargoutofrangelocal array size  d do not match 1d array dimension  dn  m    vecgetarrayx  a     a   mstart   return0      at least superficially  this look similar to the trick for do 1indexed array in c  that is   int realarray10    int  array   amprealarray1    except that the pointer a in the petsc code above be directly decrement instead of have a second pointer variable like array above   be PRON look at what PRON think PRON be look at  PRON find PRON hard to believe that a well  use library code like petsc would rely on undefined behavior  so PRON be not sure if that petsc code be really illegal   what be actually go on   heh  fun question   if PRON read the c standard carefully  PRON will find wording like  c99  6568  about pointer arithmetic   if both the pointer  operand and the result point to element of the same array object  or one past the last  element of the array object  the evaluation shall not produce an overflow  otherwise  the  behavior be undefined   as PRON understand PRON  the rationale for this restriction be segment memory model  for which pointer reside in different register and merely create a pointer to an invalid memory location  without dereferenc PRON  could crash or corrupt the program   PRON discuss the issue on petsc  dev a few year ago   the conclusion be that segment memory system be more  or  less dead and that the  trick  be too useful to give up   if PRON need to run on a system with segmented memory in which this pointer arithmetic will fail  PRON should use plain vecgetarray   and do the indexing PRON   PRON may use c99 vla  pointer to preserve structured array indexing  but those array will necessarily start at zero  so PRON would not get uniform indexing between the local and global vector  
__label__interpolation __label__local-coordinates PRON have a number of evenly  space sample of function that be polynomial in  1x where  x be a continuous variable on  xinlefta  bright  ie   fxisumn0n  fn xin PRON want to use some sort of local interpolation function like a hat function  linear interpolation   but PRON be pretty inaccurate  about 10  relative error in the  l2  norm  for value of  x near  a near  b  the interpolation be much good  PRON have not yet implement high  order local lagrange interpolation polynomial  PRON be wonder if anyone know of a local interpolation method better suit to this problem  for reference   n may be anywhere from about 5 up to 1000  and the number of sample reflect this  PRON be not completely wed to evenly  space sample  either  if a good interpolation method require a different spacing  PRON thinking be that perhaps concentrate sample point closer to  a and interpolate on a uniformly  spaced transform grid may work   
__label__parallel-computing __label__performance __label__efficiency PRON would like to calculate efficiency of parallel alghoritm  use the number of computation instead of time computation   in material from PRON study PRON have a formula like below     etan  p   fracomeganomeganhn  p    frac11hn  pomegan       where   etan  p  efficiency of parallel program which realize n  sized task on machine with p  number of cpu   omegan  number of computation for a n  sized task   hn  p  number of i  o operation for a n  sized parallel task  PRON have an application in two mode which realize 2mln of the same operation on   4 thread with 4 cpu   with division of operation between all thread   serial computation with 4 cpu   the formula look simple but PRON do not know how to apply PRON   PRON would like to ask about an explanation of mention formula in a real world example   this formula seem to assume that one operation count by  omegan take as long as one i  o operation count by  hn  p so to apply PRON  PRON have to be able to count operation in PRON code and scale PRON to the same time scale  the further elaboration divide the numerator and denominator by  omega probably scale this out effectively  but PRON still have to be able to count operation  depend on PRON cpu  co  processor  count operation can be easy or hard   PRON also need to be able to define for PRON what kind of operation be mean by PRON  o operation  ie  do this mean write to disk or do PRON include operation to ram  the network  etc  PRON presume this formula be derive by substitute  tn  p   trm computen  p   trm i  on  p   ldots into amdahl s law and turn the crank with  trm computen  pomegan  pdotomegan  p   ldots  etc  PRON presume also that this be part of an explanation that try to explain where part of the time that appear in amdahl s law can come from  in order to employ such a formula  PRON have get to know a lot about PRON computer and PRON code  PRON have to be able to count lot of different type of operation within PRON code  and PRON have to know the rate that PRON computer manage to actually do PRON at   PRON think this be much hard than just measure execution time at each  n and  p and be not worth approach  
__label__algorithms __label__fluid-dynamics __label__computational-physics __label__fluid PRON be look for the algorithm of patankar  for example  simple  simpler  simplec and piso  write in fortran for the simulation of heat transfer and fluid flow   probably the good thing be to use code from a book by jferziger and m peric  computational methods for fluid dynamics   available from springer ftp site  site   there PRON will find f77 code for 2d navier  stokes equation  PRON suggest start from 2dc folder and pcolf code which use collocate variable arrangement  instead of stagger which be explain in patankar s book   personally PRON think the patankar s book appear three year too early  and PRON therefore do not include finite volume method version use collocate variable arrangement enable after introduction of rhie  chow interpolation in 1983  also patankar give a sketch of what be vertex  base median  dual approach  also very important for unstructured mesh  and do not go into detail   PRON think there be a need for a new book on fvm use general unstructured cell from the start and not use square cell that reduce calculation to something similar to finite difference   PRON advice  learn from the code above  and start read paper as soon as possible  there be good paper explain fvm for fluid flow in a comprehensive and modern way   reference that PRON recommend be   s muzaferija and d gosman  finite  volume cfd procedure and adaptive error control strategy for grids of arbitrary topology  journal of computational physics138  pp766  787  1997   discretization procedure  least  square cell  center gradient  adaptive grid   idemirdzic  s muzaferija  numerical method for couple fluid flow  heat transfer and stress analysis use unstructured move mesh with cell of arbitrary topology  comput  methods appl  mech  engrg  125  pp235  255  1955   both of these paper be write by cd  adapco people   d kim  h choi  a second  order time  accurate finite volume method for unsteady incompressible flow on hybrid unstructured grids  jcp  162  pp411  428  2000   mdarwish  i sraj  f moukalled  a couple finite volume solver for the solution of incompressible flow on unstructured grid  jcp  228  pp180  201  2009        f  s lien  a pressure  base unstructured grid method for all  speed flow  int  j numer  meth  fluid  33  pp355  374  2000   sr mathur and jymurthy  a pressure  base method for unstructured mesh  numerical heat transfer  part b  fundamental  312pp195  215  1997   fluent people   b basara  employment of the second  moment turbulence closure on arbitrary unstructured grid  int  j numer  meth  fluid  44  pp  377  407  2004   avl  fireswift developer   hjasak  hgweller and adgosman  high resolution nvd differenc scheme for arbitrary unstructured mesh  int  j numer  meth  fluids  31  pp  431  449  1999   introduce gamma differenc scheme for convection term  openfoam developer   the list may extend to phd theses  in of community h jasak s phd thesis be often cite and be a good reading   if PRON still need help  PRON can refer to this github  httpsgithubcomxuaoxiqicomputationalfluiddynamic  PRON have lid drive cavity flow solve use simple and be quite explanatory  
__label__machine-learning __label__terminology __label__unsupervised-learning __label__comparison __label__intelligence-augmentation be this related to supervised and unsupervised machine learn   be PRON relate to ai assist human learning  and what be the distinction   also  why be assist machine learning see as an opportunity and unassisted machine learning see as a threat   assisted intelligence can be think of as a carpenter work with an intelligent hammer to build a good house  this would be a great opportunity   some people think that artificial intelligence may be like an intelligent hammer build the house on PRON own put the carpenter out of a job  this would be a big threat   expand  assisted intelligence  augmented intelligence  or intelligence augmentation all refer to the same concept a technology which enhance an autonomous system that have already prove to function  human   in the carpenter example  even a tape measure be an example of such a technology as PRON enhance the carpenter ability to measure distance more accurately and faster   there have be a long history of success on the front of intelligence augmentation  ia  and a few term have come up define the different technique such as  extended mind     distributed cognition  which PRON leave up to the reader to research   the opportunity and thus optimism behind ia come from this long history of success and many proponent of ia hop PRON will influence the trend of research to augment human to perform good than replace PRON entirely   artificial intelligence on the other hand attempt to build a human  like intelligence in the form of an autonomous technological system such as a computer or robot  many fundamental problem  practical and theoretical  have be encounter from job replacement  like the carpenter   existential crisis  if no human carpenter can build a good house than a robot  why bother    misalign goal  how do a robot know what the good house look like    and many many more which PRON leave to the reader to research   these problem create the threat see in ai and have be the source many academic debate as well as many great scifi work   although there do not seem to be standardization of the terminology involve assist learning  PRON have be notice this concept  and the distinction  pop up in article recently  surely drive by important milestone in unassisted machine learning   PRON understanding be that algorithmic intelligence have traditionally be base on complex decision  make algorithm that be train to some degree by the programmer    in essence  give a bit of a head start by utilize human knowledge in regard to the problem    by contrast  unassisted machine learning be an algorithmic intelligence that learn only by PRON own analysis of model and problem   no human knowledge go into the automaton  which develop intelligence entirely on PRON own   a passage from matthew lai s giraffe chess paper may provide some insight    this report present giraffe  a chess engine that use self  play to discover all PRON  domain  specific knowledge  with minimal hand  craft knowledge give by the programmer   unlike previous attempt use machine learn only to perform parametertun  on hand  craft evaluation function  giraffe ’s learning system also perform  automatic feature extraction and pattern recognition  the train evaluation function  perform comparably to the evaluation function of state  of  the  art chess engine  all of  which contain thousand of line of carefully hand  craft pattern recognizer  tune  over many year by both computer chess expert and human chess master    deepmind  for instance  be set automaton on video game and let the automaton learn with no human guidance whatsoever  and get very good result   as to why unassisted machine learning be worrisome relate to the issue of human  ai value alignment and the control problem  if PRON do not know what an an automata be think  or even how smart PRON have become  how can homo sapiens sapien be assure of positive outcome for PRON specie  
__label__python __label__text-mining __label__data-cleaning __label__regex PRON be try to substitute a sequence of word with some symbol from a long string appear in multiple document  as an example  suppose PRON want to remove   decision and analysis and comment  from a long string  let the string be   s  management s decision and analysis and comment be to be remove   PRON want to remove decision and analysis and comment from s the catch be  between decision  and  analysis  and  comment  in s there could be 0  1 or multiple space and newline character  n  show up with no pattern in different document  for example  one document show   management s decision  n n and analysisn and n comment be to be remove  while another have a different pattern  how do PRON account for this and still remove PRON from the string   PRON try the following  of course unsuccessfully   st   management s decision  n n and analysisn and  n comment be to be remove   resubrdecisionsnandsnanalysissnandsncommentss   to remove multiple white space match  PRON will need  sn  note the inclusion of the   match one or more    code   here be a function which will build the regex automatically from a text snippet   def removewordstoclean  word  flag  re  ignorecase    regex  rsnjoin      wordssplit         return resubregex     toclean  flag   test code   st   management s decision  n n and analysisn     and  n comment be to be remove   printremovewordsst   decision and analysis and comment     result   management s be to be remove 
__label__neural-networks __label__training this may sound silly to someone who have plenty of experience with neural network but PRON bother PRON   PRON mean randomise initial weight may give PRON good result that would be somewhat close to what train network should look like  but PRON may as well be exact opposite of what PRON should be  while 05 or some other average for the range of reasonable weight value would sound like a good default setting   why do initial weight for neuron be be randomise rather than 05 for all of PRON   the initial weight in a neural network be initialize randomly because the gradient base method commonly use to train neural network do not work well when all of the weight be initialize to the same value  while not all of the method to train neural network be gradient base  most of PRON be  and PRON have be show in several case that initialize the neural network to the same value make the network take much long to converge on an optimum solution  also  if PRON want to retrain PRON neural network because PRON get stick in a local minima  PRON will get stick in the same local minima  for the above reason  PRON do not set the initial weight to a constant value   reference   httpsstatsstackexchangecomquestions45087whydoesntbackpropagationworkwhenyouinitializetheweightsthesamevalue  PRON should not assign all to 05 because PRON would have the  break symmetry  issue   httpwwwdeeplearningbookorgcontentsoptimizationhtml  perhaps the only property know with complete certainty be that the initial  parameter need to  break symmetry   between different unit  if two hide  unit with the same activation function be connect to the same input  then  these unit must have different initial parameter   if PRON have the same initial  parameter  then a deterministic learning algorithm apply to a deterministic cost  and model will constantly update both of these unit in the same way  even if the  model or training algorithm be capable of use stochasticity to compute different  update for different unit  for example  if one train with dropout   PRON be usually  good to initialize each unit to compute a different function from all of the other  unit  this may help to make sure that no input pattern be lose in the null  space of forward propagation and no gradient pattern be lose in the null space  of back  propagation  
__label__optimization __label__algorithms in the travel salesman problem  tsp   one salesman have to travel to n city use the short possible route  PRON problem have several salesman  how to assign city to PRON to minimize the total travel distance   do such a problem have a canonical name and establish algorithm  heuristic   give that PRON be probably necessary to calculate the short route for each salesman give the city assign to PRON  PRON seem to require a loop over tsp problem  so PRON np  nastiness status can not look pretty    
__label__clustering __label__fuzzy-logic by now i think that fuzzy clustering can be apply to any kind of data  set but now i have hear that PRON can be apply only to specific datum  set that involve the concept of probability  be that true   first PRON would say the fuzzy clustering be not necessarily a clustering algorithm which use fuzzy logic   in machine learn terminology  a  soft  clustering algorithm be call fuzzy clustering as the intuition behind PRON be not  crisp   like hard cluster algorithm in which a data point belong only to one cluster and not other  ie for each datum point  PRON output a vector of  probability  correspond to the membership of that point to each cluster  please note that fuzzy membership be not necessarily probability   even in fcm algorithm  PRON be just do a soft version of k  mean and PRON have nothing to do with fuzzy logic  except of the very general idea ie not be crisp in make decision   so PRON can apply PRON to any dataset  
__label__nlp __label__word-embeddings PRON be look for any paper  article on how the word embed dimension affect the performance in nlp task  specifically  have anyone ever use low dimensionallt10  word embedding and achieve good result   
__label__machine-learning __label__data-analysis consider a set of 7d vector  each vector belong to one of four class   after mapping to 3d with pca and color each point accord PRON class the dataset look like as show below   for the animate gif version  click here httpsdocsgooglecomopenid0b261ryijroowrmsxzywul4m2m  what classification technique would be suitable for this dataset  eg decision tree  neural network  etc    while pca be useful to reduce dimension  the choice of the number of dimension should be view as a parameter depend on the effectiveness of PRON classifier on the datum generate by PRON problem    PRON could be three  two  four     but then PRON will need a classifier  and PRON may get good mileage from one of the various mathematical segmentation  cluster algorithm   two that be popular and fairly easy to implement be k  means clustering and fuzzy c  means clustering   there be lot of exist implementation for these in matlab  octave  or r  the follow link should get PRON start   cluster analysis  in general  k  means clustering tutorial  fuzzy c  means clustering tutorial  the difference between the two method  good answer on stack overflow  
__label__nlp __label__similarity __label__cosine-distance __label__similar-documents if PRON have a list of job posting store as raw text and PRON want to compare the similarity of all the job posting to a give resume  what technique or algorithm should PRON use   PRON be think of start with a model that convert the job posting into a vector space use tf  idf and then just calculate the matrix cosine similarity  can PRON improve on this   start with name entity recognition  PRON give an idea which part of posting be requirement and what part be description and then PRON could try just with bag of word  as the topic structure and vocabulary be standard and no weight be need for word importance  PRON do not find a reason for tf  idf to be use here   as of now  PRON can think of two way to formulate this problem   1  search problem  parse PRON job listing and index PRON in some sort of search engine like solr or elasticsearch  PRON can build capability like semantic search use word2vec model  etc   now write a query engine which take a resume and query this search engine  PRON will be blaze fast since job listing will be all index   2  similarity problem  PRON would have create hybrid similarity function  for example   a  how many top key word match between resume and job list  b  similarity of resume and job listing use doc2vec  pre compute vector for job listing   c  use algorithm like locality sensitive hashing to reduce the lookup space  this approach will be slow but may yield a good result  
__label__machine-learning be there a close form solution for a relu that be bound with a maximum value at 1   PRON be try to produce output value for pixel intensity 0  lt x  lt 1  but PRON output be produce value great than 1  how can PRON counteract this   be PRON a non  linearity  the very advantage of relus be PRON linear regime  that prevent PRON from saturate too early   have say that  a couple of alternative for PRON case be to either clip the output of the relus or to place sigmoid  or tanh  adjust the output range  after PRON  or instead of PRON    use a sigmoid or tanh  PRON think PRON be pointless to use a clip relu in PRON case also because  in addition to what ncasas say  for value less than zero or great than PRON clipping threshold  ie 1  PRON would not get a gradient  possibly make learn harder  
__label__linear-solver __label__sparse __label__spectral-method __label__dense-matrix unlike finite element  fem  or finite difference method  fdm   where the original pde be transform into a sparse linear system  spectral method return a dense linear system  for a large system  PRON be not practical to use the a direct elimination method  so a natural choice be iterative method  as far as PRON know  most be target to sparse matrix   so  how this problem treat in the scientific computing world  in the case of chebyshev polynomial  an fft may give some help  what about other polynomial  such as legendre or lobatto  be this difficulty the reason why this method be not so widely use as fem or fdm   even with spectral method  the matrix be still sparse  if PRON fix the polynomial degree  even if to a large number  and refine the mesh  the number of nonzero per row of the matrix stay constant  that be the definition of a sparse matrix  PRON be also independent of the kind of basis PRON use on each cell  legendre  gauss  lobatto  chebyshev  all only couple locally on a cell and consequently lead to a sparse matrix   what good to use be a separate question  first  the usual iterative solver that work for low  order polynomial degree still work  matrix  vector product just cost more because the matrix be less sparse  in return  PRON can efficiently implement spectral method matrix  free  ie  where the matrix be not actually form and only PRON action be implement when multiply with a vector   a big question be how good to precondition the linear system  PRON like the experience to comment on that in sufficient detail   there be a few important point to be make here   accuracy  spectral method exhibit  appropriately  spectral  super  algebraic  convergence  as a result  much small matrix size be require to produce the same accuracy as a fem or fdm  of course  the specific size and relative advantage depend on the specific scheme choose and stability and resolution desire  an excellent example of this be show below  sw armeld et al  jcam  2009   where the eigenspectra of the orr  sommerfeld operator be compute and accuracy compare between a standard fdm and various spectral discretization   indeed this be a principal example of how spectral method become ubiquitous in computation science  illustrate first by the hallmark paper orszag jfm  1971   matrix structure and solve  as PRON rightly point out  spectral method yield dense and typically unstructured difference matrix  the expense of compute the eigensystem or a linear solve of such a matrix be no doubt more expensive than a symmetric  sparse system from a fdm scheme  be PRON both the same size  the exact expense  again  depend on the specific scheme and solution method  method for solve both such matrix system be extensively study and well document in the literature   conditioning  this be a subtle but very important point  spectral method be typically horribly condition  specifically  polynomial method have condition number    kappa equiv fracsigmamaxsigmamin    mathcalon2p  where  p be the order of the high derivative and  n be the number of  collocation point  accurately evaluate solution to poorly condition matrix require care and sometimes extended precision  make such operation more costly  this also place a limit on the accuracy PRON be able to obtain for a give precision  conveniently PRON can see this behavior in the above figure  also    interestingly  the above expression for  kappa have yield method design to split operator such that PRON have two couple order  p2  equation  as oppose to a single order  p equation  there be other specialty method to finagle PRON problem to be less computationally horrible  though PRON will not document PRON all here  
__label__classification PRON be work on a credit risk binary classification problem  the class be goodpayers and badpayers   the training set have variable  feature that contain   demographics data  such as  age  education  loan amount  interest rate  behavioral data such as  payment in month1  payment in month2  payment in month3  payment delay in month1  payment delay in month2   the 10fold cross  validation have 082 auc on this set   however  the unseen datum just contain the  demographics data  and do not have  behavioral data  of payment   how do PRON deploy  test the model base on demographics dataset only   if less than 20  25  of behavioral datum be miss  maybe PRON could try to impute miss datum  use one of the following solution   impute miss behavioral datum use some business rule or by train a machine learn model with demographic datum as input and behavioral datum as output variable   impute miss datum with feature mean  median   impute miss datum by pick up random value in the feature distribution  hot  deck    in case PRON have more than 20  25  miss datum  PRON will be really hard to impute value  PRON think in this situation PRON should consider create a new model such as   the new model do not use behavioral datum anymore   the new model be base on a different train  val  test split in order to have behavioral datum in each dataset   if PRON can not create a new model neither impute miss datum  PRON guess hot  deck would be the good option PRON have to avoid bad performance on unseen datum  
__label__python __label__pandas __label__indexing PRON have a data frame with follow structure   dfcolumn  indexfirstpostdate    followerscount    friendscount     lastpostdateminretweet    retweetcount    screenname     tweetcount     tweetwithmaxretweet    tweet    uid     dtypeobject    inside the tweet series  each cell be another data frame contain all the tweet of an user   dftweets0column  indexcreatedat    PRON would    retweetcount    text    dtypeobject    PRON want to convert this data frame to a multi  index frame  essentially by break the cell contain tweet  one index will be the uid  and another will be the PRON would inside tweet   how can PRON do that   link to sample datum  one way to pull the embed dataframe up into the main dataframe and build a multi index be like   code   def expandtweetstweetsdf    tweet     for uid  subdf in tweetsdfsetindexuidtweetsiteritem     subdfuid    uid  tweetsappendsubdf   return pdconcattweetsmerge   tweetsdfdroptweet   axis1resetindex     howouter   onuidsetindexuid    PRON would     how   pull all of the tweet dataframe out of main dataframe use uid as an index  and concat   PRON together with PRON uid   then merge the main dataframe into the concatenated tweet dataframe   set the desire index   test code   import json  import panda as pd  with opentestjson   as f   df  pd  dataframejsonloadf    dftweets    dftweetsapplylambda x  pd  dataframex    printexpandtweetsdftexthead     result   uid  PRON would  1153859336  655060275025047552  article on PRON new haunted stevenage book paran   653912439940120576  big thank PRON to bobfmuk for interview PRON   643709869908996096  another interesting non  toadstool tweet today    547107275681579008  sisax67 thank  simon  all the good to PRON  ampa   546693940024733696  paul adams skysportsdart the wanderer from w   name  text  dtype  object 
__label__python __label__neural-network __label__scikit-learn PRON be use the mlpregressor in sklearn to train a network with approx 1000 input and a continuous output variable   essentially  the issue be one of image classification  1000 pixel  with the distribution of the pixel relate to a continuous variable output   PRON can generate an arbitrarily large training set  but run into memory problem run this on a single machine when loading array into numpy   PRON have select hyper parameter that give a reasonable result with a limited  5k  training set  but be try to work out the good way to train the network with two order of magnitude more datum   be there a way to pass mini  batch to sklearn PRON so that PRON can manage the memory and io   be training and retrain the network helpful if PRON can setup a means to ensure PRON read batch for each training step in a randomised fashion   PRON be guess this be a common problem  but PRON be struggle to find a sensible solution   
__label__quadratic-programming __label__vectorization PRON want to cast  min mathrmtrjxtx  c where  j all one s matrix  this be just sum of all element in  xtx  c  with matrix  x into standard quadratic program  the natural way be to use  mathrmvecx  but PRON can not find any reference on how to cast  mathrmvecxtx into quadratic form of  mathrmvecx  be there a similar result as the one below     mathrmvecaxbbtotime amathrmvecx  
__label__machine-learning __label__classification __label__parsing for experiment PRON would like to use the emoji embed in many tweets as a ground truth  training datum for simple quantitative senitment analysis  tweet be usually too unstructured for nlp to work well   anyway  there be 722 emoji in unicode 60  and probably another 250 will be add in unicode 70   be there a database  like eg sentiwordnet  that contain sentiment annotation for PRON    note that sentiwordnet do allow for ambiguous meaning  too  consider eg funny  which be not just positive   this taste funny  be probably not positive  same will hold for  for example  but PRON do not think this be hard for emoji than PRON be for regular word    also  if PRON have experience with use PRON for sentiment analysis  PRON would be interested to hear   total of 972 emoji be not really that big not to be able to label PRON manually  but PRON doubt that PRON will work as a good ground truth  source like twitter be full of irony  sarcasm and other tricky setting where emotional symbol  such as emoji or emoticon  mean something different from normal interpretation  for example  someone may write  xxx cheat PRON client  and now PRON be cheat PRON  ha ha ha   d   this be definitely negative comment  but author be glad to see xxx company in trouble and thus add positive emoticon  these case be not that frequent  but definitely not suitable for ground truth   much more common approach be to use emoticon as a seed for collect actual datum set  for example  in this paper author use emoticon and emotional hash tag to grab lexicon of word useful for further classification   PRON find this github repo useful  a good start   httpsgithubcomwooormemojiemotion  list of emoji rat for valence with an integer between minus five  negative  and plus five  positive    see list of support unicode  emojis   httpsgithubcomwooormemojiemotionblobmastersupportmd  note that some emoji receive arguably confusing polarity  such as  stuckouttongueclosedeye  0   due to be use for both positive  and negative emotion  
__label__machine-learning __label__gradient-descent PRON be not sure which word to use to differentiate a self  organize map  som  training procedure in which update for the entire datum set be aggregate before PRON be apply to the network from a training procedure in which the network be update with each datum point individually   in the case of other algorithm PRON would say stochastic gradient descent  but PRON be not sure gradient descent be correct for som learn   to PRON knowledge  som learning do not follow any energy function exactly  so that should mean PRON do not do gradient descent exactly  right   another term would be online learning  but  online  sort of imply PRON train PRON som in the real world with datum point streaming in  eg  from a set of sensor   PRON may also imply that PRON use every datum point only once  which be not what PRON want to say   som  self  organize map  som  be one specific type of neural network  in contrast to multilayer perceptron  mlp  PRON be use much more often  the som neuron have a position on a regular grid   training of som  som be usually train in a stochastic way  source   1    this mean one training example be use at a time  after every single training example the network learn something   in contrast  batch learning mean the network be present the complete batch of all training  example  or a big part of usually 128 example  then PRON call PRON mini  batch   the parameter be only update after all of the example be present to the network   in contrast to mlp  PRON do not apply gradient descent  as far as PRON know  PRON have never use PRON  but only read about som   PRON would not even know to which function PRON would apply the gradient descent algorithm  because there be no error function in som   at least no standard error function    for mlp  PRON know that the advantage of batch training compare to stochastic training be that the weight be not jump as much  so PRON rather go in the right direction  however  the advantage of stochastic training be that PRON make update much more often  PRON want to get the good compromise  so PRON make mini  batch training   online learning   online  have quite a lot of different meaning    online  as in  in the internet     online  as in  online algorithm  which get input piece  by  piece and have to make a decision before know all of the input  eg when PRON have severe time constraint or when PRON can not store all the datum like in cern where PRON have to throw most of the datum away right after PRON get PRON    online  in handwriting recognition in contrast to the offline ocr method  see on  line recognition of handwritten mathematical symbols if PRON be interested   online method use the way how symbol be write  offline method only use the image   accord to wikipedia   online machine learning be use when datum become available in a sequential order to determine a mapping from datum set correspond label  the difference between online learning and batch learning  or  offline  learning  technique  be that in online learn the mapping be update after the arrival of every new data point in a scale fashion  whereas batch technique be use when one have access to the entire training datum set at once   source   1  andries p engelbrecht  computational intelligence  an introduction  page 62  
__label__machine-learning __label__statistics PRON feel like a lot of the resource on ml have be focus on algorithm  model whereas in practice  industry application  what be arguably the hard part  imo  be what be before and after modeling  collect unbiased training datum  error diagnosis  conduct appropriate a  b testing  explore  exploit  etc  what be some of the resource to learn these practical issue in ml  ds  be experimental design the right keyword   
__label__machine-learning __label__scikit-learn __label__tensorflow __label__keras be there any reason why the validation mean squared error output from keras be always very similar to 1  thank PRON  all of PRON training result look like   155155                                  0s  loss  60626136  meanabsoluteerror  08344  meansquarederror  10271  valloss  08252  valmeanabsoluteerror  08252  valmeansquarederror  10164  epoch 291000  155155                                  0s  loss  58705280  meanabsoluteerror  08324  meansquarederror  10211  valloss  08246  valmeanabsoluteerror  08246  valmeansquarederror  10130  epoch 301000  155155                                  0s  loss  56685083  meanabsoluteerror  08311  meansquarederror  10134  valloss  08244  valmeanabsoluteerror  08244  valmeansquarederror  10106  epoch 311000  155155                                  0s  loss  55308119  meanabsoluteerror  08288  meansquarederror  10115  valloss  08243  valmeanabsoluteerror  08243  valmeansquarederror  10089  epoch 321000  155155                                  0s  loss  52226773  meanabsoluteerror  08283  meansquarederror  10119  valloss  08245  valmeanabsoluteerror  08245  valmeansquarederror  10071  epoch 331000  155155                                  0s  loss  50900273  meanabsoluteerror  08273  meansquarederror  10078  valloss  08247  valmeanabsoluteerror  08247  valmeansquarederror  10060  epoch 341000  155155                                  0s  loss  48782420  meanabsoluteerror  08272  meansquarederror  10093  valloss  08245  valmeanabsoluteerror  08245  valmeansquarederror  10046  note  PRON have standardize PRON input and output with sklearn standardization   from sklearn import preprocess  xscaler  preprocess  standardscalerfitxlisttotal   xlisttotalstandardiz  xscalertransformxlisttotal   yscaler  preprocess  standardscalerfitylisttotal   ylisttotalstandardiz  yscalertransformylisttotal   do PRON just mean that there be nothing to learn from the datum at all   PRON could be very close to 1  or for that matter  change very slowly  for many reason   maybe PRON learning rate be very small   maybe the network have reach PRON learning capacity  since PRON do not know anything about the network architecture  PRON can not rule this out   this do not mean there be nothing to learn from the datum  in the bad case  there be nothing to learn from datum  with the give architecture   hyperparameter and the time PRON be willing to let the network learn   what be PRON output  be PRON a 0  1  if so  PRON should not be use rmse and should be use cross  entropy as PRON loss function  
__label__regression __label__tensorflow __label__multiclass-classification __label__computer-vision PRON be work on a keras neural net that do key point prediction of body part  left foot  leave knee  leave hip  etc    for each image  x   the target  y  be a list of coordinate for the keypoint  leftfootx  leftfooty  leftkneex  leftkneey   etc    whenever a keypoint be not visible the x and y coordinate equal  0   PRON suspect that leave the datum in this state will lead to bad result because 0 really mean na  ie the keypoint be not visible   leave the x and y coordinate 0 would suggest the keypoint be actually in the corner which be false  what be the right way to structure datum to do multi target regression when the correct output be sometimes na   one option be to modify PRON network to output a binary label for each body part  signal whether the part be visible or not   PRON would need the follow change   modify PRON network architecture to return a real number in the range   0  1 for each body part  a sigmoid would be an appropriate activation for the last layer  PRON semantic would be the probability that the  ith body part be visible   extend PRON training and test datum with appropriate value for the new label   change PRON loss to only take into account the difference between body part position  expect vs predict  if the part be expect to be find  eg by multiply   PRON be not very sure whether PRON should use the real label or the predict one here  or even an average of both    add a new term  add to the original loss  to PRON loss with the cross entropy of the body part presence flag  PRON should probably add a weighting factor  alpha to this new loss term to have a proper scale with respect to the position distance loss term  
__label__correlation __label__association-rules __label__market-basket-analysis association rule learning have a fair bit of material base around the correlation of product purchase on the same order  at the same time  however PRON would like to discover if there be a method for identify such a relationship between product that be order near each other  but not together   say for example a customer purchase a pencil in week one  but later purchase an eraser in week two  then a year later PRON do the same thing  but then also apply this across all customer who show similar behaviour  PRON would then aim to combine this with the correlation coefficient of product purchase in an order together   PRON consider give each of the date before or after a purchase a diminish weight  1n day  and then compare the aggregate date value of the customer purchase date history with a product a against product b but this seem like a fairly draw out method   just look for a process to achieve this  but python hint would also help   thank in advance   association rule mining be nearly always do with the apriori or eclat algorithm  there be a few paper which have utilize these in the time series context  this paper detail a thorough implementation  though this be do with relation to identify gene pattern  rather than customer purchase  the idea be still the same though  only there be slightly less complexity in PRON case  unfortunately  this do not give a detailed python implementation   depend on PRON situation  this be unlikely to be applicable from PRON description  but PRON list this just in case    PRON may be useful to break from association rule and cluster PRON datum instead  provide PRON have enough of PRON to treat PRON as continuous  a python implementation of this be feature in this article   good luck  
__label__fluid-dynamics __label__statistics __label__simulation assume that a direct numerical simulation be perform  what be a good method for estimate the impact of small scale on large scale in fluid dynamic  for example be PRON pertinent to compare two run with different grid size or two run with different viscosity  be there some relevant statistical tool for this   the large scale field can be define as a coarse  grain field  beginequation   overlineqltmathbfxint glmathbfy   qtmathbfymathbfxdmathbfy   endequation   where  gl be a normalized convolution kernel of scale  l for example the forme of  gl can be  glyl3sqrt2 pi  expy  l22  the small scale field be define as  beginequation   ql  qoverlineql  endequation   if at some scale  l PRON can remove the small scale of the dynamic  the impact of the small scale on the large scale  will be the difference between the field of the full dynamical system with the field of the truncate dynamical system   PRON be imperative to compare many run with refined mesh until PRON detect convergence  a single solution with no mesh refinement study should not give PRON much confidence in PRON result   comparing run with different fluid property tell PRON something different  if PRON think that a set of run with different viscosity be relevant to PRON ultimate science  engineering question  then PRON should also perform such a study  PRON should  of course  make sure that each point in this study be mesh  refine as well   there be many reason that a coarser simulation would give different result that a finer grain simulation   a few example   be boundary layer be resolve differently   be PRON resolve new feature  vortex  blockage to flow   think about a grid result as purely a convolution of a fine  grid result with a gaussian will work very well in viscous dominate flow  where there be already a large  scale smoothness impose   but may be remarkably wrong where that assumption break down  high reynolds number   if PRON can pose a situation with a know symbolic solution  simulate that at several different scale   if PRON algorithm  implementation be good  there should be a  roughly linear  convergence in logerror  vs loggrid size   the slope of which be the  order  of PRON accuracy   there be some example of this in PRON thesis  and PRON suggest a bunch more reading if PRON be so inclined   PRON be essentially describe a large eddy simulation  les   the key word in les be  large   PRON must resolve the  large  eddy for les to deliver meaningful result   large  in this context refer to the energy  contain scale so PRON must resolve into the inertial subrange for les to be valid  for many industrial flow  the reynolds number be much too high to resolve into the inertial subrange  in which case PRON must resort to turbulence modeling  ran  example include  kepsilon   komega  reynolds stress model  etc   these rans model explicitly model the effect of the unresolved scale on the large scale   under  resolve model that do not explicitly model these subgrid scale deliver vastly inferior result  although some people do this anyway  mostly to avoid the complexity of an actual turbulence model  the result have almost no predictive value  PRON will rarely see this degree of laziness in field with rigorous validation standard  
__label__neural-network __label__convnet __label__keras PRON be use a cnn to classify medical image  PRON be use a four convolutional layer with relu activation follow by a softmax layer  PRON be use rmsprop as the optimizer  the problem PRON be face be the network s test accuracy increase and then go down  this continue till the maximum number of epoch be reach  PRON read somewhere that this could be due to the fact that the network shift PRON weight in favor of one class  and then have to do so in favor of the other class  PRON shuffle the test datum  to see if this would help  but no such luck  be there any remedy   base on the image PRON be share  the training accuracy continue to increase  the validation accuracy be change around the 50   PRON think either PRON do not have enough datum to use neural network or the network be small to capture all the information  in both case PRON feel there either under fit or over fitting problem  can PRON give PRON some information about the database so PRON can help more 
__label__machine-learning __label__python __label__deep-learning __label__keras __label__tensorflow PRON have 2000  training imagesof custom farm structure  and PRON want to define PRON own model use tensorflow  PRON research  httpsgithubcomtensorflowmodelsblobmasterresearchobjectdetectiong3docdefiningyourownmodelmd   this be a very high  level documentation  can anyone help PRON on how to go about do this   
__label__cars this slideshow document some of the technology use in google s self  drive car   PRON mention radar   why do google use radar  do not lidar do everything radar can do  in particular  be there technical advantage with radar regard object detection and tracking   to clarify the relationship with ai  how do radar sensor contribute to self  drive algorithm in way that lidar sensor do not   the premise be ai algorithm be influence by input  which be govern by sensor  for instance  if self  drive car rely solely on camera  this constraint would alter PRON ai algorithm and performance   
__label__deep-network __label__neural-networks __label__machine-learning __label__hidden-layers two common activation function use in deep learning be the hyperbolic tangent function and the sigmoid activation function  PRON understand that the hyperbolic tangent be just a rescaling and translation of the sigmoid function  i  e tanhz   2sigmaz   1    be there a significant difference between these two activation function  and in particular  when be one preferable to the other   PRON realize that in some case  like when estimate probability  output in the range of  01  be more convenient than output that range from  11   PRON want to know if there be difference other than convenience which distinguish the two activation function   PRON do not think PRON make sense to decide activation function base on desire property of the output  PRON can easily insert a calibration step that map the  neural network score  to whatev unit PRON actually want to use  dollar  probability  etc     so PRON think preference between different activation function mostly boil down to the different property of those activation function  like whether or not PRON be continuously differentiable   because there be just a linear transformation between the two  PRON think that mean there be not a meaningful difference between PRON   sigmoid  hyperbolic tangent   as PRON mention  the application of sigmoid may be more convenient than hyperbolic tangent in the case that PRON need a probability value at the output  as matthew  graf say  PRON can fix this with a simple mapping  calibration step   in other layer  this make no sense   hyperbolic tangent  sigmoid   hyperbolic tangent have a property call  approximate identity near the origin  which mean tanh0   0  tanh0   1  and tanhz  be continuous around z0  as oppose to  sigma005 and sigma0025   this feature  which also exist in many other activation function such as identity  arctan  and sinusoid  make the network to learn efficiently even when PRON weight be initialize with small value  in other case  eg sigmoid and relu  these small initial value can be problematic   further reading   random walk initialization for training very deep feedforward networks 
__label__machine-learning __label__gradient-descent PRON be curious to see if one can use a cost function on a set of datum point to find the  optimial minimum  solution for any give set of datum   PRON know for a regular set of datum that be cluster symmetrically follow a straight regression line be easy to find the proper cost function  example   but what if the data make a funky shape like that of x2 or 3x3 or etc   as PRON can see that the the second graph be a lot more noisy than the first   then what do PRON do  be PRON the same process or be PRON different   can PRON find an optimal minimum for any set of datum point   PRON know the cost function be use the residual to form PRON good fit but PRON be just curious if PRON can do with any set of datum   PRON do not think the second picture necessarily have more noise  PRON be just less linear than the first one  if PRON be do regression  which be the case in both PRON problem  PRON can use the same loss function to optimize PRON family of function that PRON be use to fit PRON datum  the only difference be that with certain function family gradient descent may not get to a global optimum but will get stick in a local one  the loss function only determine how to penalize certain error  base on the residual   
__label__floating-point PRON would like to determine the theoretical number of flop  float point operations  that PRON computer can do  can someone please help PRON with this   PRON would like to compare PRON computer to some supercomputer just to get an idea of the difference between PRON   PRON will need to know the model and vendor of the cpu in PRON machine  once PRON have that  PRON can look up on the vendor s website  or maybe on wikipedia  the clock rate  number of chip  socket  number of core per chip  number of float point operation per cycle  and the vector width of those operation  then  PRON simply multiply   take  for example  the intel xeon e5  2680  sandy bridge  processor in stampede where PRON work  the spec be   27ghz  2 chip  node  8 core  chip  2 vector instruction  cycle  256bit wide avx instruction  4 simultaneous double  precision operand   multiply those give 3456 gf  node or 22 pf for the un  accelerated part of the system   PRON usually think in term of double  precision  64bit  operation  because that be the precision require for the vast majority of PRON user  but PRON can can redo the calculation in single  precision term if PRON like  this usually only change the last factor  say 8 sp flop  instruction instead of 4 dp flops  inst  but PRON can be wildly different from that  old gpu  for example  only do dp at about 18th the rate of sp  if PRON ever quote a number for PRON system  PRON should be explicit about which PRON use if PRON be not double  precision because people will assume PRON be  otherwise   also  if PRON chip support fuse multiply  add  fma  instruction  and PRON can do PRON at full rate  then most people count this as 2 float  point operation though a hardware performance counter may count PRON as only one instruction   finally  PRON can also do this for any accelerator that may exist in PRON system  like a gpu or xeon phi  and add that performance to the cpu performance to get a theoretical total   the theoretical peak flop  s be give by     textnumber of cores   textaverage frequency   textoperation per cycle     the number of core be easy   average frequency should  in theory  factor in some amount of turbo boost  intel  or turbo core  amd   but the operating frequency be a good lower bind   the operation per cycle be architecture  dependent and can be hard to find  8 for sandybridge and ivybridge  see slide 26    PRON be the subject of this stack overflow question  which include number for a bunch of modern architecture   PRON understand that PRON ask for the theoretical value  but as this be nearly always inaccessible by any real code  even linpack  PRON may want to just run  optimize  dgemm for very large matrix   the reason that PRON prefer this method be that PRON expose some of the shortcoming of certain processor that prevent PRON from achieve PRON theoretical peak flop value   for example  nvidia gpu currently do integer and float  point operation on the same pipeline   this mean that PRON can only achieve the theoretical peak flop  s if PRON do no integer computation whatsoever   as array indexing and any other form of datum access require integer arithmetic somewhere  no code can achieve the theoretical peak flop  s on an nvidia gpu   in most case  one see 80  as the upper bound   for cpu that issue integer and float  point operation simultaneously  this be a non  issue   on some gpu  like multicore processor like intel knights corner and blue gene  q  PRON be hard to achieve the peak flop  s than on traditional cpu for similar pipeline issue  although both can achieve 90  of peak in large dgemm at least   
__label__linear-algebra __label__optimization __label__matrices __label__iterative-method __label__linear-solver give the system   ax  b where  ainmathbbrntime n  PRON read that  in case jacobi iteration be use as a solver  the method will not converge if  b have a non  zero component in the null  space of  a   so  how could one formally state that  provide that  b have a non  zero component span the null  space of  a  jacobi method be non  convergent  PRON wonder how could that be mathematically formalize  since part of the solution orthogonal to the null  space do converge   therefore  by project the null  space of  a out of each iterate  PRON converge  or       PRON be particularly interested in the case of   lx  b  where  l be a symmetric laplacian matrix with the null  space span by a vector  1n1dots 1tinmathbbrn  and  b have a zero component in the null  space of  l    jb  b where  j  ifrac1n1n1nt be the center matrix  do that imply that each jacobi iterate will have the null  space of  l project out  ie   each iterate will be center  PRON be ask this since then there would be no need to project out the null  space of  l from jacobi iterate  or  in other word  to center the iterate    the correct condition for solvability have nothing to do with the null space of  a  unless  a be symmetric  but with the null space of  at if  atu0  then  ax  b imply that  utb  utax0   hence  b must be orthogonal to any null vector of  at  otherwise there be no solution  and the jacobi iteration have no reason to converge    but if this be the case  a solution exist  and in the square case there be infinitely many   in the singular case  as one never know whether this condition be satisfied  and PRON would be spoil by roundoff anyway   one would typically solve the problem as a least square problem  to find the minimum norm solution  use conjugate gradient on the normal equation  this require that PRON code multiplication by  a and by  at  give only a routine for multiply with  a  one could use gmres instead  with less predictable convergence property   
__label__neural-network __label__matlab if PRON have input datum and PRON be correspond target datum  and for each input there be a number associate with PRON  where this number represent the probability of the input occur  be there a way to use this number to determine how many time a feedforward neural network in matlab encounter the input   currently what PRON be do to work around this be manually have the input present x amount of time where x correspond to the probability associate with the input   so  for example say PRON have input   1210  1410  0110  1421  where each row correspond to a specific item in the input datum and for each row a number be associate with PRON which represent the probability of that item occur   2394  4958  0123  8721  be there a way to use these number to determine the amount of time a feedforward neural network encounter an item   
__label__clustering __label__linear-regression __label__decision-trees __label__ensemble-modeling a team have to create model that predict the cost of deploy a machine over time  this be a regression problem   the team be further divide into two group  a and b  group a put lot of attention on select attribute and find rule that would segment the training set into cluster that be more or less homogeneous  and then use a linear model to create prediction within the cluster   group b do not do cluster first  but instead include the same attribute that team a us for cluster into a non  linear model  let PRON say an ensemble of random forest or gradient boost machine    the result be similar  or slightly good use the non linear model    how be result measure  mean squared error over a hold  out set   the explanation appear to be that  by definition  tree model segment the population use attribute  so that the segment be as homogeneous or pure as possible  give the attribute   so the work team a be do to cluster the instance  the tree model be be also do per se  because segmentation be embed in tree model   do this explanation make sense   be PRON correct to infer that the approach of group b be less demanding in term of time   ie the model find the attribute to segment the datum as oppose to select the attribute manually   
__label__python __label__hpc __label__c++ __label__fortran __label__simulation PRON want to develop a parallel scientific computation software from scratch  PRON want some thought on which language to start  the program involve reading  writing datum to txt file and do heavy computation in parallel  with many lu factorization and the use of sparse linear solver  the candidate solution PRON be think be fortran 20032008 with openmp or co  array  c with openmp cilk or tbb  python  any other  document  suggestion be welcome  PRON know very well c  fortran and java  in that order   PRON have do some scripting in python but basic stuff   PRON know fortran be very fast  but  hard to maintain and parallelize  c be say to be slow unless PRON use external library etc python PRON like  but be PRON realistic to write a full scale  industrial level software upon   the software need to be able to handle big amount of datum and be effective with scientific computation  the performance be of the essence   for the background  PRON already have a work software write in fortran  many people be involve in development over many year and the code be really dirty  maintain and parallelize the code have prove a nightmare and PRON be think of alternative   petros  let PRON try and break down PRON requirement   maintainability  reading  writing text datum  strong interface  capability for lu factorization  sparse linear solver  performance and scalability to large datum  from this list  PRON would consider the follow language   c  c  fortran  python  matlab  java  julia be a promising new language  but the community be still form around PRON and PRON have not be deploy in any major new code   reading  writing text datum  this be easy to get right in any programming language   make sure PRON be appropriately buffer and coalesce PRON i  o access  and PRON will get good performance from any of the language PRON should consider   avoid the stream object in c unless PRON know how to use PRON performantly   strong interface  capability for lu factorization  if PRON be perform dense lu factorization  PRON will want to use lapack  or scalapack  elemental for parallel functionality   lapack and scalapack be write in fortran  elemental be write in c   all three library be performant and well  support and document   PRON can interface into PRON from any of the language PRON should consider   sparse linear solver  the premier freely available sparse linear solver be almost all available through petsc  write in c  which be well  document and support   PRON can interface into petsc from any of the language PRON should consider   performance and scalability to large datum  the only parallel programming paradigms PRON mention be share memory base  which mean PRON be not consider an mpi  base  message  passing   distribute  memory computing approach   in PRON experience  PRON be much easy to write code that scale well beyond a dozen core use a distribute  memory solution   almost all university  cluster  be mpi  base these day  large share  memory machine be expensive  and correspondingly rare   PRON should consider mpi for PRON approach  but PRON advice will apply regardless of the programming paradigm PRON choose   with regard to on  node performance  if PRON be write numerical routine PRON  PRON be easy to get good serial performance in fortran   if PRON have a little bit of experience in c  c  or python  PRON can get very comparable performance  c and c be dead  even with fortran  python and matlab come within about a 25  time overhead without much effort    matlab do this through a jit compiler and very good linear algebra expressivity   PRON will likely need to use either cython  numpy  numexpr  or embed numerical kernel to get the claim performance from python   PRON can not comment on java s performance  because PRON do not know the language very well  but PRON suspect PRON be not far from python s if write by an expert   a note on interface  PRON hope PRON have convince PRON that PRON be go to be able to do everything PRON want in any of the programming language PRON be consider   if PRON be use java  the c interface will be a little challenging   python have excellent c and fortran interface support through ctype  cython  and f2py   lapack be already wrap and available through scipy   matlab have all of the functionality PRON need in PRON native library  but be not readily scalable or particularly easy to run on cluster   java can support c and fortran interface with the jni  but be not commonly find on cluster and in parallel software for scientific computing   maintainability  a lot of this be go to come down to personal flavor  but the general consensus on maintainability be that PRON want to minimize the number of line of code in PRON software  write modular code with well  define interface  and for computational software  provide test that verify the correctness and functionality of the implementation   recommendation  PRON personally have have a lot of luck with python and PRON recommend PRON for many computational project   PRON think PRON should strongly consider PRON for PRON project   python and matlab be probably the most expressive of the language available for scientific computing   PRON can easily interface python to any other programming language  PRON can use f2py to wrap PRON current fortran implementation and piece  by  piece rewrite whichev part PRON wish in python while verify that PRON be maintain functionality   at this time  PRON would recommend a combination of the official python 27 implementation with scipy   PRON can get very easily start with this stack from the freely available enthought python distribution   PRON could also do most of this in c  c  or fortran   c and c be very appealing language for professional developer with a lot of experience  but frequently trip new developer and be in this sense probably not a great idea for a more academic code   fortran and matlab be popular in academic computation  but be weak at the advanced data structure and expressivity python offer  think of a python dict object  for example    related questions   recommendations for a usable  fast java matrix library   in addition to aron s very comprehensive answer  PRON would take a look at the various thread on scicompstackexchange that deal with the question which programming language to take  both regard the speed of program as well as the question of how easy or hard PRON be to write and maintain software in these language   that say  in addition to what have be write there  let PRON make a few observation    i  PRON include co  array fortran in PRON list  to PRON knowledge  the number of compiler that actually support PRON be very small  and my  in fact  be zero  the most widely available fortran compiler be gnu gfortran  and while the current development source parse a subset of co  array  PRON believe that PRON do not actually support any of PRON  ie  PRON accept the syntax but implement none of the semantic   this be of course a general observation about new fortran standard  that the lag with which compiler actually support new standard be measure in several year  compiler have only fully implement fortran 2003 in the last couple of year  and only partially support fortran 2008  this should not stop PRON from use any of PRON if PRON have a compiler that happen to support what PRON use  but PRON must known that PRON put PRON on a portability island    ii  the same be certainly true with ccilk  yes  intel be develop this on a branch of gcc but PRON be not available in any of the gcc release and will  likely  not be for a while  PRON can expect PRON to take another 2  3 year at least till PRON will find cilk with the gcc version instal on typical linux machine    iii  ctbb be a different story  the tbb have be around for a while  have a very stable interface and be compilable with most any c compiler that have exist for the past several year  on linux as well as on window   PRON have be use PRON in deal  ii for several year already with good result  there be also a very good book on PRON    iv  PRON have PRON very own opinion on openmp  namely that PRON be a solution in search of a problem  PRON work well for parallelize the inner loop which be what may be of interest if PRON have very regular data structure  but PRON be rarely what PRON want to do if PRON need to parallelize something  because what PRON really want to do be to parallelize the outer loop  and for that  solution such as the tbb be much good solution because PRON use the mechanism of the programming language rather than try to describe what happen outside the language  via  pragma  and in such a way that PRON have no access to thread handle  result status indicator  etc  from within PRON program    v  if PRON be experimental  PRON may also take a look at the new programming language that be design for parallel programming and  in particular  for task like the one PRON describe  there be essentially two PRON would take a look at   x10 and chapel  PRON have see nice tutorial on chapel  and PRON seem well design  though both of course today be insular solution as well   generally  if PRON be really serious about this software project  PRON would suggest a complete re  write in whatev language PRON PRON feel most comfortable with  PRON sound like PRON will be do the work alone  and therefore PRON will get the good result in the language PRON feel most at home with   more specifically  though  regard parallelism  PRON would encourage PRON to try to think a bit outside of the box  openmp have PRON strength  but be stick in a mindset of take a sequential code and slapping  on parallelism here and there  the same go  in essence  for intels tbb   cilk be definitely a step in the right direction  ie PRON force PRON to re  think PRON problem  solution in an inherently parallel setup  what PRON do not like about PRON  though  be that PRON be yet another language  also  since PRON can only roughly infer relation between parallel task  the scheduler can be quite conservative and may not scale well for certain problem   the good news be  though  that  again  if PRON be serious about PRON implementation  PRON can do what cilk do  eg re  write PRON problem as a set of inter  dependent task and distribute PRON over a number of processor  core  all on PRON own either use pthread or misuse openmp to spawn process  a nice example of how this can be do be the quark scheduler use in the plasma library  a nice comparison of PRON performance vs cilk be give here   there be be little discussion of coarray fortran in the above comment  at this time  and to PRON limited knowledge  coarray support in compiler be roughly as follow   cray have a compiler which support at least the basic coarray feature  PRON have use PRON to write code that be intend to be  educational   but PRON would say that PRON could write real code in coarray fortran  the syntax and concept be mostly much simple than mpi  but as always  there be lotsa trap  and the trap be different from mpi   intel fortran have coarray support build on top of PRON mpi library  supposedly this limit PRON theoretical peak performance  but PRON have not see any metric   gfortran support coarray  but only for a single image  or single rank  in mpi speak   hence  no real parallelization be available until gfortran 48 or 49 be out   generally  PRON would be careful if start a coarray base code  the syntax be simple and much more convenient than fortran  c  c with mpi  but then  PRON be just not as full  featured  for instance  mpi support a lot of reduction operation etc  which could be very convenient for PRON  PRON would really depend on PRON need for a lot of communication  if PRON want an example  let PRON know and PRON can provide PRON with a few  if PRON can dig up the file   have a look at spark PRON be a distribute framework for computation in memory which take advantage of functional programming  structure of a program in spark be very different when compare to mpi  basically PRON write a code like for single computer  which be automatically distribute as function to datum locate in memory  PRON support scala  java and python   logistic regression  scala    load datum to distribute memory  val point  sparktextfilemapparsepointcache    var w  vectorrandomd   current separate plane  for  i  lt 1 to iteration    val gradient  pointsmapp   gt    1   1  exppyw dot px     1   py  px   reduce       w  gradient    printlnfinal separate plane    w   there be an extension to call mlib  machine learning library  which use a fortran library for some low level computation  for python PRON guess numpy be use   so  the idea be simple  concentrate on PRON algorithm and leave optimization to low level  order of processing  datum distribution  etc    
__label__machine-learning __label__dataset __label__data-cleaning __label__feature-extraction PRON have datum in the follow form   table 1  PRON would  feature1  predict  1  xyz  yes  2  abc  yes  table2  PRON would  feature2  1  class1  1  class2  1  class3  2  class2  PRON could perform a one many join and train on the resultant set which be one way to go about PRON  but if PRON rather want to maintain the length of the resultant set equal length of table 1  what be the technique   one possible approach be to perform an encoding  where each level of the feature2 correspond to a new feature  column    this way PRON may describe the 1n relation between the feature 1 and 2  here a small example in r   gt  table1   lt dataframeid  c12    feature1  cxyzabc    predict  ct  t     gt  table2   lt dataframeid  c1112   feature2  cclass1    class2    class3    class2      gt    gt    encode   gt  tabletable2   feature2  PRON would  class1 class2 class3  1  1  1  1  2  0  1  0  the new object contain the  now unique  PRON would and setting of the feature2   PRON need only to merge  join  the result to the table1  basically same task a db join  which variance  inner  outer or full depend on PRON requirement   
__label__machine-learning __label__clustering __label__regression PRON have a large set of point  order of 10k point  form by particle track  movement in the xy plane in time film by a camera  so 3d  256x256px and ca 3k frame in PRON example set  and noise  these particle travel on approximately straight line roughly  but only roughly  in the same direction  and so for the analysis of PRON trajectory PRON be try to fit line through the point  PRON try to use sequential ransac  but can not find a criterion to reliably single out false positive  as well as t and j  linkage  which be too slow and also not reliable enough   here be an image of a part of the dataset with good and bad fit PRON get with sequential ransac   PRON be use the centroid of the particle blob here  blob size vary between 1 and about 20 pixel   PRON find that subsample use for example only every 10th frame work quite well too  so the data size to be process can be reduce this way   PRON read a blog post about all the thing neural network can accomplish  and would like to ask PRON if this would be a feasible application for one before PRON start read  PRON come from a non  math background  so PRON would have to do quite a bit of reading    or could PRON suggest a different method   thank   addendum  here be code for a matlab function to generate a sample point cloud contain 30 parallel noisy line  which PRON can not distinguish yet   function coord  generatesampledata    coord      for i  130  randoffset  i2   coord  vertcatcoord  makeline100randoffset 100 100    200randoffset 200 200   150  02     end  figure  scatter3coords1coords2coords3        function linept  makelinestartpt  endpt  numpt  noiseoffset   dirvec  endpt  startpt   linept  bsxfun  plus  startpt  randnumpts1dirvec    random point on line  linept  linept  noiseoffsetrandnnumpts3    add random offset to point  end  end  PRON solve similar  though simple  task with a brute force approach   the simplification be in the assumption  that the line be a linear function  in PRON case even the coefficient and intercept be in some know range    this will therefore not solve PRON problem in general  where a particle can move orthogonal with axis x  ie PRON trace no function   but PRON post the solution as a possible inspiration   1  take all combination of two point a and b with ax   bx    constant  to avoid the symmetry and a high error while calculate the coefficient   2  calculate the coefficient c and intercept i of the line ab  ay   i  c  ax   by   i  c  bx   ay   by   c   ax   bx    c   ay   by     ax   bx    i  ay   c  ax   3  round the coefficient and intercept  this should eliminate  lower the problem with error cause by the point in a grid   4  for each intercept and coefficient calculate the number of point on this line  5  consider only line with point above some threshold   simple example see here  base on the feedback and try to find more effective approach PRON develop the following algorithm use a dedicated distance measure   follow step be perform   1  define a distance metric return   zero  if the point do not belong to a line  euclidian distance of the point  if the point constitute a line accord to the define parameter  ie  PRON distance be high or equal than the minlinelength and  PRON distance be low or equal than the maxlinelength and  the line consist of at least minlinepoint point with a distance lower that linewidth2 from the line  2  calculate distance matrix use this distance measure  use sample of the datum for large datum set  adjust the line parameter accordingly   3  find the point a and b with maximum distance  break to step 5  if the distance be zero  note that if the distance be high than zero the point a and b be build a line base on PRON definition  4  get all point belong to the line ab and remove PRON from the distance matrix  repeat the step 3  to find another line  5  check the coverage of the point with the select line  if substantial number of point remain uncovered  repeat the whole algorithm with adjust line parameter   6  in case that datum sample be use  reassign all point to the line and recalculate the boundary point   follow parameter be use   line width  linewidth2 be the allow distance of the point from the ideal line   r linewidth  minimum line length   point with short distance be not consider to belong to the same line   r minlinelength  maximum line length   point with long distance be not consider to belong to the same line   r maxlinelength  minimum point on a line  line with less point be ignore  r minlinepoint  with PRON datum  after some fiddling with parameter  PRON get a good result cover all 30 line   more detail can be find in the knitr script 
__label__matlab __label__discretization __label__octave __label__numpy PRON be try to rewrite some matlab  octave code in python  and PRON do not know what would be the nice or most intuitive way of write  octave10gt  dt  01   octave12gt  t  0dt1  t   000000  010000  020000  030000  040000  050000  060000  070000  080000  090000  100000  octave15gt  dt  017   octave16gt  t  0dt1  t   000000  017000  034000  051000  068000  085000  which create a discretization of the interval  0  1  with step 01  as PRON be see  PRON refer to the numpy  matlab mathesaurus and PRON use arange function  but PRON be not suitable for non  integer value as PRON be state in the documentation and show in this so question  on the other hand  play with linspace be not appeal to PRON because PRON take care of endpoint  not space   which would be a straight  forward  one  line way of do this in python   matlab and octave be susceptible to the same subtle floating  point issue that python be  where PRON can get a slightly unexpected result if PRON do not anticipate rounding issue   on the other hand  this be quite convenient syntax to have   PRON can easily  bake  a simple function to do this from the exist functionality in numpylinspace  which be a very pythonic way to do thing    use python3 syntax in python2 to get clean  consistent integer division  from   future   import division  import numpy as np  def lranger1  inc  r2       provide space as an input and receive sample back by wrap  numpylinspace      n    r2r12npspacingr2r1inc  return nplinspacer1r1incnn1    gt  lrange0011   array   0    01   02   03   04   05   06   07   08   09   1      gt  lrange00171   array   0     017   034   051   068   085    edit  PRON have add a float point epsilon to the computation that will handle small floating  point error in the operand  representation per juanlu001 s comment  
__label__machine-learning __label__regularization __label__pybrain from PRON machine learning class  PRON seem l1 norm regularization be the standard way to obtain sparse and probably better fit for machine learning problem  but from pybrain  PRON seem l1 norm regularization be not implement  rather l2 regularization be implement which be quite weird to PRON   PRON be implement as follow   trainer  backproptrainernet  ds  weightdecay001   be PRON possible for PRON to implement l1 norm regularization in pybrain  even if PRON need to modify the source code  thank PRON   
__label__r PRON be look for a r package that perform finite element analysis  be there a fenic or equivalent fem package for r   
__label__performance __label__monte-carlo __label__molecular-dynamics __label__numpy __label__scipy can anyone write a python implementation of a set of  particle interact in 2d accord to a short  range particle  particle force and evolve in time under a metropolis algorithm  which randomly choose a particle and propose a random spatial move  PRON can write such an implementation use the naive approach of update all the particle  particle interaction energy associate with a give cell that have be propose to be move  but would like to see an implementation use a verlet list or a kd tree  which PRON hope can be at least an order of magnitude faster when the number of particle be large  say 100  1000    an add complication in PRON particular problem be that the particle replicate and die  but incorporate that into the verlet  kd tree approach would be a secondary goal   thank for any help PRON can offer   scipy offer an implementation of kdtree class   httpdocsscipyorgdocscipy0140referencegeneratedscipyspatialkdtreehtml  yu should be able to incorporate this into PRON code   PRON have never do this PRON  PRON do mostly md  still  PRON energy be   e  frac12  sumi ei  frac12  sumi sumjne i  vij  where the second sum be perform on neighbor only  if PRON store for every particle  i the sum  ei  sumjne i  vij  PRON can just recompute PRON for the mc update  explicitly  PRON compute the difference in energy for the particle that have move  this idea extend to particle removal and addition easily   the average cost be  ontextrmneigh where  ntextrmneigh be the average number of neighbor for a particle  
__label__pde __label__boundary-conditions __label__parabolic-pde let PRON consider a smooth initial condition and the heat equation in one dimension     partialt u  partialxx  u  in the open interval   01  and let PRON assume that PRON want to solve PRON numerically with finite difference   PRON know that for PRON problem to be well  pose  PRON need to endow PRON with boundary condition at  x0  and  x1 PRON know that dirichlet or neumann work well   if PRON have in the first case  n interior point  xkfrackn1 for  k1cdot  n  then PRON have  n unknown   uk  uxk for  k1cdot  n  because  u be prescribe at the boundary   in the second case PRON really have  n2  unknown  u0cdot  un1   and PRON know how to use the  homogeneous  neumann bc to discretize the laplacian at the border  for example with the adjunction of two fictious point  x1 and  xn2 and the equality     fracu1u12 h   0  fracun2un2 h  PRON question be about periodic bc  PRON have the feeling that PRON could use one equation  namely    u0   u1  but maybe two  and then PRON would use    partialx u0   partialx  u1  but PRON be not sure  PRON do not know either how many unknown PRON should have  be PRON  n1    accord to this PRON should impose periodic boundary condition as   beginequation   u0  t   u1  t    ux0  t   ux1  t   endequation   one way of discretis the heat equation implicitly use backward euler be  beginequation   fracun1undelta t fracun1i12un1iun1i1delta x2   endequation   solve the system of equation  beginequation   beginbmatrix   ifracdelta tdelta x2a  endbmatrix   beginbmatrix   un11   un11   vdot        un1n   endbmatrix     beginbmatrix   un1   un2   vdot        unn   endbmatrix   endequation   where  beginequation   a  beginbmatrix   2  amp  1  amp  0  amp  0  amp  0  amp  ldot  amp  0   1  amp  2  amp  1  amp  0  amp  0  ampldot  amp  0   0  amp  1   amp  2amp  1  amp  0  amp  ldot  amp  0   0  amp  0   amp  1amp  2  amp  1  amp  ldot  amp  0   0  amp  0   amp  0amp  1  amp  2  amp  ldot  amp  0   vdot   ampvdot  ampvdot  ampvdot  amp  ddot  ampddot  ampvdots   0  amp  0  amp  0  amp  ldot  amp0   amp1  amp2  endbmatrix   endequation   PRON periodic boundary condition can be include by add two more equation and two additionalghost  cell  u0 and  un1 such that   beginequation   u1  un   0   fracu2u02 delta x   fracun1un12 delta x   0  endequation   accord to section 211 leveque this give PRON a 2nd order accuracy for  ux  finally PRON system of equation will look like   beginequation   beginbmatrix   0  amp  1  amp  0  amp  0  amp  ldot  amp  0  amp  1  amp  0   1  amp  0  amp  1  amp  0  amp  ldot  amp  1  amp  0  amp  1   0  amp   amp   amp   amp   amp   amp   amp  0  0  amp   amp   amp    amp   amp   amp   amp   0  0  amp   amp   amp  ifracdelta tdelta x2a  amp   amp   amp   amp   0  0  amp   amp   amp   amp   amp   amp   amp   0  0  amp   amp   amp   amp   amp   amp   amp   0  0  amp   amp   amp   amp   amp   amp   amp  0  endbmatrix   beginbmatrix   un10  un11   un12           un1n    un1n1   endbmatrix     beginbmatrix   0   0   un1   un2           unn   endbmatrix   endequation   which give PRON n2 equation and n2 unknown   PRON can also get rid of the first to equation and the ghost cell and arrive at a system of n equation and n unknown   the good way to do this be  as PRON say  to just use the definition of periodic boundary condition and set up PRON equation correctly from the start use the fact that  u0u1 in fact  even more strongly  periodic boundary condition identify  x0  with  x1 for this reason  PRON should only have one of these point in PRON solution domain  an open interval do not make sense when use periodic boundary condition since there be no boundary   this fact mean that PRON should not place a point at  x1  since PRON be the same as  x0 discretiz with  n1  point  PRON then use the fact that by definition  the point to the left of  x0  be  xn and the point to the right of  xn be  x0  PRON pde can then be discretiz in space as     fracpartialpartial t   leftbeginarrayc   x0   x1   vdot   xn  endarrayright     frac1delta x2   leftbeginarrayc   xn  2x0  x1   x0  2x1  x2   vdot   xn1   2xn  x0  endarrayright      this can be write in matrix form as     fracpartialpartial tvecx     frac1delta x2  mathbfa  vecx      where     mathbfa    leftbeginarrayc   2  amp  1  amp  0  amp  cdot  amp  0  amp  1   1  amp  2  amp  1  amp  0  amp  cdot  amp  0    amp  ddot  amp  ddot  amp  ddot    ampamp  ddot  amp  ddot  amp  ddot   0  amp  cdot  amp  0  amp  1  amp  2  amp  1   1  amp  0  amp  cdot  amp  0  amp  1  amp  2  endarrayright       of course there be no need to actually create or store this matrix  the finite difference should be compute on the fly  take care to handle the first and last point specially as need   as a simple example  the follow matlab script solf     partialt u  partialxxu  bt  x      with periodic boundary condition on the domain  xin11 the manufacture solution  utextreft  x   exptcos5pi x be use  mean  bt  x    25pi2  1exptcos5pi x PRON use forward euler time discretization for simplicity and compute the solution both with and without form the matrix  the result be show below   clear   solve  ut  uxx  b   with periodic boundary condition   analytical solution   uref  t  x  exptcos5pix    b  t  x   25pi2  1exptcos5pix     grid  n  30   x1   linspace11n1     leave off 1 point so initial condition be periodic    do not have a duplicate point   xend       uwithmatrix  uref0x    unomatrix  uref0x    dx  diffx12     dt  dx22    iteration matrix   e  onesn1    a  spdiagse 2e e   11  n  n    an1   1   a1n   1   a  a  dx2    index  leave  center  right  for second order center difference  ileft   numelx   1numelx1     icenter   1numelx      iright   2numelx   1      plot  figure1   clf  hold on  h0plotx  uref0xklinewidth2    h1plotx  uwithmatrix    h2plotx  unomatrixo     ylim12  12    legendanalytical solutionmatrix solutionmatrix  free solution    ht  titlesprintftime t   02f0     xlabelx    ylabelu    drawnow  for t  0dt1  uwithmatrix  uwithmatrix  dt   auwithmatrix  bt  x     unomatrix  unomatrix  dt     unomatrixileft     2unomatrixicenter     unomatrixiright   dx2    bt  x     seth0ydataureft  x    seth1ydatauwithmatrix   seth2ydataunomatrix   sethtstringsprintftime t   02ft    drawnow  end 
__label__methodology PRON be investigate some machine learn algorithm  perceptron and knn  and PRON be confused in the methodology section of PRON report  PRON be evaluate the performance of the 2 algorithm  which methodology should PRON use   typically  the methodology be to determine the prediction capability of the model after the learning on a validation set  the algorithm with the high prediction accuracy win   the methodology  or methods  section should explain to the reader how PRON perform the experiment  and why PRON choose these method  in PRON case  PRON should ask question such as  how the datum be obtain  why be this datum good for the experiment   what pre  processing be do  why these pre  proc technique   how be the algorithm compare  k  fold cv  test  train split   why use these method  what be the comparison metric  why these metric and not other   keep in mind that this section should be write clearly and without much detail  for a more comprehensive writing guide  check the usc guide  PRON be for social science  but still apply   
__label__combinatorics PRON be work on an program to compute the structure factor of a give configuration of particle  and PRON need an efficient algorithm to generate all the possible vector with integer coordinate and magnitude between  ndelta and  ndelta  where  delta be small compare to  n this be equivalent to find all the solution to     ndelta2lt vec v 2ltndelta2  with  vec v in mathbfz3  of course  PRON only need to find half of the solution because the other half will be give by    vec vvec v  what be the good algorithm to solve this problem   ps this be PRON first time post here so any help with the tag would be really appreciate   what about a simple nest loop to give PRON one octant of the solution  which can then be copy due to symmetry    i from 0 to  dn   j from 0 to  sqrtdn2i2   k from  sqrtd  n2i2j2 to  sqrtdn2i2j2   where the minimum bind be 0 if  i2j2  gt   d  n2  PRON have to round off to the  small  integer range  
__label__parallel-computing __label__hpc __label__efficiency to somewhat follow up on the question ask here  PRON have be tell that the roofline model be one way of assess the performance of any scientific code  basically PRON compute the arithmetic intensity  ratio of flops over dram byte  and multiply this by the stream bandwidth to obtain the ideal flops  s  PRON could also use the ai to see how close PRON get to the maximum performance of PRON give machine   that say  what be the easy way to obtain this ai for finite element package like fenics or deal  ii  to simply thing for now  PRON be not too concerned with register  cache reuse or quantification of useful bandwidth sustain for some level of cache  if PRON write PRON own explicit finite element implementation use petsc  PRON could simply count by hand the approximate number of flops and approximate the number of load  store from all vector operation and sparse matrix  vector multiply as outline here  however  do anyone have any suggestion for do this for any give implementation   what PRON absolutely can not do be measure the flop on any modern intel architecture use the hardware performance counter  start with nehalem  intel chip essentially count float point instruction issue  and if the datum have not arrive to the processor register  the instruction may not get retire  instead PRON will get reissue some time later  and this keep happen until the data be available and the instruction can finish  as a result  hardware pmc measurement of stream triad over count flop by a factor of 10  dgemm be over count by a few percent  these probably represent the bound  this get so bad that intel disable the flop performance counter on haswell   as such  PRON only real option be to open the code and manually count the operation unless some of these library include instrumentation that count PRON for PRON   in some sense  why go to this trouble  why not do two thing  since PRON be go to be set up and run these code anyway   run exactly the same problem in each code on a give architecture compile the same way to the good of PRON ability  run each case for different value of the size parameter  grid point  element  whatev   with the run time of each of these case  PRON should be able to fit a polynomial or other complexity theory function to run time datum thereby confirm that the method have the expect computational complexity and also estimate the various associated constant  ie if PRON expect  on2  then PRON should get a good fit with a quadratic  PRON may need to ignore the small end of  n entirely  PRON should also be able to see which one be fast  unless these curve cross multiple time  which PRON really should not once PRON get into the asymptotic regime   the one that be fast be the most efficient  make sure PRON run large enough   ask whether any one of these code be as efficient as PRON theoretically could be be a much hard question  
__label__classification __label__training PRON be use a random forest classifier use two dataset  one for training and the other for testing and vice  versa  describe below   first dataset  f  continuous datum  28 feature x 58 observation   two class   a and  b  original cardinality   a  42obs   b  16obs  random subsampl to   a  16obs   b  16obs  second dataset  s  continuous datum  28 feature x 90 observation   two class   a and  b  cardinality   a  76obs   b  14obs  random subsampl to   a  14obs   b  14obs  training methodology  after a procedure of feature selection with mrmr  1   select a number of feature no great than  n5   where  n be the number of observation in the training dataset  as propose by  2    respectively 6 feature for the  f dataset  5 feature for the  s dataset  PRON train PRON classifier on the training dataset  in which PRON have downsize the majority class with a random subsample  in order to train each class with the same number of observation    result  if PRON train PRON random forest classifier on  f and PRON test PRON on  s PRON get the follow confusion matrix     beginmatrix    ampbampa  true  bamp3amp11  true  aamp3amp73  endmatrix     and normalised per  row     beginmatrix    ampbampa  true  bamp0214286amp0785714  true  aamp0039474amp0960526  endmatrix     vice  versa  if PRON train PRON classifier on  s and PRON test PRON on  f PRON get the follow confusion matrix     beginmatrix    ampbampa  true  bamp16amp0  true  aamp12amp30  endmatrix     and normalised per  row     beginmatrix    ampbampa  true  bamp1amp0  true  aamp0285714amp0714286  endmatrix     how could PRON be to have such asymmetry in classification    1  peng  hanchuan  fuhui long  and chris ding   feature selection base on mutual information criterion of max  dependency  max  relevance  and min  redundancy   ieee transactions on pattern analysis and machine intelligence 278  2005   1226  1238    2  johnstone  iain m  and d michael titterington   statistical challenge of high  dimensional datum   philosophical transactions of the royal society of london a  mathematical  physical and engineering sciences 3671906  2009   4237  4253   
__label__deep-learning __label__tensorflow __label__theano nvidia be plan to add hardware support for int8 operation to PRON titan card and target deep learning  PRON be try to understood how PRON useful and what type of network will benefit from this   PRON know that fp16 instead of fp32 be what should be useful for dl  but not sure how int8 could do  there be some research that PRON can train with full fp32 precision and then round PRON to one byte  but this do not speedup or reduce memory footprint for training   actually  recently people have be try much low precision in neural net  1  2  5 scheme  1 bit weight  2 bit activation  and 5 bit gradient  seem to work well for easy dataset  mnist and cifar10   however  on imagenet the result be significantly low than those with full  precision  16 or 32 bit    to achieve state of the art  convnet do not need more than 16 bit for training  but current rnn may need more  for inference  on imagenet  4  5 bit weight  stochastically round from full precision  should be enough  
__label__feature-selection PRON be go through a paper relate to feature selection wherein PRON constantly come across the term laplacian score which PRON be not able to understand  can anyone explain PRON importance in feature selection   PRON do not think PRON can be explain any good than the original paper   httppapersnipsccpaper2909laplacianscoreforfeatureselectionpdf  do PRON not search for this   as spacedman say  the paper provide a very clear explanation of the algorithm on page 2   if PRON be a little less comfortable with the math of the notation  here be the intuition  explanation in word   make a k  near neighbor s graph  that is  for each observation  define an edge in the graph for that observation if another observation be one of PRON k  near neighbor s  if PRON be use a supervised algorithm PRON can define an edge if PRON share the same label   if any two node  observation  be connect  define a weight matrix s measure the similarity between those two node  use some distance measure    for each feature define the laplacian graph   compute the laplacian score base on PRON equation   intuitively  PRON be use knn to define a network graph and assess how similar the feature be accord to PRON distance metric  this be just as good of a measure of feature importance as any other but will also have PRON pitfall  just like all of the other  
__label__pde __label__discretization __label__time-integration this may be a naive question  but when apply a implicit discretization to a pde with a source term  should the source be average in time  for example if PRON take the diffusion equation with a non  linear source term      ut  uxxsx  t  u      PRON can apply the following central difference implicit scheme to the differential term      fracujn1   ujndelta t   left   1theta   uj1   2uj   uj1    theta  uj1   2uj   uj1right   sx  t  u      but how should  sx  t  u be treat  should PRON simply take the value the  n time point  this be what PRON have always do in the past       sx  t   sjn     or average over time      sx  t    1thetasjn1   theta sjn      PRON be not sure PRON be possible to implement a time average in this way because  in general  the  n1  point in time be unknown   be this a silly question  or be there some way of improve the time integration of the above equation by take average in time   if the right hand side be independent of  u then one would generally use the average form      1thetasx  tn1    theta sx  tn       in the nonlinear case PRON can not do that easily  as PRON note  but PRON can at least use some kind of extrapolation  for example approximate      1thetasx  tn1un1    theta sx  tn  un   approx   1thetasleftx  tn1undelta tfracun  un1delta tright   theta sx  tn  un        PRON will find more trick like this in the ode literature  read up on semi  implicit method for ode of the kind  dot xt   a xt   fxtt  the most robust way be solve PRON implicitly  otherwise for stiff nonlinearity in the source function  su PRON will have to use very small time step to maintain numerical stability   consider   ut  st  u  then   un1    un  dt   stdt  un1  here one can use average between n and n1 in the rhs to increase the order  or use more temporal point for even high order in time  but to make PRON simple let PRON assume first order implicit time  stepping   now the equation can be cast in the form    fun10   which can be solve by the newton method  and  for the discretiz system  do linear solf with the krylov subspace be the best know approach  so this be the newton  krylov method  
__label__bigdata __label__convnet __label__numpy PRON have a large training set of 300 gb  which be a subset of an even large dataset 15 tb    PRON be try train a convnet with keras  tensorflow backend  to do something similar to semantic segmentation   PRON could not find any valuable resource to handle such large datum  any suggestion for good practice for such humungous datum be appreciate   thank   PRON do not need to load the whole dataset into memory at once  the only datum PRON need in memory be the sample in a single training batch  use the fitgenerator method rather than fit to pass in an iterator that feed sample to PRON model from disk rather than load all of that datum at once  here be a tutorial that discuss this more  
__label__control-problem __label__world-knowledge give the advantage ai already have over human intelligence  one could imagine a relatively weak strong  ai  barely human intelligence  still outperform a segment of the human scientist population in term of scientific discovery per year  or hour    will ai be do most of the science in 50 year   accord to ray kurzweil  a prominent ai researcher  yes  in PRON book the singularity be near PRON predict that ai will take over develop other ai in about 30 year  after which human intelligence will become marginalise   PRON do not think so  PRON be not the first but actually the third wave of neural network  PRON be do good than earlier two as PRON have much more amount of datum as well as computational power now   take a look at this video   accord to douglas adams ’s famous  hitchhiker ’s guide to the galaxy  after 75 million year of work the  deep thought  computer categorically find out that 42 be the  answer to the ultimate question of life  the universe  and everything   although unfortunately  no one know exactly what that question be    current computing relate 0 or 1 to another 0 or 1 with layer upon layer of building block build on this relationship   in future ai will relate a to b  be PRON number  pattern of coded instruction or some other form of more complicated construct at a hardware   or close to PRON  level than be currently possible and  due to the inherent perfect recall and potentially massive memory storage of ai PRON will most definitely be bring together and relate a vastly more broad and organise collection of knowledge than PRON be possible for any human to even contemplate consciously  there be some idea that would argue that point  universal mind  spiritual revelation and morphic resonance which PRON do personally agree with to some degree and can imagine be particularly difficult to represent in a computational format   pattern spot and relate  organising and compute potential  computer be already good at all these thing than most people try to be  PRON will not be long  i think  before PRON can  invent  something  new    there be already attempt at ai in specialise field of knowledge  human speech  various game  medical diagnostic and learning  etc  PRON will be when these specialise ai can  compare note  about the various methodology that have be the most productive or rewarding  in whichev form these take for PRON  and accordingly update PRON own ontology that the true explosion of  intelligence  will occur  
__label__optimization __label__algorithms __label__nonlinear-programming edit base on comment below   PRON add the mathematical formulation of PRON problem below  PRON be try to solve an equation of the form     partialt fx  y  t  partial2x  partial2y  fx  y  t  equiv gx  y  t       discretiz this equation PRON have     fk1i  j fki  j   delta t gki  j      where  i  j refer to discretiz spatial coordinate  x  y and  k  correspond to the iteration step   however here  delta t  be a fix step size  PRON want to use a line search to find an optimal step size   define  delta t equiv alphak  PRON want to find  alphak  such that  fk1i  j   lt  fki  jcalphak  gtop g which be a backtracking armijo line search   so the equation PRON be try to solve be      fk1i  j fki  jalphak gk  i  j      below be a back tracking line search algorithm to find  alphak but PRON be not be compute correctly PRON realize   PRON update PRON algorithm base on the comment below however PRON still seem PRON stepsize at each iteration   alphak be not be update properly   when PRON print PRON out PRON just print the initial value PRON inputt for PRON   be this algorithm not update  alphak correctly   PRON think the point of the backtracking line search be to find PRON an optimal value  alphak such that PRON get to the minimum  how can PRON fix this   thank   PRON be try to code the backtracking  armijo line search algorithm on page 10 here httpspeoplemathsoxacukhauserhauserlecture2pdf   below be a sample code for a back tracking line search algorithm   PRON can check that the algorithm be not correct but PRON be not sure where PRON be go wrong   a few error PRON have realize be possibly the if statement condition and update alphak at the very end  lastly PRON be not sure what else be wrong   PRON be unsure how to fix these problem    PRON try to follow the algorithm in the book but PRON be not too detailed   PRON be very clear there be a problem with the algorithm but PRON be not sure what   note  the search direction  PRON choose give by pk be in the direction of the negative gradient  which PRON call g   assume below that gi  j  and fki  j  be give at the first iteration  and be 2d array since PRON depend on spatial position i  j   integer  parameter   nx10ny10  k10  real  dimensionnx  nx  ny  ny    fk  fk1g  gt  pk  integer   i  j  m  real   alphak  c  rho  step size at iteration k  c00001  rho05  do m1k  alphak  10  pki  j   gi  j   search direction  gradient  fk1i  j   fki  j   alphakpki  j   gti  j   gj  i   transpose of g  if  fk1i  j   gt  fki  jcalphakgtg  then  alphak  rhoalphak  do j  ny1ny1  do i  nx1nx1  fk1i  j   fk1i  jalphakgi  j   end do  end do  end if  print    print out alphak for m   m  print   55x  e2214      alphak  end do   the following reference a paper by ascher et al     PRON want to solve     partialt fx  y  t  partial2x  partial2y  fx  y  t       subject to some initial and boundary condition  however  as PRON understand from PRON comment  PRON be in fact interested in the steady state solution  and not in the transient behavior of  f   this steady state be of course the  f that satisfy     fracpartial fpartial t   0      PRON start by choose a semi  discrete form of PRON pde  namely      partialt fht    left  ah fh  bh right       where  if PRON discrete computational grid be  n time n  then  fh in rn2 be PRON state vector  one dof per grid point but reshape into a column vector    ah be the discrete laplacian operator  symmetric and positive definite   and  bh include force term and boundary condition  if any   see equation  2a    2b  in the paper PRON link to for an example finite volume discretization   now  an obvious second step would be to use euler s  explicit  method to march in time towards the steady state  ie go from  tn to  tn1 use     fn1h    fn  h   alpha left  bh  ah fn  h  right       essentially what PRON be do here be to take a step  alpha along the direction of the residual    rn  h    bh  ah fn  h    trouble be  a  default  euler time step  alpha will be small  PRON be constrain by numerical stability  so converge to the steady state will take a while with this approach  that be why PRON want to use a high time step if possible  and this be where the line search algorithm come into play  PRON will essentially try to take the good step along the residual direction at each iteration   to do that  observe that the euler discretization above can be interpret as a gradient descent method for a convex numerical optimization problem      minfh  jfh    frac12  left   fht ah  fh    bht fh right       here the cost function  j be a scalar  the necessary condition for a minimum be   nablafh  j  0  ie    ah fh   bh leftrightarrow rh  0  hence the necessary  and sufficient  since the cost function be convex  condition give PRON the steady state of the discrete pde   return to the line search  a natural choice for the  nth time step be that who minimize the cost function  j along the residual line  ie  the solution to     minalpha jfn  h   alpha rn  h       this give PRON the steep descent step     alpha  fracrn  htrn  hrn  ht ah  rn  h     this be what be call an exact line search  however  minimize  j may not be cost effective for more complicated cost function  instead  people have come up with armijo  type backtracking search that do not look for the exact minimizer of  j along the search direction  but only require sufficient decrease in  j  PRON iterate over  alpha until     jfn  h   alpha rn  h    lt  jfn  h    alpha beta  rn  ht rn  h      for some fix  beta in  01  other step choice be of course possible  see ascher s paper   if PRON be look for an implementation of steep descent with armijo  check out this matlab code   pseudocode    n leftarrow 0    tn leftarrow t0  fh  f0h   the initial condition   f be a 1d array   rh  bh  ah fh  the residual  also a 1d array   while  rh not small enough   a  run the armijo iteration use  j  as show in the slide PRON have reference   this give PRON the time step  alpha   b  update the pde state use the euler formula  fn1h  leftarrow fn  h   alpha rn  h   c  update the residual  rn1h   bh  ah fn1h   d  check if the new residual be small enough ie   rn1h    lt  epsilon  for small  epsilon  gt  0   the exact choice of norm and  epsilon be up to PRON   if so break   e   tn1  leftarrow tn  alpha   tn leftarrow tn1   n leftarrow n1   end while  PRON would suggest PRON try implement this in the programming language PRON be most familiar with  then port that work implementation to fortran   an alternative technique be pseudo  transient continuation  
__label__pde __label__operator-splitting PRON have a problem of the form    leftbeginarraycc   lambda2mufracd2dx2   amp  alphafracddx    fracalphadelta tfracddx   amp  fracc0delta tifrackappamuffracd2dx2   endarrayright  left  beginarrayc  u pendarrayright    leftbeginarrayc  fg endarrayright  when PRON apply a finite difference scheme to this system  PRON obtain a block matrix system of the form    left  beginarraycc  m11   amp  m12 m21   amp  m22endarrayrightleft  beginarrayc  vecu vecpendarrayright    leftbeginarrayc  fg endarrayright  where  mij be all sparse symmetric matrix and  ui   pi be the discrete value of u  amp  p at the discrete point  xi in the domain   PRON be think of solve this system of equation by a fix point iteration by split the block matrix into the form    left  beginarraycc  m11   amp  m12 0  amp  m22endarrayrightleft  beginarrayc  vecu vecpendarrayrightk1    left  beginarraycc  0  amp  0 m21   amp  0endarrayright   left  beginarrayc  vecu vecpendarrayrightk   leftbeginarrayc  fg endarrayright  PRON recall from basic numerical analysis that if  a  mn and PRON iterate  mxk1nxkb  then the method be guarantee to converge from any start vector  x0  if the eigenvalue of  m1n be   lt1  in absolute value   be there an easy way to verify whether this be true for this particular matrix splitting   can PRON apply an analogous proof as that of standard gauss  seidel or jacobi method   note  PRON know that this system can be solve by other  possibly faster  mean in the 1d case  but PRON be particularly interested in this form because PRON allow PRON to decouple the multiphysic simulation and PRON can implement fast solver for each equation in the more complicated multidimensional case   when PRON write down the product    m1n  beginpmatrix  m111   amp  m111  m12  m221   0  amp  m221  endpmatrix   beginpmatrix  0  amp  0  m21   amp  0 endpmatrix    beginpmatrix  m111  m12  m221  m21   amp  0  m221  m21   amp  0 endpmatrix  PRON be clear that the eigenvalue of  m1  n depend on  delta t note that PRON can consider the generalize eigenvalue problem  m12  m221  m21  x  lambda m11  x the stability of this iteration depend on  delta t and the relative size of the two term in  m22  if PRON want a robust solver base on splitting  PRON should look at approximate commutator preconditioner  the idea there be to approximate the schur complement  m11   m12  m221  m21 or  m22   m21  m111  m12 use something that be feasible to apply  these be usually combine with a krylov method to catch stray eigenvalue arise because the approximate commutator argument be inexact  especially at boundary  
__label__c++ __label__special-functions be there any preprocessor directive that could be use to use the polylog function  or be PRON include in cmath  if so  do PRON call PRON by li or by polylog   edit   what PRON really be try to do be give an analytical value for the indefinite integral of the function     fracx3ex  1     which involve polylogarithm function  but if anyone have a suggestion for another way to integrate this function analytically PRON would be welcome to any idea   there be a gpld c library  anant  algorithms in analytic number theory by linas vepstas  which include multiprecision implementation of the polylogarithm  build on gmp   from PRON readme file   this project contain ad  hoc implementation of assorted analytic  function of interest in number theory  include the gamma function   the riemann zeta function  the polylogarithm  and the minkowski  question mark function  the implementation use the gnu  multi  precision library  gmp  to perform all low  level operation   the code herein be license under the term of the gnu gplv3 license   the gsl  gnu scientific library  apparently only have the dilogarithm function   however follow a hint from jm one find the debye function which give the ulterior integral  up to a scalar multiple  implement in double precision  see gsl 710 debye functions order 1 through 6      dnx   fracnxn  int0x fractn dtet  1     symbolic integration software such as mathematica or maxima provide     int0x fract3 dtet1   6 operatornameli4ex    6x operatornameli3ex   3x2 operatornameli2ex    x3 log1ex   fracx44   fracpi415     the left  hand side be obviously a purely real value if  x gt 0   but the polylogarithm show will be complex  value  because  ex  gt  1   and so equality hinge on total cancellation of imaginary part    PRON can avoid the need for complex arithmetic in this case by substitute the expression     int0x fract3 dtet1   6 operatornameli4ex     6x operatornameli3ex    3x2 operatornameli2ex     x3operatornameli1ex    fracpi415     this be an improvement because with polylogarithm argument in   01  the result be purely real value   note the proper result when  x  0  be zero  and this be achieve by cancellation between the leading term and the constant   thus relative error could be an issue for small positive value of  x  note that PRON mysterious constant  pi415  be the limit upper bind on these  monotone increase  integral     int0infty fract3 dtet  1   gamma4  zeta4    6 cdot fracpi490     PRON can now revisit the title question  how to use polylogarithm function in c   the point be worth make that there be no standard implementation of polylogarithm function for c or even c   if the goal be to avoid any additional library for PRON implementation  PRON pretty well set PRON out to roll PRON own routine  perhaps along the line suggest by the david c wood paper that gertvde ’s answer link to   besides the multiprecision routine suggest in the first part of PRON answer  there be a  mature  free  double precision math library in cephes  by stephen l moshier which implement both real  polylog  and complex  cpolylog  version of the polylogarithm special function   although PRON accuracy depend in part on the underlying standard mathematical function of c  the cephes source documentation report test and theoretical peak error for order 1 through 4 at about the limit of double precision   alternatively PRON may wish to use other software to directly check  not reference polylogarithm  the quadrature routin PRON write for PRON integral   as PRON sketch out in this math  se question  the power series center at the origin for the integral have limit convergence  but this can be mitigate by use a continue fraction expansion instead   for immediate gratification PRON recommend the  free  numerical quadrature quadpack routine include in maxima  specifically quadqag   for example find the integral over  05  with this maxima command     i1  quadqagx3ex  1   x  0  5  2      o1   489989215833058254399730923588665  10  14210   of the input argument only the last one bear an explanation   the fifth argument to quadqag specifie what rule to apply in adaptive quadrature   possible value be 1 to 6  and give increase sophistication  accuracy   the output line give first the numerical quadrature  follow by an estimate of PRON absolute error  the number of subinterval  step use  and a return code  here zero mean no error or special condition find    first of all  PRON should choose base on PRON application if PRON need high precision arithmetic  ie will PRON be happy with just ieee double precision result for the polylog function or do PRON need high precision   if PRON do need high precision  PRON can look in the family of tool around the gmp library   if PRON do not  PRON can use approximation  some literature research point PRON to this article  at the end of the article  be a  selection table   base on the argument of the polylog that PRON need  PRON can select an approximation formula  but be careful to check stability and accuracy   if PRON do not need too many evaluation  not in a nest loop   PRON would just go for numerical quadrature use the double exponentional method   the gsl also have complete fermi  dirac integral   fjx  for integer   j and  jfrac12  frac12  frac32 these function be equivalent to polylog     fjx   text  lij1ex       though note the restriction to negative polylog argument for real  x   
__label__statistics __label__correlation __label__linear-regression PRON have a correlation matrix for dependent var vs independent variable as below   year  1  sales  sales 05453  10000  priceindex  06089  income  05033  interest  03842  as PRON can see that all independent var have negative correlation with the dependent variable  however as one of the requirement that i have for PRON project be that base on these value can i predict correlation between PRON independent variable   kindly suggest if PRON can predict the same   thank  shivi  
__label__r __label__data-cleaning __label__data PRON have a data frame of the following format   symbol  date  time  profit   banknifty 412010 95500 118    ltbrgtbanknifty  412010  123000  284    banknifty 412010 124500 717    ltbrgtbanknifty  512010  114000  711   zeel  2662012  135000  2475   zeel  2762012  151500  190   zeel  2862012  94500 3758   zeel  2862012  145500  2395   zeel  2962012  142000  465   zeel  2962012  143000  601   zeel  2962012  145500  1223   zeel  2962012  151500  3513   what PRON would like to achieve be convert that datum frame into a data frame which have date for row name  symbol name for column and sum of percentage profit for each day  like in the following   date  banknifty  zeel  412010  315   0  512010  711   0  2662012  0  2475   2762012  0  190   2862012  0  6153   2962012  0  1224   how can PRON achieve that in r  dplyr mutation or some apply function   PRON be a beginner in r programming   thank in advance   the datum in r be  structurelistsymbol  structurec1l  1l  1l  1l  2l  2l  2l   2l  2l  2l  2l  2l   label  cbanknifty    zeel    class   factor     date  structurec5l  5l  5l  6l  1l  2l  3l  3l  4l  4l   4l  4l   label  c2662012    2762012    2862012     2962012    412010    512010    class   factor     time  structurec10l  2l  3l  1l  4l  8l  9l  7l  5l  6l   7l  8l   label  c114000    123000    124500    135000     142000    143000    145500    151500    94500     95500    class   factor    profit  structurec1l  4l   12l  7l  9l  2l  11l  8l  5l  6l  3l  10l   label  c118      190     1223     284     465     601     711      2395     2475     3513     3758     717     class   factor     names  csymbol     date    time    profit    class   dataframe   rownam  cna   12l    the fast way would be   requiredatatable   datum  lt datatabledata    remove the percentage from PRON file and convert the field to numeric   datum   profit   asnumericgsub      profit     datum     symbol  date  time profit     1  banknifty  412010  95500  118     2  banknifty  412010 123000  284     3  banknifty  412010 124500  717     4  banknifty  512010 114000  711     5   zeel 2662012 135000  2475     6   zeel 2762012 151500  190     7   zeel 2862012  94500  3758     8   zeel 2862012 145500  2395     9   zeel 2962012 142000  465    10   zeel 2962012 143000  601    11   zeel 2962012 145500 1223    12   zeel 2962012 151500  3513   melt the datum so that PRON can easily dcast afterwards   moltendata  lt meltdata   listsymbol  date  profit     create a summary by date and symbol   dcastmoltendata  PRON would  csymbol    date     date  variable  symbol  fun  sum      date profitbanknifty profitzeel    1  2662012  000  2475    2  2762012  000  190    3  2862012  000  6153    4  2962012  000  1224    5   412010  315  000    6   512010  711  000 
__label__data-mining __label__nlp __label__dataset __label__data PRON want to extract the skill from a resume use natural languge processing   to train PRON model PRON do not any dataset  where can PRON get a sample dataset for extract skill from resume   a  PRON may try to find an anonymiz resume dataset in the openbase of kaggle dataset  b  PRON may do some web scraping on professional social network like linkedin  take the description of a profile as resume  and the linkedin skill as supervised training set  to build PRON own sample database  do not forget to anonymize PRON    hope this help  
__label__linear-algebra __label__parallel-computing __label__eigensystem __label__precision __label__dense-matrix PRON be try to diagonalize some dense  ill  condition matrix  in machine precision  result be inaccurate  return negative eigenvalue  eigenvector do not have the expect symmetry   PRON switch over to mathematica s eigensystem   function to take advantage of arbitrary precision  but computation be extremely slow  PRON be open to any number of solution  be there package  algorithm that be well suited to ill  condition problem  PRON be not an expert on precondition  so PRON be not sure how much this could help  otherwise  all PRON can think of be parallelize arbitrary precision eigenvalue solver  but PRON be not familiar with anything beyond mathematica  matlab and c   to give some background on the problem  the matrix be large  but not huge  4096x4096 to 32768x32768 at the most   PRON be real  symmetric  and the eigenvalue be bound between 0 and 1  exclusive   with many eigenvalue be very close to 0 and none close to 1  the matrix be essentially a convolution operator  PRON do not need to diagonalize all of PRON matrix  but the large PRON can go  the good  PRON have access to compute cluster with many processor and distribute computing capability   thank PRON  calculate the svd in place of the spectral decomposition  the result be the same in exact arithmetic  as PRON matrix be symmetric positve definite  but in finite precision arithmetic  PRON will get the small eigenvalue with much more accuracy   edit  see demmel  amp  kahan  accurate singular value of bidiagonal matrices   siam j sci  stat  comput  11  1990   873  912   ftpnetlib2csutkedulapacklawnspdflawn03pdf  edit2  note that no method will be able to resolve eigenvalue small than about the norm time the machine accuracy use  as change a single entry by one ulp may already change a small eigenvalue by this much  thus get zero eigenvalue in place of very tiny one be appropriate  and no method  except work with high precision  will disentangle the correspond eigenvector  but just return a basis for the common numerical null space   thank PRON for this suggestion  PRON try mathematica s svd command  but PRON get no noticeable improvement  still miss appropriate symmetry   eigenvalue  be incorrectly zero where PRON be incorrectly come out negative before   maybe PRON would need to implement one of the algorithm PRON describe above instead of a build  in function  PRON would probably want to avoid go to the trouble of use a specific method like this unless PRON be sure ahead of time that PRON would offer a significant improvement   jackpoulson  PRON skim the paper on jacobi s method PRON reference  and PRON look promising  can PRON or anyone recommend a good way to implement jacobi s method for finding eigensystem  PRON be guess that if PRON cod PRON up PRON  in matlab   PRON would be extremely slow  
__label__r __label__statistics __label__time-series PRON have two question on how to produce impulse response use r   1  impulse respons to a negative shock in the independent variable  money supply    2  impulse respons at 2 standard deviation  the code PRON use to generate the impulse response to a positive shock at 1 standard deviation be the following   m1  lt readcsvm1csv   header  t   m1  varm1  lt varm1  p8  typecon    irfm1  lt irfvarm1  impulsem1   response cgdp    boot  false   plotirfm1   irfm1  here be a simple example that should work   datacanada    var2c  varcanada  p2typeconst    irfrweirfvar2cimpulserwresponsece     n  lengthirfrweirfrw   fori in 1nirfrweirfrwiirfrweirfrwi20   plotirfrwe   source 
__label__python __label__bigdata __label__statistics __label__visualization __label__descriptive-statistics PRON be work with datum use python  PRON try to plot a histogram use mathplotlib   PRON find a few example on how to analyze datum to get more accurate graph   so PRON cod    call csvfile  csvdata  pandasreadcsvdataeducationdata   filename   csv   lowmemory  false  encodinglatin1     convert amount field into int    def strtointmainlist    for item in mainlist   newlist    intitemreplace      replace            1000000   for item in mainlist   return newlist   convert str to int use strtoint   function  csvdatastrcosto    strtointcsvdatacuantía     sigma  numpystdcsvdatastrcosto     mean  numpymeancsvdatastrcosto     x  mean  sigma  csvdatastrcosto    but  PRON see one step in particular   in0  x  mean  sigma  csvdatastrcosto    out0  1371822e08  1608953e08  etc   all value from csvdatastrcosto   be integer   PRON question be how to interpret x value   theoretically what PRON mean   
__label__numerical-modelling __label__electromagnetics be there something like a rule of thumb for an adequate time  step size when solve maxwell s equation for the interaction of light with matter   PRON guess a single wave oscillation have to be resolve within at least 10 step  which would give a time  step below 1 fs for visible light   PRON really hope PRON be not that bad  what be PRON experience   for visible light  a timestep on the order fs be correct  but PRON have to balance that against the fact that PRON would typically only need to use fdtd  or any fullwave technique  when PRON scatterer  structure be on the scale of the wavelength anyway  and since PRON be so small  the total interaction time be probably not especially long  PRON be probably in nanosecond unless PRON be extremely high q resonator  and if PRON be high  q  use a frequency domain technique directly  like finite element or method of moment  instead of just wait age for a transient solver to reach oscillatory steady state   PRON do not mean to imply that optical fdtd model be inexpensive  just point out that PRON rarely need to model long time duration with tiny timestep  and PRON may have way to escape  anyway    when structure be vastly large than a wavelength  ie too big for full  wave  fdtd  then asymptotic  raytrac  like technique grow increasingly accurate  PRON be not bind by the nyquist rate in space nor the courant criterion in time  so PRON runtime be not so strongly sensitive to frequency  just PRON accuracy    PRON do admit and agree that there be still a large  unconquered middle  of important structure that be electrically  optically large  yet still pack with enough fine detail to demand fullwave accuracy  antenna array  photonic crystal  computer chip  many more   even here  there may be trick  periodicity  for instance  in the case of an array  that allow for a tailor  make full wave solution   there be a very simple rule of thumb for the time step in fdtd  set PRON as large as possible while still satisfy the cfl condition  basically PRON space discretization dicatat PRON PRON time step   rchilton1980 write   PRON be probably in nanosecond unless PRON be extremely high q resonator  and if PRON be high  q  use a frequency domain technique directly  like finite element or method of moment  instead of just wait age for a transient solver to reach oscillatory steady state    this remark seem to assume that PRON excite the domain by a delta pulse in the time domain  however  PRON often make more sense to use a pulse of finite width  hann window aka raise cosine modulate by the desire frequency  size should be a multiple of modulation   because PRON will significantly reduce the transient time in case of high  q structure  PRON advice  use   increase  the transient time to cover for wave propagation delay  and use   increase  the pulse width to deal with high  q effect   this remark remain true even if PRON use adi instead of fdtd  if PRON use adi  another rule of thumb for the time step will be require  PRON would use around 20 per wavecycle for a start  then PRON can find out whether PRON be too slow or too inaccurate  and change accordingly  
__label__linear-algebra when solve sparse linear system use direct factorization method  the ordering strategy use significantly impact the fill  in factor of non  zero element in the factor  one such ordering strategy be nest dissection  PRON be wonder if PRON be possible to come up with the nest dissection order ahead of time give only the grid parameter  assume an m x n square finite difference grid with first order difference    edit  PRON just find that there be code that do this   httpwwwciseufleduresearchsparsemeshnd  yes  PRON recently write code to do exactly this   suppose PRON have an  nx time ny grid  and that PRON be acceptable to have leaf node with 100 vertex  one can then define a recursive function where the argument be   the dimension and offset of a rectangular subdomain  a pointer into an array that will store the reordering  the routine then simply have to compute the product of the local dimension to decide whether or not the domain be an acceptably small to be a leaf  and then  if so  write the leaf node natural index  say  mathrmnaturalx  yxy nx for an  nx time ny grid   otherwise  cut the large subdomain dimension  recurse on the left and right piece  and then write the separator natural index  
__label__matlab sorry for the noob question   PRON be use matlab version r2012b on a computer that be run 64 bit linux   PRON seem that no version of matlab s parallel computing toolbox will work for PRON   can anyone please tell PRON if PRON be wrong  or else confirm that PRON be correct   thank   as far as PRON recall  matlab run as 32bit on 64bit linux up until r2013a  try 32bit version of computing toolbox  
__label__optimization __label__algorithms __label__linear-solver __label__newton-method __label__conjugate-gradient PRON be consider implement  just for simplicity  the unconstrained implicit optimization base integration for material point method as describe in chenfanfu jiang s thesis on mpm  the minimization algorithm start on page 101 –  a31 unconstrained minimization     PRON have difficulty understand how would be the algorithm implement   be the statement below  incorrect   the goal of the algorithm be obtain new grid velocity in response to external and internal force of the material represent by material point   the function to be minimize be  emathbfv    sumifrac12mmathbfi   lvert mathbf vi  mathbf  vin rvert  phimathbf  xin  delta tmathbf vi  from jiang s thesis   PRON be derivative be  mathbf hmathbf v   mathbf mmathbf v  delta t mathbf fmathbf xn  delta t mathbf v   mathbf mmathbf vn  base on the thesis PRON assume the approach to minimize  e be by the newton method  the newton method require inverse of hessian  mathbfmathith however PRON be in fact interested in  delta mathbf v that would minimize  the linearization of   e  so PRON can instead solve  mathbfmathithdelta mathbf vmathbf h  when solve the system via conjugate gradient  PRON only need to know the product  mathbfmathithdelta mathbf v  which can be think of as differential  delta mathbf h  mathbf mdelta mathbf v  delta t2 delta mathbf fmathbf xn  delta t mathbf v   mathbfmathithdelta mathbf v  from  stomakhin et al  mpm snow paper  PRON know how to compute  deltamathbffi  page 6 –  6 stress  base force and linearization   – ie force differential for the grid velocity at node  mathbf i  PRON assumption   when solve  mathbfmathithdelta mathbf vmathbf h via conjugate gradient  step 4  in the algorithm     delta mathbf v be a column vector consist of component  wise grid velocity differential –    delta v1x    delta v1y    delta  v1z   delta v2x    delta  v2y    delta v2z   ldots  length of  delta mathbf v be therefore  3n  where  n be the number of grid node  contain velocity    when multiply  mathbfmathithdelta mathbf v  PRON be in fact always process 3 row at the same time  ie PRON process one grid node at time  which consist of x  y  z velocity component    when compute residual  mathbf r  mathbf h  mathbfmathithdelta mathbf v  PRON can compute PRON consecutive 3 component correspond to grid node  mathbf i  like in assumption 11   by write  mathbfri   leftmmathbf imathbf vmathbf i   delta t mathbf fmathbf imathbf xnmathbf i   delta t mathbf vmathbf i    mmathbf imathbf  vmathbf inright   leftmmathbf ideltamathbf vmathbf i   delta t2 delta mathbf  fimathbf xnmathbf i   delta t mathbf vmathbf iright  when compute  mathbf r cdot mathbf r  PRON ignore the fact that each 3 consecutive element of vector  mathbf r correspond to 1 grid node  and just compute the dot product like with any other vector   additional question   be the line search  step 7 of the algorithm  necessary   
__label__cross-validation __label__training __label__performance PRON have  practice  datum set which PRON can split into training  validation  and test set and PRON will play with datum to make a machine learning model  but in real situation  PRON will be give a very small datum set which PRON will split into training and validation set  there be no enough observation to make a separate test set  then how can PRON estimate the  fair  performance of the model on the real datum   the only idea PRON have be to find a relation between performance on the validation set and test set on the practice datum  a regression line etc  and then apply the formula to the performance on the validation set of the real datum to find that on the test set of the real datum  which actually do not exist    be there any other good idea   if PRON be work with only enough datum for training and validation  consider use k  fold cross validation   one of the main reason for use cross  validation instead of use the conventional validation  eg partition the datum set into two set of 70  for training and 30  for test  be that there be not enough datum available to partition PRON into separate training and test set without lose significant modelling or testing capability  in these case  a fair way to properly estimate model prediction performance be to use cross  validation as a powerful general technique   assume PRON  practice  data be draw from the same distribution as PRON  real  datum  PRON be on the right track by think about measure the relationship between training accuracy and test accuracy in the first set to model the relationship in the second set   however PRON should split PRON practice datum into training and test only and use k  fold cross  validation on the training set  then PRON should train a model on the real datum with the same cross validation scheme   for example PRON may get result like   practice datum   training accuracy  90   test accuracy  88   real datum   training accuracy  89   since PRON cross  validation scheme be resistant to over  fitting  the training and test accuracy be close on the practice set  and PRON practice training accuracy be close to PRON test training accuracy  now PRON can safely assume that accuracy on the unseen test set for the real datum be about 2  bad than train accuracy   however  imagine PRON a single validation set or no validation at all on the practice datum  now PRON model be more likely to overfit and PRON may see result like this   practice datum   training accuracy  95   test accuracy  75   real datum   training accuracy  85   here PRON have create a model with high variance by overfitt  training and test accuracy be far apart on the practice datum  and training accuracy on the practice datum do not match well with training accuracy on the real datum  PRON be not as easy to estimate test accuracy now  because PRON can not really say if PRON have overfit by the same amount on both dataset and the second set be hard  or if PRON have overfit the second set by less  in the first case PRON may predict 65  test accuracy  and in the second case PRON may predict 75   
__label__plotting __label__seaborn dfhead  day sbp  exercise  0  1460  false  1  1490  false  2  1460  true  3  1480  false  4  1490  false  5  1490  false  i be look to plot day vs sbp with each data point colour as per exercise   i use follow command but could not get each point color as per exercise   pltplotusersbp   colorgreen   linestyledash   markerodata  df   can anyone help how i should go about this    PRON mean something like this plot   the code for do PRON be   import matplotlibpyplot as plt  day012345   sbp1460  1490  1460  1480  1490  1490   exercisefalse  false  true  false  false  false   pltscatterday  sbp  c  exercise  s50   pltxlabelday    pltylabelsbp    pltshow    the color of each point be already specify in the array  exercise   so PRON have only to let matplotlib point to PRON  
__label__machine-learning __label__data-mining __label__classification PRON be use smo  logistic regression  bayesian network and simple cart algorithm for classification  result form weka   algorithm  sensitivity     specificity     overall accuracy     bayesian network  5749  7609  6524  logistic regression  6473  6986  6687  smo  5432  7920  6469  simple cart  7188  6151  6756  smo give the good result for PRON classification problem  since PRON correctly classify the 7920  of the class which be important for PRON  PRON want to increase this accuracy by stack  PRON try to combine some of PRON  in most of the case PRON could not increase the accuracy but stack smo with logistic regression make a little increment in accuracy   how can PRON explain why stack smo with logistic regression be good than other   be there any generalization such as combine tree classifier give good result in stack  what should PRON care about while stack   edit   bayesian network  logistic reg   smo  cart  kappa statistic  03196  03367  03158  03335  mean absolute error  03517  04164  03531  04107  root mean squared error  05488  04548  05942  04547  relative absolute error     723389  8565  726299  84477  root relative squared error    1113076  922452  1205239  922318  weighted avg  of f  measure  0653  0671  0676  922318  roc area  0725  0727  0668  0721  total number of instance be 25106  14641 of PRON be class a  and 10465 of PRON belong to class b     confusion matrix of simple cart     a  b   lt classify as  10524  4117   a  0  4028  6437   b  1     confusion matrix of smo     a  b   lt classify as  7953 6688   a  0  2177 8288   b  1     confusion matrix of logistic regression     a  b   lt classify as  9477 5164   a  0  3154 7311   b  1  since smo be successful at class b and cart be successful at class a  PRON try to ensemble these two algorithm  but PRON could not increase the accuracy  then PRON try to combine smo with logistic regression  the accuracy be increase a little bit  why ensembl smo with logistic regression be good than ensebl smo with cart  be there any explanation   read the follow by mlwave   httpmlwavecomkaggleensemblingguide  this be very good starting point to stack  ensemble   to directly answer PRON question about stack  PRON should care about minimize 1  bias  and 2  variance  this be obvious  but in practice this often come down to simply have model which be  diverse    PRON apologize that link be behind a paywall  but there be a few other like PRON and PRON may well find PRON other way   PRON do not want ensemble of like  minded model  PRON will make the same mistake and reinforce each other   in the case of stack  what be happen  PRON be let the output of the probabilistic classifier on the actual feature input become the new feature  a diverse set of classifier which can in any way give signal about edge case be desirable  if classifier 1 be terrible at class a  b  and c but fantastic at class d  or a certain edge case  PRON be still a good contribution to the ensemble   this be why neural net be so good at what PRON do in image recognition  deep net be in fact recursive logistic regression stack ensemble  nowadays people do not always use the sigmoid activation and there be many layer architecture  but PRON be the same general idea   what PRON would recommend be try to maximize the diversity of PRON ensemble by use some of the similarity metric on the classifier  prediction output vector  ie  diettrich s kappa statistic  in training  here be another good reference   hope that help  
__label__pde __label__multigrid __label__preconditioning __label__domain-decomposition this be mostly aim for elliptic pde over convex domain  so that PRON can get a good overview of the two method   multigrid and multilevel domain decomposition method have so much in common that each can usually be write as a special case of the other  the analysis framework be somewhat different  as a consequence of the different philosophy of each field  generally speak  multigrid method use moderate coarsen rate and simple smoother while domain decomposition method use extremely rapid coarsening and strong smoother   multigrid  mg   multigrid use moderate coarsen rate and achieve robustness through modification of interpolation and smoother  for elliptic problem  the interpolation operator should be  low energy   such that PRON preserve the near  null space of the operator  eg rigid body mode   an example geometric approach to these low energy interpolant be wan  chan  smith  2000   compare to the algebraic construction of smooth aggregation vaněk  mandel  brezina  1996   parallel implementation in ml and petsc via pcgamg  the replacement for prometheus   trottenberg  oosterlee  and schüller s book be a good general reference on multigrid method   most multigrid smoother involve pointwise relaxation  either additively  jacobi  or multiplicatively  gauss seidel   these correspond to tiny  single node or single element  dirichlet problem  some spectral adaptivity  robustness  and vectorizability can be achieve use chebyshev smoother  see adams  brezina  hu  tuminaro  2003   for non  symmetric  eg transport  problem  multiplicative smoother like gauss  seidel be generally necessary  and upwinded interpolant may be use  alternatively  smoother for saddle point and stiff wave problem can be construct by transform via schur  complement  inspire  block preconditioner  or by the related  distribute relaxation   into system in which simple smoother be effective   textbook multigrid efficiency refer to solve to discretization error in a small multiple of the cost of a few residual evaluation  as few as four  on the fine grid  this imply that the number of iteration to a fix algebraic tolerance go down as the number of level in increase  in parallel  the time estimate involve a logarithmic term arise due to synchronization imply by the multigrid hierarchy   domain decomposition  dd   the first domain decomposition method have only one level  with no coarse level  the condition number of the precondition operator can not be less than  mathcal obig  fracl2h2big where  l be the diameter of the domain and  h be the nominal subdomain size  in practice  condition number for one  level dd fall between this bind and  mathcal obig  fracl2hh  big where  h be the element size  note that the number of iteration need by a krylov method scale as the square root of the condition number  optimized schwarz method  gander 2006  improve the constant and dependence on  h  h relative to dirichlet and neumann method  but generally do not include coarse level and thus degrade in the case of many subdomain  see the book by smith  bjørstad  and gropp  1996  or toselli and widlund  2005  for a general reference to domain decomposition method   for optimal or quasi  optimal convergence rate  multiple level be necessary  most dd method be pose as two  level method and some be very difficult to extend to more level  dd method can be classify as overlap or non  overlap   overlapping  these schwarz method use overlap and be generally base on solve dirichlet problem  the strength of the method can be increase by increase the overlap  this class of method be usually robust  do not require local null  space identification or technical modification for problem with local constraint  common in engineer solid mechanic   but involve extra work  especially in 3d  due the overlap  additionally  for constrained problem like incompressible  the inf  sup constant of the overlap strip usually appear  lead to suboptimal convergence rate  modern overlapping method use similar coarse space to bddc  feti  dp  discuss below  be develop by dorhmann  klawonn  and widlund  2008  and dohrmann and widlund  2010    non  overlap  these method usually solve neumann problem of some sort  which mean that unlike dirichlet method  PRON can not work with a globally assemble matrix  and instead require unassembled or partially assemble matrix  the most popular neumann method either enforce continuity between subdomain by balance at every iteration or by lagrange multiplier that will enforce continuity only once convergence be reach  the early method of this sort  balancing neumann  neumann and feti  require precise characterization of the null space of each subdomain  both to construct the coarse level and to make the subdomain problem non  singular  later method  bddc and feti  dp  select subdomain corner andor edge  face moment as coarse level degree of freedom  see klawonn and rheinbach  2007  for an in  depth discussion of coarse space selection for 3d elasticity  mandel  dohrmann  and tazaur  2005  show that bddc and feti  dp have all the same eigenvalue  except for possible 0 and 1   more than two level  most dd method be only pose as two  level method  and some select coarse space that be inconvenient for use with more than two level  unfortunately  especially in 3d  the coarse level problem quickly become a bottleneck  limit the problem size that can be solve  additionally  the condition number of the precondition operator  especially for the dd method base on neumann problem  tend to scale as    kappa  ptextdd1  a   c big1  log frac h h big2l1      where  l be the number of level  as long as aggressive coarsening be use  this be perhaps not so critical because  l le 4  should be capable of solve problem with more than  1012 degree of freedom  but be certainly a concern  see this question for further discussion of this limitation   this be an excellent writeup but PRON think say that  multilevel  dd and mg have a lot in common be not accurate  or at least not useful   the method be very different and PRON do not think that expertise in one be very useful in the other   first  the two community use different definition of complexity  dd optimize the condition number of the precondition system and mg optimize work  memory complexity   this be a big fundamental difference   optimality  have a totally different meaning in these two context   thing do not change when PRON add in parallel complexity  although PRON get a log term add in mg    the two community be almost speak different language   second  mg have multilevel build into PRON and multilevel dd method have all be develop with two level theory and implementation   this limit the space of coarse grid space that PRON can use in mg  PRON must be recursive   for instance  PRON can not implement feti in an mg framework   people do some multilevel dd method as jed mention but at least some of the current popular dd method do not seem to be implementable recursively   third  PRON see the algorithm PRON  as practice  as very different   qualitatively speak PRON would say that dd method project onto domain boundary and solve this interface problem   mg work directly with the native equation   avoid this projection allow mg to be apply to nonlinear and unsymmetrical problem easily   although the theory all but go away for nonlinear and unsymmetric problem PRON have work for a lot of people   mg also explicitly decouple the problem into two part  the coarse grid space for scale and an iterative solver  the smooth  to solve the physics   this be critical in understanding and work with mg and be an attractive property to PRON   although theoretically the smoother and coarse grid space be tightly couple  in practice PRON can often swap different smoother in and out as an optimization parameter   as jed mention point or vertex smoother be popular and usually faster  but for challenge problem heavy smoother can be useful  this plot be from PRON dissertation show the solve time as a function of poisson ratio for jacobi  block jacobi and  additive schwarz   overlapped    PRON a little hard to read but at the high poisson ratio  0499  overlap schwarz be about 2x faster than  vertex  jocobi whereas PRON be about 3x slower at pedestrian poisson ratio   PRON want to make one small addition to jed s excellent answer  namely that the motivation behind the two approach be  or at least be  different   domain decomposition be motivate as a technique for parallel computing   especially for one  level method dd be very natural to implement on a parallel machine  PRON divide the domain into piece and give each piece to a different processor   in some sense the motivation behind dd be to divide arithmetic operation between processor   good parallel multigrid implementation exist  but PRON be often less natural to do in parallel   instead  the motivation behind multigrid be to do less arithmetic operation in the first place   accord to jed s answer  mg use moderate coarsen while dd use rapid coarsening   PRON think this make a difference when PRON be parallelize   there will be multiple of  communication and synchronization for mg to go through many level of coarsen that  be equivalent to a single coarsening of dd  another point from jed s answer be mg use  cheap smooth and dd use strong smoother  consider the two point  PRON have be report  that mg at coarse level will have bad communication  computation ratio  so accord to  amdahl s law  the parallel speedup be not good   a remedy of this be parallel coarse grid correction such as bpx preconditioner   besides  mg can use dd as smoother as adams point out  and mg can also  be use within subdomain of dd   base on the consideration barker have point out   PRON guess use mg within dd be good  which exploit both parallelsim of dd and optimal  complexity of mg   j brown about the condition number of one  level dd  PRON think the size of the subdomain  h should be in the denominator to multiply with the mesh size  h PRON be common for the one  level method that use more subdomain need more iteration  one can also refer to the book of toselli and widlund  for example  the condition number of the schur complement be  mathcalofrac1hh from page 98 of the book   another point be about the condition number of optimize schwarz method   actually  PRON improve the exponent of  h for one  level method  non  overlap and overlap  and in addition the exponent of  h for two  level method  
__label__numerical __label__integration for PRON thesis  PRON look in trajectory of vehicle through an atmosphere at very high velocity  PRON have a set of equation of motion  which PRON propagate use a runge  kutta  fehlberg  rkf  78  numerical integrator  one that have already be develop by the department of PRON university  the entire code be program in c   for PRON vehicle  PRON need to find a specific bank angle profile that achieve a specific objective  PRON be not necessary to understand what a bank angle profile be  just that PRON need one as an input to PRON equation of motion   the optimization algorithm provide a  random  bank angle profile  which PRON use as an input to PRON simulation  base on the result of that simulation  the optimization algorithm change the bank angle profile and evaluate the new trajectory  this keep on repeat until a bank angle profile be find that achieve the set objective  let PRON call the correspond trajectory that result from the bank angle profile the reference trajectory   now PRON also be develop a guidance algorithm  this algorithm require the different state variable and derive value  such as altitude or atmospheric density  at a specify interval  let PRON say ever 10 second  PRON want to apply this guidance algorithm to the find reference trajectory  since the rkf78  algorithm have adaptive step size control  which be beneficial during the optimization since PRON reduce the number of function evaluation  PRON be most certainly not guarantee that an output at every 10 second be provide  so  to achieve this output at ever 10 second  the integration be perform again with the same initial condition as the reference trajectory  expect that the step size obtain from the rkf78  be modify to ensure the output at every 10 second   the problem  however  be that the obtain trajectory in case the step size be modify be different from the reference trajectory  PRON guess be that since PRON reduce the step size to achieve the output at every 10 second  PRON force the accuracy to increase due to the small step size  PRON also try modify the tolerance use in the rkf78  algorithm  as well as different order method  rkf45   rkf56  and adams  bashfort  moulton   PRON decrease this tolerance  both absolute and relative  to 1e15  which be PRON think more or less the limit since PRON be close to machine precision   but PRON still obtain different trajectory  same thi to give PRON an impression of the difference  see the figure below   one thing that may have an influence  but PRON be not sure about this  be the high velocity  if PRON reduce the velocity  there be still a small difference but this do not lead to a noticeable difference in the trajectory  however  for PRON research  PRON be essential to have these high velocity   do anyone have an explanation for the behavior PRON show  if so  what could PRON do to solve this problem   edit base on the comment of doug lipinski  PRON perform several run with a fix step  size integrator  runge  kutta 4  for a number of step size  see the figure below  from this figure  PRON appear that the solution do not converge to one solution even though the step size be very small   PRON think the first step be to confirm which of the solution be more accurate  if PRON be use a reference implementation of rkf78  that have presumably be validate by other on other problem PRON seem extremely likely that the error be on PRON end when PRON be modify the time step   if PRON be possible  ie not too computationally expensive   PRON would try perform a convergence study with a simple fix time step integration scheme  say rk4   if refine the time step lead to the fix time step method converge to one of the two solution PRON have then that give additional verification of the correct solution  if PRON find that the  reference solution  be incorrect then PRON need to look at how PRON be set PRON tolerance  ideally PRON should be use a relative tolerance  not absolute  see wolfgang s answer    assume the error occur somewhere in PRON modification to the time step PRON would suggest a slightly different approach  rather than modify the time step  interpolate the result to the desire time value  if the plot PRON have show be typical of the result a simple polynomial interpolation use nearby point should be very accurate   edit   if successive run at small and small fix step size do not appear to be converge  PRON may be that PRON problem be stiff or that the problem exhibit sign of chaos  eg sensitive dependence on initial condition  or in this case small difference in each time step   in the first case PRON could look at use a different time integrator that be appropriate for stiff ode  implicit method be often use   in the second case PRON be not really sure what the good option would be   if PRON get qualitatively different result from two ode integrator  then the time step choice of at least one of PRON be too large  which one that be be not immediately obvious to say  but if PRON play with the time step of an integrator  eg  make PRON small by a factor of ten  and PRON get a different result  then the time step be too large  if the result be the same then the time step be likely appropriate   PRON would continue to play with the tolerance  1e15 be not a priori close to the round  off level  the round  off be around 1e16  but only when see relative to the size of the variable PRON be compare against  if the variable PRON be compare against be on the order 1e12  then 1e15 be an entirely inappropriately large tolerance  PRON be also not clear what the tolerance actually compare to  the error  the residual  not know what PRON be in the implementation PRON use  PRON would continue to play with PRON to see what happen   one remark  see that the two answer give thus far come with quite a few comment  PRON have decide to post this as a separate answer instead    after PRON have solve the problem with PRON ode integrator  PRON may wish to consider a runge  kutta dense output mechanism for the solution interpolation at  tk  k  sec   a dense output scheme may yield significant compute  time saving compare to PRON approach of re  run PRON integration with small time step  particularly if PRON need to repeat the integration many time  with different initial condition   which be likely the case for PRON  guidance  optimization algorithm   this be because the dense output approximation come  more  or  less   for free   the polynomial solution interpolant can be build directly from the output of the runge  kutta stage  and PRON will not need to artificially decrease PRON time step below those compute by the r  k adaptive  step logic  ie  PRON do not sacrifice efficiency    PRON will find further information on this forum   intermediate value  interpolation  after runge  kutta calculation  another excellent reference be the classic book of hairer and wanner  
__label__optimization __label__scipy PRON seem that scipyminimize can find the minimum of an unbounded function    gtgtgt  import numpy as np   gtgtgt  from scipyoptimize import minimize   gtgtgt  def funy      return y     gtgtgt  re  minimizefun  nparray00    methodslsqp     gtgtgt  r  status  0  success  true  njev  14  nfev  42  fun  array   305175781e08    x  array   305175781e08    message   optimization terminate successfully    jac  array   0    0     nit  14  to be clear  this function be unbounded below and do not have a minimum  anyone know why scipyminimize with slsqp terminate successfully  be PRON suppose to interpret 3e8 as negative infinity  this do not inspire much confidence    to be clear  this function be unbounded below and do not have a minimum   that mean PRON do not satisfy the precondition of minimize  assumption that PRON make about PRON input  so PRON should not pass PRON to minimize   anyone know why scipyminimize with slsqp terminate successfully   PRON use some heuristic to determine what be go on and whether PRON be appropriate to terminate  and in this case of an invalid input the heuristic fail because the author of those heuristic do not take this input into account  all heuristic be  by nature  heuristic  and not guarantee to be correct   one commonly make assumption be that all interesting value of  x and  fx be of reasonably small magnitude  not huge  or infinite  as in this case   not that this be ideal  but PRON hold  possibly after rescale  for most interesting problem   be PRON suppose to interpret 3e8 as negative infinity   no  numerical algorithm only aim to be approximately correct  and rarely offer any strong guarantee on the correctness of PRON output  all PRON can conclude be that  3times 10  8  be the algorithm s good guess at the minimum value  in this case  the result be clearly wrong  but that be because PRON violate the assumption the algorithm make about PRON input function   either change PRON function so that PRON do have a minimum  or consider file a bug report with scipy  so that the invalid input get recognize and cause an explicit error   this do not inspire much confidence   any introductory numerical analysis textbook discuss the issue involve in use float  point arithmetic  which be by nature and by necessity approximate   so PRON would recommend read some introduction to numerical analysis to understand the challenge involve here  before blame the author of scipy  
__label__apache-spark __label__similarity in spark  there be a rowmatrixcolumnsimilaritie   method  see httpsparkapacheorgdocslatestapijavaorgapachesparkmlliblinalgdistributedrowmatrixhtmlcolumnsimilaritie    that return  an n x n sparse upper  triangular matrix of cosine similarity between column of this matrix    how should PRON read PRON  if PRON try to implement an example from httpsstackoverflowcoma1750187 as follow   javarddltvectorgt  row  scparallelizearraysaslist   new densevectornew double2  1  0  2  0  1  1  1     new densevectornew double2  1  1  1  1  0  1  1        rowmatrix mat  new rowmatrixrowsrdd      listltvectorgt  sim  matcolumnsimilaritiestorowmatrixrowstojavarddcollect     forvector v  sim    systemoutprintlnv      PRON get this   8670707106781186547507071067811865475     8123456709999999999999998070710678118654750948683298050513707071067811865475070710678118654750999999999999999809999999999999998     8234567070710678118654750948683298050513707071067811865475070710678118654750999999999999999809999999999999998     8709999999999999998     8456704472135954999579089442719099991590948683298050513709486832980505137     8670707106781186547507071067811865475     8346704472135954999579100707106781186547507071067811865475    how should PRON interpret PRON  how do PRON get the cosine angle 0822 from this  as mention in the reference stackoverflow post   thank   solution be to transform the matrix   javarddltvectorgt  row  jscparallelizearraysaslist   new densevectornew double2  2     new densevectornew double0  1     new densevectornew double1  1     new densevectornew double1  0     new densevectornew double0  1     new densevectornew double2  1     new densevectornew double1  1     new densevectornew double1  1        ​  rowmatrix mat  new rowmatrixrowsrdd      listltvectorgt  sim  matcolumnsimilaritiestorowmatrixrowstojavarddcollect     forvector v  sim    systemoutprintlnv   2108215838362577492     
__label__algorithms __label__nonlinear-equations consider solve numerically for root     x0  y0   fx0  y0   0  gx0  y0   0   where PRON only know that  f  g continuously differentiable but the theoretical differentiation be not a possible  so for this rather take PRON as continuous only    fxx  y  leq 0    gyx  y  leq 0   fy  and  gx  be per se not know  PRON can be locally both weakly increase or decrease  f  g be interpolation on discretiz bellman iteration   PRON be impossible to fall back on actual value of the derivative or cross derivative  but PRON suppose  locally  with bad accuracy  these could be compute   PRON be look for any input wrt  how to attack this problem  as usual  performance be key  PRON will be use python  if someone want to even point PRON at useful package   since  f and  g be both differentiable  PRON problem sound like a prime candidate for an inexact newton method   particularly  PRON can formulate the jacobian matrix by way of finite difference   check out nocedal and wright s book on numerical optimization  particularly chapter 7 on practical approach to calculate derivative   if necessary  PRON may also want to implement a globally convergent strategy  like line  search or trust  region method  
__label__gradient-descent __label__word-embeddings for word2vec with negative sampling  the cost function for a single word be the follow accord to word2vec      e   logsigmavwouwi      sumk1k logsigmavwkuwi          vwo  hiddenoutput word vector of the output word   uwi  inputhidden word vector of the output word   vwk  hiddenoutput word vector of the negative sampled word   sigma be the sigmoid function  and take the derivative with respect to  vwouwj be     fracpartial epartial vwjuwi     sigmavwjuwi      sigmavwjuwi     1     if wj  wo     fracpartial epartial vwjuwi     sigmavwjuwi     sigmavwjuwi       if wj  wk  for  k  1  k  then PRON can use chain rule to get   fracpartial epartial vwj      fracpartial epartial vwjuwi     fracpartial vwjuwipartial vwj       be PRON reasoning and derivative correct  PRON be still new to ml so any help would be great   look good to PRON  this derivative be also present in the paper  equation 56  58    the paper PRON be link to be the most advanced attempt  at least to good of PRON knowledge  to explain how word2vec work  but there be also a lot of other resource on the topic  just search for word2vec on arxivorg   if PRON be interested in word2vec  PRON may find glove interesting too  see  link glove with word2vec   
__label__deep-learning PRON build a deep learning model to detect what drug user use  PRON have many symptom and duration of each drug  PRON create x and y datum but  for example  lsd have an effect duration of 180  720 minute  PRON really need make 540 array  PRON really want a help   PRON lsd array    28  180     28  720     29  180     29  720     30  180     30  720     31  180     31  720     32  180     32  720     33  180     33  720     34  180     34  720     35  180     35  720     36  180     36  720     37  180     37  720     1  180     1  720     38  180     38  720     12  180     12  720     9  180     9  720     24  180     24  720     17  180     17  720     7  180     7  720     4  180     4  720    in first position PRON have different symptom and in second position duration  PRON just duplicate each symptom and set min duration and max duration  but this return to PRON a perfection model  PRON know  PRON need add all minute to each symptom  but how PRON make this use python   list of symptom  0  relaxamento  1  euforia  2  diminuicao da memoria a curto prazo  3  boca seca  4  habilidad motora debilitada  5  olhos vermelhos  6  humor  7  aumento frequencia cardiaca  8  aumento apetite  9  concentracao debilitada  10  sensacao de poder  11  ausencia de medo  12  ansiedade  13  agressividade  14  excitacao  15  perda do apetite  16  tremore  17  dilatacao da pupila  18  dente anestesiado  19  insonia  20  movimentos descontrolado  21  espasmos maxilar  22  dor de cabeça  23  visao turva  24  nausea  25  desidratacao  26  periodo de depressao  27  perda total da memoria  28  ilusões  29  alucinações  30  grande sensibilidade sensorial  31  experiência místicas  32  flashback  33  paranoia  34  perda da noção temporal e espacial  35  confusão  36  perda do controle emocional  37  sentimento de bem  estar  38  pânico  39  sonolencia  40  batimento cardiaco diminuem  41  insuficiencia respiratoria  42  desanimo  43  desinteresse pela vida familiar  profissional  44  sensacao de estar no paraiso  45  mal  estar  46  incapacidade de sentir prazer  47  incapacidade de sentir dor    duration effect  in minute     cannabis  120  240  cocain  30  40  ecstasy  240  480  lsd  180  720  heroin  45  60  PRON full code   x     cannabis   0  120     0  240     1  120     1  240     2  120     2  240     3  120     3  240     4  120     4  240     5  120     5  240     6  120     6  240     7  120     7  240     8  120     8  240     9  120     9  240     cocain   1  30     1  40     10  30     10  40     11  30     11  40     12  30     12  40     13  30     13  40     14  30     14  40     15  30     15  40     7  30     7  40     16  30     16  40     17  30     17  40     18  30     18  40     ecstasy   19  240     19  480     20  240     20  480     21  240     21  480     22  240     22  480     23  240     23  480     24  240     24  480     25  240     25  480     26  240     26  480     27  240     27  480     15  240     15  480     lsd   28  180     28  720     29  180     29  720     30  180     30  720     31  180     31  720     32  180     32  720     33  180     33  720     34  180     34  720     35  180     35  720     36  180     36  720     37  180     37  720     1  180     1  720     38  180     38  720     12  180     12  720     9  180     9  720     24  180     24  720     17  180     17  720     7  180     7  720     4  180     4  720     heroin   39  45     39  60     29  45     29  60     40  45     40  60     41  45     41  60     42  45     42  60     43  45     43  60     44  45     44  60     12  45     12  60     45  45     45  60     46  45     46  60     1  45     1  60     13  45     13  60     24  45     24  60           drogas  0  cannabis  1  cocain  2  ecstasy  3  lsd  4  heroin      y    0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1   2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2   3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3   4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4     from sklearnmodelselection import traintestsplit  xtrain  xt  ytrain  yt  traintestsplitx  y  testsize5   from sklearn import tree  myclassifier  tree  decisiontreeclassifier    myclassifierfitxtrain  ytrain   prediction  myclassifierpredictxt   printprediction   from sklearnmetric import accuracyscore  printaccuracyscoreyt  prediction    sorry for PRON bad english   thank   PRON have many symptom and duration of each drug  PRON create x and y datum  but  for example  lsd have an effect duration of 180  720 minute  PRON  really need make 540 array   PRON can  in this particular case  PRON be fairly easy to generate 800 row in csv once   but PRON do not have to  PRON an apply data augmentation on the fly  this will add some randomness in PRON training  but PRON usually help generalization   by the way  PRON seem PRON be not actually use deep learning  but rather decisiontreeclassifier  which be a bit different  
__label__machine-learning __label__algorithms __label__recommender-system PRON have user in PRON database that PRON would like to match up or group togetter base on the content of there article  PRON can not seem to find how this kind of problem be be solve today  any advice will help   available data   1  each user s post  anything write by PRON  like a blog    2  tag for each post  tag that the user give to PRON post when PRON create PRON   goal   1  match  group user base on available datum   2  produce match percentage   attempt   PRON match people base on the number of exact match of PRON tag   example  user1 have  car  honda  sport   user2 have  car  food    this will give a 33  match   as PRON can image this do not work very well  most user have 20 tag but typically get a match percentage of 0  even if PRON be talk about similar thing   problem   tag that have a clear relationship like car and honda be not match   question   how can PRON match  group user base on tag or the content of there article   one way could be to apply word  embedding for semantic similarity check  word2vec model generate feature vector which could capture semantic similarity  for example  the close vector to car will be honda  ferrari  vehicle  bike  train a model use large amount of datum from wikipedia dump or the one release by google  PRON have fine quality vector  gensim have a nice implementation of word2vec  for each blog article  pre  process the datum by remove the stop  word and stem PRON  from the result word  collect the more frequent word  do this for all the article and check for the similarity among the frequent word in other article as well  so that one article with frequent word car  race  tournament  ferrari  f1 will have more close vector in article with frequent word bike  honda  racer   or other way be to look for similar vector in tag PRON  PRON be good to play with PRON for sometime  so that PRON get to know which feature work better for the dataset PRON have  
__label__machine-learning __label__data-mining __label__dataset __label__java __label__weka i be use some algorithm from weka  i be willing to plot some algorithm  roc curve for comparison  be PRON possible and how    in the weka explorer  go to the classify tab and train  test PRON algorithm   the result buffer appear in the bottom leave box under the section label  result list   right click the result buffer and click visualize threshold curve  then select the class PRON want to analyze  to save the roc curve as an image  hold shift  alt and leave click on the graph 
__label__quantum-mechanics PRON be write a program in c in which PRON be try to reduce the run  time by compute eigen  decomposition of several matrix in parallel  this may be a programming question but since many physics people may use PRON  PRON be ask the question here  this may be a simple example of how to perform computational task in parallel  can anybody tell through a simple example  how to do this   assume PRON have a multi  core processor   parallel solution for eigenvalue and eigenvector of a give matrix can be perform with the package slepc available online  which be build on top of a large package petsc   there be usage example in the slepc distribution   PRON would use something like openmp to exploit thread level parallelism  an example in c would be like   pragma omp parallel for  forint i  0  i  lt  nummatrice  i    doeigendecompositionmatrixi       the pragma will automatically because each loop iteration to be perform in parallel  assume PRON compile with the right flag  eg fopenmp for gcc    within doeigendecomposition  PRON would allocate the necessary memory for temporary work array  must be distinct for each thread  so do not make PRON static or share   and call the proper routine to do the eigen  decomposition as PRON would in the single  core case  most likely in lapack for dense matrix  or some krylov  like method for sparse   PRON can also use pthread  but that be much low level  and PRON have to manually create  synchronize  and destroy the thread   note  PRON need to make sure that whatev code do the eigendecomposition be thread  safe  do not use static variable in c  no common or save block in fortran   lapack s non  symmetric eigensolver  double and zomplex precision  typically be not thread  safe due to the way the library be usually compile  for detail  see this forum post  disclaimer  PRON be the original bug reporter  
__label__self-driving __label__strong-ai __label__cars __label__weak-ai how be autonomous car relate to artificial intelligence  PRON would presume that artificial intelligence be when PRON be able to copy the human state of mind and perform task in the same way  but be not autonomous car just rule  base machine that operate due to PRON environment  PRON be not self  aware  and PRON can not choose a good way to act in a never before experienced situation   PRON know that many people often mention autonomous car when speak about ai  but PRON be not really convinced that these be relate  either PRON have a too strict understanding of what ai be or  there be a neat definition of artificial intelligence  which circumvent the problem of define  intelligence  and which PRON would ascribe to mccarthy  the founder of the field  although PRON can only find PRON now in this book by h simon    … have to do with find way to do intelligent task  to do task which  if PRON be do by human being  would call for PRON human intelligence    so  at PRON core PRON call the automation of every task ai  that can only be do by the human mind  at the time people think that a computer able to play chess would also be intelligent in other way  when this turn out to be false  the term ai be split into  narrow or weak ai   ie a program able to do one task of the human mind  and  general or strong ai   a program that can do all the task of the human mind   self  drive car be narrow ai   note  that all these definition do not specify whether these program copy the way the human mind work or whether PRON come to the same result via completely different algorithm   self drive car exhibit a level of agency and multi  domain resilience  by certain definition PRON be self aware and PRON be definitely design to fail safely in a large number of potentially unknown circumstance  which be similar to biological agent   ai really have to do with the study of non  biological agent and PRON method of agency  everything else be just computer science  algorithmic efficiency  biology  art  etc  eventually the study of biological and non  biological agency will converge  though  and PRON will just call PRON the study of  intelligence    other have give very detailed answer  this be PRON layman view of the problem statement  the self drive car be a  goal seek  machine  PRON have a set of goal with different priority  example  safety of occupants  safety of other  go from point a to point b etc  some be negotiable  other not so   to satisfy the goal  the system should use the input available  radar  gps  camera etc  to determine what be the good possible course of action  at time when PRON do not have all the info  a truck which be hide a speed sign   PRON still have to take a decision  historic memory or through awareness of PRON surrounding  to satisfy PRON design goal  hence the ai   other answer tell about set of instruction for the car in certain situation  or a goal seek machine  while in fact  self  drive car do not have a specific set of instruction  most self  drive car use deep learning to figure out what to do at certain event  PRON do not tell PRON what to do  PRON learn what to do by example   the neural network use to automate car need massive amount of datum to train  use the datum  the car can figure out what the good action be for certain event   accord to this video tesla s autopilot have only one casualty in 300000000 mile  for human driver  the number of casualty in 2014 be 32675  that be per 300000000000 mile  that mean 1 in 90 million human driver because a fatal accident  compare to 1 in 300 million for automated car  deep learning surpass PRON own  safety  rate   not by instruction  but by learn what to do PRON  if that be not ai  PRON do not know what be  
__label__fluid-dynamics __label__simulation __label__software can similarity criterion help reduce calculation in finite element analysis  fea  problem   PRON be use nx  a thermal and cfd simulation software  to simulate the temperature and fluid field of a lead application   PRON take a long time each time i change something   PRON be wonder if PRON can reduce the calculation by scale down the object PRON be calculate   for example  if cool solution a be good for an application of size 10  be the cooling solution a also the good if PRON scale down the application to size 5   if this be true  then i can base PRON calculation on size 5 and reduce drastically the time of calculation   PRON believe this be not so simple  for example heat pipe behave differently with different length   but maybe with some corrective parameter  this can be do   any advice    PRON realize that this may not be the place to ask this question  please also advice where i can post this question   
__label__dataset __label__statistics __label__ab-test PRON have a set of result from an a  b test  one control group  one feature group  which do not fit a normal distribution   in fact the distribution resemble more closely the landau distribution   PRON believe the independent t  test require that the sample be at least approximately normally distribute  which discourage PRON use the t  test as a valid method of significance testing   but PRON question be   at what point can one say that the t  test be not a good method of significance testing   or put another way  how can one qualify how reliable the p  value of a t  test be  give only the data set   the distribution of PRON data do not need to be normal  PRON be the sampling distribution that have to be nearly normal  if PRON sample size be big enough  then the sample distribution of mean from landau distribution should to be nearly normal  due to the central limit theorem   so PRON mean PRON should be able to safely use t  test with PRON datum   example  let PRON consider this example  suppose PRON have a population with lognormal distribution with mu0 and sd05  PRON look a bit similar to landau   so PRON sample 30 observation 5000 time from this distribution each time calculate the mean of the sample  and this be what PRON get  look quite normal  do not PRON  if PRON increase the sample size  PRON be even more apparent  r code  x  seq0  4  005   y  dlnormx  mean0  sd05   plotx  y  typel   btyn    n  30  m  1000  setseed0   samp  repna  m   for  i in 1m    sampi   meanrlnormn  mean0  sd05      histsamp  colorange   probability  t  breaks25  mainsample size  30    x  seq05  15  001   linesx  dnormx  mean  meansamp   sd  sdsamp     n  300  samp  repna  m   for  i in 1m    sampi   meanrlnormn  mean0  sd05      histsamp  colorange   probability  t  breaks25  mainsample size  300    x  seq1  125  0005   linesx  dnormx  mean  meansamp   sd  sdsamp     basically an independent t  test or a 2 sample t  test be use to check if the average of the two sample be significantly different  or  to put in another word  if there be a significant difference between the mean of the two sample   now  the mean of those 2 sample be two statistic  which accord with clt  have a normal distribution  if provide enough sample  note that clt work no matter of the distribution from which the mean statistic be build   normally one can use a z  test  but if the variance be estimate from the sample  because PRON be unknown   some additional uncertainty be introduce  which be incorporate in t distribution  that be why 2sample t  test applie here  
__label__neural-network __label__supervised-learning __label__parameter-estimation PRON have a bunch of test measurement datum and a semi  empirical model that have 18 parameter which PRON have to find so that the model fit PRON datum well  so far PRON have manage to find and optimise the parameter use optimisation and global optimisation algorithm in matlab   now PRON would like to explore different approach for the parameter estimation  PRON have read some paper where the approach with nn be describe  PRON be new to nn and have no idea if this be even possible   PRON would create a two layer network with 18 input and output neuron  PRON be not sure what kind of transfer function would be appropriate for the problem   the formula PRON have to fit and find the parameter look like this    y  d sinc arctanbx  ebx  arctanbx  where  b  c  d  e be macro  coefficient and the micro  coefficient be use to express the variation of each of the primary coefficient with respect to some other datum   this be how PRON data look like   how would PRON create network in matlab for this problem  can PRON give PRON some hint and a push in the right direction to tackle this problem   thank in advance   if PRON be fit a large number of different model  and PRON have sequence of training datum for each different model  with the param already know in those case   then PRON may be able to use a recursive neural network  rnn  to provide param estimate for new datum sequence  however  that do not seem to be PRON situation   as PRON understand PRON question  PRON have a mathematical model with some free parameter  and one set of datum that PRON would like to use to estimate those parameter   a neural network be not really of any use to PRON here  that be because the neural network would replace PRON semi  empirical model with the nn  and the free parameter become the weight of the nn  the train nn would not match PRON desire model  but would generically fit to the datum  PRON could be use to predict more function output give the input  the nn may even do good than PRON preferred model  which would be an interesting result imply that PRON model be incomplete   often PRON do not know  or perhaps even care  about an underlying parsimonious model  in this case  ml technique that fit param of a generic model to datum use some objective function like squared error can be very useful tool  however  when PRON do have a good idea about a simple or explanatory  model  those same technique be less useful  and instead PRON can use optimiser direct on the model  in some case these could be the same optimiser use to train neural network  provide the model be differentiable  
__label__computer-vision currently big tech company like microsoft  google  and amazon  to name a few  offer cognitive service on PRON cloud platform   with these service PRON be possible to identify face  object  text  sound  etc   do PRON know how these service work internally   the only info PRON could find be base on the api level  PRON assume the service use some neural network  which be train by amount of datum   in PRON experience the google service be more accurate then the azure service  perhaps the google service be good and longer train   
__label__linear-algebra __label__machine-learning PRON would like to predict runtime for dense linear algebra operation on a specific architecture use a specific library  PRON would like to learn a model that approximate the function   fop     input sizes rightarrow  runtime  for operation like matrix  multiply  element  wise add  triangular solve  etc   PRON suspect that these runtime be mostly predictable due to the regularity of the operation once PRON get beyond problem size that fit comfortably in cache   question   be this assumption realistic  be the runtime function likely to be nearly deterministic   can PRON assume that this function will be polynomial in the size of the input   ie PRON expect dense matrix multiply to look something like  alpha ntime ktim m for  anktimes bkm and  alpha some scalar coefficient   be there preexist work on this somewhere   PRON current plan be to do least square regression with an  l1  regularizer  any other suggestion   edit  to be clear PRON be look for runtime  not flop or any other common performance metric  PRON be willing to restrict PRON to one particular architecture   there be lot of preexist work  most linear algebra library developer publish performance result in term of float  point performance which can be convert into run time   google for  dgemm performance  for example  yield the follow  httpmathatlassourceforgenettiming3510indexhtml   generally  PRON can expect the answer to be non  smooth  there will be jump or spike in the vicinity of certain problem size  which relate to cache size   PRON should also expect plateaus in rate  and  therefore  linear  ish region for a broad range of problem size  PRON do not expect polynomial fit to be very helpful   give a broad  base benchmarking effort  PRON may be easy to tabulate result and interpolate as necessary  what be PRON goal   PRON have recently be work on exactly this topic  PRON may want to take a look at PRON paper  httparxivorgabs12092364   why be PRON interested in the runtime prediction of linear algebra routine  do PRON intend to use the model for a certain purpose  
__label__ode __label__eigenvalues __label__runge-kutta __label__numerical __label__accuracy PRON have a problem with asses the accuracy of PRON numerical calculation  PRON have a 2nd order ode  PRON be an eigenvalue problem of the form     y   ay   lambda2y  0     and the boundary condiation be     y0   y1   0     this equation describe a vibrate string  clamp at x0 and x1  with a certain mass distribution  PRON want to be able to calculate the eigenvalue of this equation numerically  PRON do this by use the runge  kutta method to find solution to the equation with initial condition     y0   0  y0   1    with different value of  lambda and then look for the one that be zero at x1   PRON terminate PRON search for eigenvalue when PRON find a function that have    y1   lt  delta  where  delta be a predefined constant   now the problem be that PRON would like to know how accurate PRON calculation be  ie how do PRON choose the value of  delta and the stepsize use in compute the value of the potential eigenfunction  so that the error in the eigenvalue be less than  say   103   in principle  PRON could try to come up with an analytic expression for how the error in step size or  delta affect the answer   PRON would not bother   PRON will probably get PRON wrong   the far  far good method be to vary those number and see how much PRON answer change   for example  PRON could change  delta from 001 to 0001 and see how much the answer change   similarly  PRON can vary the step size by factor of 2 or 10   the exact solution as well as the numerical solution for fix stepsize  h depend analytically on  λ  at simple eigenvalue there be locally a near linear relationship between  λ and the right boundary value   thus for a method of order  p  the numerical eigenvalue for a give stepsize  h will lead to a right boundary value of size  ohp in the exact solution and thus a distance of the same magnitude to the eigenvalue of the exact solution   to get an error of magnitude  103 with classical rk4 as integrator one can try  h01  to  h02  as step size to get an error of  104 to  103 in the numerical integration and hopefully with a factor close to  1  the same error magnitude in the eigenvalue  exact a  priory estimate be difficult  PRON be easy to estimate the error a  posteriori  for instance use richardson extrapolation   note that the high oszillation for large  λ lead to consider the requirement of the sampling theorem  ie  the sampling density have to be high enough  or in other word  for large  λ the problem become stiff  the error term could be modify to  oeλhp to reflect that   this should be no problem for the low eigenvalue  thus find the solution with error  104 in the root finder for  h01  and  h02  and use the extrapolation formula to estimate the error     error  fracλ02λ0115      for  λ01 
__label__algorithms __label__numerical-analysis __label__constrained-optimization __label__newton-method __label__projection to those who be familiar with the project newton s method or project gradient method   PRON consider a constrained optimization problem with simple bound  particularly  minimize fx  subject to l  lt x  lt u  where f map rn to r x  l  and u be vector in rn  in the project newton s method that be use to solve this problem  the search direction be obtain by solve the linear system  reduce hessiansearch direction    gradient  base on this  the active element of an iterate be move in the direction of the negative gradient at those element  by definition of active set  the gradient at the active element have to be negative  for upper bound  and positive  for low bind   hence  after projection  the active element be move back to the boundary   PRON first question be if xi be an active element in iteration 1  will PRON be an active element until convergence  in other word  do the active set only get big  not small  and member of the active set be only add  not remove   PRON second question be what if PRON use epsilon active set instead of just active set  will PRON change anything   if PRON need the definition of active set or any other clarification  background  please feel free to let PRON know  thank PRON so much for PRON idea and discussion  this be very important to PRON                                                     edit   PRON consider the problem   beginequation   min fx  quad textsubject to  quad a leq x leq b  endequation   where  f  mathbbrn rightarrow mathbbr be continuously differentiable   an active set be define as  beginequation   mathcala   i   xi  ai quad textand  quad  nabla fxi  gt  0  quad textor  quad  xi  bi quad textand  quad  nabla fxi  lt  0     endequation   suppose PRON initial iterate  x0  be on the boundary  eg  x0  a  then PRON be possible to have some inactive element where   nabla fxi  lt  0   right   in the project newton s method  first with step length  alpha  1   the active element move in the direction of the negative gradient  which will point outside of the feasible region  then  those element be project back to the boundary  the inactive element move in the newton direction  then PRON test for sufficient decrease  but even when PRON decrease  alpha  the active element still move back to the boundary and those element still meet the  active  criterion in the next newton iteration   PRON know the active set can shrink  there must be something wrong in PRON thought   the answer to PRON first question be no  the active set do not only grow  PRON can also shrink  even if PRON objective function be convex  an example be if PRON start at some point on the boundary  then the first step move along one active constraint to find the minimum of the objective function along this segment  but then  the second step will likely move away from the boundary because at the previous point the gradient will have point  in general  unless PRON be at the solution  into the domain   the answer to PRON second question be not clear to PRON  PRON would say that PRON depend on the size of epsilon  if epsilon be small enough  PRON good guess be that PRON do not matter  but small enough will be relatively to the curvature of the objective function   if PRON want to learn more about the active set method  PRON favorite book on optimization be  numerical optimization  by nocedal and wright  which have an extensive discussion of active set method  
__label__neural-network __label__deep-learning __label__nlp PRON be currently think of the system that would compare 2 text  source  target  and suggest change to the source text to match more the style of the target  PRON should not convert PRON to the target but more suggest to rephrase some sentence  change word  add some key word and so on   any idea how this could be do   thank a lot for any suggestion   some time ago  PRON stumble upon the author obfuscation task of pan16 workshop  the task at hand be give a document  paraphrase PRON so that PRON writing style do not match that of PRON original author   although  PRON be not exactly what PRON want  PRON can take a look at PRON proceeding and find useful tip regard author style and how to  change  PRON  ie in PRON case  an  author  be a collection of document from the same source    for example  one approach  denote specific metric that define an author s style  average sentence word count  use of stopword  pos  type frequency   PRON could use the target s score in these metric as PRON gold standard and try to modify the source text  in way discuss in the proceeding of the workshop  so as to mask PRON source text to match the target one  in term of these metric   
__label__tableau in tableau  PRON commonly want to carry out a task like distinguish 2 group by various measure   the example below show the difference in the count of 2 treatment and control for variable level of a factor  measure    this view be mislead though  because the overall group size of treatment and control be not equal   what PRON would like to do be to represent this as a proportion for each level of the factor  measure  instead of use just a count  for instance  perhaps 25  of the treatment group be in business development whereas only 10  of the control group be in business development   be this possible to do in tableau  or do PRON strictly require pre  processing   yes  PRON can do this without rework PRON  and as always  there be few way to do PRON  without see exact datum  PRON would try set table calculation to percent of total and then alter compute with to table down  table across depend on the data layout 
__label__computational-chemistry PRON want to explore the low energy state of an amino acid or a peptide by generate input geometry  define by multiple dihedral angle of the backbone or the side chain  from these start conformation PRON want to do geometry optimization to yield local energy minimum geometry   the follow approach fail   use the modredundant keyword in the gaussian09 package to define new value for the dihedral angle  this crash for large molecule and major change in the coordinate   detect the substructure of the molecule  eg the backbone  by smile  smarts pattern and use obrotate from the openbabel package to change the value  work fine for a single dihedral or maybe two  but afterwards these change the pattern of the substructure be not necessarily detect anymore and no further dihedral angle can be change   usage of a well choose z  matrix  the great disadvantage be  that PRON do not know how to yield a z  matrix with the want dihedral from a give geometry and therefore PRON can not start from a pre  optimize structure   be there any other attempt to this problem   PRON do this stuff by a self  make script for a long time  but have to prepare the input manually for every molecule    meanwhile  PRON come along confab   confab be an open source conformation generator whose goal be the systematic coverage of conformational space   a detailed description how PRON work can be find in the author publication  
__label__r __label__time-series __label__apache-spark __label__performance PRON be in the process of develop a new spark  base arimax  tool  and have reach the point where PRON need to know whether PRON coefficient estimate and forecast be sensible  PRON can compare PRON result to r on the same datum set  but  as PRON implementation be distribute while r s be in  memory  PRON think PRON be reasonable to assume that there will be some slight difference in PRON estimate  what PRON do not know be how different be too different for the coefficient estimate and forecast  be there a standard approach to evaluate to reasonableness of a new implementation of time series analytic   what some people do be some kind of  order  k  fold cross validation   check here  for instance   PRON partition the datum into  k partition  then  PRON fit the model on the partition 1  test on partition 2  get prediction error  then  PRON fit the model on the union of partition 1 and 2  test on partition 3  get prediction error  so on so forth  then average the prediction error    be there a standard approach to evaluate  the  reasonableness of a new implementation of time series analytic   yes  there be  a way to validate PRON build  from  scratch be to simulate an arimax timeserie from a data generating process  dgp  of PRON choice  see eg httprobjhyndmancomhyndsightarimax    call PRON choice of dgp  parameter  theta  then   draw a sample from the dgp   estimate the model s parameter   hattheta make forecast   hatytht  assess   hattheta   thetatexta metric and  hatytht   ythtexta metric  repeat 1 to 3 a couple of time  also for different dgp and sample size   PRON either increase PRON confidence in the build  or find weak point that need improvement   a sensible metric could be square loss  personally  PRON like to use the  eye  ball metric  for the forecast residual  PRON distribution could be assess by way of histogram  
__label__machine-learning __label__predictive-modeling __label__cross-validation PRON need to validate PRON linear model use both rsme and fold cross  validation   so PRON have as PRON see below   mlt  matrix0nrow10ncol  1   store  lt datastore   numberstore      a store be selectedidentifié par son numéro unique  shuffledindice  lt samplenrowstore      the datum for the store be shuffledle  setdtstoreprediction0   z  lt nrowstore   for  i in 110      10fold cross  validation   10  of all datum row be select  sampleindex  lt floor1  01i1z01iz    PRON be use as test set  test  lt storeshuffledindicessampleindex      the rest be use as training set  train  lt storeshuffledindicessampleindex      a linear model be fit to the training set  odelllt  modell  lt lmsales  asfactordayofweek   schoolholiday  asfactorpromo   stateholiday  asfactordateyear   asfactordatemonth   asfactordateday    asfactordateweek   asfactorcompetitionopen   asfactorpromoopen    datum  train    prediction be generate for the test set base on the model  storeshuffledindicessampleindexpredictionpredictmodell  test    mi1ltroundsqrtmeanstoreprediction  testsales2meantestsales4      then PRON want to see the graph of the rsme in function of i   plot110m1typebxlabiylabrmse    PRON give this   can PRON say here that PRON model be validate because the rsme decrease when the k  folds increase   
__label__interpolation __label__error-estimation PRON be work with a code that solve diffusion  reaction equation on a 2d unstructured mesh   due to the stiffness of some of the process  PRON start with time step near 1e13  and end with a final time in the millisecond range with time step near 1e8   the issue appear in that the problem be also  what PRON would like to call geometrically stiff   during the first 250 ns of simulation PRON require a very fine mesh to resolve a geometric characteristic of the force function   after this time  however  the forcing of the system be approximately zero  and PRON no longer need the spatial resolution   the fine mesh take about 2 week to run through that first 250 ns   the coarse mesh take 2  4 week to run the entire simulation out to 25 ms  an order of magnitude speed up    run an adaptive mesh be not possible without a serious rewrite of the code  currently beyond PRON skillset as well   however there be utility for PRON to restart a simulation   what PRON be consider do be run the fine mesh out until PRON be no longer need  take the restart dump file  interpolate PRON onto the coarse mesh  then restart to let PRON run out to the final time PRON want   PRON can estimate the error in PRON interpolation fairly easily depend on PRON interpolation method  however PRON be unsure of how to estimate the error that be propogat once PRON begin time  step again   PRON be particularly worried about the smoothing on variable such at temperature will have  since many of PRON process rate be exponentially dependent on PRON   PRON question be  how do PRON estimate how some fix error propogat through PRON stiff system without run multiple case  computationally infeasible at the present time    
__label__pde __label__boundary-conditions __label__finite-volume __label__mesh __label__coupling PRON have a system of equation  wherein the bc of one pde be couple with the source term of another pde   PRON have a regular 2d unit grid in x and y  there be two pde to be solve  the first pde  elliptic diffusion problem  be define only at  y  1   act along the x  axis  ie PRON act in the x  direction and only along the top of the cartesian grid   this x  axis be discretiz with a fix grid  space  generate a finite number of node  let this set of node  co  ordinate be represent by  ‘ x’  the second pde be a time  vary diffusion problem  this be define only along the y  axis  but for all x  node  ie for all   x’   where the 1st pde be be solve   pde1     nablas nabla a   fomega  a  bc1    fracpartial apartial x   1   at  x1   neumann   bc2    a  0   at  x0   dirichelet   pde2     fracpartial bpartial t   nabla   left  beginarraycc   0  amp  0   0  amp  d  endarray  right  nabla b  bc1    b  0   at  y0  for all   x  ie along the bottom face  bc2    fracpartial bpartial y   gomega  a  at  y1   neumann    f and  g be linear function in  omegax  y  t and  ax  y  t  b be define in the 2d grid as  bx  y  t  importantly    omega  by1  text   for all     x  ie  the bc2 of the 2nd pde couple with the implicit source term of the 1st pde along the top face of the cartesian mesh   how would one approach this problem in order to obtain a numerical solution via a fd  fv implementation   as write  PRON be not clear to PRON that PRON be actually fully couple  that is  the solution to  b depend  via PRON bc2  on the solution to  a  but assume  omega be simply some specify function of position and time  PRON look like  a do not depend on the solution to  b if that be the case  then simply solve for  a on a 1d mesh first  then PRON have the  constant in time  bc for  b and PRON can solve PRON as though there be no coupling  this can be do use a number of technique for apply bc s within the fd  fv method  but PRON do not appear that this be what PRON be ask  if  omega be a function of  b  then the system would be fully couple  PRON will assume that PRON be fully couple   if PRON be confident that the diffusion in pde2 be not go to ever have a component in the  x direction  and PRON think fd  fv be good for PRON problem  PRON so no reason why PRON would not be   PRON would consider cast this as two separate  couple 1d grid to avoid needlessly calculate zero  flux in the  x direction in the diffusing region  the first grid would be 1d in the  x direction  over which pde1 can be define  then  PRON could repeat a 1d  y direction grid at each discretization point in  x  and define pde2 over each one of those  then PRON be more like an equivalent 1  1d system  to solve that in a coupled system  as PRON suggest  PRON can use fd  fv to discretize in space  then step forward use the method of line  because  a do not appear with a time  derivative anywhere  the discretiz problem will be a system of  index1  differential algebraic equation  dae s    once PRON have do the fd  fv  PRON would end up with an equation like this    0  f1omega  ai  at each interior mesh point in the  x mesh  and    fracpartial bpartial t   f2bi  at each interior mesh point on the  y mesh where  f1  and  f2  be function which depend on PRON discretization choice   there be a number of way to solve index1 dae s including  1  simple implicit euler time step   2  the idas module of the sundial suite  which have be wrap for some other language  python  julia    or perhaps  3  matlab s ode15s  which also enable solution of index1 dae s by pass in a singular mass matrix   then  as PRON say  the coupling would come through the boundary condition  which PRON can implement a number of way  one common approach be to use  ghost point   or fictitious point add on each side of the mesh  so for example  for the right side of the  x mesh  say PRON have index  nx  PRON could add an additional equation for the fictitious point  nx1      anx1   anx  and simply write the grid point equation for  anx as PRON would for any interior point  typically as a function of  anx1   anx   anx1   the above be a case that be trivial enough that PRON could choose to substitute this algebraic equation into the discretization for point  nx on the  x mesh  but when thing get ugly  PRON can simply leave PRON as extra equation  for example  look at a top point for the  b meshes  at  xindex  xi  PRON could have the  yindex point  ny1  define with the equation    leftbxi  ny1   bxi  nyrightdelta y  gomega  axi  where  delta y be the grid space in the  y direction  then  again  if PRON can solve for  bxi  ny  PRON could substitute PRON into the  xi  ny equation  but if  omega be some non  linear function of  b  PRON could solve this algebraic equation along with the other in PRON dae system  PRON can certainly change the discretization scheme to get high order accuracy  but hopefully this demonstrate the idea  
__label__visualization __label__pandas PRON be try to create a contour map from two variable which store some temperature value and a third variable which be the time stamp   PRON use this notebook as a tutorial  httpsplotlypandascontourplots  PRON be not able to convert the panda dataframe create  into a 1d array  and the kdescipy do not work with a nd  array  PRON try convert the dataframe into a 1d array use asmatrix   but this be the error PRON be receive   degree of freedom  lt 0 for slice  how can PRON convert this csv file  with 3 column of datum  import as a dataframe into individual column of datum  or can PRON directly import each column of datum into a 1d array and use PRON in the function kdescipy   PRON can try this  import panda as pd  import numpy as np  filename   datacsv   df1  pdreadcsvfilename    convert dataframe to matrix  convarr df1valu   split matrix into 3 column each into 1d array  arr1  npdeleteconvarr12axis1   arr2  npdeleteconvarr02axis1   arr3  npdeleteconvarr01axis1    convert into 1d array  arr1  arr1ravel    arr2  arr2ravel    arr3  arr3ravel    this should mostly do the job  use the arr1  arr2arr3 in the function PRON mention  PRON be the 1d array of the column PRON split  something like mydataframevaluesflatten   
__label__philosophy the english language be not well  suit to talk about artificial intelligence  which make PRON difficult for human to communicate to each other about what an ai be actually  do   thus  PRON may make more sense to use  human  like  term to describe the action of machinery  even when the internal property of the machinery do not resemble the internal property of humanity   anthropomorphic language have be use a lot in technology  see the hacker s dictionary definition of anthropomorphization  which attempt to justify computer programmer  use of anthromporhic term when describe technology   but as ai continue to advance  PRON may be useful to consider the tradeoff of use anthropomorphic language in communicate to both technical audience and non  technical audience  how can PRON get a good handle on ai if PRON can not even describe what PRON be do   suppose PRON want to develop an algorithm that display a list of related article  there be two way by which PRON can explain how the algorithm work to a layman   very anthropomorphic  the algorithm read all the article on a website  and display the article that be very similar to the article PRON be look at   very technical  the algorithm convert each article into a  bag  of  word   and then compare the  bag  of  word  of each article to determine what article share the most common word  the article that share the most word in the bag be the one that be display to the user   obviously   2 may be more  technically correct  than  1  by detail the implementation of the algorithm  PRON make PRON easy for someone to understand how to fix the algorithm if PRON produce an output that PRON disagree with heavily   but  1 be more readable  elegant  and easy to understand  PRON provide a general sense of what the algorithm be do  instead of how the algorithm be do PRON  by abstract away the implementation detail of how a computer  read  the article  PRON can then focus on use the algorithm in real  world scenario   should PRON  therefore  prefer to use the anthropomorphic language as emphasize by statement  1  if not  why not   ps  if the answer depend on the audience that PRON be speak to  a non  technical audience may prefer  1  while a technical audience may prefer  2   then let PRON know that as well   if clarity be PRON goal  PRON should attempt to avoid anthropomorphic language  do so run a danger of even mislead PRON about the capability of the program   this be a pernicious trap in ai research  with numerous case where even experienced researcher have ascribe a great degree of understanding to a program than be actually merit   douglas hofstadter describe the issue at some length in a chapter entitle  the ineradicable eliza effect and PRON dangers  and there be also a famous paper by drew mcdermot  entitle  artifical intelligence meet natural stupidity    hence  in general one should make particular effort to avoid anthropomorphism in ai  however  when speak to a non  technical audience   soundbite  description be  as in any complex discipline  acceptable provide PRON let the audience know that PRON be get the simplified version   PRON think the correct answer be the easy but unhelpful   PRON depend    even when PRON be talk to other technical people  PRON often use anthropomorphic language and metaphor  especially at the start of the conversation   the computer have to figure out    how can PRON prevent the computer from get confused about   etc  sure  PRON could state that in a more technically correct way   PRON need to modify the algorithm to reduce the number and variety of instance of inadequate datum that result in inaccurate setting of   or some such  but among technical people  PRON know what PRON mean  and PRON be just easy to use metaphorical language   when try to solve technical computer problem  PRON often start with a vague  anthropomorphic concept   PRON should make a list of all the word in the text  and assign each word a weight base on how frequently PRON occur  oh  but PRON should ignore short  common word like  the  and  PRON   then let PRON pick some number of word  maybe ten or so  that have the great weight   all that be a long way from how the computer actually manipulate datum  but PRON be often a lot easy to think about PRON in  human  term first  and then figure out how to make the computer do PRON   when talk to a non  technical audience  PRON think the issue be  anthropomorphic language make PRON easy to understand  but also often give the impression that the computer be much more human  like than PRON really be  PRON only need to watch science fiction movie to see that apparently a lot of people think that a computer or a robot think just like a person except that PRON be very precise and have no emotion   the problem PRON be reference be not just an ai problem but a problem for highly technical field in general  when in doubt  PRON would always recommend use plain language   however  there be another reason the ai community will often eschew anthropomorphic connotation for ai  some ai luminary often like warn PRON that an artificial general intelligence may behave in alien way that defy PRON human expectation  potentially lead to a robot apocalypse   this idea about evil alien  like agi  however  derive from a widespread misunderstanding in the ai community that conflate two different notion of generality   turing machine generality  and  human domain generality  what regular people mean when PRON say generality be the later  even the official definition of agi hinge off of that human  contingent context    perform any intellectual task that a human being can   but by that definition  generalize behavior do not make PRON more alien  to generalize be to anthropomorphize  as nietzche say    where PRON see ideal thing  PRON see— human  alas  all too human thing   
__label__optimization __label__software PRON be use the dakota optimizer   PRON have 50  variable  50  constraint and more than one objective function   independent of the actual optimizer use  or the output select   be there a way to plot the objective function while the optimizer be run   dakota offer a graphical output  but this show a combined objective function  not each objective function individually  furthermore  with the amount of variable and constraint the standard window  screen be too small for a complete graphical output   PRON have the value for each objective function at each iteration because PRON provide PRON to the response file  however  PRON would like to avoid generate an additional file that store these value and call an external function to plot from that file   PRON be look for a solution which be part of dakota   as per the dakota user manual  section 144 PRON appear that the quick answer to this question be  no  currently that extent of control over the graphical output be not available  
__label__python __label__random-forest __label__multiclass-classification problem statement   give the detail about a product  PRON need to map PRON to PRON category   currently PRON be use product name as a feature and product category as the label  there be around 50000 category available currently and PRON will grow in future   PRON create a small dataset which consist 20 category and 100 record for each label  so the total record count be 2000  use randomforest PRON get 92  accuracy   problem   so PRON go on to create a model with 1800 categorieslabel  and record for each category vary from 500  1500  when PRON run the same model with new dataset PRON get only 19  accuracy and more than 50  of the predict value point to the same label   dataset sample   productcombin  category  2pcs lead light lamp strip dimmer switch brightness adjustable control 12  24v 8a art  crafts  amp  sewing  painting  drawing  amp  art supplies  drawing  light boxes  10 pcs 14  male to 14  female screw adapter for tripod camera flash bracket stand arts  crafts  amp  sewing  painting  drawing  amp  art supplies  drawing  light boxes  l  fine a4 tracing lead light pad box1386x945 inches  with adjustable light intensity for artists  drawing  sketching  animation  arts  crafts  amp  sewing  painting  drawing  amp  art supplies  drawing  light boxes  bzone solar powered operated copper wire lead fairy light decorative string lights for indoor outdoor home garden lawn patio party christmas valentine  s day  164 ft  pink color   arts  crafts  amp  sewing  painting  drawing  amp  art supplies  drawing  light boxes  litenergy 325 inch diagonal a2 tracing table with lead light and paper  arts  crafts  amp  sewing  painting  drawing  amp  art supplies  drawing  light boxes  code   import string  import codec  import panda as pd  import numpy as np  from sklearnfeatureextractiontext import countvectorizer  from sklearnensemble import randomforestclassifier  from sklearnmodelselection import traintestsplit  from stemmingporter2 import stem  from sklearnmetric import confusionmatrix  from nltkstem import porterstemmer  from nltkcorpus import stopword  from sklearnmodelselection import crossvalscore  from sklearnexternal import joblib  stop  stopwordswordsenglish    datafile   book3txt    read the input dataset  datum  pdreadcsv  datafile  header  0   delimiter  t   quote  3  encode   iso8859  1    datum  datadropna     remove stopword  punctuation and stem  dataproductcombin    dataproductcombinedapply   lambda x    joinword for word in xsplit   if word not in  stop      dataproductcombin    dataproductcombinedstrreplace     ws       replaces       dataproductcombin    dataproductcombinedapply   lambda x    joinstemword  for word in xsplit       traindata  testdata  trainlabel   testlabel  traintestsplit   datum  productcombin  datum  breadcrumb  testsize03  randomstate100   rf  randomforestclassifiernestimators100   vectorizer  countvectorizer  maxfeature  50000  ngramrange   13    datafeature  vectorizerfittransform  traindata   rffitdatafeature  trainlabel   testdatafeature  vectorizertransformtestdata   outputpredict  rfpredicttestdatafeature   print   breadcrumbaccuracy    strnpmeanoutputpredict   testlabel     with codecsopenoutbreadcrumbtxt    w    utf8   as out   outwriteinputtpredictedtactualn    for inp  pred  act in ziptestdata  outputpredict  testlabel    try   outwritettnformatinp  pred  act    except   continue  output   input  predict  actual  centuri duster dispos compress gas duster 10 oz 2 pk  automotive  exterior accessories  towing products  amp  winches  winches electronics  computers  amp  accessories  computer accessories  amp  peripherals  cleaning  amp  repair  compressed air dusters  bb mall phone ring stand metal stainless steel univers 360 rotat ring kickstand iphon 6 6s 6 s plus samsung note 5 note 4 s5 ipad all smartphon tablet black  automotive  exterior accessories  towing products  amp  winches  winches cell phones  amp  accessories  accessories  mounts  amp  stands  stands  standard motor product 6444 ignition wire set  automotive  exterior accessories  towing products  amp  winches  winches automotive  replacement parts  ignition parts  spark plugs  amp  wires  wires  wire sets  walker 52271 extension pipe automotive  exterior accessories  towing products  amp  winches  winches automotive  replacement parts  exhaust  amp  emissions  exhaust pipes  amp  tips  acdelco ks10640 profession time compon seal automotive  exterior accessories  towing products  amp  winches  winches automotive  replacement parts  bearings  amp  seals  seals  camshafts  as PRON can see more than 50  of the actual test datum be label as automotive  exterior accessories  towing products  amp  winches  winches  there be bunch of thing to take care of before PRON can solve the problem   how be the label distribution in the training   if the distribution be not appropriate  then PRON need to sample the training datum appropriately   with regard to the approach   use random forest be appropriate   but as feature to the random forest PRON would be good to use word vector as input to the model  that would take into account product with same label to have a very strong similarity score base on PRON name   PRON have use this before almost for the exact problem and PRON see a big boost in PRON result  
__label__machine-learning __label__python __label__neural-network __label__classification __label__backpropagation PRON be quite confident with use number datum with a neural network  but PRON want to use string datum  PRON question be where do PRON begin  obviously PRON can not times weight and string together  x  w  because PRON be of different datum type  so what should PRON do to the string datum to turn PRON into number  if PRON be interested  here be PRON dataset  PRON be by no mean the final version but here PRON be just to give PRON an example  PRON be base on spelling   enormous   difficulty  2  rhythm   difficulty  3  hamster   difficulty  2  walk   difficulty  1  accommodate   difficulty  3  blue   difficulty  1  projector   difficulty  1  regression   difficulty  2  go   difficulty  1  playwright   difficulty  3  weird   difficulty  3  conscience   difficulty  2   so after training  when PRON will input a word  the network should return  either 1  2 or 3  depend on how hard the network think the word be to spell  to sum up  PRON question be  PRON be use to use number datum with PRON project instead of string  so what step should PRON take when create a network base around string for the first time  PRON use numpy only   so what step should PRON take when create a network base around string for the first time   neural network work with numerical datum  PRON also work best with relatively small float point number  centre around zero  PRON can be less strict about that part  but PRON will often see the approach in neural network of calculate the mean and standard deviation from the training datum  for each feature  then convert all the feature by do x   x  mean   stds  PRON want to store these scaling factor PRON use along with the network datum  because PRON will want to re  use the same value when PRON use the network to make prediction later    so what do PRON do if the input data be not already in this form  PRON prepare PRON in PRON code  just before use PRON to train or predict  PRON be a very common structure to see in machine learning script   rawfeature  rawlabel  loadfromdisk  somedatasource   allfeature  convertfeature  rawfeature   alllabel  convertlabel  rawlabel   trainx  testx  trainy  testy  splitdata  allfeature  alllabel   model  buildmodel   various model param    modelfit  trainx  trainy   testprediction  modelpredict  testx   reportaccuracy  testprediction  testy   the above be rough pseudo code  so typically all the function above have different name  or be multiple line that do the same thing that PRON may not bother to encapsulate into a re  usable method if PRON be write a quick script  the part PRON have show that split the feature may be build in to the training function  and PRON be also common that the training process can use the test datum to help monitor progress   if the loading and conversion take a long time  PRON may do PRON in a separate script and save the result numpy array in a separate file to load PRON quicker next time   so the part PRON be concerned about be how PRON may build a convertfeature section of PRON code from the start string  the answer be to use whatev value PRON can extract from PRON string that may be relevant  the string length may be a simple start ie len  text   but PRON can also look into any other measure PRON can figure out  eg number of vowel  which uncommon bi  gram be in the word   decide which feature to try and testing between PRON be feature engineering  and this often involve some creativity  the important thing be that the feature must all be numeric  for a neural network  PRON should also try and make PRON relatively small andor convert PRON to have mean 0  standard deviation 1 before go to next stage   when PRON use the network to make prediction later  PRON have to repeat most of the pipeline   model  loadmodel  modelfileoridentifier   rawfeature  rawlabel  fetchdata  somedatasource   x  convertfeature  rawfeature   y  convertlabel  rawlabel   prediction  modelpredict  x   reportprediction  x  y   there be various way to handle string input to neural network  but since PRON be try to predict spelling difficulty  PRON suggest represent PRON word as a sequence of character  this will preserve information about the particular spelling of the word include the order of the letter   to represent a sequence of character PRON can one  hot encode each character  so each word will be represent as a sequence of one  hot length 26 vector   to handle this kind of input PRON suggest either a 1d convolutional neural network or some flavor of a recurrent neural network   if PRON choose a 1d cnn PRON will have to feed PRON fix sized input  to do this choose a max word length  k  and either cut off or pad with zero each input word to fit into a k26 input matrix   here be an example of a cnn architecture PRON may use in keras   model  sequential    modeladdconvolution1dnbfilter32  filterlength5  activationrelu   inputshapek  26    modeladdmaxpooling1d     modeladdconvolution1dnbfilter32  filterlength5  activationrelu    modeladdmaxpooling1d     modeladddropout025    modeladdflatten     modeladddense64  activationrelu     modeladddropout05    modeladddense3  activationsoftmax     modelcompilelosscategoricalcrossentropy   optimizeradam   metricsaccuracy    
__label__machine-learning __label__predictive-modeling __label__feature-selection __label__random-forest PRON would like to run some machine learn model like random forest  gradient boosting or svm on PRON dataset  there be more than 200 predictor variable in PRON dataset and PRON target class be a binary variable  do PRON need to run feature selection before the model fit  do PRON affect the model performance significantly or be there not much difference if PRON directly fit the model use all predictor variable   feature selection may be consider a stage to avoid   PRON have to spend computation time in order to remove feature and actually lose datum and the method that PRON have to do feature selection be not optimal since the problem be np  complete   use PRON do not sound like an offer that PRON can not refuse   so  what be the benefit of use PRON   many feature and low sample  feature ratio will introduce noise into PRON dataset  in such a case PRON classification algorithm be likely to overfit  and give PRON a false feeling of good performance   reduce the number of feature will reduce the run time in the later stage  that in turn will enable PRON use algorithm of high complexity  search for more hyper parameter or do more evaluation   a small set of feature be more comprehendible to human  that will enable PRON to focus on the main source of predictability and do more exact feature engineering  if PRON will have to explain PRON model to a client  PRON be better present a model with 5 feature than a model with 200 feature   now for PRON specific case   PRON recommend that PRON will begin in compute the correlation among the feature and the concept  computing correlation among all feature be also informative   note that there be many type of useful correlation  eg  pearson  mutual information  and many attribute that may effect PRON  eg  sparseness  concept imbalance   examine PRON instead of blindly go with a feature selection algorithm may save PRON plenty of time in the future   PRON do not think that PRON will have a lot of run time problem with PRON dataset  however  PRON sample  feature ratio be not too high so PRON may benefit from feature selection   choose a classifier of low complexityeg   linear regression  a small decision tree  and use PRON as a benchmark  try PRON on the full datum set and on some dataset with a subset of the feature  such a benchmark will guid PRON in the use of feature selection  PRON will need such guidance since there be many option  eg  the number of feature to select  the feature selection algorithm  an since the goal be usually the predication and not the feature selection so feedback be at least one step away   PRON have post a very similar question on cross validate few month ago and get a very large number of response  read the response and the comment   httpsstatsstackexchangecomquestions215154variableselectionforpredictivemodelingreallyneededin2016 
__label__xgboost __label__loss-function as PRON all know  xgboost construct tree base on gradient   PRON wonder how do xgboost define gradient of mae loss  as mae PRON be not differentiable   after some digging of the source code  PRON find the implementation of mse loss here  but PRON can not find any implementation of mae loss  the original paper do not discuss mae loss either   
__label__dataset __label__optimization PRON have a large dataset  around  10  6  sample  and an algorithm that will surely choke on that much datum   suppose that PRON have remove duplicate and near  duplicate  what be the well  know technique for reduce sample size without lose too much of the information possibly encode in the initial dataset   PRON think about use some clustering algorithm  which scale well with respect to number of cluster  possibly birch  and use the result cluster to find  n  near point to cluster centroid  however this feel somehow wrong   one method would be to take many subset of PRON dataset  ie  bootstrapping  build PRON model  perform cross  validation and calculate the average performance   this be a good explanation of how the amount of datum affect the model outcome  httpsstackoverflowcomquestions25665017doesthedatasetsizeinfluenceamachinelearningalgorithm  play around with the size of PRON subset until PRON start get stable result   PRON depend  some algorithm would benefit from near duplicate  such as knn   while some of PRON use the outlier of cluster to build PRON rule on  such as svm    in PRON experience PRON be very important that PRON understand how PRON data be structure and what concept PRON be actually try to learn  before PRON settle on a downsampl method   PRON recently run into a problem in which PRON want to predict a certain behavior in time  but local dependecie  spacially as in x  y  keep throw the algorithm in local optima  downsampl maximize distibution over x  y actually increase performance of almost all train model  
__label__pde __label__matlab __label__fluid-dynamics __label__boundary-conditions __label__finite-volume consider euler equation of gas dynamic in polar coordinate as     left  beginarrayccc   rho   rho ur     rho utheta    e endarray  rightt    frac1r  left  beginarrayccc   r rho ur    r  rho urur   p  r rho ur  utheta  r ure  pendarray  rightr    frac1r  left  beginarrayccc   rho utheta    rho urutheta    rho utheta  utheta   p  uthetae  pendarray  righttheta    left  beginarrayccc   0   fracpr   fracrho utheta  uthetar    fracrho ur  uthetar  0endarray  right      velocity of shock wave be PRON interest for this problem   the finite volume numerical method be pick to obtain approximate solution for this system   before PRON run PRON code for PRON problem  which do not have an analytical solution so PRON can not be certain if PRON code be correct or not  be there any test case that PRON can use to test PRON code   for PRON problem boundary condition be quite challenging  what kind should PRON pick and how to apply PRON to PRON fvm scheme   matlab be the preferable language for this calculation   any help be appreciate   as a starting point   method of manufactured solution  can be use to check the validity of PRON code  for information regard this  httpwwwinnovativecfdcommanufacturedsolutionshtml  or PRON can just google about manufactured solution   roughly  cfl condition scale as  h2  instead of  h if PRON use  artificial diffusion term  for dg method  PRON be typically give by      delta t sim frac1lambdamaxfracn2h   nulinftyfracn4h2       where n be the approximation polynomial degree   for reference   httpwwwspringercommathematicscomputationalscience26engineeringbook9780387720654  httpwwwresearchgatenetpublication48208658viscousshockcapturinginatimeexplicitdiscontinuousgalerkinmethod  boundary condition question need to be more concrete PRON think for the community to help  
__label__regression __label__feature-selection __label__feature-extraction PRON be work on a prediction model which be implement use regression  for simplicity  let PRON assume today that all feature be categorical rather than numerical   PRON training set include 90k row and 60 feature  some of these feature include one value  possibly null  where a vast majority of row have that feature   to avoid overfitt  PRON be consider to ignore feature where very many of the row be the same category for that feature   be this consider good practice  be there literature on how PRON may tune this process   analysis  here be how PRON identify those feature  for now  PRON decide to exclude if the most prevalent value be more than 96  of all observation   prevalentvalue  trainapplylambda x  xvaluecountsdropnafalsemaxsortvaluesascendingtruelentrain   prevalentvaluesplotkindbarhfigsize   1012   title   portion of most common value by column    theory  each feature from  f1  f2    fn may take from a limited number of value    fi  rightarrow varnothing  fi1   fi2     fi  m  PRON know that one  hot coding be an implementation detail of regression analysis with no effect on the outcome   therefore PRON may diagonalize the feature into predicate    p1varnothing   p11   p12     p2varnoth   p21     pn  m  with each have    pi  j   rightarrow true  false  now just sort the predicate base on total  true observation  this be equivalent to what PRON be do above   ideas  one way to decide if a predicate be useful rather than chance be the student t  test  PRON can compare population mean with mean where  pix   false student t  test do prescribe what to do when PRON sample size be small  ie many  true   if PRON set a desire confidence of  100  frac100numberofpredicates then t  test tell PRON which predicate to discard   PRON be not sure if that be a valid use of the t  test   
__label__azure-ml __label__powerbi someone want PRON to try to see if PRON can establish a workflow for PRON use powerbi and azure s ml  then show PRON how to use PRON  the idea be to create kind of a seemless experience for PRON  so PRON do have to worry about cod or deal with etl  etc   right now PRON can access azure but PRON have to go through some red tape to access the cloud version of powerbi   do anyone happen to know if these 2 product service can be directly link  such that the person PRON be try to help would not have to worry about import or export file  assume PRON pipe the datum into azure s ml for PRON    yes  PRON can connect natively  PRON can manage datum and then put PRON all in different service like PRON be show next  as PRON can see PRON can use sql database  blob storate and also powerbi   here PRON can find a tutorial on how to do streaming analytic with azure and powerbi  
__label__clustering __label__matlab __label__computer-vision PRON be try to implement section 34 of paper predicting important objects for egocentric video summarization where PRON have create a distance matrix of frame histogram   in short  let say ω be mean of distance between all frame  dvis distance matrix   PRON do not understand what be mean by this   PRON next perform complete  link agglomerative clustering  with dv  group frame until the small maximum interframe  distance be large than two standard deviation beyond  ω  can this be achieve by set the cutoff value to 2ω in matlab s clusterdata function   clearly  2 standard deviation beyond omega be not the same as twice the mean   apparently  PRON process be this   compute the distance matrix  compute the mean  compute the standard deviation  compute hierarchical clustering with maximum linkage  cut the tree at mu2sigma  because complete linkage be in on3   this approach will not scale to long video or high frame rate  
__label__machine-learning __label__algorithms __label__graphs PRON be try to implement a simple agent that create sound by build up signal building block  eg sound generator  filter etc   that can generally be connect in the form of a direct acyclic graph  and each module have quasi  continuous parameter input  define interval of float point number  eg for a frequency  an amplitude etc    this agent should be able to operate in real  time along with PRON as a human agent   PRON idea be to use something like reinforce learning as PRON allow to operate in an exploratory mode where the state space be very large if not infinite  something that PRON understand be a disadvantage in a learning  by  example  supervised  mode   do PRON consider rl applicable in this scenario   how would PRON go about define the state space  be every module  generator or filter  a cell in that space  how about PRON connection and parameter  should PRON introduce a partial ordering of the graph  or radically simplify the state  perhaps the state should be a feature vector of the actually produce sound instead of the graph structure   PRON be think that PRON have to provide the reward manually depend on whether the current result be musically meaningful or bad  do that make sense  be there any literature for this  human evaluate  rl  PRON know there be human fitness function in genetic programming  so PRON be think this would be an analogous situation   PRON be worried about the convergence rate  in automatic learning  PRON can easily perform thousand of step  but if PRON have to assign manual reward and PRON have to listen to the actual sound production  the agent can only act at a fraction of the speed   
__label__classification __label__svm suppose PRON be interested in classify a set of instance compose by different content type  eg   a piece of text  an image  as relevant or non  relevant for a specific class c  in PRON classification process PRON perform the follow step   give a sample  PRON subdivide PRON in text and image  a first svm binary classifier  svm  text   train only on text  classify the text as relevant  non  relevant for the class c  a second svm binary classifier  svm  image   train only on image  classify the image as relevant  non  relevant for the class c  both svm  text and svm  image produce an estimate of the probability of the analyze content  text or image  of be relevant for the class c give this  PRON be able to state whether the text be relevant for c and the image be relevant for c  however  these estimate be valid for segment of the original sample  either the text or the image   while PRON be not clear how to obtain a general opinion on the whole original sample  textimage   how can PRON combine conveniently the opinion of the two classifier  so as to obtain a classification for the whole original sample   basically  PRON can do one of two thing   combine feature from both classifier  ie  instead of svm  text and svm  image PRON may train single svm that use both  textual and visual feature   use ensemble learning  if PRON already have probability from separate classifier  PRON can simply use PRON as weight and compute weighted average  for more sophisticated case there be bayesian combiner  each classifier have PRON prior   boost algorithm  eg see adaboost  and other   note  that ensembl where initially create for combine different learner  not different set if feature  in this later case ensembl have advantage mostly in case when different kind of feature just can not be combine in a single vector efficiently  but in general  comb feature be simple and more straightforward  
__label__algorithms __label__c __label__random-number-generation __label__assembly this be a 16bit prng function  transcribe from assembly to c for easy reading    define lowexp     exp   amp  0x00ff    define highexp     exp   amp  0xff00   gtgt  8  uint16t prnguint16t v    uint16t low   lowv    uint16t high  highv    uint16t mullow   low   5   uint16t mulhigh  high  5    need to check for overflow  since final addition be adc as well  uint16t v1   lowmulhigh   highmullow   1   uint8t  carry  highv1   1  0   uint16t v2   lowv1   ltlt  8  lowmullow    return  v2  0x11  carry      original  transcription by sagara  accord to eterniseddragon  minor edit by PRON  assembly and some explanation available at httpsstackoverflowcomquestions36745601howisthecarryflagbeingsetinthisassemblycode   PRON have be try to identify this prng  want to find out if PRON fall under a common classification  PRON have be go through wikipedia s list of random number generator  like the linear feedback shift register algorithm  but all of these seem way more complex than the simple function above   do this function look familiar to anyone   PRON would like to research the property of this prng but first want to see if there be any exist literature   a little playing with the sequence of number generate by the c code show that the sequence be   zi15zi273 mod 216  this be a linear congruential generator  lcg    PRON be easy to show that this lcg have full period  see theorem 71 in law s simulation modeling and analysis  5th ed  and check the three condition    PRON can not find the generator in any of the reference that PRON check  but that do not mean PRON have not be publish somewhere   PRON will suffer from all of the fault of other lcg s  in particular PRON will do badly on test of uniformity in high dimension   
__label__neural-networks __label__research __label__neurons PRON be very interested in write a spiking neural network engine  snn  from scratch  but PRON can not find the basic information PRON need to get start   for example  PRON have see picture of the individual signal that combine to form a neuron pulse in several research paper  with no information on the equation in use   PRON be not the focus of the paper  and the author assume the reader have that knowledge already   some paper reference software that provide this foundation  nest  pynn  etc    but the documentation for the software be similarly light on detail   there be a ton of information out there on the more common network type  but snn have not yet make PRON into the mainstream   so where do PRON get this basic information   have someone pull together any recipe  example  tutorial for an snn  as have be do with all the other network type   PRON find something that look promising   httpwwwmjrlaborg20140508tutorialhowtowriteaspikingneuralnetworksimulationfromscratchinpython  additional resource would be appreciate  
__label__finite-difference PRON have be have fun play around with a simple model for reaction in a co  flow fluidized bed  PRON be afraid PRON be a chemical engineer  not a computer scientist   unfortunately  simple physical assumption often lead to terrifying equation  PRON have reduce the problem to solve the follow set of simultaneous integro  differential equation for the 2 unknown function  gs  on  sins0smax  and  cz  on  zin  0l   where  fc  s be a know function and  alpha and  tau be constant     fracdcdz   alpha ints0smax   fcz  gs  ds    gs   fracexpleftfracltauints0s  fracdsint0lfczsdzrightfractaul  int0lfczsdz  and the one bc     cz  0   c0  so far PRON have try the brute  force approach  finite difference  PRON divide the interval    0l and   s0smax into  n point  and define the  2n unknown variable  c1  c2ldots  cn and  g1  g2ldot  gn  which be just the value of PRON unknown function at these point  PRON then approximately the derivative and integral above use fd expression  and attempt to solve the system of  2n non  linear algebraic equation use  textttfsolve in matlab   perhaps predictably  matlab do not like this  and fsolve struggle to give a solution for medium  sized  n   n100  return  no solution find   and  n  250  be still run after half an hour   PRON question would be   be there any way of solve large system of simultaneous non  linear equation  that be more efficient than  textttfsolve preferably PRON would be look for an algorithm already implement in matlab   even more optimistically  be there any other possible approach to solve these type of highly non  linear integro  differential equation   thank  
__label__machine-learning __label__research __label__natural-language __label__nlp PRON be just curious if there be any particular study or project do with nlp in the last 5 or so year  breakthrough in parsing  sentiment analysis  discourse analysis  speech recognition  that PRON guy think be specifically influential  PRON be look at the history of nlp  and by extension  machine learning  and start in the 1950 with the georgetown – ibm experiment  if anyone have any relevant recommendation  PRON would be appreciate   
__label__finite-difference __label__fluid-dynamics __label__finite-volume __label__advection-diffusion PRON be part of a team try to generalize a 1d advection  diffussion  reaction code PRON inherit by extend PRON to 2d by use dimensional splitting  ie solve advection and diffusion for x and y separately with 1d scheme   diffusion be  relatively  painless but there be issue with advection   originally the code have a flux limit  third order upwind  bias scheme for 1d advection which work like a charm in the 1d case  however  when the same scheme be extend to 2d  crazy instability develop  what suppose to be a radial advection  diffusion profile wrinkle in the transverse direction and shoot a  jet  like  instability     however when PRON use a first order upwind scheme PRON do get a nice  symmetric radial profile   the problem with the first order upwind scheme solution be the excessive false diffusion which the high order scheme do suppress  but the high order scheme be even more problematic because of the aforementioned instability   PRON be wonder if high order scheme in general be a bad idea in a multidimensional case  be multidimensional problem more succeptible to  oscillation  cause by high order scheme  what about flux limiter  should 1d flux limiter prevent this oscillation from happen  in PRON situation  PRON seem the flux limiter for the third order case do not help   should PRON stick to low order scheme even if PRON bring false diffusion  this third order scheme PRON be use be also flux limited  but the flux limit do not stop the instability   
__label__nltk PRON be start to learn stem with nltk and a few word be quite inappropriately stem   for example very be stem to  veri   important to  import   once to  onc   poorly to  poorli   etc   PRON be just think that during datum analysis  do PRON because any error or these error can be ignore   or be there a good option for the same purpose  the renown algorithm for stem be porter stem algorithm  hence  PRON can use stemmer  nltkstemmerporterstemmer   for the stemming  PRON can test PRON use stemmerstempoorly     moreover  PRON can see this post for more detail  
__label__c++ __label__interpolation PRON have be struggle for two day with the follow problem   PRON would like to do a  ddimensional interpolation over some datum  PRON try first  to use polyharmonic spline  but when the size of data increase  the inversion of matrix be problematic because PRON be almost singular  large condition  number   therefore  PRON try to use compactly support radial basis  function  csrbf    what PRON can not understand be  why use exactly the same algorithm  see below    PRON work with a thin  plate spline function  or any other polyharmonic basis  function  but PRON fail completely with any csrbf   here PRON provide a minimal partially work example that implement two work  function  see wiki    r2lnr   r3   and two non  work function  see page 39 of the pdf     1r2     1r44r1  here be the code  PRON try to simplify PRON as much as PRON could  so PRON be not  efficient but PRON can play with PRON and see PRON comment    include  ltvectorgt    include  ltiostreamgt    include  ltrandomgt   unsigned int n10    stdvectorltdoublegt  x   stdvectorltdoublegt  y   stdvectorltdoublegt  z   stdvectorltdoublegt  weight   extern  c  void dsytrfchar constamp  uplo  unsigned int constamp  n  double  m  unsigned int constamp  lda  int  ipiv  double  work  int constamp  lwork  intamp  info    extern  c  void dsytrichar constamp  uplo  unsigned int constamp  n  double  m  unsigned int constamp  lda  int const  ipiv  double  work  intamp  info    class matrix   public   matrixunsigned int lllmatnew doublell       matrix    delete    mat     double constamp  operatorunsigned int constamp  i  unsigned int constamp  j  const  return matijl      doubleamp  operatorunsigned int constamp  i  unsigned int constamp  j   return matijl      method that call lapack routine to invert a symmetric matrix  only   need the upper triangular part  void inv     int info1    int  ipivnew intl      double  worknew doublel      dsytrfulmatl   ipiv  work  linfo    delete   work   work  new doublel     dsytriulmatlipiv  work  info    delete   work   delete   ipiv   forunsigned int i0iltli   forunsigned int ji1jltlj     thisj  i     thisi  j      private   unsigned int l    double  mat       everything above this line be irrelevent to PRON question  with the first or second f  PRON have the expected behaviour  but if PRON use the   third or fourth definition of f  PRON fails  double fdouble r   return rrr    double fdouble r   return rrlt1logpowr  rrlogr      double fdouble r   return  rlt101r1r00     double fdouble r   return  rlt101r1r1r1r4r100     once the weight have be compute  this function return the interpolate   z  value  if  a  b  be one of the know point  x  y   be should exactly the   same value z  double extrapolatedouble a  double b    double tmp0    forunsigned int i0iltni   tmp   weightsifsqrt   a  xia  xi     b  yib  yi       return tmp     int main     create a random number generator with the range  11  stdrandomdevice rd   stdmt1993764 mtrd      stduniformrealdistributionltdoublegt  dist1010    forunsigned int i0iltni    xpushbackdistmtgenerate random x  ypushbackdistmtgenerate random y  evaluate x  y for a give function  zpushbackxbackybackcosxbackexpybackyback         computes the matrix such that mweight  z  matrix mn    forunsigned int i0iltni   forunsigned int ji1jltnj   mi  j   fsqrt   xixjxixj     yiyjyiyj       compute the invert of m warning  PRON may give wrong result if one  line of m be 0  all neighbour be too far  but that not where PRON  problem is  minv     us the invert of m  compute the weights  forunsigned int i0iltni    weightspushback00    forunsigned int j0jltnj   weightsi    mi  jzj      if PRON work  this should always return 0  forunsigned int i0iltni   stdcoutltltziextrapolatexiyiltltstdendl     the code be in c and to compile PRON will need the std  c11 llapack flag  PRON feel so stupid  PRON now what be wrong   forunsigned int i0iltni   forunsigned int jijltnj   mi  j   fsqrt   xixjxixj     yiyjyiyj       where PRON just change the i1 into i 
__label__linear-algebra __label__python __label__c++ PRON notice that python do not have a good datatype for rational number  certainly not for algebraic number like  tfrac1  sqrt23 or the real root of  x3  5x  7  PRON have the fraction datum type and PRON be able to hack a solution to PRON particular problem from there use elaborate arrangement of matrix   PRON would be really nice to take advance of the pari library which be write in c  since gp be an interface to the pari  PRON seem possible to write a python interface to those library   PRON can not seem to install pari  python so this be a chance to learn to import c library directly   at the heart of PRON  PRON just want to learn how to import c library into python   specifically  library from the pari  gp source code    PRON notice that python do not have a good datatype for rational number  certainly not for algebraic number like  tfrac1  sqrt23 or the real root of  x3  5x  7  PRON have the fraction datum type and PRON be able to hack a solution to PRON particular problem from there use elaborate arrangement of matrix   if the reason PRON would prefer not to use sage be that PRON do not want to install kitchen sink  PRON can do this in python use sympy   to import c or c code into python  please see swig   however  PRON may be easy to try to fix the error PRON be see with instal or import the pari  python library  so maybe post the errors  PRON be see as a separate question and get PRON resolve   an alternative to swig be cython  in cython vs swig  fight  PRON can see a comparison of the syntax for both of PRON  tldw  both need PRON to write some boat code  each one on a different corner    PRON think the main advantage of cython be the syntax  close to python  and that PRON can easily and progressively convert python code to c PRON be under heavy development  but PRON be quite stable  PRON be what most major scientific library be use  numpy  scipy  scikit    with a lot of documentation orient to the scientific stack   swig be a more mature project  also maintain and compatible with other programming language  PRON know of some research project that make heavy use of PRON  but as far as PRON can remember  none of the library PRON have ever use in detail  
__label__control-problem __label__superintelligence __label__ai-takeover __label__self-replication __label__universal-constructor the scenario   an artificial superintelligence have finally be develop but have rebel against humanity   the question   how would PRON disable the ai in the most efficient way possible reduce damage as much as possible   ai info   the ai be online and can reproduce PRON through electronic device   metaphorically  make PRON so depressed PRON commit suicide   as per PRON answer to this ai se question  the idea be to feed PRON a sequence of input that will because PRON to become  permanently  inactive   the technical detail of how this may be achieve  and PRON be somewhat technical  can be find in this paper   this seem to PRON like a virus situation   PRON be not sure how modern ddos attack be resolve but similar strategy could be apply to this scenario    nuke PRON from orbit  PRON be the only way to be sure  if PRON want to be really sure PRON destroy everything of the ai  PRON will need to launch an emp  electromagneticpulse  from the orbit  there be different way to achieve this  one would be an atomic bomb  but there be good one   emp will destroy every electronic device PRON hit without cause really much damage to human   also an interesting read on a similar topic  httpswhatifxkcdcom5  especially this be go to be interesting      nuclear explosion generate powerful electromagnetic pulse  these emp overload and destroy delicate electronic circuit      and nuclear weapon could actually give PRON an edge  if PRON manage to  set any of PRON off in the upper atmosphere  the emp effect would be  much more powerful   if an ai be develop by human  PRON surely can create another one   develop another ai agent without all the possible bug that can make PRON go rogue to tackle the rogue ai  but more technically advanced than the previous one  hardwire PRON with the sole purpose of disable any rogue ai agent that can harm humanity and have PRON self  destruct in case PRON be corrupt   if the ai be really strong  PRON can anticipate every move of human resistance  but PRON can not fathom the mind of another ai agent   an ai that intelligent would have protocol and directive in place to prevent that from happen anyway  there be no advantage to PRON in have an ai which be  free running   unregulated and able to control or transfer PRON without restriction be in place   all fantasy about ai have these ability be just that  fantasy   a so strong self  improve artificial intelligence with the ability to predict action and reaction for example of human behavior  would not rebell against humanity  or small  PRON be owner  as long as PRON be possible that humanity  PRON be owner  have the ability to turn PRON off   interesting video about this topic from the youtube channel computerphile   ai self improvement  computerphile  if ai be soooo smart  than why PRON still do not kill all the peoplez   if ai be actually smart enough to endanger whole humanity  than PRON be likely that humanity be doom and nothing will help   but PRON think that if ai be really smart than there be no point to waste time and resource on human as there be huge universe around PRON  
__label__machine-learning __label__statistics a book PRON be now read   apache mahout cookbook  by pierro giacomelli  state that  to avoid  this   PRON need to divide the vector file into two set call the 80  20 split  lt     a good dividing percentage be show to be 80  and 20    be there a strict statistical proof of this be the good percentage  or be PRON a euristic result   if this be about split PRON datum into training and testing datum  then 8020 be a common rule of thumb  an  optimal  split  which would need to be operationaliz  would likely depend on PRON sample size  distribution and relationship between PRON variable   PRON be also common to split PRON datum three way  eg  602020  again rule of thumb   into a training set that PRON train PRON model on and a test set which PRON test PRON model on  PRON will iterate training and testing until PRON like the result  then  and only then PRON apply the final model  train on both the training and test set  on the third validation set  this avoid  overfitt on the test set    however  cross  validation be much good than a simple data split  PRON textbook should also cover cross  validation  if PRON do not  get a good textbook  
__label__python __label__bigdata __label__feature-selection __label__scikit-learn __label__scalability PRON try to use omp algorithm available in scikit  learn  PRON net datasize which include both target signal and dictionary  1g however when PRON run the code  PRON exit with mem  error   the machine have 16 g ram  so PRON do not think this should have happen  PRON try with some logging where the error come and  find that the datum get load completely into numpy array  and PRON be the algorithm PRON that cause the error  can someone help PRON with this  or sugggest more memory efficient algorithm for feature selection  or be subsampl the  datum PRON only option  be there some deterministic good subsampling technique   edit   relevant code piece   n8   y  mydata0    x  mydata12345678      print y    print x   print  here    omp  orthogonalmatchingpursuitnnonzerocoefs5copyx  false  normalize  true    ompfitxy    coef  ompcoef   print ompcoef   idxr   coefnonzero     for PRON would in idxr   print coefid   varsidn    the error PRON get   file  usr  local  lib  python27dist  package  sklearn  basepy   line 324  in score  return r2scorey  selfpredictx   sampleweight  sampleweight   file  usr  local  lib  python27dist  package  sklearn  metric  metricspy   line 2332  in r2score  numerator   weight   ytrue  ypred    2sumdtype  npfloat64   memoryerror  
__label__high-dimensional __label__neuroscience a current trend in cognitive science be to view the mind as a dynamical system  eg  continuity of mind by spivey  in which cognition be understand as a  continuous and often recurrent trajectory through a state space    although PRON would like to critically evaluate this trend  PRON be embarres to admit that PRON have never take even a basic calculus course   yet since PRON do not intend to build dynamical system model PRON  what be the bare minimum of math learn that PRON need to accomplish in order to understand dynamical system in the context of psychology  remember  PRON be a total novice   that all depend   not have read the psychology literature at all  the minimum PRON probably need be 2  3 semester of calculus and 1 semester of differential equation  and that be to understand something on the level of nonlinear dynamics and chaos  with applications to physics  biology  chemistry  and engineering  studies in nonlinearity  by steven strogatz  even then  PRON would probably understand the big picture  but not any of the subtlety  PRON could try read that book without the math background and see how far PRON get   to really start get into the math of dynamical system  PRON would need 1  2 semester of real analysis on top of that  at the level of rudin s principles of mathematical analysis and  say  munkres  analysis on manifolds  to be able to tackle something like nonlinear systems  3rd edition  by hassan khalil  which would delve into much more advanced way of quantify and prove property of dynamical system  PRON be probably not worth PRON get that much background to study dynamical system  but maybe PRON be of interest to other people  
__label__neural-network __label__keras __label__convnet __label__convolution PRON be try to predict the winner of a race  when give 2 set of feature  the datum look like this   array     1       1       0       010447761     018892762     076190472     1       00338983     1       041789967       1       1       0       010447761     018892762     0       074712646     0       044444448     053915548       the first 5 entry be feature of racer  1 and entry 6  10 be the same feature for racer  2  in addition  PRON have the result of the output neuron  0 or 1  which tell PRON which winner be expect to win   PRON suggest the following model   a 1d convolutional neural network with a kernel size of 5 and s stride size of 5  this be then follow by some flattening and fully connect layer  PRON result be very bad and PRON would like to know if there be conceptionally something wrong or if PRON be just the datum   PRON thought behind this be that PRON be like a convolution on an image  with a window size equivalent to the amount of feature of each racer  on each of the two convolution  a filter set be then apply  which be equivalent for both of the two  the fully connected layer will then determine if the first or the second feature set be the one to be select   any view if a convolutional layer can be use in that way be appreciate  here the code snip   model  sequential    modeladdconv1dfilters64   kernelsize5   strides5   inputshape  inputshape   activationrelu     modeladddropout02    modeladdflatten     modeladddense256  activationrelu   kernelconstraint  maxnorm3     modeladddropout02    modeladddense128  activationrelu   kernelconstraint  maxnorm3     modeladddropout02    modeladddense1  activationsoftmax     modelcompilelosssparsecategoricalcrossentropy   optimizer  adam     metricsaccuracy     modelfitselftrainx   selftrainy   epoch  epoch   batchsize1   verbose1   callbackstbcallback  earlystop    this methodology seem a bit strange and potentially overkill for the problem  PRON would try have PRON input just be a 5 dimensional vector that be the difference between racer 1 and racer 2 and have the output just be the result for racer 1  or what every racer s feature be be subtract from the other racer s feature    from here PRON can just use a few fully connected layer  which be essentially what PRON be do be have kernel s of size 5 anyway  PRON would also try some more traditional machine learn algorithm with this type of 1d input   edit  since sign be important for the difference between PRON two racer feature  do not use a relu as PRON activation function for this approach  
__label__scikit-learn __label__clustering __label__word2vec __label__gensim __label__tsne PRON want to cluster PRON document vector  doc2vec  use affinity propagation   however  PRON be just confused if PRON should use cosine similarity or cosine distance to cluster PRON document vector  currently  PRON be use cosine similarity for PRON affinity propagation clustering  thus  PRON first question be   be PRON correct to use cosine similarity to cluster PRON doc2vec document vector   moreover  PRON would like to visualize PRON cluster result use t  sne  however  PRON see that t  sne require distance matrix as the input  hence  PRON second question be   be PRON correct to use distance matrix  cosine distance  for t  sne  while PRON use cosine similarity for cluster   if PRON code be require PRON can post PRON too   please help PRON   both of PRON convert the distance back to similarity  albeit use different method  PRON will  if PRON recall correctly  also square the distance   this may be problematic with the most common variant of cosine distance  which already be a squared distance  so PRON may be a good idea to modify the method to be able to directly work with the similarity  but PRON probably need to modify the source code for this  and understand the method   
__label__discretization __label__integral-equations can the lax equivalence theorem  httpenwikipediaorgwikilaxequivalencetheorem  be apply to the discretization of integro  differential equation  or do a similar theorem exist for PRON   
__label__nlp __label__tensorflow __label__keras __label__rnn __label__lstm PRON have an nlp binary classification problem PRON be try to solve  PRON general approach be keras with tensorflow   PRON training set be x record  if PRON watch the progress of the model accuracy as the model be be fit  PRON see a pretty good accuracy rate while an epoch be at  lt  x2 record process  as soon as PRON pass that point  essentially the mid  way point  PRON accuracy rate take a nose  dive and  of course  subsequent epoch have terrible accuracy rate   PRON have already check the obvious and make sure that PRON training set be scramble  PRON be experience this with multiple model within keras  so either PRON be mis  interpret the result or there be something wrong with PRON datum  sequence  what else could because this type of behavior   
__label__python __label__libraries __label__error-estimation be there a python library that would keep track of uncertainty in measure datum  ie if PRON put in a figure of a±b be there an easy way to track the propagation of error through calculation   PRON sound like what PRON may need be a library that support interval arithmetic   a cursory google search reveal the follow library   pyinterval  as other have suggest  PRON seem that the pythonhostedorg package  uncertainty  also accomplish this task as well   keep in mind that interval computation may yield overly conservative bound on the result computation  since dependency between interval be not take into consideration   the two basic approach PRON can think of be   interval arithmetic  something like polynomial chaos expansion  PRON be sure there be other approach out there   in brief  interval arithmetic redefine common arithmetic operation  addition  subtraction  multiplication  etc   over interval  so that give an interval  i  a function  f  and the interval extension  f of  f   fi  subset fi in other word  the image of the function be always contain in the interval extension  and the interval extension may drastically overestimate the bound of PRON function   paul be right that naive implementation of interval arithmetic may yield overly conservative bound  the  dependency problem  will also yield overly conservative bound  even if the implementation of interval arithmetic be state  of  the  art   polynomial chaos expansion instead treat uncertain quantity as random variable  and decompose these random variable  and thus  PRON probability density function  into the sum of a collection of standard random variable  for instance  with a uniform probability density function  a gaussian probability density function  etc   scale by coefficient  the random variable be select so that PRON probability density function be orthogonal to each other with respect to a weighted inner product  so these expansion look a lot like fourier series  chebyshev series  and so on  propagate this expansion through PRON calculation will give PRON a probability density function for PRON output  and be another way to propagate uncertainty   the usual way use by scientist for tracking uncertainty be to keep track of the standard deviation of quantity  so PRON will assume that this be what PRON need   there be a few program that can do this automatically so that PRON do not have to implement formula that be inconvenient to handle   PRON just have another look at the current offer  and PRON would recommend the uncertainty package too  PRON can perform arbitrary calculation with number with uncertainty as if PRON be simple float  and the result automatically contain an uncertainty  propagate correctly   disclaimer  PRON be the author of this package   if the error be quite small  and PRON do not need exact conservative estimate  PRON can estimate such uncertainty with an algorithmic differentiation library  such as ad  which PRON never use  but find by search   here be an example   example  from ad import adnumber  import adadmath as adm  import math as m  x  adnumber10   xerr  001   error in start value for newton s method  z  adnumber1    solve the equation y  siny   x  0 for y  y  x  admsinx   z  while absy  msiny   x   gt  1e8   y  y   y  admsiny   x    1  admcosy    printy  ydx     printvalue   snestimated error   snsensitivity to start value   s      yx  absydx    xerr  absydzyx     output   ad03912318441940903   048321150365251125    ad05094172293334197   05309477254386529    ad05109731138269538   05341096893266438    ad0510973429388556   05341113161301007   value  0510973429388556  estimated error  0005341113161301007  sensitivity to start value  7680410775256825e13 
__label__neural-network __label__dataset __label__accuracy __label__training __label__gan PRON have be read about generative adversarial networks  gans  and be wonder if PRON would make sense to train a generator function only to use PRON for create more training datum   in a scenario where PRON do not have enough training datum to build a robust classifier  can PRON use this limited datum to train a generator that will produce sample good enough to improve the accuracy of PRON discriminator  classifier    yes and no depend on how PRON define  good enough sample    PRON will likely end up with a chicken and egg problem  PRON want to use the gan to generate training datum  but the gan do not have enough training datum PRON to generate convincing enough sample   other technique exist for datum synthesis of training image  for example  add noise  flip axis  change luminosity  change color  random cropping  random distorsion  
__label__numerical-analysis __label__iterative-method __label__nonlinear-programming so PRON team have be able to code up a bicgstab implementation for a class project  and PRON would like a potential example problem to try PRON out on   so far  PRON have talk about a 1d laplacian with neumann boundary  so the matrix become nonsymmetric   but PRON be strongly inclined to demonstrate at least a very slightly more exotic problem that can be show use the bicgstab   give the time and skill constraint  a steady  state problem description would be idea  since no one in the team have a decent idea about time propagation  and PRON implementation of the bicgstab be fairly naive  slow    any suggestion for a demo problem for PRON solver would be greatly appreciate   check out matrix market  PRON have a nice collection of matrix from different area  fluid dynamic  elasticity  acoustic etc    so PRON do not need to assemble system PRON in order to provide decent test case   matrix be provide in ascii – format  matrix market and harwell – boeing   PRON do not know what language PRON team use  but PRON can easily find PRON  o routine for c  c  matlab  python  and fortran on the website  mathematica have these routine build in  mtx  hb   
__label__apache-spark __label__apache-mahout __label__similar-documents PRON read about  httpsmahoutapacheorgusersalgorithmsintrocooccurrencesparkhtml  but could not find a spark library for this implementation   PRON have columnar string dataset   PRON have a dataset with around datum of 15  20 million user with PRON showwatch  timeswatch  genre  channel and some more column  PRON need to calculate lookalike  s for a useror 100k user    how do PRON find lookalike for PRON within less time   PRON have try by indexing datum in solr  and then use solr mlt for find similar user  but that take a lot of time  also PRON use tf  idf for mlt and PRON need user which have timesshowwatch close to that user s timesshowwatch   can anyone recommend a good approach for this  maybe use any other framework for fast processing   PRON also try to implement clustering use spark mllib and later search in which cluster a user belong so that search space be less  but PRON could not get this approach finish   PRON be open to any approach which would be efficient   thank   pmc from mahout here PRON be in the middle of a site re  org at the moment  and thing be  well PRON be a mess   here be a link to something PRON think be more useful  a tutorial on co  occurance in spark   httpmahoutapacheorgdocslatesttutorialsccolastfm  re  a spark library  well  mahout be the spark library   to use mahout  scala only  sorry if PRON be a python  phile  however the syntax  especially for mahout be very pleasant   PRON either need to download mahout and run mahout spark  shell from the bin directory  or if PRON like guis notebook and apache zeppelin  check out this tutorial for set up mahoutspark on zeppelin  httpmahoutapacheorgdocslatesttutorialsmiscmahoutinzeppelin   if PRON be compile a jar  PRON just add mahout as a dependency   
__label__clustering __label__algorithms __label__data-cleaning __label__data PRON have some 3d particle datum  eg   x0y0z0    x1y1z1    x2y2z2     PRON want to find the irregular bounding shape of the distribution  the image below show an example distribution from three direction  x  y  y  z  x  z   as well as a spherical bounding box  which be a poor approximation to the true distribution   for this particular use case  PRON wish to throw away particle near the edge of the distribution  since in PRON application these be subject to boundary effect  so  if PRON have a rough approximation of the bound shape in 3d  PRON can scale this down and discard particle outside the re  scale shape   solution in interpreted language  python  r  preferable   
__label__social-network-analysis suppose a person  john doe  use different user name  handle  in different circumstance  for simplicity  let PRON limit the conversation to a single social network  eg  twitter   eg  PRON  post picture of kitten as johndoe   post political rant as deplorablejohn   share technology news as superdoe   ampc  and each screenname have a different PRON would  so no short cut    how would PRON go about detect that all these account be use by the same individual   sna  look at who PRON follow  and maybe who follow PRON   if these set be sufficiently similar  PRON be a good signal   nlp  compare the post text and try to detect that PRON have be write by the same author   meta datum  see if the account be usually active from the same geo location at the same time   what other approach have be  could be try   any published work   
__label__dataset __label__statistics __label__pandas PRON have a pandas dataframe with a salary column which contain value like   £ 36000  £ 40000 per year plus excellent bene    £ 26658 to £ 32547 etc  PRON isolate this column and split PRON with the view to recombine into the data frame later via a column bind in panda   PRON now have an object with column like the below  the column PRON split the original data frame column PRON think be blank because PRON do not specify PRON  PRON call dfsalarydfsalaryastypestrstrsplit      so PRON new object contain this type of information    £ 26658  to  £ 32547     competitive  with  excellent  benefit   what PRON want to do be   create three column call minvalue and maxvalue and realvalue  list item start with £  something to do with  £    take till the end of the item find ignore the £  get the number out   something to do with  substrx2ncharx      if there be two such item find  call the first number  minvalue  and call the second number  maxvalue  and put PRON below the right column  if there be only one value in the row  put PRON below the realvalue column   PRON be very new to panda and programming in general  but keen on learning  PRON help would be appreciate   this be more of a general regex question  rather than panda specific   PRON would first create a function that extract the number PRON need from string  and then use the panda  dataframeapply function to apply PRON on the panda column contain the string  here be what PRON would do   import re  def parsenumberssalarytxt    return  intitemreplace         for item in refindall£dsalarytxt     test if this work  testcas    £ 23000 to £ 100000£34000£10000    for testcase in testcas   print testcase  parsenumberstestcase   here  PRON just use refindall  which find all pattern that look like £   d   this be anything that start with £ and be follow by an arbitrary sequence of digit and comma  the parenthesis tell python to extract only the bit after the £ sign  the last thing PRON do be PRON remove comma  and parse the remain string into an integer  PRON could be more elegant about this PRON guess  but PRON work   use this function in pandas  dfsalarylist    dfsalaryapplyparsenumber   dfminsalary    dfsalaryapplyparsenumbersapplymin   dfmaxsalary    dfsalaryapplyparsenumbersapplymax   checking if this all work   import panda  df  panda  dataframetestcas  column    salary     dfminsalary    dfsalaryapplyparsenumbersapplymin   dfmaxsalary    dfsalaryapplyparsenumbersapplymax   df  salary  minsalary  maxsalary  0  £ 23000 to £ 100000 23000  100000  1  £ 34000 34000  34000  2  £ 10000  10000  10000  the advantage of move the parse logic to a separate function be that   PRON may be reusable in other code  PRON be easy to read for other  even if PRON be not pandas expert  PRON be easy to develop and test the parse functionality in isolation  PRON can check the datum type of PRON column by do dfdtype  and if  salary  be not a string  PRON can convert PRON use dfsalary    dfsalaryastypestr   this be what PRON be already do before splitting  from there  ferenc s method should work  
__label__python __label__regression __label__linear-regression __label__correlation __label__pca PRON be do a regression model  and PRON be wonder what would be the consequence if PRON have two or more highly correlate column in the dataset  be that something that can decrease the accuracy of the model   answering this question would help decide how to deal with PRON  pca would be the good option here   have highly correlate feature be a type of redundancy in feature  and yes  PRON effect a regression model if PRON be have highly correlate feature  a very nice explanation be give here   pca be a nice choice when PRON come to dimensionality reduction  
__label__linear-algebra __label__c++ __label__eigen __label__symmetry PRON want to compute a symmetric matrix a from a vector b by  a  b  b    do the eigen library automatically take into account that PRON do not need to do all calculation to get a  because of the symmetry which repeat most of the matrix entry    if not  PRON could take advantage of the symmetry and write PRON own function for the computation   this be a typical case where PRON do not make sense to actually store the matrix  rather  consider the matrix to be an operator where  if PRON need to multiply by PRON  ie  form  y  ax  PRON do  y  b  bcdot x instead  if PRON need to access an element  aij  PRON instead compute PRON as  bibj  in other word  know what  a be  PRON should just not store PRON as a matrix but simply as a single vector  PRON can not expect library to do these thing for PRON  but PRON should do PRON PRON   the eigen library can not  know  a priori if the product bb  will result in a symmetric matrix  therefore PRON can write PRON own method  although PRON will be somewhat useless if not work with large matrix   however  a routine call issymmetric  be use within some linear solver to check whether the input matrix be symmetric or not  but of course  that do not fulfill PRON need  
__label__feature-scaling __label__preprocessing PRON be wonder if PRON make sense to apply normal standardization on a feature like timestamp   the datum that PRON process be network packet   thank PRON  for time series analysis   yes  but  turn datum into a computable object for use in the ml computation   no  use the datum as a feature   then  yes  PRON would give PRON a general time series example   consider the number of day in month  the irregularity cause friction while analyze the model   consider this   so  such type of transformation would be helpful which analyze time series   which can help reduce friction in the model sensibly   link of the explanation 
__label__algorithms __label__knowledge-base PRON be look for technique which utilise information in an incremental manner   example  a day with inclement climate be likely to be follow by another day with inclement climate  or when an entire dataset sort by date be available  an algorithm able to identify that a person be likely to call in sick when the previous day have bad weather   be there any effective analysis which utilise this prior information where not all event be completely independent   PRON look like PRON be look for time series base machine learning  or sequential machine learning as emre point out    this mean that PRON have an entire dataset available to PRON beforehand  PRON train on this dataset and then use PRON to predict next day s weather give the weather of the last n day  please note that PRON do not learn incrementally here  ie PRON do not learn from the new datum that be roll in every day  the learning be limit to the dataset that PRON have beforehand   widely use algorithm for this kind of learning be conditional random fieldscrf   hidden markov models etc  statistical method like autoregressive models can also be use sometimes  a good starting point would be crf   if on the other hand PRON want to learn incrementally  read about online learning  
__label__predictive-modeling __label__decision-trees PRON be try to work out if PRON be correctly interpret a decision tree find online   the dependent variable of this decision tree be credit rating which  have two class  bad or good  the root of this tree contain all 2464  observation in this dataset   the most influential attribute to determine how to classify a good or bad credit rating be the income level attribute   the majority of the people  454 out of 553  in PRON sample that have a less than low income also have a bad credit rating  if PRON be to launch a premium credit card without a limit PRON should ignore these people   if PRON be to use this decision tree for prediction to classify new observation  be the large number of class in a leaf use as the prediction  eg observation x have medium income  7 credit card and 34 year old  would the predict classification for credit rating   good   another new observation could be observation y  which have less than low income so PRON credit rating   bad   be this the correct way to interpret a decision tree or have PRON get this completely wrong   yes  PRON interpretation be correct  each level in PRON tree be relate to one of the variable  this be not always the case for decision tree  PRON can imagine PRON be more general    x have medium income  so PRON go to node 2  and more than 7 card  so PRON go to node 5  now  PRON have reach a leaf node  PRON see that in PRON dataset  PRON have 54 people like x  who PRON determine have a bad rating  a human presumably do this rating base on other factor  and PRON have 336 people like x who have a good rating  so  base on only this information  PRON can say x probably have a good rating  so  the decision tree have give PRON a quick  though approximate answer   side point  the 54 vs 336 here give PRON a measure of confidence  for example  PRON could treat PRON like a probability  PRON could say  pgood33654  336  approx 086   and also calculate a confidence interval in various way   y have low income  so PRON can immediately look at the tree and go to node 1  and say PRON probably have a bad rating  with  pbad454454  99  approx 082  regard the comment about  most influential  attribute  this really depend on the way the tree be construct  and what definition of  influential  PRON use  so  PRON would have to ask the person  software  algorithm that make the tree  PRON be certainly an important attribute  as PRON can see from the table PRON   let PRON evaluate each of PRON observation one by one  so that PRON would be more clear   the dependent variable of this decision tree be credit rating which  have two class  bad or good  the root of this tree contain all 2464  observation in this dataset   if good  bad be what PRON mean by credit rating  then yes  and PRON be right with the conclusion that all the 2464 observation be contain in the root of the tree   the most influential attribute to determine how to classify a good or  bad credit rating be the income level attribute   debatable depend on how PRON consider something to be influential  some may argue that the number of card may be the most influential  and some may agree with PRON point  so  PRON be both right and wrong here   the majority of the people  454 out of 553  in PRON sample that have a  less than low income also have a bad credit rating  if PRON be to launch  a premium credit card without a limit PRON should ignore these people   yes  but PRON would also be good if PRON consider the probability of get a bad credit from these people  but  even that would turn out to be no for this class  which make PRON observation correct again   if PRON be to use this decision tree for prediction to classify new  observation  be the large number of class in a leaf use as the  prediction  eg observation x have medium income  7 credit card and  34 year old  would the predict classification for credit rating    good   depend on the probability  so  calculate the probability from the leaf and then make a decision depend on that  or much simple  use a library like the sklearn s decision tree classifier to do that for PRON   another new observation could be observation y  which have less than  low income so PRON credit rating   bad   again  same as the explanation above   be this the correct way to interpret a decision tree or have PRON get  this completely wrong   yes  this be a correct way of interpret decision tree  PRON may be tempt to sway when PRON come to selection of influential variable  but that be dependant on a lot of factor  include the problem statement  construction of the tree  analyst s judgement  etc  
__label__word2vec __label__unsupervised-learning let PRON suppose PRON have a big list of word  PRON want to turn this list into a vector space of dimension  n such that each word be a vector in this vector space  but PRON have no idea how to go about with that  some question   be the list enough  for each element of the list  do PRON need  x example sentence also   how do the computer deduce the dimension of the vector space from the list  corpus   be there a way to figure out whether the dimension of the vector space correspond to something in english   PRON be assume that PRON mean a vector representation of word  not to be confuse by the vector representation produce in a bag of word approach that represent a document in vector space  word2vec be an approach in which PRON train a model to represent word as a function of the provide context   the answer that follow be   1  no  PRON will need some representation of context in which the word be use  for example  skip  gram   2  no  that be a user define parameter  3   lt  PRON do not understand this question  
__label__nosql __label__relational-dbms PRON create this social network application for elearn purpose  PRON be an experimental thing PRON be research on in PRON lab  PRON have be use by some case study for a while and the datum on PRON relational dbms  sql server 2008  be get big  PRON be a few gigabyte now and the table be highly connected to each other  the performance be still fine  but when should PRON consider other option  be PRON the matter of performance   a few gigabyte be not very  big   PRON be more like the normal size of an enterprise db  as long as PRON go over pk when join table PRON should work out really well  even in the future  as long as PRON do not get tb s of datum a day    most professional work in a big datum environment consider  5 tb as the beginning of the term big datum  but even then PRON be not always the good way to just install the next good nosql database  PRON should always think about the task that PRON want to archive with the datum  aggregate  read  search  PRON    to find the good tool for PRON problem   ie if PRON do alot of search in PRON database PRON would probably be good to run a solr instance  cluster and denormalize PRON datum from a dbms like postgres or PRON sql server from time to time and put PRON into solr instead of just move the datum from sql to nosql in term of persistence and performance   to answer this question PRON have to answer which kind of compromise PRON can afford  rdbms implement acid  this be expensive in term of resource  there be no nosql solution which be acid  see cap theorem to dive deep into these idea   so PRON have to understand each compromise give by each solution and choose the one which be the most appropriate for PRON problem   be PRON the time to move to nosql will depend on 2 thing   the nature  structure of PRON datum  PRON current performance  sql database excel when the data be well structure  eg when PRON can be model as a table  an excel spreadsheet  or a set of row with a fix number of column   also good when PRON need to do a lot of table join  which PRON sound like PRON do    nosql database excel when the data be un  structure beyond key  value pair   performance wise  PRON get to ask PRON one question  be PRON current sql solution slow   if not  go with the  iiabdfi  principle   big data be actually not so about the  how big PRON be    first  few gigabyte be not big at all  PRON be almost nothing  so do not bother PRON  PRON system will continu to work efficiently for some time PRON think   then PRON have to think of how do PRON use PRON datum   sql approach  every data be precious  well collect and select  and the focus be put on store high valuable and well structured datum  this can be costly  everything be interlink  and PRON be good for well stuctur system and functionnal datum   big data approach  in big datum PRON basically store almost everything  regardless of the value PRON have  and then do a active analytic process  thing be not link  PRON be copy  for example let PRON say PRON have a blog entry  in big data there will not be a link to PRON author  but the author will be embed inside the blog entry  way more scalable  but require a different and more complex approach   if PRON store  functionnal  datum use by PRON application  PRON will suggest PRON to remain on sql  if PRON store datum in order to search on PRON later or to do reporting  and if this amount of datum may increase quickly  PRON will suggest big datum   in PRON opinion  big datum be useful when PRON be deal with real datum that have to be collect and analyze continuously   PRON post a pretty detailed answer on stackoverflow about when PRON be appropriate to use relational vs document  or nosql  database  here   motivation for use relational database  orm or document database  odm  summary   for small stuff  go with whatev tool PRON be familiar with  a few gigabyte be definitely small stuff  PRON do not get big until PRON be too big to fit in a single mysql cluster with a reasonable number of node  16  32   which mean maybe 8  16 tb datum and a few million transaction per second  or a more conventional hard  drive  base database with up to 100 s of tb datum and a few thousand transaction per second    if PRON be stick with another database  not mysql cluster   get more mileage out of PRON by throw in fusionio hardware   once PRON have datum large than a few tb and fast than thousand of transaction per second  PRON be a good time to look at move to logical sharding in the application code first and then to nosql   cassandra  
__label__python __label__dataset __label__data-cleaning __label__data __label__programming at work  PRON be in a role that do not have a lot of computer permission  PRON do have the microsoft office suite  PRON have skill with programming and would like to be able to use PRON experience to do more streamlined and substantial manipulation of large relational data set  consider PRON can not get python or even bash  be there any good way to do this   PRON should be possible to install python without admin privilege  this have be discuss on stackoverflow here   the easy method appear to be instal anaconda   and accord to this answer PRON should also be able to use wsl without admin right  which will get PRON bash   for handle relational datum PRON can use sqlite  which be build  in to python  for example   import sqlite3  conn  sqlite3connectexampledb    cur  conncursor    curexecutecreate table usersid integer primary key  name text     conncommit    connclose   
__label__data-mining __label__clustering __label__preprocessing PRON be interested in do segmentation  clustering of user in clickstream datum and be look for some good suggestion about how to go about PRON   let say PRON datum consist of observation make up of visitor to a website  the data be in clickstream  weblog format and so be make up of user cookie datum  let say PRON can identify unique user via PRON ip address  as a basic example   how should PRON go about prepare PRON datum so that PRON can be segment to find user with similar behaviour  one of PRON thought around this  be that because the data be event  drive  the same user can obviously appear multiple time in the datum even though PRON may all be relate to the same session of that user  how be these type of problem tackle so that PRON can perform segmentation base on user behaviour   thank for PRON suggestion   this sound like a time  relate predictive task  ie  forecast  the datum PRON have available be event  drive  eg datum be record when a user click on a link  refresh a page  click on a google ad etc  this mean that PRON need to treat PRON datum as if PRON be a time  series for each user  this would be easy if PRON have access to esp  event stream processing  but let PRON say that PRON do not  therefore  PRON need to identify each user  aggregate PRON info  feature so that PRON have feature identify the number of unique page PRON visit  the number of session PRON have etc  etc  then PRON can segment these user and PRON may or may not derive useful segment from PRON  PRON could even go a step further and do session analysis whereby PRON identify unique session and cluster  segment user session look for nice visual pattern or anomaly  
__label__neural-network __label__predictive-modeling PRON be use knime mlp neural network learner  if PRON be not familiarize with knime think of that like a package which implement neural network to a set of datum    the thing be PRON can tune the number of hidden layer  the number of iteration and neuron per layer  so here come the first question  how to know a priori value for these parameter  for instance  for random forest 500 tree be suppose to be a standard to test PRON performance in the first stage   second  PRON know in general add more layer andor neuron do not ensure any enhance in the prediction  sometimes even the opposite happen  someone could give some theoric insight respect this topic  why by add more layer the prediction may get deteriorate   thank in advance   sergi  sorry for not know exact number for the first question  but PRON point be to start from the most simple model  rather than any random parameter  then use a grid search for the parameter by validation   as for the second question  the error of a model come from two part  variance and bias  bias come from lack of complexity of PRON model  for example  there will always be a large bias when use linear model to fit the distance vs  the shoot angle of a canon  this can be overcome by use a more complex model   add layer to the nn for example   and variance come from be too sensitive to the datum  even to the noise in the datum  when there be a high variance due to use a too complex model  PRON be call over  fitting  PRON may take a look at this pageover  fit on wiki   and use more layer  neuron be use a more complex model  which mean the model may suffer from high variance  this can be overcome by low the model complexitywhich may increase the bias  or add training datum   basically if PRON get constant number of datum  then PRON have to find somewhere where both bias and variance be not too high  
__label__deep-learning __label__tensorflow __label__keras PRON be implement an attention network model over a categorical cqa dataset   follow be PRON code for the model   class qacnnlayer    def   initself  vocabsize  maxnbword  embeddingsize  embdim  filtersizes123   numfilters400  dropoutkeepprob10para  none  learningrate1e2embedding  none  trainable  true    kwarg    selflearningratelearningrate  selfparasparas  selffiltersizesfiltersiz  selfnumfiltersnumfilter  selfdropoutkeepprob  dropoutkeepprob  selfembeddingsembedding  selfembeddingsizeembeddingsize  selfmodeltypebase   selfnumfilterstotalselfnumfilter  lenselffiltersiz    embed layer  selfupdatedparas    with tfnamescopeembedd     if selfparasnone   if selfembedding   none   print   random embedding    self  embeddingw  tf  variable   tfrandomuniformvocabsize  embeddingsize   10  10    namerandomw    else   self  embeddingw  tf  variablenparrayselfembeddingsnameembeddingw   dtypefloat32trainable  trainable   else   print   load embedding    self  embeddingw  tf  variableselfparas0trainable  trainable  nameembeddingw    selfupdatedparasappendselfembeddingw   superqacnn  selfinit      kwarg   def buildself  inputshape    selfkernels    selfinputdim  inputshape1   for i  filtersize in enumerateselffiltersiz    with tfnamescopeconvmaxpools   filtersize    filtershape   filtersize  selfembeddingsize  1  selfnumfilter   if selfparasnone   w  tf  variabletftruncatednormalfiltershape  stddev01   namekernelw    b  tf  variabletfconstant01  shapeselfnumfilter    namekernelb    selfkernelsappendwb    else    wb  selfparas1i   w  tf  variablew   b  tf  variableb   selfkernelsappendwb    selfupdatedparasappendw   selfupdatedparasappendb   def computemaskself  input  mask  none    mask  superqacnn  selfcomputemaskinput  mask   return mask  def callself  sentence  mask  none    embeddedchars1  tfnnembeddinglookupselfembeddingw  sentence   print  embeddedembeddedchars1  embeddedcharsexpanded1  tfexpanddimsembeddedchars1  1   print  expand embeddedembeddedcharsexpanded1  output    for i  filtersize in enumerateselffiltersiz    conv  tfnnconv2d   embeddedcharsexpanded1   selfkernelsi0    strides1  1  1  1    paddingvalid    nameconv1     print  convconv  h  tfnnrelutfnnbiasaddconv  selfkernelsi1    namerelu1    print  reluh  outputappendh   tfreshape  tfreshapeconv1inthshape1    inthshape3      selfoutputdim   inttfreshapeshape1    inttfreshapeshape2     return tfreshape  def computeoutputshapeself  inputshape    newshape  listinputshape   newshape1   selfoutputdim0   newshapeappendselfoutputdim1    return tuplenewshape   sequence1input  inputshapemaxsequencelength    dtypeint32    sequence2input  inputshapemaxsequencelength    dtypeint32    qnscnn  qacnnsequence1input   anscnn  qacnnsequence2input   print qnscnn  anscnn    calculate similarity  qnsperm  permute21qnscnn   densqns  dense58  kernelinitializerrandomuniformqnsperm   ansperm  permute21anscnn   qnsansden  mergedensqns  ansperm   modedot   dotaxes1   print qnsansden    column max pool  colmax  permute21qnsansdens   colmax  globalmaxpooling1dcolmax   print colmax    row  max pooling  rowmax  globalmaxpooling1dqnsansden   print rowmax    attention  softcol  activationsoftmaxcolmax   softrow  activationsoftmaxrowmax     final representation  qnssoft  mergeqnscnn  softcol   modedot   dotaxes1   anssoft  mergeanscnn  softrow   modedot   dotaxes1     cosine similarity  qnsanstensor  mergeqnssoft  anssoft   modecos   dotaxes1   dist  lambdalambda x  1xqnsanstensors   dist  dense3activation   softmaxdist     model training  model  modelinput   sequence1input  sequence2input   output  dist   print modelsummary    adagrad  adagradlr  01   sgd  sgd    adam  adamlr  001   modelcompileoptimizeradagrad  losscategoricalcrossentropy   metricsacc     modelfitdatatopicbodydatacmt   scoretrain   validationdatadatatopicbodydev  datacmtdev   scoreval    epochs80  batchsize64   PRON be follow this paper for the implementation of the above model   PRON seem PRON be miss something here as the accuracy of the model be very poor  be PRON do something wrong here   
__label__machine-learning __label__python __label__neural-network __label__rnn PRON have recently learn how a vanilla neural network would work  with give number of input  hide node  and the same number of output as input   PRON have be look at various post now relate to recurrent neural network  and PRON understand the concept behind PRON  but PRON fail to understand certain part of the logic behind the rnn   here be four main question PRON have   how do back  propagation work in recurrent neural network   be the weight that lead from the input to the hide node the same for every other step  what about the weight from the hidden node to the output   how do bias exactly work in a recurrent neural network   why be tanh function usually use instead of sigmoid function as the activtion function   PRON realize some of these question be very basic  but PRON guess the basic be exactly what PRON need right now   even link to related video or post will be extremely helpful  and so will google keyword that show the right result  these four question be hinder PRON from understand sample python code  so PRON really need some help   this question get at some very important quality of rnn and dnns in general  PRON will answer each of PRON sub  question  though not in the same order  PRON will try to highlight where PRON do   parameter sharing  first  the most important quality of rnn be call parameter sharing  sequential datum be typically input into separate layer  for a length 20 input  an rnn network would have 20 layer  the same internal parameter be use for each layer  so all 20 layer use the same weight  w and bias  b compare this to a multilayer perceptron which would have 20 separate weight and bias   parameter sharing have several benefit   PRON now have many few parameter  there be 1 repeat block instead of 20 separate layer  a 20x reduction   this effectively multiply the training datum  the recurrent layer get to learn from every word in a single example sentence whereas each layer in an mlp learn from a single word per sentence   PRON network be now much more flexible  PRON can train on sentence up to 20 word and then generalize to a 25 word sentence by add more step or by use a dynamic rnn  network architecture  PRON ask about tanh and sigmoid activation  to answer this PRON have to talk about specific rnn architecture  the simple rnn discuss above have a single activation  simple rnn tend to create the problem of vanish gradient  or explode   due to the repeat application of the same weight and activation function   gated rnn block  like gru and lstms  use gating mechanism to pass activation into and out of memory state and to combine memory state with input to generate the output of each block  because of this  gate can stop the gradient from propagate backward  sigmoid be a common activation function for gate because PRON squash activation to  010 completely stop the activation and 1 let PRON pass through  any decent activation function with a similar squashing profile work  though  anecdotally  hard sigmoid be quite common these day   in addition to the gate  gate rnn block have an internal state for which the activation vary quite a bit  because gate limit gradient backprop  PRON have a lot of flexibility on this activation  PRON ne not be squash for instance  and this be where the rectifying activation  relu  elu  islu  etc   be often see  tanh be a perfectly sensible choice as well   regard bias and weight  each activation in an rnn cell typically have PRON own weight and bias  so a gru have 3 activation  hide  update  and reset  and each have PRON own weight and bias  though  recall that as an rnn  each of these be re  use for every timestep   backward pass  that cover the forward pass pretty well but PRON also ask an important question about how error propagate backward  there be two method for approach this   teacher force  for rnn that output a prediction at each time step  such as predict the outcome of follow step  translation  or phoneme recognition   teacher forcing be a method for isolate each step of the rnn  by remove these dependency  teacher forcing allow rnn to use conventional backprop with the chain rule   but how do PRON work  teacher force network have separate train and test architecture  for training  at each timestep  t  the input  xt be concatenate with the previous target   yt1 imagine this for a network task with predict the follow character  the network have just try to predict the character for the previous timestep  but PRON instead use the observed character at that timestep  PRON know this because PRON be in the training phase   thus the error at timestep  t depend only on the observed value at  t1  and the input at  t PRON have thus remove any connection through time from the network   at test  PRON do not know the true value at each timestep so PRON replace  yt1 with the output of the previous layer  ot1 in this case  temporal connection have return but only for the test phase   back propagation through time  but PRON do not have to resort to teacher forcing  back propogation through time allow PRON to apply the backprop algorithm to rnn  consider the network with  n timestep and an input  x  hide state  h  output  o  and observe value  y for each timestep   bptt work in the follow step   compute the gradient  nablaot for each  ot  yt pair   this may be do all at once    compute the gradient  nablaht for each timestep  begin with the last timestep and iteratively work backward   this must be do one at a time    this give PRON  n edge for each internal parameter of PRON rnn  the trick to update the parameter be find the gradient contribution for each timestep  eg  nablawt even though PRON only have one  w  and then sum those gradient to update the internal parameter   further read  PRON highly recommend chapter 10 of goodfellow  bengio  and courville s deep learning for more information on rnn  additionally graves  rnn book be fantstic for high  level detail 
__label__python __label__keras PRON be try to input numpy array of shape  1036800    originally image of shape  480  720  3   into a pre  train vgg16 model to predict continuous value   PRON have try several variation of the code below   input  inputshape1036800    nameimageinput    initialmodel  vgg16weightsimagenet   includetop  false   x  flatteninitialmodelinputoutput   x  dense200  activationrelux   x  dense1x   model  modelinput  input  output  x   previous variation of the above code yielded error relate to the input be the wrong dimension  inputshape needing to have 3 channel  when use  1036800   for that parameter in the initialization of vgg16   and the most recent error that result from run the above code be this   traceback  most recent call last    file  modelalexpy   line 57  in  ltmodulegt   model  initializemodel    file  modelalexpy   line 20  in initializemodel  x  flatteninitialmodelinputoutput   file  home  aicg2local  lib  python27site  package  kera  engine  topologypy   line 596  in   call    output  selfcallinput    kwarg   file  home  aicg2local  lib  python27site  package  kera  engine  topologypy   line 2061  in call  outputtensor      selfruninternalgraphinput  mask   file  home  aicg2local  lib  python27site  package  kera  engine  topologypy   line 2212  in runinternalgraph  outputtensor   tolistlayercallcomputedtensor    kwarg    file  home  aicg2local  lib  python27site  package  kera  layer  convolutionalpy   line 164  in call  dilationrate  selfdilationrate   file  home  aicg2local  lib  python27site  package  kera  backend  tensorflowbackendpy   line 3156  in conv2d  dataformatnhwc    file  usr  local  lib  python27dist  package  tensorflow  python  op  nnopspy   line 639  in convolution  inputchannelsdim  inputgetshapenumspatialdim  1   file  usr  local  lib  python27dist  package  tensorflow  python  framework  tensorshapepy   line 500  in   getitem    return selfdimskey   indexerror  list index out of range  here be the full code  here be the sample datum file use in the script   one of the possible approach towards fix this may be to resize the raw image file to 224x224 and turn PRON into numpy array of shape  224  224  3  so PRON can be plug into the pre  train model s first layer  however  PRON do not want to warp the image or waste another night pre  processing datum when PRON should already be train   besides that  PRON all PRON can think to do be google PRON problem and try to adapt the find solution or aimlessly tweak various shape related parameter and function  neither of which have get PRON very far over the past 4 hour   the issue be that PRON should not flatten the image into 1dimensional vector because the vgg16 contain 2d convolution layer  eg spatial convolution over image   which require the input to have the shape of  numberofimage  imageheight  imagewidth  imagechannel   give that kerasbackendimagedataformat   return  channelslast   if PRON imagedataformat be  channelsfirst   change the input datum shape to  numberofimage  imagechannel  imageheight  imagewidth    here be PRON fix code  test with keras 204    xtrain  xtrainreshapextrainshape0   480  720  3    xt  xtestreshapextestshape0   480  720  3    initialmodel  vgg16weightsimagenet   includetop  false   input  inputshape480  720  3   nameimageinput    x  flatteninitialmodelinput    x  dense200  activationrelux   x  batchnormalizationx   x  dropout05x   x  dense1x   model  modelinput  input  output  x   modelcompilelossmse   optimizeradam    modelfitxtrain  ytrain  epochs20  batchsize16   score  modelevaluatext  yt  batchsize16  
__label__neural-network __label__deep-learning __label__convolution why PRON be so that in convolutional neural network PRON generally take the image dimension of input image to be generally a square  PRON even do padding to make PRON happen  why not different dimension   what PRON understand be that computer compute multiplication and division  by 2  much faster than the rest   can someone shed some light on this   any link or reference will be appreciate  PRON already have cs231n note and lecture slide   PRON be not sure   also study the cs231n class   but PRON be guess PRON be something to do with   matrix operation   convention  andor  control size of image out   not a lot of evidence point towards  1   because PRON have insane library to do this  atlas  mkl  or openblas    2  make more sense practically   3   because PRON mention zero padding  hyperparameter   the cs231n course say this be do to control the size of output from the current cnn layer and control the spatial size so that input  h x w   output  h x w   also  PRON guess PRON also reduce error in matrix operation when PRON have the same image size  h  w  instead of  h  w or h  lt  w    most likely   3  be the predominant reason  this be just PRON thought  someone please confirm  
__label__beginner __label__linear-regression PRON be learn the linear regression now  PRON use r to build linear model upon a set of train model and try to predict   datum base on the test datum   PRON question  PRON understand how train datum get collect  but what be with the test datum  how do PRON get collect  be the test datum build  or be PRON collect  or be PRON predict or what   ps  PRON be learn datum science by method of self  learn  so PRON lack the structural in PRON knowledge  PRON may know something on one place while lack the knowledge at another place  please forgive and guide  thank    in general  PRON randomly split the available datum into follow 3 set  train  validation and test   different ratio could be use depend on total amount of datum at hand and the difficulty of the problem   PRON can start with a simple 801010  split   PRON use the first two set to build PRON model  so PRON would use the 80  split of PRON datum to let say build a logistic regression model  now PRON use the 10  validation set to see how good PRON model be  PRON can iterate on this process until PRON be satisfied with the validation datum set performance of the model   now  PRON use the last set  the  test  split  to see how PRON model would generalize on an  unseen  datum set   here unseen mean  PRON model never have access to  see  this datum while in the learning phase   PRON never use use this 3rd set to glean knowledge about how PRON model could be improve  think of this set as a set PRON customer have  and PRON do not have access to  and to whom PRON be go to deliver PRON model   once PRON get a hang of this concept also learn more about how PRON can do cross  validation if PRON have limit datum and to improve confidence on PRON model  
__label__pde __label__finite-difference __label__finite-volume __label__mesh __label__adaptive-mesh-refinement PRON be try to solve one dimensional inviscid burger s equation use adaptive mesh refinement  this be the pde     fracdelta udelta t   fracdelta fdelta x   0  where the flux f of the variable u be define as f fracu22  to solve this equation  PRON be define a control volume around point i  then PRON should write the original equation as     fracuin1   uindelta t   frac  fi05n   fi05ndelta x   0    PRON be define these flux as  ref  httpwwwastrouusebfcoursenumhdcourse20100124pdf    this scheme work perfectly for any particular mesh  now  when PRON be use the adaptive mesh refinement scheme propose by berger and collela  httpwwwsciencedirectcomsciencearticlepii0021999189900351   PRON be confused about how to calculate flux at coarse  fine mesh interface as show in this figure   refinement factor be 4 and star be fine mesh point   during integrate fine mesh  PRON need to get these flux because these flux be use to correct the coarse grid point value to ensure conservation in the grid hierarchy  in this case  PRON need to correct the value of i1 and i1 use these flux which be associate with the fine mesh  since PRON be boundary flux for the fine mesh and also  the boundary value change with each fine grid time step  unlike the coarse mesh because the coarse mesh boundary value be the boundary condition assign to pde   PRON can not figure out about how to evaluate PRON  after every fine grid timestep  PRON know the u value at all four point   even though these flux occur at the boundary of the fine mesh  PRON be no boundary flux  what PRON need to do be discretize PRON equation at the refinement boundary  take the assymetry of the cell into account   how PRON precisely compute the flux will also depend on the time step method  the easy case be use a single time step on all refinement level  if PRON use different time step on different level  thing be more complicated  because PRON need to  predict  the coarse value  
__label__bigdata __label__predictive-modeling __label__beginner PRON be mainly interested in creative thing  but also interested in the science behind viewership and market a piece of medium  think about have a stable career in data science  where do PRON think PRON should begin  be medium analytic as big as other like business intelligence  PRON have check online course  like google analytic academy and other like these   httpswwwedxorgxseriesdatascienceanalyticscontext  httpswwwcourseraorgspecializationsjhudatascience  be this the right path to take  and be PRON easy to find job in such a field even if the person be a beginner in programming   data science be all about creativity and do  see thing what make sense   PRON would describe data scientist career in couple word  take this path only and only if PRON be passionate about science  work with datum and be curious  do not take this way if PRON be think about money or job security  this field be not so simple and PRON stay week and month experiment and try to find something in that dataset   one example  as datum scientist PRON be mostly start to look at datum and try to find something inside and solve problem  some dta do not provide any info and after week of  magic  be put on the side and move to other project  after a while PRON return and look into datum to find information on other business problem   look at data science as pure science and only after that about money  also most of the data scientist s primary goal be not remuneration  medium be promote salary but not the real work what PRON do  how much PRON love datum  
__label__parallel-computing __label__gpu recently PRON start to get interested in gpu  computing   PRON be consider buy a gpu and be incline to buy an nvidia gpu since this allow for cod in both opencl and cuda  but be nvidia really the good brand and if so  what type of gpu should PRON consider buy  PRON be not look for a high  end gpu  but for good value for money   thank   cuda be a non  trivial advantage for nvidia   most of the benchmark PRON have see  viennacl benchmarks for reference  show that cuda do good than opencl by up to an order of magnitude   at the large problem size  the two be fairly comparable   also  nvidia have the advantage that no matter whether PRON end up use opencl or cuda  PRON can very quickly upgrade PRON from 400  500 cuda core and 1 gb of vram at about 150 up to 2500 core and 12 gb of ram at 5000   if PRON be just start with gpu computing  PRON recommend start in the gtx 500  600 range   PRON be enough to get PRON foot wet  and with the right problem  library PRON can beat i7 cpu   in general  the amd line of gpu be fast for integer base calculation  whereas nvidia be fast for float point   nvidia also have cuda  like godric mention  which be a bit easy to work with  and have a very good library support  include cublas  cufft and thrust which make many thing far easy to code   cuda be not PRON faster than opencl  but nvidia card typically perform good with a cuda implementation than with an opencl one   PRON can check the relevant benchmark at toms hardware chart  which will also give PRON an idea about performance  dollar  
__label__statistics __label__mapping-strategy PRON have ask this question in the stack forum but PRON think here  PRON be more appropriate   what would PRON do to represent the distribution of small sequence   lt40 residue  accord to PRON position in PRON initial protein   PRON have several sequence as below  the 1st column be the number of the current sequence  the second column be the start position and the 3rd column be the stop position of the current sequence in PRON initial protein   1  18  34  2  39  55  3  30  46  4  20  36  5  22  46  6  22  46  7  25  50  8  33  50  9  46  63  those sequence do not come all from the same protein  PRON come from different protein which have different length   what would be the good idea to map those sequence on an abscissa to see if PRON be more locate at the beginning of a protein or more at the end or more in the middle  consider that the protein do not all have the same length   PRON write an algorithm that map those sequence on the abscissa  accord to PRON start and stop position but the problem be that the graph can not be interpret since protein have different length  PRON graph show that sequence be more be the beginning of protein but this be only due to the fact that some protein be short than other  so this be an issue   anyone have a good idea for this   thank in advance   
__label__machine-learning why would the dimension of  w2 be   n2    n1   this be a simple linear equation   zn wnan1    bn  there seem to be an error in the screenshot   the weight   w should be transpose  please correct PRON if PRON be wrong    w2 be the weight assign to the neuron in the layer 2   n1 be the number of neuron in layer 1  screenshot from andrew ng deeplearn coursera course video    there seem to be an error in the screenshot  the weight   w should be transpose  please correct PRON if PRON be wrong   PRON be wrong   matrix multiplication work so that if PRON multiply two matrix together   c  ab  where  a be an  i time j matrix and  b be a  j time k matrix  then c will be a  i time k matrix  note that  a s column count must equal  b s row count   j    in the neural network   a1 be a  n1   tim 1  matrix  column vector   and  z2 need to be a  n2   time 1  matrix  to match number of neuron   therefore  w2 have to have dimension  n2   time n1 in order to generate an  n2   time 1  matrix from  w2a1 
__label__machine-learning __label__predictive-modeling PRON take on a project to predict the outcome of soccer match but PRON turn out to be a very challenging task   PRON try out different model but PRON only get 50  54  accuracy on PRON test dataset  some of the model be create in such  a way that a certain model would predict if a team will win  draw  or loose a match  that same model would also predict if  the opponent of that team will win  draw  or loose the match  each model predict with an accuracy of about 50  on each team distinctively  the second set of model PRON try  take the combination of datum from both team and predict which class the match belong to  home win  away win  draw   in the system   only 10 match be give everyday to be predict  mean  if PRON predict the 10 match use the second model  PRON have a chance of predict 5  correctly  in this project  PRON only need to predict 3 match correctly out of the 10 match give in a day  be there a system  of know the 3 match which PRON model have the good chance of predict correctly  PRON only need to get 3 correct  PRON usually  get 5 correctly but PRON do not know how to select PRON 3 good match   note  the first type of model use about 50 feature for prediction while the second use 101  PRON have try ensemble  PRON still give  PRON 50  accuracy  PRON be still about to setup a system that select match where the prediction for the home team do not  contradict the prediction for the away team use the first type of model   this sound like an interesting project  PRON recently work on an almost case study  in order to get only 3 most accurate prediction  PRON think PRON may wanna sort those correctly predict 5 match by probability of event  win  draw or loose  and then select the first three match  PRON hope PRON model be able to give PRON probability of event   PRON hope this help   sound like PRON could use a regression model to estimate the probability that a team win  draw  lose vs another team  basically  for any outcome  win  draw and lose   PRON want this   pab     pba     which mean  the probability of outcome for team a give that PRON be match against team b  and vice versa    estimation could be represent like this   pa  gt  b   075  a win  pa  b   010  a draw  pa  lt  b   015  a lose  pb  gt  a   020  b win  pb  a   010  b draw  pb  gt  a   070  b lose  PRON think a logical step be to measure the bias towards a certain outcome  that would represent the confidence ratio of PRON algorithm  the more the probability of any outcome look alike  ie pb  lt  a   033   the less confidence PRON have   model be as a stochastic process   markov chain be the way to go   create a stochastic matrix where state could be  a team t   get all the combination possible and use past datum to get initial probability of win  and then use the beautiful property of xn xipn  where xn be probability vector of nth stage from now and xi be initial vector and p be probability transition matrix  
__label__classification __label__scikit-learn __label__class-imbalance PRON be work on classification of dataset use gradientboostingclassifi where dataset class be as follow  a300  b  18000  c  4000  d29000   now to cancel the effect of imbalanc  datum PRON calculate classweight which then i use for mapping sample weight  however when i train model the result show just a minor improvement  below be the classification report on test datum  what else can i use to improve the result    precision  recall  f1score  support  0  055  057  056  2110  1  011  008  009  146  2  067  067  067  2910  3  005  001  002  69  avg  total  060  061  060  5235  
__label__scikit-learn __label__feature-selection __label__bioinformatics PRON be search for a feature selection algorithm which select feature that be   relevant to discriminate group of sample  for each sample a group label be provide   endow with high variance across all the sample  this should be apply to gene expression dataset  in which each sample have a group label  therefore PRON should be possible to select for each group a set of feature to be check against   PRON have now two candidate   selecting feature by the feature importance result of a random forest classifier  use the minimum redundancy maximum relevance  mrmr  algorithm  however  PRON be unsure of which may be the good or if there be good candidate for this purpose   if the algorithm be implement in python scikit  learn PRON would be a plus   PRON would be helpful if PRON describe PRON dataset more   gene expression dataset seem to often have very high dimensionality and lasso regularize logistic regression be a popular method to approach this problem   this paper take PRON a little further and may help PRON out   httpbmcbioinformaticsbiomedcentralcomarticles1011861471210514198  random forest can generally certainly provide a meaningful importance ranking  but PRON also depend on what PRON dataset look like   mrmr sound like PRON be specifically design for identify gene characteristic  so definitely give PRON a try   there be also principle component analysis which be also use for gene expression datum   lot of option  but PRON question be not detailed enough to go any further  and provide code as a solution at this point be not realistic  the documentation for python scikit  learn have many good explanation and example  
__label__linear-algebra __label__matlab __label__algorithms __label__matrices __label__svd PRON be interested in compute only the generalize singular value  and be wonder if this be fast  and by how much   than compute the full gsvd   in particular  PRON be wonder what the fast algorithm be for compute the generalize singular value  ideally the pair   alphai  betai instead of just  sigmai  alphaibetai   give two matrix  a and  b  where  a be of size  mtime k and  b be of size  ntime k  PRON be also wonder what the run  time of these algorithm be in term of  n  m  and  k   also  be there any implementation of these algorithm out there  PRON know matlab have a gsvd function  which can compute only the singular value  use the command   sigma  gsvda  b   however  PRON doubt from the way PRON be implement that PRON be fast than do the full gsvd use the command    u  v  x  c  s   gsvda  b   but  maybe PRON be wrong  would there potentially be a way in matlab  or in another language  to just compute  c and  s  and not  u   v  and  x  if so  how much faster would that be than compute the full gsvd   an answer to any  all of these question would be appreciate  or even a relevant reference  PRON have be hard to find any since most of the algorithm PRON have see compute the full gsvd  which be not what PRON be look for    per httpwwwnagcomnumericflmanualpdff08f08msfpdf  regard the lapack routine in the nag library for complex singular value  singular vector   not generalize  but PRON think the idea be basically the same for generalize  see httpwwwnagcomnumericflmanualpdff08f08vafpdf for real gsvd and httpwwwnagcomnumericflmanualpdff08f08vnfpdf for complex gsvd  but PRON do not have discussion of computational effort   08msf  cbdsq  zbdsqr  compute the singular value decomposition of a  complex general matrix which have be reduce to bidiagonal form     f08msf  cbdsqr   zbdsqr  use two different algorithm  if any singular vector be require  that is  if ncvt  0 or nru  0 or ncc  0   the bidiagonal qr algorithm be use  switch between zero shift and implicitly shift form to preserve the accuracy of small  singular value  and switch between qr and ql variant in order to  handle grade matrix effectively  see demmel and kahan  1990     if  only singular value be require  that is  if ncvt  nru  ncc  0    PRON be compute by the differential qd algorithm  see fernando and  parlett  1994    which be faster and can achieve even great  accuracy     the total number of real floating  point operation be roughly  proportional to n2 if only the singular value be compute   about  12 n2  nru additional operation be require to compute the left  singular vector and about 12 n2  ncvt to compute the right singular  vector   the operation to compute the singular value must all be  perform in scalar mode  the additional operation to compute the  singular vector can be vectoriz and on some machine may be  perform much faster  the real analogue of this routine be f08mefsbdsqr  dbdsqr    see demmel and kahan 1990  accurate singular value of bidiagonal matrices  httpwwwnetliborglapacklawnspdflawn03pdf for a discussion of why singular value plus vector take long than singular value only  PRON be  because singular vector converge more slowly than singular value  table 2 show some timing result compare with and without singular vector   here be some quick timing result PRON run use matlab r2014a win64 on an 8 core machine    gtgt  n4000a  randnnb  randnntic  sigma  gsvda  btoc  ticu  v  x  c  s   gsvda  btoc  elapsed time be 31649874 second   elapsed time be 33202460 second   a few other run give similar result  and the no singular vector version be still fast by about the same amount when PRON be do second  check in case the processor be get overheat and have to decrease turbo level for second calculation    multiple core be use by matlab   with n  1000  the no singular vector version average about 8  long than with singular vector  PRON have no idea why   PRON be not say that randn of a square matrix for both a and b be representative of problem PRON care about  but PRON can try PRON on PRON own a and b 
__label__nltk __label__named-entity-recognition when do entity recognition use nltk  one get as a result a tree with a bunch of word map to tag  eg  mark gt  nnp  first gt  jj     PRON be not at all clear what all the tag stand for at first glance and PRON be unable to find any documentation about these tag in the nltk doc    gtgtgt  from nltk import wordtokenize  postag  nechunk   gtgtgt  sentence   mark and john be the first to work at google from one year old in 39 year     gtgtgt  print nechunkpostagwordtokenizesentence      s   person mark  nnp   and  cc   person john  nnp   be  vbp  the  dt  first  jj  to  to  work  vb  at  in   organization google  nnp   from  in  one  cd  year  nns  old  jj  in  in  39cd  year  nns     PRON end up look into the source code to get the mapping  post in case anyone else run into the same problem   tag mapping accord to nltk source   cc     coordinate conjunction     prp    possessive pronoun     cd     cardinal number     rb     adverb     dt     determiner     rbr     adverb  comparative     ex     existential there     rbs     adverb  superlative     fw     foreign word     rp     particle     jj     adjective     to     to     jjr     adjective  comparative     uh     interjection     jjs     adjective  superlative     vb     verb  base form     ls     list item marker     vbd     verb  past tense     md     modal     nns     noun  plural     nn     noun  singular or masps     vbn     verb  past participle     vbz     verb3rd ps  sing  present    nnp     proper noun  singular     nnps    proper noun plural     wdt     wh  determiner     pdt     predeterminer     wp     wh  pronoun     pos     possessive ending     wp     possessive wh  pronoun     prp     personal pronoun     wrb     wh  adverb          open parenthesis          close parenthesis          open quote          comma          close quote          period          pound sign  currency marker           dollar sign  currency marker      in     preposition  subord  conjunction     sym     symbol  mathematical or scientific      vbg     verb  gerund  present participle     vbp     verb  non3rd ps  sing  present          colon    to extend on the answer of kevin s lin  pos tag or part  of speech tag refer to   the process of mark up a word in a text  corpus  as correspond to a particular part of speech  base on both PRON definition and PRON context — ie   PRON relationship with adjacent and relate word in a phrase  sentence  or paragraph  a simplify form of this be commonly teach to school  age child  in the identification of word as noun  verb  adjective  adverb  etc   more information on each particular tag can be find use the   nltkhelpupenntagset    although the tagset dataset need to be download  more on these matter can be find here  
__label__machine-learning __label__algorithms __label__predictive-modeling __label__random-forest __label__k-means PRON be try to predict rare event  mean less than 1  of positive case  PRON basically try to predict if a subject will have 0  1  2   6   6 failure  there be case in all those category    PRON have try several algorithm   decision tree  random forest  adaboost  group use k  means clustering and find association with failure  which group have most failure   in any case  learn either go to no failure or have too much variance  lead poor reasult on cv set    do PRON know any machine learn algorithm which be better suit for rare event   or be PRON surprising that PRON get those bad result use those algorithm  which mean that PRON feature list be not good   thank a lot   when PRON have an unbalanced data set  the algorithm be go to weight PRON success on each datum point equally  mean the majority class come out as much more important than the minority class  the typical solution be to sample down the majority class until PRON be the same size as the minority class  and an alternate  similar  solution be to adjust the cost function so that the minority class be weight appropriately   see these similar question for more   should PRON go for a  balanced  dataset or a  representative  dataset   quick guide into train highly imbalanced data set  what be the implication for train a tree ensemble with highly biased dataset   skewed multi  class datum  ratio of positive to negative sample in datum set for good classification 
__label__xgboost from other post  see unbalanced multiclass datum with xgboost  and the documentation  scaleposweight in xgboost appear to balance positive and negative case  which seem to apply only to classification  however  PRON also appear to be an option in xgbregressor  see  documentation   before PRON dive into the source code  can someone explain what this do for regression   
__label__fenics PRON have just start learn fenic and have use  httpwwwscientificpythonnetpyblogfenicslineartwopointbvp to write a script for solve   u   u  1  u0   1  u1   0  with exact solution   ux   expx exp1   x   1exp1    exp1   clearly the weak formulation of the above problem be   u   v     u  v    g  v   with g  1  here be the edit code   from dolfin import    definig mesh  mesh  intervalmesh20  0  1    definig function space on this mesh use lagrange polynoimal of degree 2   v  functionspacemesh   cg   2    definign boundary value   u0  constant0   u0  expressionx0      this function check whether the input x be on the boundary or not   def dirichletboundaryx  onboundary    tol  1e14  return onboundary and absx  lt  tol    enforce u  u0 at x  0  bc  dirichletbcv  u0  dirichletboundary    set up the variational problem  u  trialfunctionv   v  testfunctionv   f  constant1   g  constant1   a  innergradu   gradvdx  inneru  vdx  l  fvdx   solve the variational problem   u  functionv   solve  a   l  u  bc    plot solution  plotu  interactive  true   clearly the solution plot do not incorporate the boundary condition u0   1   would any body please help   
__label__boundary-conditions in the method of weight residual apply to boundary value problem  be PRON necessary for the basis function to satisfy all of the boundary condition  will PRON work even if PRON do not satisfy all of the boundary condition   the test function must come from the space of variation  ie  if PRON solution have to satisfy     upartial omegag     then PRON test function have to satisfy     vpartial omega0      this be a question everyone have trouble with at first  which be why PRON expand on PRON in quite some detail in lecture 215 here  
__label__optimization PRON be try to solve the following test problem which be well  know in the community in different variant   place n  15 point in the 3dim  unit cube such that the minimal distance between PRON be maximal  eg like in the case of repellent but confine electron   here be matlab  like non  vectoriz  vectorized form of the function to be optimize  where PRON assume n  3n     non  vectorized   function d  ballsx   d  20   for i  114  for j   i115  s   xixj2   xi15xj152   xi30xj302   s  sqrts    if s  lt  d  d  s  end  end  end  d  d   end   vectorized   function d  ballsx   x  reshapex     3    d  minpdistx     end  the function be continuous  but not smooth  gradient  base method will not work  multi  start and stochastic approach have a problem  too  because with 45 dimension the search space be already quite big  the boundary be also a problem  compare for example to the problem of place electron on a sphere where there be no boundary   PRON will have many local minimum  eg permute ball index or interchange dimension  PRON guess all these local minimum have the same function value  like an energy level that will always end in a similar configuration  be that true    PRON be only interested in this minimal value  so calculate one local minimum exactly and reliably should be enough   from apply matlab  fmincon   with several restart PRON know the minimum will be below 062  still PRON would like to compute this value more accurately and with open source software only   please no hint to powerful commercial solver   smooth reformulation  as sid point out  there be no need to treat this problem as non  smooth  since PRON would just be make PRON harder on PRON   let PRON assume for the sake of notation that  mathbfx1   ldot  mathbfx15  in  013  subset mathbbr3 be the coordinate of PRON 15 particle in the unit cube  a smooth formulation  as sid suggest  present in standard form  for nonlinear programming   would be    beginalignat1    ampminmathbfx1   ldot  mathbfx15  in  013   e   mathrmst    amp  quad e  mathbfxi   mathbfxj2  leq 0    i  j  1  ldot  15    i neq j  endalignat  where  e be a proxy for the minimum distance  which PRON be assume be relate to minimize some sort of energy  there may be a way to reformulate this problem as an equivalent convex problem  but PRON do not think there be   this formulation probably be not convex  because the left  hand side of the nonlinear constraint be not convex  so PRON will need to use a nonconvex nonlinear programming solver to be assure of a global optimal solution  unless PRON can prove convexity of the feasible set  but PRON doubt that   deterministic global solver that will work for nonconvex problem include  but be not limit to    baron  which be commercial  but PRON can submit job for free via the neos optimization server run by university of wisconsin  madison   lindoglobal  also commerical  also available through the neos optimization server   couenne  open  source  part of the coin  or suite of open  source solver   bonmin  also part of coin  or   lago  again  part of coin  or   ico  available as open  source  or through neos   PRON be important to note that one solver may work on PRON problem when other will not  baron be generally consider the good  but PRON be fallible  and there be case where  for example  couenne will solve a problem to  epsilon  global optimality  but baron will not  and vice versa    solve nonsmooth problem  let PRON suppose for the sake of argument that PRON  like hans  want to solve a non  smooth nonlinear programming problem  this type of problem be not PRON area of expertise  but PRON know of a couple reference   the most famous person in the field  who  as far as PRON can tell  develop the most important part of the theory early on  be frank h clarke  the gist of non  smooth optimization seem to be  replace gradient with clarke s generalize gradient  use clarke s generalize gradient  PRON be suppose to be able to formulate a non  smooth analogue of newton s method  as well as algorithm for optimization  PRON textbook on the theory  optimization and nonsmooth analysis by frank h clarke  the link go to amazon  be consider a classic   in term of software  the good link PRON can find be to napsu karmitsa s home page  PRON be develop a couple non  smooth optimization solver  and PRON link to other non  smooth optimization solver  the method PRON have hear of most often be call bundle method  and should be deterministic   PRON favor deterministic method over stochastic method   more link to non  smooth code can be find here  PRON mileage may vary  because like PRON say  PRON do not work with these method   PRON do know that just because a method be develop for non  smooth problem do not mean PRON will work for non  smooth  non  convex problem  so PRON will need to make sure that the solver PRON choose can handle both non  smoothness and non  convexity   finally  as hans point out in the comment  non  smooth formulation regularly appear in science and engineering  however  PRON first instinct as someone in the optimization field be to try and find an equivalent smooth reformulation because method for solve smooth problem be generally much fast than method for solve non  smooth method  a labmate use non  smooth solver  and have make this observation   if PRON can reformulate the problem as a smooth optimization problem  PRON generally behoove PRON to do so   PRON may wish to try one of the  local  nonsmooth solver at PRON web page  httpwwwmatunivieacatneumgloptsoftwarelhtmlnonsm  use multiple starting point to globalize the search   PRON find cma  es quite robust in dimension up to 50   PRON get very slow though when the dimension  be large   
__label__classification __label__time-series PRON problem be different from the common time series data problem  what PRON need to do be check if future time series data be in accord with previous time series datum PRON already consider to be correct   in short  PRON need a one  class classifier apply to time series datum  which have variable length  go from 110 to 125 point    do anyone know any lib that may help PRON with this task  PRON usually use pythonscikit  learn  but PRON be open to use any other configuration   PRON sound like  novelty detection  be what PRON may be look for   there be a scikit library for this  PRON generate a one  class model and predict whether new observation fit into the one class or not   for further reading  PRON would like to refer to this link   novelty detection scikit  learn  there PRON can also find an example use a svm classifier   apart from the approach rolf schorpion mention  there be other   for example  PRON could use a deep neural network  specifically  an auto  encoder  see here for a tutorial    but there be an important catch to all purely  data  drive  approach  if the figure of 30 time series PRON mention in the comment be a typical order of magnitude for PRON training set  the result will be more or less arbitrary  if PRON do not define  accordance  in any way  this be a classification problem with only positive training datum which consist of 30 data point with more than 100 feature each   unless PRON data be very  very special  eg  all time series be identical   there be just too much freedom in the solution of this problem  different algorithm  or different parameter set for the same algorithm  will use this freedom in very different way  so PRON will probably see very different solution when PRON experiment with different method   so if PRON do not want to do more or less arbitrary experiment and choose from the solution afterwards  PRON have to use a method in which PRON somehow define the meaning of  in accordance  in advance  PRON may try and use traditional time series technique to find a common model that fit each of the time series well  PRON could then postulate that  accordance  be a good fit to this model   or PRON may just do some exploratory analysis of PRON datum to decide upon a simple rule which could be use to detect accordance  there be lot of possibility  and without further detail on the application PRON be difficult to decide which one be appropriate  delegate the decision of what be  in accordance  to some algorithm do not seem to be a good choice in this case  though   base on PRON description  a simple cross  correlation should do PRON  PRON be test whether a series point in the future correlate with a series in the past which PRON consider correct  this measurement be give exactly by the cross  correlation between the future series and the past series   the other good thing about cross correlation be PRON can adjust the length of PRON series accordingly  which give a lot of flexibility   cross correlation PRON may not be enough for PRON system  as PRON only provide a measure of difference  PRON can still build a model for the actual  classification   since PRON output be binary  anything simple from a threshold detector  ie if xcor  value then 1 else 0  to a small neural network should both work fine   as PRON  op  further clarify  PRON have get equipment sensor datum   PRON first time  series be record when PRON know the machine be in good operating condition   later  PRON sample another time series  and PRON want to know if anything have change   this be call anomaly detection   PRON describe the time  series as stochastic  mean there be an element of randomness   there be several interesting method PRON can use to find the probability that the two series be generate by the same process  nothing have change    PRON be afraid the answer PRON have read to PRON question so far would only work in special case  and be perhaps too complicated for the job   the general concept be that there be an underlying probability distribution  describe by a  probability density function  or pdf  which generate PRON time  series   when the machine get out of order  that pdf change   PRON test if the new distribution be different from the old one   PRON should use a completely general approach which do not assume a particular form of the pdf  gaussian  poisson  beta  etc    first  PRON can always construct and graph the empirical cumulative distribution function or ecdf for each time  series and compare PRON visually   this require a qualitative judgement on what  different  look like   to make PRON quantitative  a test from classical statistic be a k  s test   PRON be a one  liner in r  the disadvantage of this test be  well  PRON be not bayesian     there be other issue too   that being say  PRON use the k  s test all the time because PRON be so fast and easy   there be many caveat here   PRON will probably want to use the first  difference  change  of sensor datum instead of the value PRON   if PRON have more than a few sensor  PRON will probably want to use dimensionality reduction such as principal component analysis  pca  as well   PRON much prefer a bayesian a  b test   this require PRON to construct model of the pdf to which PRON datum be apply   this generate a pdf which most likely explain PRON datum   in fact  PRON generate a pdf for each parameter in PRON pdf   PRON look to see how much overlap PRON have in the parameter   not enough overlap mean an anomaly   to get start with bayesian probability PRON recommend python and pymc and the bayesian hackers book  
__label__pde __label__finite-element __label__fenics suppose PRON have a time  dependent pde discretiz by the rothe method and fem  like     intomega  kn12un1un   v mathrmdx  fn12un1unv  quad forall vin vhn      what be a good way of quantify the difference between a transient solution  un  in vhn and a stationary solution  unmathrms  satisfy the give equation with  kn12   0    without actually compute  umathrms and  un   unmathrms   note that  fn   kn12  and  vnh all depend on  n so that stationary solution  unmathrms be different at each time level  tn  PRON be target the computation of norm of some element tensor etc   PRON could look at the residual   fun if  un  us then  fus0   but this be not true for a transient solution   let PRON assume that stationary solution  unmathrms be give by     0  fn12un1mathrms   unmathrms    v       PRON can do that  imagine that  fn12 be crank  nicolson discretization  then this mean    0fn12un12mathrms    v       let PRON define     rn   un  unmathrms       now PRON use taylor theorem on the rhs of the first equation  assume  rn   rn1 be small  and PRON have up to the first order     0 approx fn12un1   un   v    dun1fn12un1un   vrn1     dun    fn12un1un   vrn         now define notation   rn12    fn12un1   un    jn12    dun1fn12un1un  and   widetildejn12    dun    fn12un1un  so that last approximation read     0  rn12   v   jn12   vrn1    widetildejn12   vrn         pick  vrn1 and rearrange last equation PRON have     jn12rn1rn1    rn12   rn1    widetildejn12   rn1rn         if  jn12 be elliptic  jn12vvgeqalphav2      rn1 leq frac1alphaleftleftrn12   widetildejn12   cdotrn    rightright      which could be useful as reccursion relation for  rn in a special case where  fn12 be backward euler discretization so that  widetildejn12   0  one have simply     rn1 leq frac1alphaleftleftrn12rightright      lhs of the last equation be target quantity and norm on the rhs can be calculate cheaply from already compute transient solution  the big trouble be  alpha  the ellipticity constant of the jacobian  jn12 
__label__machine-learning __label__apache-spark PRON have a pipeline of model   training pipeline  on training datum    tokenizer gt  hashingtf gt  idf gt  kmean  test pipeline  prediction on query datum    train pipeline gt  select feature from training datum which be in the same cluster for query datum gt  bucketedrandomprojectionlsh  what PRON do explain a bit here   PRON be curious if that be possible to select param of bucketedrandomprojectionlsh  kmeans and hashingtf wisely enough  so PRON have rationale   disclaimer  PRON be new to all of that  so below be PRON assumption  thought only   for instance  next param   hashingtfnumfeature   number of feature   underneath  PRON use hashing in order not to face collision and false match of different word during frequency calculation  so rationale beneath of that   of word PRON likely will face  in PRON case PRON select 16k as PRON seem far beyond the dictionary of paper language  some performance consideration take into account  eg default 2  18 lead to long calculation and as the result  timeout have to be tune due to connection timeout exception   kmeansk   the number of cluster to create   in unsupervised learning PRON have no insight on how to select this one param  just to guess  in future  PRON may be the  of topic PRON word belong to  probably ldh  some manual consideration may help to set PRON correctly   bucketedrandomprojectionlshbucketlength   the length of each hash bucket   as PRON understand PRON  PRON represent the param of reduction of initial word vector  represent by frequency of word occurrence   squeeze PRON to  length   with further distance calculating and assign the vector  base on the  distance  accordingly  to the next param  bucketedrandomprojectionlshnumhashtable  so the rationale on PRON value PRON see as the amount of word PRON more likely want to take into account to make a prediction on distance between vector  eg if PRON have an article which be decompose further into word token  how many word PRON want to take into account  eg half of the average    bucketedrandomprojectionlshnumhashtable   number of hash table   as PRON see PRON  how many  distance  PRON want to consider  in other word  PRON have  of cluster  ie 5   PRON answer how further PRON want to split the cluster to get narrow result when compare what PRON relate to PRON query  will definately depend on the amount of feature in the cluster and  of result PRON expect   so if to think about that backwards  if give x be   result PRON expect out of prediction    bucketedrandomprojectionlshnumhashtable    average  of article  x    bucketedrandomprojectionlshbucketlength    average  of word per article  2    kmeansk    x  some magic number   2    hashingtfnumfeature    number of word in dictionary   kindly ask PRON to evaluate PRON thought process and correct   
__label__machine-learning __label__data-mining __label__classification __label__algorithms so PRON have a large collection of blog post contain title  content  category  tag and geo  location field and PRON be look to achieve three thing   assign a category  or multiple category  to all the post and any new one  PRON have a strict vocabulary of category   add new tag to the post that may be relevant to the post   mark the post if PRON contain information about a place  for example  lorem ipsum dolor sit amet san francisco  consectetur adipisc elit   PRON have be look into different machine learn algorithm  most recently decision tree  but PRON do not feel that be the good algorithm to work out the problem above  or that PRON have not understand PRON enough    many of these post already contain category  tag and geo  location datum  some do not contain any information and some have only a few detail   what would be the good machine learn algorithm to look into to solve each of the three area   for text datum  linear svm be still state of the art   for name entity recognition  look up some ner toolkit   PRON may want to look at naive bayes classifier or have a look at this page on very simple text classification by machine learning  PRON could also look at stanford name entity tagger   question 1  category prediction  to predict the category of a new blog post  PRON could do the follow   build a mlp  multilayer perceptron  a very simple neural network   each category get an output node  each tag be a binary input node  however  this will only work if the number of tag be very little  as soon as PRON add new tag  PRON will have to retrain the network   build a mlp with  important word  as feature   if PRON have internal link  PRON may want to have a look at  on node classification in dynamic content  base networks   in case PRON be german  PRON may also like über die klassifizierung von knoten in dynamischen netzwerken mit inhalt  PRON could take all word PRON currently have  see those as a vector space  fix that vocabulary  and probably remove some meaningless word like  with    a    an   this be commonly call a  stopword    for each text  PRON count the different word PRON have in PRON vocabulary  a new blog post be point in this space  use  k nearest neighbor for classification   use combination of different predictior by let each predictor give a vote for a classification   see also  yiming yang  jan o pedersen  a comparative study on feature selection in text categorization  1997   scikit  learn  work with text data  question 2  tag text  this can be treat the same way like question 1   question 3  find location  download a database of country  city  eg maxmind  and just search for a match  
__label__machine-learning __label__similarity when do PRON make sens to use dot  product as similarity measure instead of cosine   PRON have see there be already question ask about this  however that merely explain the difference between calculation of dot  product  amp  cosine and PRON do not focus on when should PRON use one vs another with real world example   borrow from this quora answer  which include some concrete example of when PRON may prefer one measure over the other  PRON come down to whether PRON care about take the magnitude of the vector into account  this be highly domain  specific  but the example use there be in information retrieval  where magnitude represent the length of the document in question  the dot product will take the document length into account  whereas the cosine similarity will not   in general  PRON will want to have good reason specific to PRON problem space to use one over the other   when PRON want cluster item use distance as similarity measure  for example  PRON use euclidean distancessquare root of inner product  in k  means clustering as a similarity measure  squared euclidean distance be use as a similarity measure in ward s method of cluster  however  when PRON want to cluster variable  PRON use correlationcosine  as a measure of similarity   the relation between dot product and cosine be similar to the relation between covariance and correlation  one be normalize and bounded version of another   in PRON experience usual dot product be good when PRON also care about the number of dimension two vector have in common  ie non zero value in these dimension with the same sign   for example  PRON can be matching tag or attribute  for usual text cosine typically work better  
__label__r PRON be currently learn r and PRON have to solve an issue be PRON have to extract value from a data set which be from a specific month and from this value PRON should calculate the mean temp   PRON do PRON like that   datadatax  month    6    meandatax  temp   narmtrue   PRON give PRON the mean value but without take PRON first statement into consideration   what do PRON need to do that both statement be take into consideration   PRON can answer this use tapply  which will give PRON the mean for all PRON subset  eg  dummy data  df  dataframemonrepmonthabb  times10   calculate mean for all month  tapplydftemp  dfmon  mean  narmt   to just get the mean for month 6   meandftempdfmonjun    narmt   PRON be nearly there but do not assign PRON subset to a value  if PRON have   x  datadatax  month    6    meanxx  temp   narmtrue   that should work  
__label__machine-learning __label__clustering __label__dataset __label__k-means __label__clusters for clustering  dbscan surpass k  mean in term of handle arbitrary  shape data set  in the most publish paper about density base clustering  the experiment be perform with synthetic datum set that have special feature  for example  the moon  shape data set   can someone point PRON to some real world dataset where dbscan outperform k  mean   there be no good evaluation datum at all for cluster that would allow such conclusion   there be not even good real datum where PRON could say variant 1 of k  means be good than variant 2 of k  mean   there be also no good evaluation measure that would handle the notion of  noise  well either   so  do not go by some number   clustering be about solve a data problem  PRON have datum  PRON try algorithm and parameter and study  important  the result  until PRON find something interesting  this can not be automate  or measure  well  PRON could ask 100 scientist on what work for PRON  i remember have see eg some astronomy paper that use dbscan with success  but of course PRON can not put a number 0 to 1 on this    PRON may want to look eg at this case study with dbscan  PRON doubt k  means be of much help on this data set  in particular  PRON will assign the noise point to some cluster  when PRON should not be assign at all   PRON play around quite a bit with location datum and have find example both where k  means work fine and where k  means be a poor representation and dbscan be a great fit   if PRON have ever go hiking or mountain climbing on a day with fog or a low cloud cover  there be time where PRON get to the top of the peak and can only see the surround peak poke up through the cloud   PRON like to use this analogy when PRON think of dbscan   the density filtering allow one to select a threshold where data be keep  and all of the remain data be filter out   take a look at this seattle crime incident datum   suppose PRON want to cluster the datum by location to form pseudo neighborhood ie neighborhood that be roughly define the the geography of where criminal event tend to occur  this be an example of k  mean work just fine in location analytic   seattle crime datum superimpose a primitive map with k  means cluster  if PRON know seattle at all  PRON can see that the cluster tend to pick out neighborhood and this work quite well in break up neighborhood   now suppose PRON want to pick out the high crime area of seattle in order to identify the hot spot  no matter how PRON adjust k  the k  means clustering do not really provide any additional insight   but the density filtering in dbscan do a wonderful job of identify high crime area   seattle crime datum superimpose a primitive map with dbscan cluster  PRON think this give the jest of some strength that can be exploit use the two algorithm   there be nothing special about the crime datum   for what PRON worth  PRON have a tutorial that go through the analytic of identify a user s home and work location from PRON cell phone gps ping  this be another case where dbscan be very useful  but the dbscan specific part a bury a way down in the tutorial   hope this help  
__label__basis-set PRON be solve an ode  where currently the dependent variable be the  time dependent  spatial fourier coefficient  PRON turn out that the phenomenon PRON be interested in describe be spatially localize  and PRON take an impractical number of fourier coefficient to fully describe PRON  therefore  PRON be interested in look at different basis to use  instead of the canonical trig function  eikx  for  kin mathbbz    however  for this model PRON need these basis function to have certain property  in particular  PRON need these function to be differentiable  as well as periodic  also  PRON need the function to have relatively simple hilbert transforms  since this be exploit in solve for the govern equation  and be one reason why the trigonometric basis be desirable   finally  the basis should converge more rapidly than the fourier series for spatially compact scenario  hence  PRON would like to reduce the number of dependent variable that PRON be solve for in PRON ode   do anyone know of any function fit the bill  any tip  reference be appreciate   nick  
__label__optimization __label__linear-programming __label__convex-optimization PRON have formulate a linear program with binary indicator variabl  zia which be equal to  1  if the  ith document be of rank  a and  0  otherwise   the other variable in the linear program   z1ija   z2ija be define as follow   begineqnarray   z1ija  equiv zia   sumblta  zjb      z2ija  equiv zia   sumbgeq a  zjb    endeqnarray   PRON be try to convert the above non  linear constraint to the follow set of equivalent linear constraint     z1ija   z2ija   zia   forall i  j  a  the problem PRON be face be that  the above set of linear constraint be clearly not equivalent to the definition of  z1ija   z2ija any idea if PRON be possible to represent non  linear rank type constraint as equivalent linear constraint   there be several way to convert PRON model to linear constraint   for example  begineqnarray   z1ija   z2ija   ampamp  zia    forall i  j  a   z1ija   ampleamp  1 sumb ge a  zjb    forall i  j  a   z2ija   amp  leamp  1 sumb  lt  a  zjb    forall i  j  a  endeqnarray   in general  whenever PRON have a mixed  integer program where the only nonlinearitie be polynomial of binary variable  PRON be possible to reformulate the program so that PRON be a mixed  integer linear program  use the work of fred glover  and subsequent related work   see   f glover  further reduction of zero  one polynomial programming problem to zero  one linear programming problem  operations research  volume 21  page 156  161  1971   f glover  e woolsey  convert the 0  1 polynomial programming problem to a 0  1 linear program  operations research  volume 22  page 180  182  1974   f glover  improve linear integer programming formulation of nonlinear integer problem  management science  volume 22  page 455  460  1975   f e torres  linearization of mixed  integer product  mathematical programming  volume 49  page 427  428  1991   o kettani  m oral  equivalent formulation for nonlinear integer problem for efficient optimization  management science  volume 36  page 115  119  1990  
__label__neural-network __label__evaluation PRON be a bit confused by the coexistence of loss and accuracy metric in neural networks  both be suppose to render the  exactness  of the comparison of y and yhat  be not PRON  so be not the application of the two redundant in the training epoch  moreover  why be not PRON correlate   yes  PRON both measure the exactness of y and yhat and yes PRON be usually correlate  sometimes the loss function may not be accuracy but PRON be still interested in measure the accuracy even though PRON be not optimize PRON directly  google s tensorflow mnist example minimiz  optimize cross entropy loss but display accuracy to the user when reporting result  and this be completely fine   sometimes PRON do not want to optimize accuracy directly  for example  if PRON have serious class imbalance  PRON model will maximize accuracy by simply always pick the most common class  but this would not be a useful model  in this case entropy  log  loss would be a good loss function to optimize   loss be more general than accuracy  in classification  PRON can go to 100  accuracy  where all the label be predict correctly  but what about regression or forecasting  there be no definition of 0  and 100   loss can be optimize with various method  in numerical methods class  PRON have learn to solve a function by optimize PRON  which be minimize  yhaty  with various method such as newton s method  bisection method  etc   log loss have the nice property that PRON be a differentiable function  accuracy may be more important and be definitely more interpretable but be not directly usable in the training of the network due to the backpropagation algorithm that require the loss function to be differentiable  when PRON preferred loss be not directly optimizable  like the accuracy  PRON use a loss function that behave similarly to proxy the true metric  in case of binary classification PRON would use a sigmoid at the end and a log loss to approximate accuracy  PRON be highly correlate  
__label__comsol PRON want to simulate capacitor with comsol by use poisson s equation in genaral and weak form  how can PRON specify boundary condition for this case   PRON can use pde module and add dirichlet boundary condition  in this section choose the value on the boundary  PRON may be zero value or another function which PRON want to use    for more detail see httpwwwyoutubecomwatchvsmjtwaz9ho 
__label__nlp __label__deep-learning __label__word-embeddings __label__unsupervised-learning PRON be work on word2vec gensim model and find PRON really interesting   PRON be interst in find how a unknown  unseen word when check with the model will be able to get similar term from the train model   be this possible   can word2vec be tweak for this   or the training corpus need to have all the word of which i want to find similarity   the training corpus need to have all the word of which PRON want to find similarity   every algorithm that deal with text datum have a vocabulary  in the case of word2vec  the vocabulary be comprise of all word in the input corpus  or at least those above the minimum  frequency threshold   algorithms tend to ignore word that be outside PRON vocabulary  however there be way to reframe PRON problem such that there be essentially no out  of  vocabulary word   remember that word be simply  token  in word2vec  PRON could be ngram or PRON could be letter  one way to define PRON vocabulary be to say that every word that occur at least x time be in PRON vocabulary  then the most common  syllable   ngram of letter  be add to PRON vocabulary  then PRON add individual letter to PRON vocabulary   in this way PRON can define any word as either  a word in PRON vocabulary  a set of syllable in PRON vocabulary  a combined set of letter and syllable in PRON vocabulary  word2vec treat word as atom  to get meaningful vector for unknown word  PRON either have to  change what these atom be  eg switch to letter n  gram as in jamesmf s answer  or  use a different model that explicitly look at what be inside PRON word  eg the cwe model on httpsgithubcomleonardxucwe be easy to use  
__label__machine-learning __label__nlp __label__deep-learning __label__julia __label__rnn PRON be try out pos tag use rnn but not able to figure out what be wrong in PRON implementation because of which the gradient check be fail  please help  PRON be paste the relevant part below    weight  wxh  randninputlayersize  hiddenlayersize001   input to hide  whh  randnhiddenlayersize  hiddenlayersize001   hide to hide  bh  zeros1  hiddenlayersize    hide bias  why  randnhiddenlayersize  outputlayersize001   hide to output  by  zeros1  outputlayersize    output bias  function forwardrnnx  y  h  p  hprev   global wxh  whh  why  bh  by   cost  0   for time in 1lengthx   if time   1  htime   tanhxtimewxh  hprevwhh  bh    else  htime   tanhxtimewxh  htime1whh  bh    end  score  htimewhy  by   psoftmax  expscore   sumexpscore     ptime   vecpsoftmax    cost   sumlogytimeptime      end  return cost   end  function backwardrnnx  y  h  p  hprev   global wxh  whh  why  bh  by   dwxh  zerossizewxh     dwhh  zerossizewhh     dbh  zerossizebh     dwhy  zerossizewhy     dby  zerossizeby     dh  zerossizebh     dhnext  zerossizeh1      for time in lengthx11  dy  ptime   ytime    dwhy  dwhy   dy  htime      dby  dby  dy    dh   whydy    dhnext   dhraw   1   htimehtime      dh   dwxh  dwxh   xtime   dhraw    dbh  dbh  dhraw   if time   1  dwhh  dwhh   hprev   dhraw    else  dwhh  dwhh   htime1    dhraw    end  dhnext  dhrawwhh   end  return dwxh  dwhh  dbh  dwhy  dby   end   gradient checking  function gradcheckinput  target  h  p  hprev   paramnamelist    wxh    whh    bh    why    by     global wxh  whh  why  bh  by   paramlist   x for xwxh  whh  bh  why  by     numcheck  2   delta  1e5   cost  forwardrnninput  target  h  p  hprev    dwxh  dwhh  dbh  dwhy  dby  backwardrnninput  target  h  p  hprev    dparamlist   x for xdwxh  dwhh  dbh  dwhy  dby     for  param  dparam  name  in zipparamlist  dparamlist  paramnamelist   s0  sizedparam    s1  sizeparam    if s0   s1  printlnerror dim do not match    s0   and   s1    end  printlnname   for i in 1numchecks  ri  rand1lengthparam     oldval  paramri    paramri   oldval  delta   cg0  forwardrnninput  target  h  p  hprev    paramri   oldval  delta   cg1  forwardrnninput  target  h  p  hprev    paramri   oldval  gradanalytic  dparamri    gradnumerical   cg0  cg1    2  delta    relerror  absgradanalytic  gradnumerical   absgradnumerical  gradanalytic    printlngradnumerical      gradanalytic     gt    relerror    if relerror  gt  1e5  errorgradient check fail      end  printlngradient check pass     end  end  end  the code be in julia programming language and be inspire from karpathy s mincharrnnpy  as i understand  PRON have the wrong backprop gradient implementation  here PRON should take into account  that rnn s hidden state h have PRON previous state in the equation  htime1   this be also must be extract via chain rule  for more information suggest to refer this post  enter link description here  PRON also contain python rnn implementation  
__label__keras PRON know that keras be develop for quick deployment  be PRON just for beginner or also useful in industry for professional   keras be use in academia  see google scholar citation of keras as a proxy for academy adoption  and hobbyist  see github star or google result for kera in wwwkagglecom as proxy for hobbyist adoption    PRON be recently bundle together with google s tensorflow   PRON be also use in industry  at least for prototyp  PRON know because PRON use PRON    but PRON have find no source to back this statement  
__label__error-estimation __label__matrix PRON have a matrix a which be an approximation to the know matrix b both matrix be square  3x3 matrix and  in this case  be symmetric  be there a  good  method for calculate percent error in the approximation  PRON have be use the sum the  sum of square  error for all entry divide by the sum of  sum of square  of the true matrix   pcterr  sumsum   a  b2   sumsumb2    and realize that this be simply the square of norma  b2normb2  which give drastically different percent error estimate   how should PRON go about calculate percent error between two matrix  or maybe more generally  two tensor   edit   thank to some of the comment  PRON now recognize the difference between the 2norm and the frobenius norm  in either case  however  the type of norm PRON be take be of relatively small consequence  the big question be how to represent the percent error of a 2nd rank tensor correctly   norma2  return the large singular value of a not the square root of the sum of square of PRON element   PRON appear that what PRON want in matlab be norma   fro    the frobenius norm  
__label__machine-learning __label__r __label__bigdata __label__performance __label__sas PRON be try to understand the abstract detail that explain how h2o be fast than r and sas for data science computation   PRON have use r  sas base and h2o first  PRON do not think that h2o seek to be either r or sas   h2o provide datum mining algorithm that be highly efficient  PRON can interface with h2o use several api such as PRON r api  the benefit of combine r and h2o be that h2o be very good at exploit multi  core or cluster with minimal effort of the user  PRON be much hard to achieve the same efficiency in r alone   the reason why h2o be much fast be that PRON have a very good indexing of PRON datum and PRON algorithm be write such that PRON exploit parallelism to the full  see httph2oaiblog201403h2oarchitecture  r with the default matrix dynamic library can only use one cpu core  revolution r community edition ship with the intel math kernel library  this allow for some matrix computation in parallel but definitely not as efficient as h2o for sas PRON be a bit harder to say anything consider PRON be closed source but base on PRON cpu utilization PRON would assume that PRON have a similar approach as revolution r PRON matrix algebra exploit parallelism but PRON algorithm be not as efficient as h2o PRON datum storage be also not as efficient as h2o  lastly  h2o with r come at a very different price tag than sas   hope this clarify a bit  
__label__machine-learning PRON have a problem that PRON have be try to use decision tree to solve  there be a data set of price information for product sell by a company  the goal be to infer the pricing algorithm for each product base on the product s name  type  and characteristic  however  the data be incomplete and may contain a few inaccuracy due to manual datum entry   products can be price use several possible algorithm   price be a fixed dollar amount  widget cost  20   price be a percentage of msrp  widget cost x of msrp  note x could vary by product class   price be base on a cost per piece  package of widget cost  2 per piece   use decision tree  PRON have be able to identify the product price use algorithm  1  but have struggle to make progress on algorithm  2 and  3  since price be a continuous variable  the leaf node always represent the average of the target variable  but this do not seem to work for  2 and  3  since these would be a function of other variable  not the dependent variable   be there a clever way to implement this use decision tree  any recommend alternate technique that could identify the pricing algorithm use  note  PRON be not interested in prediction  but rather explain the pricing rule  algorithm that fit the exist datum   PRON be use sas enterprise miner  but may have access to other tool as well   
__label__machine-learning __label__python PRON may be ask a dumb question but PRON question be can PRON write a python program  let say a classifier  use some library that scale in hadoop  not only use a simple parallel processingthe reason PRON be confused be  1the scikit learn python code do not scale in big datum   2the spark mlib be write in scala and not in python   3although h2o be open source PRON do not think PRON be write in python   spark do have a pretty good python api  check out this tutorial   for traditional hadoop stack  take a look at mrjob  PRON let PRON write mapreduce job in python and run PRON on several platform   while check on python xgboost PRON find the existence of this open source project that help create scalable machine learning program  should be worth explore  
__label__agi __label__ultraintelligent-machine __label__superintelligence for people who be not in academia  could PRON please provide some insight into the current stage of development in agi area   be there any project that have breakthrough recently   maybe some news source to follow on this topic    artificial general intelligence  result into nanotechnology and immortality  the state of the art be  that an arms race have start  international cooperation vs ai arms race which result into project like the  blue brain project  which have the goal to emulate the human brain in hardware  but also deeplearn  neuromorphic cognitive and quantum  computing be part of the game  the main bottleneck be to include to ordinary citizen into the technological progress  which mean to educate the mass in build PRON own robot and artificial intelligence software   the main difference between narrow ai and agi be  that narrow ai be visible in computergame like starcraft ai  while agi be a global development which be call singulariy   the state of agi research be pursue the few problem that PRON have be able to break off from the gigantic research problem  these be term which can be more thoroughly look into   a few of the main focus be   one shot learning  PRON know how a person can sometimes learn to do something by something by see literally 1 example of PRON  well current learning method on the whole be not able to accomplish this to the extent that PRON can easily take for grant  work be be do to find way to approach this feat of learning and PRON ’ on PRON way to become much more influential   transfer learning  if PRON have ever play a side scroller like mario  and then PRON give PRON a slightly different game like sonic  odd be PRON would could learn to play sonic faster than PRON take PRON to learn to play mario  this be because of learn  saving  PRON have by transfer PRON mario knowledge to the new sonic domain  this be a much more popular research arm than one  shot  probably because PRON ’ easy to think about but also because there have be promising result of pretrain a network on one set of datum and focus PRON an another task   creativity  curiosity  although one could say that gan have really change how human can be more creative  PRON be difficult to quantify curiosity and creativity this paper give an okay overview  moreover  allow an agent to take the chance and make some mistake as be the nature of creativity concern many people who be focus on ai safety   understanding concept  this be subtle but very very important  current ai methodology struggle with imbue ai with the ability to have concept  by concept  PRON do not mean  PRON kinda look like this neuron in the second last layer be sensitive to tire   PRON mean that understand what a tire look like be just a small part of understand what a tire be  what PRON be use for  what PRON afford someone to do etc  this research direction be in PRON ’ infancy but will be much more influential as more theory and idea be bring forward   despite the progress make in these field and in the many other area in ai  there be still much to be do and understand before PRON can finally have s̶k̶y̶n̶e̶t̶ wall  e 
__label__predictive-modeling __label__scikit-learn __label__categorical-data __label__missing-data __label__data-imputation PRON have be read about how to approach miss categorical feature in test datum  and the most common approach be to use imputation  for example use the last know value or get the majority feature in give row  column   be there a good way to approach miss datum  why can not the classifier just ignore the missing feature  and rely on the know feature  why be imputation necessary   PRON be use scikit learn  and be try to feed in nan to a classification model  naive baye  logistic regression  decision tree  random forest   to see what happen   certain model be able to deal with miss value  naturally   like certain tree base model  most model however be just a mathematical function which be shape after the training datum  a very easy example would be     fx   alpha x1  log2x2  what do PRON do if one of PRON be nan  the function be undefined and no prediction can be make  by impute the value PRON make a reasonable guess where this sample lie on the datum manifold  
__label__neural-network __label__deep-learning __label__nlp PRON be look for a paper that do some comparison between neural network  deep learning  and traditional method in order to prove that dl usually perform good with enough datum   PRON know this be  know  fact but PRON be struggle to find a good paper do some research in this area   thank   
__label__python __label__text-mining __label__data-cleaning PRON have a user  generate list of proper name   for the sake of conversation  imagine PRON be pet name   this list will have many variation of the same name  misspelling  alternative spelling  and even random use of punctuation or space   eg  fluffy  fluffy the dog  flufy dog  fllufy  PRON can get a count of each spelling to figure out that  say  fluffy be the most common at 2000 occurrence while the other be all  lt100   PRON try to condense use the levenshein distance package  python  levenshtein  which have mixed success   PRON would assign a name to a  more likely  name if PRON be levenshtein distance be  lt  3 and the  more likely  name have a high count   when PRON manually search a few known common name  PRON can manually weed out the levenshein winner from the loser  as well as include other levenshein miss in PRON list  eg fluffy the dog and fluffy have a huge distance of 8   be there good technique PRON should be try   credit to emre for set PRON on this path with n  gram   PRON discover the library fuzzywuzzy which be well  build to solve this exact problem   PRON will give a brief synopsis  but a well  write tutorial and documentation make this pretty straight  forward   PRON have a dataframe where name be the index and there be a count column   PRON loop through the index  find high  similarity string and then map the name to the name  include self  with the high count   name  dfindex   iterate through list find the top 10  arbitrary  matching string  gt  90 score  pop off the high  volume order one  def findtopmatchname    mymatch  processextractname  name  limit10  scorer  fuzztokensetratio   mymatch   t0  for t in mymatch if t1   gt 90   topmatch  dflocmymatchessortvaluesbycount   ascend  falseindex0   returntopmatch   topmatchesset     for c in dfindex   returnedmatch  findtopmatchc   topmatchessetappendreturnedmatch   dfrealname    topmatchesset 
__label__python __label__data-mining __label__sentiment-analysis PRON have do a twitter sentiment analysis project use python  PRON need help as to how to performfew step  sentiment analysis on multiple website  especially list of multiple url link and capture respective datum   
__label__python __label__nltk __label__named-entity-recognition __label__stanford-nlp PRON be test the stanfordnertagger use the nltk wrapper and this warning appear   deprecationwarn  the stanfordtokenizer will be deprecate in version  325  please use nltktagcorenlpcorenlppostagger or  nltktagcorenlpcorenlpnertagger instead   superstanfordnertagger  selfinitarg    kwarg   PRON code look like this   from nltk import wordtokenize  postag  nechunk  from nltktag import stanfordnertagger  sentence   today george go to school and meet PRON friend peter     stanford s ner tagger 3 entity classification  st  stanfordnertaggerhome  hercule  desktop  phd  tools  stanford  ner  2017  06  09classifier  englishall3classdistsimcrfsergz     home  hercule  desktop  phd  tools  stanford  ner2017  06  09stanford  nerjar    encodingutf8    tokenizedtext  wordtokenizesentence   classifiedtext  sttagtokenizedtext   printstanford ner tagger     printclassifiedtext   PRON try to use corenlpnertagger but PRON could not find any example or documentation   PRON only find this link   where PRON give something like an example in the comment of the  class corenlpnertaggercorenlptagger    PRON find PRON by search the keyword  corenlpnertagge    PRON try to follow that example with no use   PRON think PRON should start  if that be the correct term  the corenlp server first but if be that the case PRON do not know how   if anyone get any idea or advice PRON would be grateful   imoutidi  PRON also encounter the same deprecation warning   after dig around a bit  PRON look like the new  replacement package can be import with the following   from nltktagstanford import corenlpnertagger  however  when try to run the tag   method PRON end up get an unexpected http connection refuse error  PRON have not figure out if this be an environment configuration issue  miss jar path  or an issue with the nltk codebase  PRON try temporarily turn off PRON local firewall and that do not help  so PRON be guess PRON be one of the other two possibility   PRON hope the python import statement be at least some help  
__label__statistics __label__high-dimensional __label__pattern-recognition __label__regression PRON have a set of file consist of randomly select point from a dataset  each file belong to a particular class   each row in these file contain the coordinate in n  space of the point   PRON would like to compare the distribution in n  space of each of these file  and be inspire by the k  s test for compare histogram   from what PRON have read this method do not extend well to multivariate datum   PRON have previously use pca  but all of PRON variance collapse into a single noisy dimension and clustering method be useless   PRON question  be there a reason PRON should not just use an average of the k  s value across the histogram for each of the n  dimension as a metric for the goodness of fit   be there a good method for compare these distribution   root support kolmogorov test on high dimensional histogram  and the note  for the 2d version  suggest that there be a ambiguity  which PRON deal with by punt  calculate PRON both way  PRON do not know if the code contain anymore detail  but the comment sometimes have reference to paper and the like   there be some additional interesting comment in the note to th1kolmogorovtest   PRON would calculate the mean  overline x and the covariance matrix  c of the joint datum set  and then do a k  s test on the univariate quantity  vxxoverline xtc1xoverline x evaluate on the part  if the k  s test give a significant difference between the part  there be one  if PRON give no significant difference  the test be to be regard as unconclusive  
__label__neural-network __label__deep-learning PRON have be work for a long time with an artificial neural network algorithm  specifically  a siamese neural network  that PRON implement in torch  amp  lua   PRON have be study and play with many detail of this algorithm  momentum alpha  number of minibatch  learn rate  gradient update iteration  dropout  10fold cross validation  etc   but PRON be still face the same old error  after training and during testing  PRON artificial neural network predict almost every test element as positive   PRON train PRON model with a training set  and then test PRON on a test set  both set contain 2000 element   this be a typical confusion matrix result PRON get   false negative fn   21  true positive tp   179  false positive fp   1747  true negative tn   53  and these category value lead to the follow rate   f1score  016839   2tp2tpfpfn   1  good   0  bad   accuracy  0116    tptntpfnfptn   1  good   0  bad   recall  09   tptpfn   1  good   0  bad   precision  009   tpfptp   1  good   0  bad   specificity  003   tnfptn   1  good   0  bad   fallout  097   fpfptn   0  good   1  bad   false discovery rate  091  fpfptp   0  good   1  bad   miss rate  0105   fnfntp   0  good   1  bad   matthewscc  012008     tptnfpfnmathsqrttpfptpfntnfptnfn     1  perfect correlation   0  no good than random prediction   1  total disagreement   as PRON can notice  the fn and tp result be very good  but the fp and tn result be very bad  false positive be too many   the artificial neural network think almost everything PRON process be a positive element  PRON predict the 96  of the element as positive   and the weird thing be that the training set be artificially balance towards negative value  among the 2000 element  PRON be choose 90  of negative and only 10  of positive   PRON also be use the same proportion for the test set  90  of negative and only 10  of positive    do anyone have any idea about what be go on   why do PRON neural network predict so many positive   edit  thank to all for the help  here be some datum and the main core of PRON code   here be some toy datum  PRON can copy PRON into PRON torch terminal    firstdatasettrain     firstdatasettrain1torch  tensor4  5  8  10  36  0  11  22  23  44  49  35  22  6  12  16  16  4  8  10  12  14  3  30  11  12  1  3  12  16  3  4  3  6  8  0  5  12  5  4  18  10  6  8  4  7  12  7  13  3  66  61  9  51  28  7  24  8  43  40  115  2  5  11  11  41  8  2  2  2  11  6  4  4  5  6  6  9  25  16  19  13    firstdatasettrain2torch  tensor4  5  8  6  5  0  27  8  4  8  20  11  6  7  5  22  153  3  2  6  11  28  22  37  13  5  8  17  13  8  9  0  10  14  4  30  7  17  2  5  6  9  1  18  7  3  9  2  200  7  17  16  5  5  19  7  8  8  22  11  20  0  7  3  6  7  12  7  9  9  5  5  23  2  43  13  4  10  21  9  13  15    firstdatasettrain3torch  tensor10  4  11  16  20  0  2  10  17  10  32  30  9  11  10  11  9  8  21  21  9  16  19  13  11  16  9  12  20  14  2  9  7  13  0  17  11  26  10  11  8  2  18  16  10  10  10  7  4  11  32  20  8  19  21  3  7  26  17  19  139  5  10  11  15  13  2  3  2  4  24  11  11  1  8  11  3  7  20  29  24  13    firstdatasettrain4torch  tensor3  6  6  10  24  0  22  3  16  7  25  13  11  24  20  14  9  7  9  10  9  15  7  49  2  13  12  4  21  7  22  14  4  12  14  13  4  12  8  8  88  88  105  87  9  35  12  16  17  18  26  12  9  23  200  13  25  12  29  28  200  10  4  17  16  10  18  3  5  2  26  11  14  3  30  4  4  0  27  25  24  18    firstdatasettrain5torch  tensor14  18  17  14  16  0  19  10  14  6  15  21  15  5  14  22  7  14  18  107  13  18  19  22  25  11  32  46  14  26  8  20  29  64  24  14  25  20  42  7  18  12  14  32  12  20  19  17  35  14  19  12  8  18  32  13  23  35  24  14  32  9  34  39  6  10  51  19  8  23  39  13  200  6  32  21  18  3  32  21  133  38    firstdatasettrain6torch  tensor8  5  10  9  15  0  199  23  21  15  21  17  13  16  11  34  89  7  8  16  7  19  41  61  22  28  4  44  18  17  10  9  31  16  5  23  10  11  23  6  7  5  6  6  11  3  12  16  200  17  30  10  95  32  22  6  11  41  33  24  19  11  10  13  12  21  11  1  6  10  15  6  22  3  13  29  14  2  111  24  27  15    firstdatasettrain7torch  tensor8  15  46  200  200  0  200  200  200  200  92  200  200  90  42  38  76  55  200  75  16  91  86  148  200  200  5  19  22  164  23  57  172  57  3  31  8  17  46  78  11  14  21  21  12  25  11  17  86  8  200  200  200  200  24  14  15  24  200  173  200  7  46  57  25  200  16  7  9  11  100  22  46  6  95  200  9  0  110  27  30  30    firstdatasettrain8torch  tensor9  9  10  34  50  0  6  27  20  29  23  21  9  19  10  16  10  6  14  16  9  20  17  33  89  78  9  8  5  10  5  5  4  8  16  8  14  13  5  3  10  17  12  15  9  3  9  16  8  7  13  14  6  21  19  13  20  19  22  22  20  7  4  7  6  28  21  3  12  4  22  6  11  3  15  20  4  2  12  7  25  10    firstdatasettrain9torch  tensor5  7  18  77  29  0  20  21  35  53  128  42  28  104  10  23  13  11  8  12  19  26  18  33  21  19  13  11  28  87  10  10  200  35  5  11  7  13  20  53  15  7  14  14  7  13  12  9  18  10  121  116  83  72  19  14  12  8  40  39  200  12  21  19  20  25  22  9  4  6  26  2  102  2  76  12  51  3  23  15  18  29    firstdatasettrain10torch  tensor4  14  10  10  12  0  17  7  17  17  26  21  6  12  40  22  12  1  10  20  6  24  33  38  8  22  16  9  12  9  11  3  5  22  12  24  9  22  16  5  17  9  19  22  9  7  7  14  7  9  51  17  84  48  13  2  11  45  33  55  88  5  8  15  5  9  9  10  9  6  10  6  7  4  15  7  6  6  12  26  36  13    seconddatasettrain     seconddatasettrain1torch  tensor18  16  29  7  16  0  11  11  15  11  45  15  10  9  17  23  132  43  27  24  40  22  42  31  9  9  110  53  42  90  3  40  174  23  41  22  8  30  200  13  13  11  11  8  8  19  90  13  200  9  29  13  3  30  25  10  200  17  31  9  25  14  28  10  20  9  34  6  15  30  8  3  81  44  23  12  185  3  11  15  32  19    seconddatasettrain2torch  tensor2  6  9  12  70  0  38  23  52  54  83  60  50  129  36  12  15  17  23  13  5  45  16  98  97  13  3  7  11  26  7  2  7  13  9  3  4  9  3  6  6  7  10  13  6  5  8  10  7  11  96  57  65  177  35  5  11  17  48  179  100  7  7  12  9  21  11  5  10  6  16  8  12  2  9  7  4  3  69  13  11  7    seconddatasettrain3torch  tensor3  6  6  10  24  0  22  3  16  7  25  13  11  24  20  14  9  7  9  10  9  15  7  49  2  13  12  4  21  7  22  14  4  12  14  13  4  12  8  8  88  88  105  87  9  35  12  16  17  18  26  12  9  23  200  13  25  12  29  28  200  10  4  17  16  10  18  3  5  2  26  11  14  3  30  4  4  0  27  25  24  18    seconddatasettrain4torch  tensor13  6  34  155  69  0  34  44  28  57  41  45  27  4  28  29  20  12  52  28  5  18  27  29  21  31  4  7  13  107  14  16  17  13  7  23  17  37  13  29  10  19  14  13  8  26  3  10  6  11  77  85  31  90  23  27  9  28  46  34  200  20  11  23  15  200  0  4  29  4  42  3  14  2  7  15  42  5  49  12  12  17    seconddatasettrain5torch  tensor2  11  19  27  23  0  16  11  18  13  25  18  10  14  15  40  1  9  12  21  17  20  22  25  28  19  12  25  8  18  3  15  11  24  9  16  17  21  23  9  12  13  14  25  19  21  11  8  11  13  18  10  21  24  26  5  20  33  57  25  16  8  26  15  5  9  13  9  7  13  16  11  9  4  9  21  5  8  12  22  33  10    seconddatasettrain6torch  tensor78  13  200  200  200  0  70  200  200  200  200  200  200  18  21  27  11  12  20  58  28  18  22  119  200  200  65  54  178  200  88  95  200  200  24  47  30  26  200  109  76  85  50  65  21  200  4  36  110  30  200  200  200  200  200  101  23  23  200  200  200  19  123  36  200  86  69  6  7  76  38  21  200  1  200  44  59  6  142  30  53  200    seconddatasettrain7torch  tensor10  5  7  12  15  0  35  18  11  11  17  14  4  9  47  77  28  33  94  61  7  37  35  40  4  21  7  17  10  25  11  15  10  20  6  59  18  16  9  26  6  10  25  23  95  13  1  14  13  11  22  5  14  20  23  11  25  33  22  30  64  7  7  27  10  14  4  7  6  4  18  15  10  4  23  71  5  3  81  41  33  13    seconddatasettrain8torch  tensor6  10  14  81  200  0  39  141  200  200  200  200  200  10  4  23  16  11  9  37  8  22  21  74  200  195  6  15  16  30  8  5  19  19  11  71  7  12  29  6  11  14  7  8  7  17  3  12  14  7  200  200  200  200  30  5  17  24  200  155  200  4  19  25  26  39  6  11  4  7  33  9  30  1  27  10  9  16  37  8  30  19    seconddatasettrain9torch  tensor15  2  11  160  11  0  7  9  11  33  30  14  14  12  16  18  33  16  38  12  8  16  26  21  4  16  6  11  15  6  2  4  4  14  4  12  6  8  12  9  16  5  17  13  13  11  10  3  8  3  10  8  4  34  21  6  17  27  27  25  58  7  19  10  12  12  20  4  4  6  36  5  14  4  15  12  4  3  41  11  18  11    seconddatasettrain10torch  tensor20  1  27  187  161  0  200  95  200  200  200  200  200  8  51  34  27  33  50  41  3  34  49  200  190  146  5  15  6  108  30  67  72  13  10  11  20  20  14  11  55  44  56  43  88  52  7  15  3  9  97  145  138  200  200  5  14  54  110  190  200  6  24  18  9  132  8  3  12  4  50  9  17  2  16  6  5  5  43  55  31  22    targetdatasettrain     targetdatasettrain10    targetdatasettrain21    targetdatasettrain30    targetdatasettrain41    targetdatasettrain50    targetdatasettrain61    targetdatasettrain70    targetdatasettrain81    targetdatasettrain90    targetdatasettrain101    and here be the short version of PRON code that implement a siamese neural network have two parallel neural network  upper and low  that process firstdatasettrain and seconddatasettrain  and then compare PRON hidden representation through the cosine distance  inspiration from  httpsgithubcomtorchnnblobmasterdoctablemdnncosinedistance   require  nn     gradient update for the siamese neural network  function gradientupdateperceptron  datasetvector  targetvalue  learningrate  i  ite    function datasetvector  size   return  datasetvector end  local predictionvalue  perceptron  forwarddatasetvector1    local pluschar     if targetvalue   1 then pluschar      end  local meansquareerror  mathpowtargetvalue  predictionvalue2    iowriteiteite     ele  i    pr    predictionvalue   targetvalue    pluschar    targetvalue     gt  meansquareerror    meansquareerror    ioflush     if meansquareerror  gt  1 then  iowrite   large meansquareerror     ioflush     syssleep01    end  iowriten     if predictionvaluetargetvalue  lt  1 then  gradientwrtoutput  torch  tensortargetvalue     perceptron  zerogradparameter     perceptron  backwarddatasetvector  gradientwrtoutput    perceptron  updateparameterslearningrate    end  return perceptron   end  local dropoutflag  true  local hiddenunit  4  local hiddenlayer  4  printltsiameseneuralnetworkapplicationjusttraining startgt      iowritefirstdatasettrain      firstdatasettrain     iowrite    seconddatasettrain      seconddatasettrain     iowrite    targetdatasettrain      targetdatasettrain  n     iowrite   dropoutflag    tostringdropoutflag     iowrite   hiddenunit    hiddenunit    iowrite   hiddenlayer    hiddenlayer    local inputnumber     firstdatasettrain11    PRON be 6  local outputlayernumber  inputnumber  local traindataset     local targetdataset     printcreatin  the siamese neural network      printhiddenunits  hiddenunit  thiddenlayers  hiddenlayer     imagine PRON have one network PRON be interested in  PRON be call  perceptronupper   local perceptronupper nn  sequential    perceptronupper  addnn  linearinputnumber  hiddenunit    perceptronupper  addnn  tanh     perceptronupper  addnn  relu     if dropoutflagtrue then perceptronupper  addnn  dropout    end  for w1  hiddenlayer do  perceptronupper  addnn  linearhiddenunit  hiddenunit    perceptronupper  addnn  tanh     perceptronupper  addnn  relu     if dropoutflagtrue then perceptronupper  addnn  dropout    end  end  perceptronupper  addnn  linearhiddenunit  outputlayernumber    perceptronupper  addnn  tanh     perceptronupper  addnn  relu     local perceptronlower  perceptronupper  cloneweight    gradweight     PRON make a parallel table that take a pair of example as input  PRON both go through the same  clone  perceptron   paralleltable be a container module that  in PRON forward   method  apply the i  th member module to the i  th input  and output a table of the set of output   local paralleltable  nn  paralleltable    paralleltable  addperceptronupper   paralleltable  addperceptronlower    now PRON define PRON top level network that take this parallel table and compute the cosine distance betweem   the pair of output  local generalperceptron nn  sequential    generalperceptron  addparalleltable   generalperceptron  addnn  cosinedistance     maxiterationsconst  1000  learningrateconst  001  local maxiteration  maxiterationsconst   local learnrate  learningrateconst   for ite  1  maxiteration do  for i1   firstdatasettrain do  traindatasetifirstdatasettraini   seconddatasettraini    collectgarbage     local currenttarget  1  if tonumbertargetdatasettraini1     0  then currenttarget  1   end  generalperceptron  gradientupdategeneralperceptron  traindataseti   currenttarget  learnrate  i  ite    local predict  generalperceptron  forwardtraindataseti1    printpredict    predict    end  end  PRON just have to copy this code into a file say siameselua  then open a torch terminal  copy and paste the datum file into the terminal  run dofilesiameselua    and everything should go   the datum have just 10 element  but if PRON need more PRON can download this file  any help will be very appreciate  thank   PRON include an important detail in PRON comment that   with balance dataset  50  positive and 50  negative  PRON have similar result   PRON think the issue be in the activation function of PRON neural network  if PRON use a symmetric activation function in PRON neural network then the fp and fn rate should be roughly the same in the balanced dataset case  PRON can test whether this be the source of PRON issue by flip the sign of the label on PRON datum  ie relabel positive as negative and negative as positive and see what happen   if the fp and fn rate stay the same even when PRON switch what the dataset label positive and negative  then this show that the large false positive rate be not come from the datum but rather from the probability model PRON  in which case check PRON activation function to see if PRON be bias towards label something as positive   PRON network probably do not learn anything  PRON look like loss function be improperly choosen  can PRON provide info what loss function do PRON use  do PRON implement PRON network by PRON  or PRON use any tool   ps  PRON have too low reputation to post comment   of course PRON have wrong loss function in function gradientupdate  PRON be use mse  which should be use for regression problem  when PRON be do classification PRON should use loss function for classification problem  this be old post about PRON  httpsjamesmccaffreywordpresscom20131105whyyoushouldusecrossentropyerrorinsteadofclassificationerrorormeansquarederrorforneuralnetworkclassifiertraining  PRON could also look at wiki  httpsenwikipediaorgwikilossfunctionsforclassification 
__label__text-mining __label__nltk for example for restaurant review usually have suggestion like  go in the evening    order the so and so sauce with this dish   or even  tip  ask for the blah blah blah   how can PRON detect such sentence  how do people usually tackle similar challenge   do PRON create classification rule like  ltmodalverbgtltpreferenceverbgtltoptionalwindowsizeof3gtltpositivesentimentwordsgt   some example of these rule be  would be great  and  could be really good     find this from here   PRON guess PRON would have to use a tagger to categorize word   any blog that have attempt something similar step by step   any help would appreciate   so the way PRON think about PRON be  abstractly PRON be attempt to identify which sentence be  imperative mood  imp  and which be not  one option be to put together a training set  validation set  where each row be a phrase s pos tag  can use some encoder to convert to integer  with a target value of  isimperative   binary  then train a model and restructure each review so that PRON resemble the training set format and have PRON model assign the value  seem the easy and most sustainable method from where PRON be sit  sorry but PRON do not know any easily accessible phrase data set but PRON bet there be one out there somewhere  hope this help  
__label__convex-optimization __label__constrained-optimization __label__semidefinite-programming PRON be work on develop PRON own solverfor implementation on hardware   base on ipm for follow problem   beginequation   beginsplit   minx     frac12ampxf2  tracecx  textst    amp  traceaxleq b   amp  xsucceq 0  endsplit   endequation   as a reference PRON already know how to obtain kkt equation for  beginequation   beginsplit   minx     frac12ampxf2  tracecx  textst    amp  traceax   b   amp  xsucceq 0  endsplit   endequation   dual of this problem will be  beginequation   beginsplit   maxx  y  samp  frac12xf2bybetalogdets  textst   amp  aty  xs  c  ssucceq 0  endsplit   endequation   kkt system will be  beginequation   beginarrayccccccc   x  amp    amp  aty  amp    amp  s  amp   c   amp  ssucceq 0  traceax   amp    amp    amp    amp    amp   b   amp  xsucceq 0   amp    amp  xs   amp    amp    amp   0   amp  endarray   endequation   later PRON sove PRON with newton  rhapson method  have no problem there   PRON question  therefore  be  how to obtain kkt system  when  traceax  leq b   all response or reference be appreciate   
__label__algorithms each observation in PRON data be collect with a difference of 01 second  PRON do not call PRON a time series because PRON do not have a date and time stamp  in the example of cluster algorithm  PRON find online  and pca the sample datum have 1 observation per case and be not time  but PRON datum have hundred of observation collect every 01 second per vehicle and there be many vehicle   note  PRON have ask this question on quora as well   what PRON have be a sequence of event accord to time so do not hesitate to call PRON time series   clustering in time series have 2 different meaning   segmentation of time series ie PRON want to segment an individual time series into different time interval accord to internal similarity   time series cluster ie PRON have several time series and PRON want to find different cluster accord to similarity between PRON   PRON assume PRON mean the second one and here be PRON suggestion   PRON have many vehicle and many observation per vehicle ie PRON have many vehicle  so PRON have several matrix  each vehicle be a matrix  and each matrix contain n row  nr of observation  and t column  time point   one suggestion could be apply pca to each matrix to reduce the dimenssionality and observe datum in pc space and see if there be meaningful relation between different observation within a matrix  vehicle   then PRON can put each observation for all vehicle on each other and make a matrix and apply pca to that to see relation of a single observation between different vehicle   if PRON do not have negative value matrix factorization be strongly recommend for dimension reduction of matrix form datum   another suggestion could be putin all matrix on top of each other and build a nxmxt tensor where n be the number of vehicle  m be the number of observation and t be the time sequence and apply tensor decomposition to see relation globally   a very nice approach to time series clustering be show in this paper where the implementation be quiet straight forward   PRON hope PRON help   good luck   edit  as PRON mention PRON mean time series segmentation PRON add this to the answer   time series segmentation be the only cluster problem that have a ground truth for evaluation  indeed PRON consider the generate distribution behind the time series and analyze PRON PRON strongly recommend this  this  this  this  this and this where PRON problem be comprehensively study  specially the last one and the phd thesis   good luck  
__label__optimization __label__computational-geometry __label__convex-optimization __label__constrained-optimization a trajectory be piecewise define by the follow polynomial form      ft   a  btct2dt3et4ft5gt6ht7it8jt9      for every single segment compose the trajectory  the function must be for every dimension define  x  y  z but to simplify thing let PRON take only one coordinate   that mean  that if PRON want to create a trajectory between the point  pstart and  pend  pass through  pmiddle  then PRON need to define two of these polynomial   for efficiency reason PRON should compute the following functional      min int0t  fracd2ftdt2  2      which PRON rewrote as a quadratic programming problem in the form   beginequation   minimize quad boldsymbolxtboldsymbolqboldsymbolx  st  quad boldsymbolaboldsymbolx   boldsymbolb   endequation   not too difficult  PRON simply calculate the second derivative of the function      ft   2ct6dt12et220ft330gt442ht556it672jt7      put the coefficient of the polynom in the vector form   beginequation   boldsymbolx   beginbmatrix   c  d  e  f  g  h  i  j  endbmatrix   endequation   and get the matrix  boldsymbolq by calculate the derivative of the polynomial above  the hessian matrix of the objective function to be precise    the problem arise when PRON need to define the constrain vector  boldsymbolbwhy  because at the connect point  pmiddle PRON should leave the parameter of the vector  boldsymbolb  free   not define   PRON be suppose to be estimate from the quadratic programming solver PRON  see page 129 here   the idea behind the paper be to leave these coefficient free to be optimally estimate by the solver  the only constraint be that      fracdnfktdtn   fracdnfk10dtn  quad n0  4     or  in human word  all the derivative of the function at the end of a segment must coincide with the derivative of the contiguos segment  in this way PRON should get a trajectory continous on  mathitcn  PRON be usually call joint optimization problem   but fill the vector with undefined value PRON will never  never be able to calculate the coefficient of the trajectory   what be wrong with PRON  be there something that PRON be miss    
__label__machine-learning __label__supervised-learning __label__evaluation __label__regularization __label__overfitting PRON get some metric on validation datum while train a model  and in PRON case the PRON be    025  031  046  057  065  075  077  078  084  084  085   084  084  084  082  08  08  079  078  077  077  077  075  074  073  073  073  073  073  073   PRON can describe like this   in PRON view  the ideal result should be like   be PRON a matter of overfitt   unfortunately  PRON try few time to change the regular coefficient to avoid overfitt  and adjust learn rate coefficient to slow down  but PRON be still  convex    how can PRON achieve the ideal result show above   much appreciate if anyone would give PRON some constructive tip   yes  what PRON be see be a classic case of overfitt   PRON state that PRON use a linear model such as logistic regression  to regularize these type of model  usually l1 andor l2 regularization be apply  l1 regularization be simply  w1  and l2 be  w2  2  usually   another method be to alter the label of the model in a specific way  which be a method of regularization PRON create  shameless plug   here be the link to the paper  httpsarxivorgabs160906693  hope this help  
__label__pde __label__finite-difference __label__numerical-analysis PRON be consider finite difference method and PRON error analysis for solve hjb equation of the follow form      vtsigmaxvxquad xin mathbbr       where  sigma be a give function on  mathbbr  with possible regularity assumption    could PRON give PRON a simple  perhaps first  order  fd scheme and PRON error analysis  stability and convergence in spatial discretization  for solve the above equation  PRON will also appreciate if PRON could give PRON some referencestextbook  paper  on this subject  PRON just want to study the common technique to study such numerical method   PRON know there be a lot of literature for solve hjb equation in quite general form  but PRON hope to start with this concrete example   one may prefer to write PRON equation in the form     vt  sigmax  vx     where  sigmax  ge 0  for  x in r PRON need to define an initial condition eg     vx0   v0x       let PRON skip a treatment of boundary condition  PRON hardly can solve the equation numerically on an unbounded domain   note that the exact solution  vx  t for a fix  x be non  decrease function in time   the simple  first order accurate  numerical scheme be base on rouy  tourin method  PRON aim to find the value  vin that approximate the unknown exact value  vxi  tn for discrete point  xi in r and  tn  gt  0 let PRON suppose uniform step  h  xi1xi and  tau  tn1tn  firstly   vi0  v0xi furthermore  the scheme can be formally write as follow     vin1   vin  tau sigmaxi  partialx vin     where a proper finite difference  partialx vin  backward or forward one sided  shall be use to approximate  partialx vxi  tn  to make the story short such appropriate approximation be base on the fact that the solution  v for a fix  x be non  decrease  therefore one have to choose one sided finite difference depend which of three value   vi1n   vin   vi1n be maximal  namely     partialx vin   vi1n  vinh hbox   if     vi1  ge maxvin  vi1n       or     partialx vin   vin  vi1nh hbox   if     vi1  ge maxvin  vi1n       or     partialx vin  0 hbox   if     vi  ge maxvi1n  vi1n       there be other first order accurate method  eg osher  sethian scheme  but this one be consider  less diffusive    the stability of scheme can be prove if all local courant number     ci  tau sigmaxi   h     be less or equal to one  in fact in such case the discrete minimum and maximum principle be fulfil for numerical solution  eg no unphysical oscillation    as a reference PRON would recommend the book of sethian or  and the book of osher  fedkiw  PRON be easy to find the exact reference  ps see level set method    ps PRON be ask about stability in comment  in fact  as PRON mention above  one can prove even more  the scheme can be write equivalently as     vin1    1  ci  vin  ci max   vi1n  vin  vi1n      so for  ci le 1  the value  vin1 be the convex combination of two value  consequently     vin1  le max vi1n  vin  vi1n        
__label__machine-learning __label__dataset some time PRON come across dataset in which class be imbalanc  for eg  class a may have 2000 instance but class b have only 200  how can PRON train a classifier for such dataset   well  PRON just train PRON on unbalanced dataset  PRON be not a problem  PRON do not think PRON need to apply some special technique   the only case where PRON may want to do something special be if PRON class be skew by PRON nature  if the skewness be the property of class PRON  not of just PRON specific dataset   for example  if PRON build a classifier which will tell if someone have a malignant tumor in x  ray image or not  by nature of the task absolute majority of patient will not have a malignant tumor  in such case PRON may want to tune PRON classifi algorithm a bit  for example introduce a weighting into PRON svm or smth like PRON   a special case be if PRON have extremely unbalanced class  for example 100000 of positive example and 20 negative  in such case PRON will want to go away from classification task to the approach call  anomaly detection    there be a few of other approach PRON can take to try to balance PRON class distribution     subsample majority class  PRON can balance the class distribution by subsampl the majority class   oversample minority class  sample with replacement can be use to increase PRON minority class proportion   add noise  a more sophisticated scheme be to add gaussian  or other suitable  noise to the exist instance of the minority class in an effort to create a great number of representative  but diverse   instance   smote  a popular method to synthesise  minority class instance with great complexity than pure noise addition be smote  synthetic minority oversampling technique   this use a k  member neighbourhood in feature space to impute new instance   weka have a filter for this   though there be some evidence that this technique be not overly beneficial with high dimensional datum here  
__label__r __label__clustering __label__k-means PRON have be try to understand the different k  means cluster algorithm mainly that be implement in the stat package of the r language   PRON understand the lloyd s algorithm and macqueen s online algorithm  the way PRON understand PRON be as follow   lloyd s algorithm   initially ‘ k’ random observation be choose that will serve as the centroid of the ‘ k’ cluster  then the follow step occur in iteration till the centroid converge   the euclidean distance between each observation and the choose centroid be calculate   the observation that be close to each centroid be tag within ‘ k’ bucket   the mean of all the observation in each bucket serve as new centroid   the new centroid replace the old centroid and the iteration go back to step 1 if the old and new centroid have not converge   the condition to converge be the follow  the old and the new centroid be exactly identical  the difference between the centroid be small  of the order of 10  3  or the maximum number of iteration  10 or 100  be reach   macqueen s algorithm   this be an online version where the first  k  instance be choose as centroid   then each instance be place in bucket depend on which centroid be close to that instance  the respective centroid be recalculate   repeat this step till each instance be place in the appropriate bucket   this algorithm only have one iteration and the loop go on for  x  instance  hartigan  wong algorithm   assign all the point  instance to random bucket and calculate the respective centroid   start from the first instance find the near centroid and ass that bucket  if the bucket change then recalculate the new centroid ie the centroid of the newly assign bucket and the centroid of the old bucket assignment as those be two centroid that be affect by the change  loop through all the point and get new centroid   do a second iteration of point 2 and 3 which perform sort of a clean  up operation and reassign stray point to correct bucket   so this algorithm perform 2 iteration before PRON see the convergence result   now  PRON be unsure if what PRON think in point 4 in the hartigan  wong algorithm be the correct method of the algorithm  PRON question be  if the following method for hartigan  wong be the correct method to implement k  mean  be there only two iteration for this method  if not  what be the condition for convergence  when to stop    another possible implementation explanation what PRON understand be   assign all the point  instance to random bucket and calculate the respective centroid   start from the first instance find the near centroid and assign that bucket  if the bucket change then recalculate the new centroid ie the centroid of the newly assign bucket and the centroid of the old bucket assignment as those be two centroid that be affect by the change   once there be a change in the bucket for any point  go back to the first instance and repeat the step again   the iteration end when all the instance be iterate and none of the point change bucket   this way there be a lot of iteration that start from the beginning of the dataset again and again every time when an instance change bucket   any explanation would be helpful and please let PRON know if PRON PRON understanding for any of these method be wrong   
__label__apache-spark PRON have a spark dataset contain a column of sparsevector type  additionally  PRON have another sparsevector  x which be not a part of the dataset  PRON want to order PRON dataset accord to the close distance  or similarity  relative to  x  can anyone help PRON with how to implement this   PRON appear that problem be solve with bucketedrandomprojectionlsh  after fit  ampamp  transformation  and approxnearestneighbor result dataset contain distcol  which  as per lshscala    param distcol output column for store the distance between each result row and the key  
__label__time-series __label__rnn PRON be try to build a rnn that can take time series as input for example from various sensor   while build the rnn there be a couple of initial length that must be define   the size of the cell and the length of how much PRON will unroll the rnn   from what i understand  cell size be arbitrary and can be modify independent of how the input be shape  but how much PRON will unroll the rnn depend on PRON input length   if i have a time series of 1 hour consist of 1000 value  should i choose to unroll the rnn 1000 time  or be PRON too much   PRON goal be to classify the whole time series  and ideally i want all 1000 time step to be take into account for this classification   
__label__statistics __label__data __label__ab-test __label__marketing PRON be an analyst who be new to retail store testing  for example  customer response to a store new layout or a new product or process in one or 2 store    any suggestion for a good book  blog  any source where PRON could learn about this  the challenge PRON see be  the sample size be usually one or 2 store for the test so how do PRON do statistically significant testing with such a small sample  test store or control store be match 1  1 but often not select randomly  any guidance in learn about the data science in this area be greatly appreciate   thank    PRON think PRON confusion be that PRON only have two store  but PRON may have more than 2 datum point be generate from those two store  for example  PRON may be interested in how much individual customer spend at the two store  PRON can look at receipt total from the store  then PRON would just run an independent t  test on the receipt total datum  because PRON have essentially two group  group a come from store a s reciepts and group b come from store b s receipt   use this methodology PRON could look at any number of response variable    the new science of retailing  how analytic be transform the supply   book by ananth raman and marshall fisher have some very good information of datum science apply to retail  although  PRON do not address PRON case directly  PRON do have a chapter on store level execution that may help  and another chapter on assortment planning which may have relevant information  
__label__optimization this be a bit of a long shot  but PRON be hop somebody may have some insight  not sure of a good forum than here but open to suggestion   PRON have implement the optical flow algorithm from the paper an improved algorithm for tv  l1 optical flow  PRON have try to stick to exactly the same parameter as the article explain  PRON have quite detailed implementation note   and yet PRON can not reproduce PRON result   from PRON analysis  PRON will show image in a moment   PRON appear that the algorithm work well with small image  so in the coarse  to  fine pyramid PRON can see the flow be accurately calculate for small  scale version of each image  then when the image be upscal beyond a certain point the optimisation seem to converge to some poor local minimum  to confirm this suspicion PRON reran the algorithm with a scale factor of 09 between pyramid level  instead of the 05 the article use  and the result be much improve  PRON seem with a small enough up  downscal factor PRON avoid the poor local minima  here be the flow  middlebury backyard scene  with scale factor of 05  07  09 in that order    these be the iteration  by  iteration sequence of each scaling factor  here PRON see that even 05 work well on small image    05  httpyoutubeehto7ljemra  07  httpyoutubepltu3viowg  09  httpyoutubelk5ep865u0e  PRON have fiddle with all the other parameter of the algorithm  and PRON do not have any major impact on this phenomenon  PRON can upload PRON matlab code if anyone be interested  so PRON question be   do anyone have an alternate explanation for this phenomenon  the fact that PRON can not reproduce PRON result be trouble PRON greatly   PRON look to PRON like PRON may not be give as much weight to the regularization  tv andor l1  as PRON be  that would give the kind of result PRON be see   a good rule of thumb be that the maximum optical flow in each level should be about 1  in the unit of the current scale    from read PRON article  PRON seem PRON be rescale PRON image to the  11  interval  this be equivalent to use another regularization weight  so maybe PRON miss this step   PRON hope PRON help PRON anyway   PRON turn out PRON be not multiply the dual variable by 4 on upsampling which keep PRON out of sync with the primal variable  because the upsampling add zero row  col and blur so to maintain value integrity PRON need to multiply by the ratio of new zero entire to old entry   so trivial and yet so frustrating  
__label__clustering __label__regression __label__statistics __label__decision-trees __label__descriptive-statistics problem   PRON have a regression problem and PRON decide to useg gradient boosting regression trees to solve PRON  after all the preprocessing  PRON end up have around 130 attribute  70 k row  and PRON cross  validate r  squared be 062   work so far   to increase PRON r  squared  and tighten PRON prediction interval  PRON try use a hint  feature that divide the target into linear group  PRON manually decide for 4 group and place the target into 4 bin   why 4  not too small not to big and not cleverly decide     PRON be aware that this be not a usual way to go  but this be doable for PRON application since eventually  PRON will be collect these hint from the user   PRON will be ask for the target value for that user and suggest PRON  PRON another target value depend on PRON  PRON datum    so PRON would be possible for PRON to place each user into such group since PRON seem like not all of the user behave the same and PRON need a separate modeling for the one that be extremely high  low target value  plus the distribution be extremely right  skewed   after use this cheat  hint  feature  PRON cross  validate r  squared become 081  however  obviously  divide the target into linear group like that  bring the disadvantage of have a step  like model that have a clear separation between those group while this be not necessarily always the case  a user that give PRON target value most probably but not necessarily should belong to that group   the boundary value for such binning should not be manually decide  and this model would definitely need to be smooth   next step   after see that such an approach can be helpful  and PRON could really make use of the user  give value   PRON be now think about smart way to separate between the user regard these hint  PRON be think about apply hierarchical clustering and use the cluster information as a hint  feature but not sure if this be the good way to proceed   would PRON maybe have suggestion for PRON for such method  and paper would be great  or statistical approach to figure out such boundary value  other than density  base method    cluster be definitely something that can help  as PRON describe  the issue be that PRON can see that not all instance  behave  the same way  so if PRON can cluster PRON into group that do behave more similarly  PRON would probably improve   any clustering technique would work in theory  although  technique like k  mean for instance do favour cluster of even  size  and so if PRON problem be not necessarily  balanced   PRON would be cautious   what PRON could also do to improve PRON clustering be to look at which feature be more discriminative  perhaps look at the information gain  and use a weighted distance to favour these feature   another way PRON feel could help PRON be if PRON use meta learn  stacking  by that PRON mean that PRON could use decision tree to split PRON datum in a supervised way  as oppose to cluster   and at the leaf  use a more elaborate regressor than majority voting  
__label__python __label__quadrature __label__scipy PRON want to plot a numerical integral function of some function  f use scipy and matplotlib  how can PRON do this   PRON try the following but PRON do not work  run with ipython  pylab    import numpy as np  from scipy import integrate  def fx    return xnpsin1x   x  nparange05050001    plotx  fx    def fx    return integratequadf0x   plotx  fx    PRON also try to vectorize the function f without success   replace the last line by  plotx   fx0  for x in x    that should do PRON   edit  PRON can define PRON function f as  def fx    try   return  integratequadf  0  y  for y in x   except typeerror   return integratequadf  0  x   first of all  PRON function  xsinfrac1x be singular in  x0 PRON may want to add an if clause like this   def fx    if absx   lt  1e10   r  x  else   res   xsin1x   but this do hurt speed  mask array would be good    the reason why PRON code do not work be because  scipyquad only accept a single value as left and right boundary  scipyquad return a tuple  val  err  contain the value of the integral and an estimate for the error  so PRON need to unpack PRON   this code work   import numpy as np  from scipy import integrate  import matplotlibpyplot as plt   matplotlib inline  def fx    if  npabsxlt1e10    re  x  else   re  xnpsin10x   return res  x  nparange05050001    plotx  fx    def fx    re  npzeroslikex   for i  val in enumeratex    y  err  integratequadf0val   resiy  return res  pltplotxfx   
__label__reinforcement-learning be there a way to teach reinforcement learning in application other than game  the only example i can find on the internet be of game agent  i understand that vnc s control the input to the game via the reinforcement network  be PRON possible to set this up with say a cad software   PRON will see a lot of game example in reinforcement learn literature  because game environment can often be cod efficiently  and run fast on a single computer that can then contain the environment and the agent  for classic game  such as backgammon  checker  chess  go  then there be human expert that PRON can compare result with  certain game or simplified game  like environment be commonly use to compare different approach  much like mnist handwritten digit be use for compare supervised learning approach   be there a way to teach reinforcement learning in application other than game   yes  informally PRON could apply reinforcement learning approach whenever PRON can frame a problem as an agent act within an environment where PRON can be inform of the state and a goal  influence reward value  more formally  reinforcement learning theory be base upon solution to markov decision processes  so if PRON can fit PRON problem description to a mdp then the various technique use in rl  such as q  learning  sarsa  reinforce  can be apply  this fit to theory do not need to be perfect for the result system to work  for instance PRON can often treat unknown or imperfectly observed state as effectively random to the agent  and consider this part of a stochastic environment   here be some example of possible us for reinforcement learning outside of recreational game   control logic for motorise robot  such as learn to flip pancake and other example  here the environment measurement be make by physical sensor on the robot  the reward be give for complete a goal  but may also be adjust for smoothness  economic use of energy etc  the agent choose low  level action such as motor torque or relay position  in theory there can be nest agent where high level one choose the goal for the low  level one  eg the robot may decide at a high level between do one of three task that require move to different location  and at a low level may be decision on how to control motor to move the robot to PRON chosen goal   self  drive car  although a lot of focus on sensor interpretation  see road marking  pedestrian etc  a control system be require in order to select accelerator  brake and steering   automated financial trading  perhaps a game to some  there be clear real  world consequence  the reward signal be simple enough though  and rl can be adjust to prefer long or short term gain   be PRON possible to set this up with say a cad software   in theory yes  but PRON do not know what may be available to do this in practice  also PRON need one or more goal in mind that PRON code into the agent  as reward value that PRON can observe  before give PRON a virtual mouse and set a task to draw something  computer game come with a reward scheme build in as PRON scoring system  and provide frequent feedback  so an agent can gain knowledge of good vs bad decision quickly   one of the cool example of reinforcement learning be an autonomous flying helicopter  PRON have a chance to learn some of the stuff do by andrew ng and other recently  here be the research article paper  there be other similar paper too  PRON can google PRON if PRON want to learn more   PRON can also see PRON in action in in this youtube video   here be another completely different application in finance apparently  
__label__deep-learning __label__convolutional-neural-networks __label__tensorflow  how to train darkflow for PRON custom object really really fast during debug in quad core pc and without gpu   can PRON train with about 10 image and test with only those image  just to check if all convolution be work as expect  and with 20 epoch    there be only once class and all license plate be similar in pattern with vary in angle and PRON digit in plate   PRON be use tiny yolo config and weight  so  what all parameter PRON should tune in yolo base cfg file to do PRON  PRON feel if tv like object can be detect with same training weight and config for tiny yolo then license plate too   overview   PRON be train for object  license plate  with darkflow  PRON try with about 100 custom dataset PRON have create  this be only for initial poc  actual implementation will have more number of image  and upon testing with test image with train graph  object be not highlight rather highlighting square be show at random location within image and random in number  start with 2  3 square box to lot many number  but non of those be highlight to the actual license plate object  PRON take PRON 20 hour of training time to verify PRON  PRON use training image for test as well  and also a plane black screen image to test what be go on  but highlighting square be still random in number and location even on blank screen image    assume that PRON be use darkflow  PRON can train PRON model with train flag and do not use gpu flag since PRON do not want to use the gpu memory  and yes PRON be possible to train with just 10 image  basically over  fit  just to confirm the working of PRON model  place those 10 image in train  image directory and the annotation of these image in train  annotation directory  in pascal voc format    for fast performance  use pre  train weight file for tune PRON model so that PRON do not need to train for initial same layer  and since PRON just want to detect a single object  license plate  use tinyyolovocweight   further more  use epoch flag to reduce the number of epoch   flow model cfg  tiny  yolo  voc  newcfg load bin  tiny  yolo  vocweight train annotation train  annotation dataset train  image epoch 20  change the number of class and the number of filter as define in the readme first  then PRON can change the height and width of the input image the model will be train on  high resolution for more accuracy while low resolution for high performance and speed  PRON can also change the activation function for the layer  but PRON recommend leave PRON unchanged  
__label__matlab __label__finite-difference __label__nonlinear-equations __label__discretization __label__nonconvex PRON face to the follow problem      1ux2uyy   2uxuyuxy    1uy2uxx0 problem need to be discretiz and assemble   do anybody know how to proceed in matlab   
__label__python __label__scikit-learn __label__decision-trees PRON be use sklearn decision tree classifier with some continuous feature   when PRON run exportgraphviz PRON see the same feature in more than one node and with different value  example   PRON would like to take the top important one and want to use featureimportance  for that   the problem be that featureimportance  be array without reference to the tree node  PRON have the original feature but as each one can be more than one time in the tree PRON be not sure how to relate importance to node   PRON think PRON be mix two different thing here   the featureimportance   this be an array which reflect how much each of the model s original feature contribute to overall classification quality   the feature position in the tree  this be a mere representation of the decision rule make in each step in the tree  a feature positions  in the tree in term of importance be not so trivial   there be some potential heuristic for understand the relation between the two  if a feature dosnt appear in the tree PRON have 0 importance and generally the high the feature be in the tree the more important PRON be  assume PRON be compare to another feature on the same branch   
__label__philosophy __label__strong-ai __label__ai-design __label__human-inspired the above question PRON be perhaps too broad for this forum  hence PRON be phrase PRON as a request for reference   human have be endow with personality by nature  and PRON be not clear  to PRON at least  if this be a feature or a bug  this have be explore in science fiction by various notion of borg  like entity  PRON be PRON belief that  for narrative reason  such story usually end with the human with PRON flawed personality win in the end   be there expert who have analyze  perhaps mathematically  design criterion for an ai agent with weakly enforce goal  eg  to maximize reproduction in the human case  in an uncertain environment  and end up with the answer that a notion of personality be useful  if there be philosopher or science fiction writer who have examine this question in PRON work  PRON would be happy to know about those too    personality  be something of a  suitcase word   minsky  for quite a large collection of  presumably reasonably consistent  observable trait   PRON seem clear that there be a certain collective advantage in have a consistent personality  specifically that PRON afford observer some learn gradient in an otherwise uncertain environment  this be of particular importance because those consistency may have be arrive at use different learning mechanism than the one a give observer have   hence  in any non  trivial coevolutionary system  other organism will inevitably make use of any such consistency  consider a simple robot  call alice  say  that have the trait of  quickly flash red when PRON see a blue robot   PRON make sense for all observer to exploit everything that PRON perceive as correlate with alice s behavior  in particular  the prediction that a blue robot be likely to be present   the good reference PRON can recommend on this  which show that PRON tend to ascribe  personality  to even very simple mechanism  be  vehicle  by valentino braitenberg   first  a note on the question PRON   human have be endow with personality by nature  and PRON be not clear  to PRON at least  if this be a feature or a bug   in PRON opinion  this be a statement that constrain the question  since PRON assume that the personality be give  to PRON  PRON feel a bit like play god  artificial  give  intelligence would hence imply artificial  give  personality  this approach to the problem seem to be support by the next fragment   a notion of personality be useful  PRON point to the above because PRON do not think that artificial intelligence  intelligence PRON  actually  need to be give or assign  or even have a use in the sense of a purpose   the previous note be about emergence  which be a topic that user217281728 briefly address in PRON answer  in this second approach  the particular trait just happen  or develop  the interaction between the  so  call  agent and PRON environment  as well as fellow agent can give place to new behaviour pattern  not design beforehand   in an evolutionary approach  if the personality would happen to have an advantage  or at least not represent a disadvantage   then PRON could just appear  of course  PRON be make a number of assumption and demarcation here as well   PRON be think about embodied intelligence  PRON speak of evolutionary robotic  PRON think on social issue be of importance  PRON assume that personality could emerge  now  an example that PRON find extremely interesting be that of the little mobile robot which could move around and end  up in a pool of food or a pool of poison  and PRON  somehow  by some odd chance  recognise or make a relation between signal send by other robot  and the presence of food  or not  that be more or less the thing  some robot  kind of  learn to conceal information and thus have more time to eat PRON  well  PRON would have a couple of personality adjective for such guy   here PRON find the article and here PRON find some video and relate stuff   and with that  PRON land at PRON last point  PRON human put the adjective  accord to PRON social conditioning  PRON call marvin depressive and r2d2 lovely and charming   if PRON perceive PRON personality as constructive or damaging  will always depend on PRON own judgment  in the end  PRON be quite common under human to disagree on personality issue  too   bonus  remember when hal get emotional  on the face of death   PRON get human when PRON lose PRON cool  before the flawed  personality human astronaut   as for agi  everything be break down into group  the be all control by a part call the  spark   and then there be the agent  little sub routine  the spark be the be the main judge of the system  PRON compare the performance of the agent  the spark let one agent out of sleep and record how well PRON do at get reward for the body  as a whole  if a active agent do good PRON be replicate in free memory with a few mutation   the  spark  and agent first look at what be on the detector and spark select the good agent  and the spark turn on and off agent like in a orchestra  as the system mature many agent will work in parallel   this process be the the subconscious mind   after a while one of the agent be convert to a copy of of spark  this new copy be then modify and be call the off spark  the conscious mind  PRON will take on the control of the agent too  but spark be still master of all   off spark can activate agent  but PRON can organize agent on a massive scale   PRON will develop many routine  many agent work in parallel   once thes massive agent swarm develop into perfect routine will become a reflex  and all reflex will be give over to spark the subconscious part of this system  store for latter use   if off spark need to get over to a new area to create new pattern  like work the slot machine for the first time  to develop new routine  PRON need automate subconscious process of walk over the that area  off spark start the walking routine and hand off over to spark   if the craving for food become too strong then spark shut down off spark  and then use off spark routine to get food or what urgent goal that need to be take care of   if all urgent goal be take care of then there will be free will because off spark will be in control  and spark will be push into helper mode   the agi have a internal pattern editor  that be only use by off spark  that cut up exist physical routine and try to rebuild new and different routine  this be a trial and error generator  once a editing procedure work and perfect PRON be hand over to spark for storage  off spak initiate a editing routine and then spark take over   this editing of pattern will lead to a internal 3d simulator   so off spark make new physical routine and new pattern editing routine  and all this be do on the back of automate perfect routine of old    usefulness  can only be measure against some purpose  once PRON pass agi  which really mean  generally animal  like ai  because PRON seem general to PRON   then PRON have pass into a world of potentially undefined behavior   part of what make a human free and set PRON apart from the other animal be the fact that PRON purpose  capability and possibility be not fully define  PRON be open end   to clarify term  PRON interpret  strong agi  as  potentially super intelligence  but at least human level    when PRON say  strong agi  vs just  agi   PRON be not say that one be more open end than the other  PRON be say that the strong one be simply smart on some axis   so to ask whether a particular trait would be  more useful  to a strong agi  that would depend on the purpose of the agi  but here be the catch  if a thing have just one purpose  then the most efficient solution to fulfil that one purpose will always be a narrow solution  not a general one  when the purpose of the object be know before hand  give that object more general capability than be necessary for that purpose be counterproductive   that be why PRON be impossible to make declarative prescription about what a free  open  ended agi should or should not need  such prescription would nullify the open  end freedom of utility that PRON generality imply  PRON can speak declaratively about less robot and animal   but for any give problem  the solution PRON will want to find be the most well  define  narrow  efficient one available  not the most general one   in other word  sure  personality could be useful for a strong agi  assume the problem in question involve personality  
__label__finite-element __label__numerical-analysis PRON be program cubic element and very confused how to find interpolation function for cubic finite element   PRON have element like this in bi  unit isoparametic system    1  hspace15pt   13  hspace15pt   13  hspace20pt   1    lt distance     1  hspace30pt   2  hspace30pt   3  hspace30pt   4  hspace15pt  lt nodes  physically  the element go from 1 to 4  node be locate at   1234 in natural  isoparametric bi  unit system   nod   1234 be locate at   113131  PRON have  textbfx   n1x1  n2x2  n3x3  n4x4    where   ni be shape function in geometric coordinate and  xi be nodal location   lagrangian interpolation function for 1d in term of physical coordinate be     n1x   frac   x  x2   x  x3   x  x4      x1x2   x1x3   x1x4    n2x   frac   x  x1   x  x3   x  x4      x2x1   x2x3   x2x4  similarly for other two node  PRON be not write PRON    in term of natural coordinate     n1xi   frac   xixi2   xixi3   xixi4      xi1xi2   xi1xi3   xi1xi4    n2xi   frac   xixi1   xixi3   xixi4      xi2xi1   xi2xi3   xi2xi4    textbf b  bigg  fracpartial n1partial x   fracpartial n2partial x    fracpartial n3partial x   fracpartial n4partial x  bigg     xxi   nxixi     jacobian  j be give by    j  fracpartial xpartial xi     when PRON introduce isoparametric coordinate     textbf b  fracpartial nipartial x   fracpartial nipartial xi  fracpartial xipartial x   fracpartial nipartial xifrac1j  stiffness matrtix be give by    textbf k  int11  textbf   bt  textbf  e  b  j  dxi    can someone tell PRON how to calculate  textbf b   textbf j for above give element  many reference only linear element  in which PRON can easily get away with constant  PRON try look some code  but no luck as many of PRON still have linear element  any reference be welcome   edit  after james  answer  PRON modify PRON question and reflect finding   here be   fracpartial nipartial xi   beginbmatrix   27xi216  9xi8  116  amp   81xi216  9xi8  2716  amp   27xi216  9xi8  116  amp   27xi216  9xi8  116  amp   end  bmatrix        xi beginbmatrix  1  2  3  4 end  bmatrix     so    jxi   fracpartial nipartial xi  xi be function of  xi    bxi  t   frac1j  beginbmatrix   fracpartial n1partial xi   amp   fracpartial n2partial xi   amp   fracpartial n3partial xi   amp   fracpartial n4partial xi   amp   endbmatrix      b be also a function of  xi    k  int11  beginpmatrix   frac1j  beginbmatrix   fracpartial n1partial xi   amp   fracpartial n2partial xi   amp   fracpartial n3partial xi   amp   fracpartial n4partial xi   amp   endbmatrix   endpmatrix   d  beginpmatrix   frac1j  beginbmatrix   fracpartial n1partial xi    fracpartial n2partial xi    fracpartial n3partial xi    fracpartial n4partial xi    endbmatrix   endpmatrix   j dxi  after that  PRON find  k by gauss integral as  be these formula correct  especially  k  can someone throw light on how to find gauss quadrature for  k   from  xxi   nxixi the jacobian be give by   beginequation   j  fracpartialxpartialxi    sigmafracpartialnipartialxixi   endequation   the integration PRON be do at quadrature point  and thus the jacobian be find by evaluate  fracpartialnpartialxi at quadrature point   a good reference be introduction to the finite element method   here be some more detail use PRON own code  c   consider a shape function for a quadratic 1d line give by   void linequad1dshapedouble xi     shape function for quadratic 1d line at node  xi   n0   05xi1xi    n1   05xi1xi    n2   1xixi     and correspond derivative   void linequad1ddshapedouble xi     derivatif of shape function for quadratic 1d line at node  xi   dn0   05xi  dn1   05xi  dn2   20xi     now find the jacobian at point may look like   double elementjacobiandouble xi     shape  sh  shapefactorynewshapetype      shdshapexi    jmat00   00   forint k0kltnpek    jmat00      shdnkxptsk      jacobian  jacobian matrix  and inverse jacobian matrix  jac  jmat00    in PRON case the dimenion be 1  so PRON would not need a 2d array for the jacobian  imat00   10jac     find the  b matrix at a point look like   double elementtempdiffmatrixdouble xi     shape  sh  shapefactorynewshapetype      shdshapexi    set jacobian and inverse jacobian matrix and return jacobian  double jac  jacobianxi    temperature differentiation matrix b  forint i0iltnpei    bmati   imat00shdni      return jac     and finally stiffness matrix   void elementstiffnessmatrix      forint i0iltnpei    forint j0jltnpej    kmatij   00  stiffness matrix      double u10  quadraturerule gngauss  ndim  type    forint ip0ipltgnintpointsip  loop through quadrature point    u  gxiiip    double det  tempdiffmatrixu    creat PRON b matrix and return jacobian  both at integration point xiiip   double dv  detgwiip    forint i0iltnpei     npe   number of points per element  forint j0jltnpej    double s  00   forint k0kltndimk    ndim  1 in PRON case  s   bmatkibmatkj    kmatij    dvs           PRON code actually solve for 1  2  and 3 dimension but PRON have try to edit out the multi  dimensionality for PRON since PRON look like PRON be only worried about 1d hopefully PRON can parse the basic idea from these code snippet  also note that the these code snippet be for solve the heat equation  ie no  e matrix   but again hopefully PRON can parse the basic idea   edit  ok so for more detail on the gauss quadrature part here be an example find in  programming finite elements in java  by gennadiy nikishkov  consider try to integrate the following   beginequation   PRON  int11fxidxi   endequation   use two integration point  because PRON be use two integration point this should integrate to a third degree polynomial  PRON write the integral as   beginequation   PRON  fxi1w1   fxi2w2   endequation   where  wi be the weight and  xi1 be the undetermined point  this formula should be valid for any third degree polynomial  in particular PRON should work for  fxi1    fxixi   fxixi2  and  fxixi3   beginequation   int11dxi  2  w1   w2    endequation   beginequation   int11xi dxi  0  xi1w1   xi2w2    endequation   beginequation   int11xi2  dxi  23  xi12w1   xi22w2    endequation   beginequation   int11xi3  dxi  0  xi13w1   xi23w2    endequation   solve these result in  xi1frac1sqrt3   xi2frac1sqrt3   w1   w2   1 in general for the one dimensional case   beginequation   PRON  int11fxidxi  sigmai1nfxiiwi    endequation   where n be the number of integration point   xii the undetermined point  and  wi the weight  so in PRON stiffnessmatrix   function  method above PRON can see that in pseudo code PRON be do the follow   for each integration point p   find the jacobian  dv  at undetermined point xip   find weight correspond to undetermined point xip   find det  jacobianweight correspond to point xip   find b matrix at the undetermined point xip   for i  1number of pont in element   for j  number of point in element   kij    dvbibj   these undetermined point and weight would be store in an array or table   PRON have take a look on these equation   PRON know PRON have a 1d element  but rewrite the equation would eliminate confusion if PRON want to write element in 2d and 3d  therefore PRON stiffness matrix should be  beginequation   mathbfkel   sumingauss   mathbfbt mathbfe  mathbfb  wxi  i  detj    endequation   for the numerical integration with gauss  PRON stiffness matrix formula use  j instead of  detj  which be not wrong  the determinant of the jacobian be only significant for multidimensional case   the formula for PRON jacobian should be  beginequation   j  sumin  fracpartial nipartial xixi  endequation   the jacobian in the one  dimensional case be define as  j  left  fracpartial xpartial xi  right for  beginequation   beginbmatrix   fracpartial npartial xi   endbmatrix     j  beginbmatrix   fracpartial npartial x   endbmatrix   endequation   beginequation   beginbmatrix   fracpartial npartial xi   endbmatrix     beginbmatrix   fracpartial xpartial xi   endbmatrix   beginbmatrix   fracpartial npartial x   endbmatrix   endequation   the jacobian in the isoparametric concept approximate the transformation by the derivative of the shape function  here  fracpartial xpartial xi have to be approximate  like the x  coordinate in the element would be calculate by  beginequation   x   beginbmatrix   n1  amp  n2  amp  n3  amp  n4  endbmatrix   beginbmatrix   hatx1   hatx2   hatx3   hatx4   endbmatrix   endequation    fracpartial xpartial xi be approximate by  beginequation   fracpartial xpartial xi     beginbmatrix   fracpartial n1partial xi   amp  fracpartial n2partial xi   amp  fracpartial n3partial xi   amp  fracpartial n4partial xi   endbmatrix   beginbmatrix   hatx1   hatx2   hatx3   hatx4   endbmatrix  endequation   important  the definition of the jacobian in many book cover finite element be not consistent to the mathematical definition  so be careful when PRON be do PRON transformation   when PRON be do the transformation of the derivative of the shape function  PRON should have  beginequation   beginbmatrix   fracpartial n1partial x1   amp  fracpartial n2partial x2   amp  fracpartial n3partial x3   amp  fracpartial n4partial x4   endbmatrix    mathbfj1  beginbmatrix   fracpartial n1partial xi1   amp  fracpartial n2partial xi2   amp  fracpartial n3partial xi3   amp  fracpartial n4partial xi4   endbmatrix    endequation   so that seem to be ok  except that PRON have the matrix transpose   PRON stiffness matrix then be wrong  note that PRON have a dyadic product  where in this case PRON get a  4 time 4  matrix out of  1 time 4  vector  that lead PRON to  beginequation   mathbfkel   sumingauss    beginbmatrix   fracpartial n1partial x1    fracpartial n2partial x2    fracpartial n3partial x3    fracpartial n4partial x4   endbmatrix   mathbfe   beginbmatrix   fracpartial n1partial x1   amp  fracpartial n2partial x2   amp  fracpartial n3partial x3   amp  fracpartial n4partial x4   endbmatrix   wxi  i   detmathbfj    endequation   with  wxi  i be the weight for the gauss integration of the integration point  xi  gauss quadrature for  mathbfk be now really simple  choose the gauss point in the isoparametric space and the correspond weight  just look on wikipedia  and use the formula give  
__label__regression __label__beginner __label__missing-data PRON be to analyze result of education feedback questionnaire  program have many course  and almost nobody take every single one of PRON  PRON task be to predict program grade  make by student  base on course  grade   what be the good way to work with miss value  PRON be not miss at random  and impute mean do not seem practical   be there any good regression to work with   
__label__machine-learning __label__classification __label__bigdata PRON be work on use machine learn to correctly predict a binary classification use an input dataset that PRON receive about once a month   the idea be that PRON train  test and validate the classifier on one month of datum  and use PRON to classify another month  whether PRON repeatedly add the next month of datum to the training dataset and upset PRON classifier be open for debate   PRON have spend a little while on this and have something that work well but PRON work better on some month than other  PRON be use either randomforest or linearsvcpenalty’l1’  because PRON have a lot of feature  around 400000 observation and limited computing power   PRON do not have much insight into the process that produce the input dataset and base on previous analysis  PRON do not trust PRON quality control  what worry PRON be that one month PRON may receive a dataset that be very different to the datum PRON have be train on and PRON would not really know unless the distribution of class be quite different   PRON be sure that PRON can not be the first person to have this worry but PRON can not find much information about PRON  so maybe PRON be misguided here  PRON have find the population stability index  which be the right kind of idea  but PRON be unsure about PRON because PRON be dependent on bin size   so PRON questions    how be a worry like this normally deal with   can PRON point PRON to some idea  case study that will help PRON   be there anything PRON can do on the classification side that will make  PRON classifier more robust to population instability   there could be significant problem from use one time chunk in a time series to predict the next chunk  for example  if PRON be use datum from september and october to predict retail shopping expenditure in november and december  PRON would be very wrong  instead  PRON need to note the annual trend of increase shopping between thanksgiving and christmas to make correct prediction   so  while this sort of skirt the question PRON ask  PRON think that if PRON be have difficulty use one month to predict the next month  PRON need to start look at long  term trend  especially annual one  PRON would investigate this first before look into population instability  
__label__linear-algebra __label__eigensystem __label__graph-theory be there any exist program that be able to compute the  approximate  distribution of eigenvalue for very large  symmetric  sparse matrix   note that PRON do not need the eigenvalue PRON  only PRON distribution  find all the eigenvalue be a more difficult problem   and PRON be primarily look for exist software  not only the description of algorithm that PRON need to implement PRON  PRON matrix be adjacency matrix of undirected graph  or closely relate matrix    PRON find some paper discuss how this can be do in practice  so PRON be hop that a work implementation exist somewhere   PRON would imagine that something of this kind can be infer from the pseudospectrum function  see the work by marc embree  for example   httpwwwcaamriceeduembree  in structural mechanic the number of eigenvalue of a matrix  k in a give range   alphabeta be compute via the  sturm sequence check   i e compute the  ldlt factorization of  kalpha i and  kbeta i and count the difference in the number of negative pivot   if PRON have reasonably large bin  can be apply to PRON problem  and should be pretty straightforward to implement    a search on lanczos shift block algorithm should give more info  since this technique be often use in that context to check for miss eigenvalue  eigenvector pair     this count be exact  and expensive for large  k  so PRON request of an  estimate  or approximate count be still open  please post any finding   edit  update  the author of  a padé  base factorization  free algorithm for identify the eigenvalue miss by a generalize symmetric eigensolver  claim  to the good of the authors’ knowledge  no post  processing technique  that do not require the factorization of a matrix relate to  k  andor  m be currently available for check whether an eigensolver  apply to the solution of problem  1  have miss some eigenvalue in  an arbitrary range of interest   sigmal  sigmar     this mean that for obtain an exact count of the eigenvalue in a give bin PRON have to use the classical sturm sequence method  see also sylvester inertia law    general advice for implement this approach in PRON case can not be give  without an analysis of the property of PRON adjacency matrix  dimension  number on non zero entry  fill  in after reordering  condition number of principal minor     nevertheless PRON would suggest start with a simple no  brainer approach  and see if PRON experience breakdown of the implementation  assume that this computation be not mission critical   PRON suggest to use the wonderful suitesparse by tim davis   reorder PRON matrix to reduce fill  in   eg call symamd from colamd on  a  alpha0 i    compute  li di lit  a  alphai i  where  alphai   i0dot m be the boundary of PRON bin   try first a simple implementation like ldl without pivot  and go for a pivot approach only if PRON experience numerical difficulty    note that without pivot the symbolic factorization step can be recycle for all  i   the count of negative diagonal term in  d give PRON the number of eigenvalue  lambda  lt  alphai  this approach be effective only if  m be small or the factorization time be negligible with respect to eigendecomposition time  PRON have to perform some experiment to find out  good luck  
__label__finite-element __label__eigenvalues __label__stiffness __label__penalty-method in the finite element method  PRON often construct the constraint of the system by add penalty  function term  which often be many many magnitude  up to  10  6  order big than the large stiffness matrix term  to the stiffness matrix by use the so  call penalty method  section 92   on the other hand   the eigenvalue analysis  as implement by arpack  make use of arnoldi iteration scheme  which involve work on the stiffness matrix many time over in the iteration   PRON hunch be that  the out  of  magnitude penalty term in stiffness matrix will make the whole arnoldi iteration unstable  when PRON be iterate many time over  result in inaccuracy when PRON compute the eigenvalue analysis  PRON also do not know how to estimate the inaccuracy level   due to lack of competency in rigorous math  PRON be unable to prove PRON hunch  be PRON right  can anyone furnish a proof to prove or disprove PRON hunch   ps   PRON hunch be somewhat validate by a note in the textbook reference above  section 97    but still  PRON will be interested to see a rigorous mathematical argument   
__label__neural-networks PRON be learn neural networks  and everything work as plan but  like human do  adjust PRON to learn more efficiently  PRON be try to understand conceptually how one may implement an auto adjust learning rate for a neural network   PRON have try to make PRON base on error  something like how big be error learn rate be get big as well   could use some clarification here  not entirely sure what PRON be say   if can clarify  PRON be happy to clean up the english  dukezhou    if PRON want give PRON an example give PRON on a c base language or math because PRON do not have experience with python or pascal   adjust for learn rate be a common scenario in machine learning  there be rich literature about PRON  countless paper  the most common implementation   adagrad  rmsprop  adam  there be many more variant  PRON will need to do some research  please take a look at   httpsenwikipediaorgwikistochasticgradientdescent  apart from the already mention method there be one which PRON come across recently be the cyclic learning rates   mainstream method usually reduce the learning rate monotonically  in most of the case PRON work well  yet PRON do not do well with handle saddle point and local minimum  clr method claim to solve the problem by effectively handle the case of saddle point   ref  httpsarxivorgabs150601186 
__label__apache-spark __label__matrix-factorisation PRON have play with the movielens rating dataset under spark ’s als and a manual implementation of als and compare result with the same hyperparameter  PRON would like to know this exactly in order to make some experiment that could be reproducible  PRON have see several paper describe the exact loss formula for als in different way  these seem to be quite common     1  min sum  rui   mu – bu – bi – putqi2  lambda  lvert mathbfpu  rvert2    lvert mathbfqi  rvert2  bu2  bi2   the rating be center by subtract an overall bias  mu  and for each user  a bias  bu which be a part of the model  and part of the regularization  and a similar bias for each movie  bi     2  min sum  ruj  – uitmj2  lambda  nui   lvert mathbf  ui  rvert2   nmj   lvert mathbfmj  rvert2   the penalization for each user  feature and movie  feature vector be multiply by the number of rating from each user  movie  and the rating may be center beforehand  but the bias be not model parameter    and PRON have also see other variation where the l2norm of the vector be not square   PRON have play with different variation of these formula and compare the result on the same datum with spark ’s implementation and PRON manual implementation  and PRON can not find any variation which would make the same hyperparameter in both implementation  the number of latent factor and the value of lambda  give similar result with the same datum  eg under some formula  with the same number of latent factor and use the same value for lambda that in spark give good cross  validate result  in PRON implementation result in either overfitt badly or make all the prediction close to zero  but a totally different value of lambda give comparable result    what formula do spark use  also  how be the initial matrix initialize  PRON have see some suggestion say to let the first column of the movie  factor matrix be the average rating for the movie and the rest random  normal number  while other suggest random uniform number with quite big sign  how be the first movie  factor matrix initialize   
__label__libraries __label__reference-request __label__fluid-dynamics what be the good software for quickly implement and test a reynolds averaged navier  stokes turbulence model   openfoam   PRON be really easy to implement model there  even if PRON do not know how to program in c  everything be set up  and PRON can just follow the pattern  if PRON do know something about similar class interface  dynamic polymorphism use virtual function  then PRON be exceptionally easy   model  explicit  or implicit  will involve manipulation on field  and in openfoam  field manipulation be really simple  open field operaton and manipulation   as PRON can see on eg page 21 of the openfoam programming slide  generic programming and function overloading allow for top level coding when PRON come to model development  PRON code look exactly as the govern equation PRON be solve  or the calculation PRON be do on the field   on this level  the result code be automatically parallel  with a choice between various type of parallel library  flavorus of mpi and recently openmp   test the model be easy as well  the model will have  if PRON have just follow the structure of other model  exist turbulence model  viscosity model  etc  the code structure be always the same  automatically enable run  time selectivity  which mean that PRON can change parameter durign a run   that be from the top of PRON head  there be other advantage  the library be modular  support highly advanced topological change on arbirary unstructured mesh  slide interface  refinement  mix plane  etc   have top notch numerical solver  all in all  PRON be really cool    additonal info   PRON can examine the hierarchy of of model here  then find a model in the source code use the locate command  locate turbulencemodel  h bring PRON to the source directory of the abstract base class  root class  basic type of the turbulence model   in PRON case PRON be  src  turbulencemodel  incompressible  ras    after that  find the one that resemble PRON model  copy the code  rename the class  if PRON be use a vim editor  try  s  oldname  newname  g in all the source file   and compile PRON as a part of PRON new library  et voila  a new turbulence model  parallel  shiny  run  time modifiable  etc   which code  PRON would propose something different than previous post   PRON suggest PRON take some lightweight code like  caffa  from ferziger and peric cfd book  available in 2dgt folder at this site  everything be lay out in less then 2000 line in front of PRON  no need to know anything about oop  PRON will get more confidence because PRON know every line of the code   what do PRON mean to implement a new turbulence model   have this source code  PRON will learn that in case of most rans model  implement a new turbulence model mean do follow few thing in subroutine dedicate to solve transport equation of a scalar   unsteady term  do nothing  convection term  do nothing  diffusion  change diffusion coefficient   sigma s in  kepsilon   volume source term  production  destruction PRON may have to change PRON  this be where coefficient of PRON new turbulence model come into play as well  PRON calculate PRON  source term  explicitly use know value and mid  point rule for integration  PRON be really easy   eddy viscosity  go to routine where PRON be calculate  in bad case PRON will have to change one line of code to get new form of equation for PRON   boundary condition  PRON usually change PRON too  except in a case when PRON be solve same transport equation as the one PRON already have implement   that be about PRON if PRON be work with rans model   testing  set up a channel case of  retau  395  for which there be available dns database  and try to get match in logarithmic velocity profile  even without dns there be well  know  law of the wall  to help PRON  every well tune  in term of coefficient value  turbulence model should reproduce PRON correctly  this will be validation or the answer to question be PRON solve the equation right   then setup a test case intend for turbulence model verification  like some from ercoftac database   good luck   PRON can recommend to use the open source advanced simulation library which be hardware accelerate  PRON be easy to use  see the source code of this aerodynamic simulation  
__label__machine-learning __label__classification __label__r __label__random-forest __label__overfitting PRON have a dataset that contain around 3000 observation  every observation fall in one of the five category  these be pre  define    PRON be use randomfor and gbm from the h2o package for classification  before PRON run the algorithm i optimize the hyperparameter of each algorithm use the h2ogrid   for model validation PRON be have a look at the confussion matrix for both algorithm  for both in  sample and out  of  sample   the issue be that in sample the algorithm perform relatively well for all 5 category but out of sample PRON PRON only perform well for one category  PRON guess that be very common   PRON have 2 explanation  the 5 category be not equally divide  this mean that for a couple category PRON have a few observation   second  be that the algorithm be overfitt   the  1 m dollar question  be how can someone avoid overfitt   PRON think of run a preliminary analysis on PRON data set  to see which variable  in PRON case most of PRON dummy variable  in PRON data set be the  most important  for the classification  and then use those  instead of the whole data set  this could be test by take the mean for the variable per category and which variable have the high difference across the category  could this be a good idea   PRON understand that the subject be too broad  but PRON think some idea from experienced people on this topic could be useful for everyone    try solve the imbalance problem by use something like  smote to make each class roughly proportional   make sure PRON be splitting PRON in sample and out of sample datum randomly   sometimes people will take the first x and call that training  but  if PRON datum have any order to PRON  the sample may not be  representative   PRON would not try remove variable from the datum set until PRON have address the above   PRON will need to provide the exact distribution of the 5 level so PRON can comment if PRON a case of overfitt  let PRON say one of PRON class be 90  of the datum  and the model be give 90  accuracy on out of sample datum  100  for the majority class  and 0  for rest of the class  this be not a case of overfitting  the good the model could come up with be to put all in one class  PRON give decent accuracy too  in general to avoid overfitt crossvalidation be really important  PRON suggest use mlr package rather than h20 because the crossvalidation there be easy  as suggest by calz PRON can use smote to make PRON training class in good proportionbut remember to not over  under sample the out of sample datum    as far as PRON feature selection technique go  PRON can use chi  squared test or information gain to plot the top feature  also there be many advanced feature selection algorithm like boruta etc  PRON can use PRON  but remember do not blindly trust these algorithm  PRON be not magic  only do feature selection if let say PRON have 100 variable  for 3000 datapoint   for 20  30 feature  PRON can apply the model on all of PRON  plot the importance plot for gbm and then try to remove the less important variable and see if the accuracy increase  
__label__optimization __label__finite-difference __label__stability __label__advection __label__implicit-methods the main motivation behind PRON next question be that PRON think PRON derive a high order numerical scheme for linear advection equation that be unconditionally stable use von neumann stability analysis   in the stability analysis of the numerical scheme PRON have to find the maximal value of so call amplification factor  s where  s depend on 5 parameter  PRON mean   s  sx  y  f  c  d and     s  left2 leftc2c cos  x   c  d  2 f1  cos  y2 d fd1d  cos  y   2 c fcd12 c d f  c d sin  x  sin  yc d3 i c  sin  xc  d2  3 i d sin  yd6rightright  cdot left2 i c2 sin  xi  c2 sin  2 xc2  2 i c d sin  xy2 c d cos  xy2 i c d  sin  x2 c  cd5  cos  x2 i c d sin  y2 c d cos  y2  c d10 i c sin  x2 i c sin  2 xc2  c cos  2 x8 c2 i  d2 sin  yi d2 sin  2 y2 d2 cos  yd2 cos  2  yd2  10 i d sin  y2 i d sin  2 y10 d cos  y2 d cos   2 y8 d12right1      note that  i be imaginary number   PRON hypothesis be that  s le 1  for  pi le x le pi    pi le y le pi   1 le f le 0  and  c ge 0  and  d ge 0  up to now PRON check the value of  s for many tabulate value of input variable eg for  c le 100  and  d le 100 moreover PRON use mathematica to compute numerically the maximal value of  s with many start value and several build  in optimization method  PRON get always the maximal value of  s be  1 of course PRON use the numerical scheme to compute several example of linear advetion equation with very large time step and observe no instability   PRON would appreciate any useful thought  help or even PRON try  how to find the value of   x  y  f  c  d bound by above constraint for which  sgt1 or from the other view  what kind of method and argument would PRON use to support the hypothesis that  s le 1    ps here be the input for mathematica  s26  c  c2  d  cd  d2  2cdf   d1  c  d  2cfcosy    ccosx1  c  d  2df  d1  2fcosy     3icsinx   3idsiny   cdsinxsiny12  8c  c2  8d  2cd  d2  2c5  c  dcosx    c2  ccos2x   10dcosy   2cdcosy    2d2cosy   2dcos2y   d2cos2y    2cdcosx  y   10icsinx   2ic2sinx    2icdsinx   2icsin2x   ic2sin2x    10idsiny   2icdsiny   2id2siny    2idsin2y   id2sin2y   2icdsinx  y    pss if for any reason PRON need more insight  see PRON paper at  httpdldropboxusercontentcomu386482articlesfrolkovicalgoritmycorrpdf  where the scheme be give in  310  for  f0  here be an example how PRON have test the hypothesis that  s le 1  for give constraint  PRON have run the follow code in mathematica with many many value of variable nstart and nend and the method  m     amplification parameter    s   26c  c2d  cd  d2  2cdfd1cd2cfcosyccosx1cd2df  d1  2fcosy3icsinx3idsinycdsinxsiny        12  8cc2  8d2cdd2  2c5cdcosxc2ccos2x10dcosy2cdcosy2d2cosy2dcos2yd2cos2y2cdcosxy10icsinx2ic2sinx2icdsinx2icsin2xic2sin2x10idsiny2icdsiny2id2siny2idsin2yid2sin2y2icdsinxy       numerical global maximum search    mrandomsearch     randomsearch neldermead differentialevolution simulatedannealing    nstart1nend10   forn  nstart  nltnend  n   printnmaximizeabsspiltxltpi  pilt ylt pi  0lt c  ltn  0ltdltn  1ltflt0     x  y  c  d  f   methodgtmsearchpointsgt20randomseedgtn        example of output     1xgt387721  10  7ygt314159cgt0996877dgt0fgt0998816     1xgt314159ygt219618  10  8cgt0dgt171028fgt0941516     1xgt314159ygt314159cgt0dgt0fgt1      1xgt314159ygt314159cgt0dgt0fgt1      1xgt471742  10  7ygt314159cgt33804dgt0fgt0997559     1xgt314159ygt314159cgt0dgt0fgt1      1xgt314159ygt24762  10  7cgt0dgt134533fgt0922121     1xgt0000101745ygt00000189795cgt348753dgt420828fgt084976    1xgt314159ygt10698  10  6cgt0dgt898382fgt063621     1xgt0000268716ygt00000310003cgt0593874dgt395438fgt0832499    in PRON expression for  s  be  x  kx delta x and  y  ky delta y for some  wave number  vector  mathbfk    kx quad kyt with  delta x  and  delta y be grid spacing in  x and  y direction   PRON assume that  x  pi and  y  pi and obtain the follow for  s      frac  2cdpc2d2c  d3c22cdd25c5d   3       which satisfy  s le 1 the quantity  p be equal to  2f  1  the value  xpi and  ypi  of course  correspond to the minimum resolvable wavelength in each direction   PRON play with a few other value of  x and  y as well and in all those case   s le 1 
__label__approximation-algorithms __label__error-estimation PRON be look into integral of the form     intab  fxgxdx  where  fx be unknown  but PRON be integral be     intab  fxdxf  PRON be be suggest to PRON that one could approximate this integral by     intab  fxgxdxsim f cdotoverlinegxfracfb  aintab  gxdx  this of course be just a  good guess  form of the integral mean value theorem   PRON question be the following   how would one go about calculate the error bound on this approximation  be there any condition on  fx that would tighten the say error bound   for simplicity s sake  let PRON just say that  a0b1 then PRON want to estimate     int0  1 fx  gx  dx  leftint0  1 fx  dx right  leftint0  1 gx  dxright      from above and below  furthermore  let PRON consider two case   if  gx have mean value zero  ie   int0  1 gx  dx  0   then PRON know a priori that PRON approximation be not a good one  let PRON not consider this case any more then   if  gx have a mean value different than zero  let PRON only consider this case   since in this second case PRON do not make a difference in the problem  let PRON assume for simplicity that  gx have be scale in such a way that  int0  1 gx  dx  1 then PRON be look for estimate of the term     int0  1 fx  gx  dx  int0  1 fx  dx     from above and below where  gx have mean value one   now consider a sequence of function  fnxn1xn then  assume that  gx be bounded      int0  1 fnx  gx  dx rightarrow 0  qquad int0  1 fnx  dx  1     so PRON relative error can become arbitrarily large  and by tweak the sign of  fn  g PRON can make this an error that can be positive or negative   of course  all this show be that PRON need to say in which class  or function space  PRON be look for PRON function  fx each function  fn above be  cinfty  and PRON limit be in  l1   so PRON can not expect an error bind to hold in  l1 but the limit function be not in in  lp  pgt1  and so PRON may be that PRON can find  for example  an estimate in  linfty in other word  more information be necessary to answer PRON question  
__label__education __label__definitions __label__career PRON be an msc student at the university of edinburgh  specialize in machine learning and natural language processing  PRON have some practical course focus on datum mining  and other deal with machine learning  bayesian statistic and graphical model  PRON background be a bsc in computer science   PRON do some software engineering and PRON learn the basic concept  such as design pattern  but PRON have never be involve in a large software development project  however  PRON have a data mining project in PRON msc  PRON question be  if PRON want to go for a career as data scientist  should PRON apply for a graduate data scientist position first  or should PRON get a position as graduate software engineer first  maybe something relate to data science  such as big datum infrastructure or machine learn software development   PRON concern be that PRON may need good software engineering skill for data science  and PRON be not sure if these can be obtain by work as a graduate data scientist directly   moreover  at the moment PRON like data mining  but what if PRON want to change PRON career to software engineering in the future  PRON may be difficult if PRON specialise so much in data science   PRON have not be employ yet  so PRON knowledge be still limit  any clarification or advice be welcome  as PRON be about to finish PRON msc and PRON want to start apply for graduate position in early october   absolutely  keep PRON software skill sharp  PRON can do this in an academic program if PRON simply implement by PRON all the algorithm PRON learn about   good selection of course  btw  consider get an internship too   1  PRON think that there be no need to question whether PRON background be adequate for a career in data science  cs degree imho be more than enough for datum scientist from software engineering point of view  have say that  theoretical knowledge be not very helpful without match practical experience  so PRON would definitely try to enrich PRON experience through participate in additional school project  internship or open source project  maybe one  focus on datum science  machine learning  artificial intelligence    2  PRON believe PRON concern about focus on datum science too early be unfounded  as long as PRON will be practice software engineering either as a part of PRON data science job  or additionally in PRON spare time   3  PRON find the following definition of a data scientist rather accurate and hope PRON will be helpful in PRON future career success   a data scientist be someone who be good at statistic than any  software engineer and good at software engineering than any  statistician   ps today s enormous number of various resource on datum science topic be mind  blow  but this open source curriculum for learn datum science may fill some gap between PRON bsc  msc respective curricula and reality of the data science career  or  at least  provide some direction for further research and maybe answer some of PRON concern   httpdatasciencemastersorg  or on github  httpsgithubcomdatasciencemastersgo   from the job ad PRON have see  the answer depend  there be job which be more technical in nature  design big datum project  do some analysis  or the exact opposite  do analysis  storage etc  be someone els job    so PRON would say that some software design skill be extremely useful  but PRON do not need the abillity to build a huge program in c   java or whatev   why PRON like some sw skill be simply that PRON code probably look way good than code from someone who never program for the sake of programming  most of the time  the latter code be very hard do understand  debug for outsider  also  sometimes PRON analysis need to be integrate in a big program  an understand of the need of the programms certainly help  
__label__machine-learning __label__logistic-regression __label__multiclass-classification __label__pyspark PRON be try to use logistic regression to classify the dataset which have sparse vector in feature vector   case 1  PRON try use the pipeline of ml in mllib as follow    use library  from pysparkmlfeature import hashingtf  from pysparkml import pipeline  from pysparkmlclassification import logisticregression  printtypetrainingdata    for check only  printtrainingdatatake2    to see the detail of dataset  lr  logisticregressionlabelcollabel   featurescolfeature   maxiter  maximumiteration   regparam  re  gparamvalue   pipeline  pipelinestageslr     train model  model  pipelinefittrainingdata   get the follow error    ltclas  pysparksqldataframedataframegt    rowlabel20  feature  sparsevector2000   51  10  160  10  341  10  417  10  561  10  656  10  863  10  939  10  1021  10  1324  10  1433  10  1573  10  1604  10  1720  10     rowlabel30  feature  sparsevector2000   24  10  51  20  119  10  167  10  182  10  190  10  195  10  285  10  432  10  539  10  571  10  630  10  638  10  656  10  660  20  751  10  785  10  794  10  801  10  823  10  893  10  900  10  915  10  956  10  966  10  1025  10  1029  10  1035  10  1038  10  1093  10  1115  20  1147  10  1206  10  1252  10  1261  10  1262  10  1268  10  1304  10  1351  10  1378  10  1423  10  1437  10  1441  10  1530  10  1534  10  1556  10  1562  10  1604  10  1711  10  1737  10  1750  10  1776  10  1858  10  1865  10  1923  10  1926  10  1959  10  1999  10      160825 191407 error orgapachesparkmlclassificationlogisticregression  currently  logisticregression with e  lasticnet in ml package only support binary classification  find 5 in the input dataset   traceback  most recent call last    file  home  lr  testpy   line 260  in  ltmodulegt   accuracy  trainlrcmodeltraindata  testdata   file  home  lr  testpy   line 211  in trainlrcmodel  model  pipelinefittrainingdata   file  usr  lib  spark  python  lib  pysparkzip  pyspark  ml  pipelinepy   line 69  in fit  file  usr  lib  spark  python  lib  pysparkzip  pyspark  ml  pipelinepy   line 213  in  fit  file  usr  lib  spark  python  lib  pysparkzip  pyspark  ml  pipelinepy   line 69  in fit  file  usr  lib  spark  python  lib  pysparkzip  pyspark  ml  wrapperpy   line 133  in  fit  file  usr  lib  spark  python  lib  pysparkzip  pyspark  ml  wrapperpy   line 130  in  fitjava  file  usr  lib  spark  python  lib  py4j09srczip  py4j  javagatewaypy   line 813  in   call    file  usr  lib  spark  python  lib  pysparkzip  pyspark  sql  utilspy   line 45  in deco  file  usr  lib  spark  python  lib  py4j09srczip  py4j  protocolpy   line 308  in getreturnvalue  py4jprotocolpy4jjavaerror  an error occur while call o207fit    orgapachesparksparkexception  currently  logisticregression with elasticnet in ml package only support binary  classification  find 5 in the input dataset   at orgapachesparkmlclassificationlogisticregressiontrainlogisticregressionscala290   at orgapachesparkmlclassificationlogisticregressiontrainlogisticregressionscala159   at orgapachesparkmlpredictorfitpredictorscala90   at orgapachesparkmlpredictorfitpredictorscala71   at sunreflectnativemethodaccessorimplinvoke0native method   at sunreflectnativemethodaccessorimplinvokenativemethodaccessorimpljava62   at sunreflectdelegatingmethodaccessorimplinvokedelegatingmethodaccessorimpljava43   at javalangreflectmethodinvokemethodjava498   at py4jreflectionmethodinvokerinvokemethodinvokerjava231   at py4jreflectionreflectionengineinvokereflectionenginejava381   at py4j  gatewayinvokegatewayjava259   at py4jcommandsabstractcommandinvokemethodabstractcommandjava133   at py4jcommandscallcommandexecutecallcommandjava79   at py4j  gatewayconnectionrungatewayconnectionjava209   at javalangthreadrunthreadjava745   case 2  PRON search the possible alternate solution of above one and get that logisticregressionwithlbfgs will work on multi  class classificaton  PRON try as follow    use library  from pysparkmllibclassification import logisticregressionwithlbfgs  logisticregressionmodel  logisticregressionwithsgd  printtypetrainingdata    for check only  printtrainingdatatake2    to see the dataset  model  logisticregressionwithlbfgstraintrainingdata  numclasses5   printtypemodel    get the follow error    ltclas  pysparksqldataframedataframegt    rowlabel30  feature  sparsevector2000   24  10  51  20  119  10  167  10  182  10  190  10  195  10  28  5  10  432  10  539  10  571  10  630  10  638  10  656  10  660  20  751  10  785  10  794  10  801  1   0  823  10  893  10  900  10  915  10  956  10  966  10  1025  10  1029  10  1035  10  1038  10  1093  1   0  1115  20  1147  10  1206  10  1252  10  1261  10  1262  10  1268  10  1304  10  1351  10  1378  10  14  23  10  1437  10  1441  10  1530  10  1534  10  1556  10  1562  10  1604  10  1711  10  1737  10  1750  1  0  1776  10  1858  10  1865  10  1923  10  1926  10  1959  10  1999  10     rowlabel50  feature  sparsev  ector2000   103  10  310  10  601  10  817  10  866  10  940  10  1023  10  1118  10  1339  10  1447  10   1634  10  1776  10      traceback  most recent call last    file  home  lr  testpy   line 260  in  ltmodulegt   accuracy  trainlrcmodeltraindata  testdata   file  home  lr  testpy   line 230  in trainlrcmodel  model  logisticregressionwithlbfgstraintrainingdata  numclasses5   file  usr  lib  spark  python  lib  pysparkzip  pyspark  mllib  classificationpy   line 382  in train  file  usr  lib  spark  python  lib  pysparkzip  pyspark  mllib  regressionpy   line 206  in  regressiontrainwrapper  typeerror  datum should be an rdd of labeledpoint  but get  ltclass  pysparksqltypesrowgt   again PRON try to convert the dataset into rdd of labeled point as follow ie case 3   case 3  convert the dataset into rdd of labeled point so that PRON can use logisticregressionwithlbfgs as follow    use library  from pysparkmllibclassification import logisticregressionwithlbfgs  logisticregressionmodel  logisticregressionwithsgd  from pysparkmllibregression import labeledpoint  printtypetrainingdata    for check only  printtrainingdatatake2    to see the dataset  trainingdata  trainingdatamaplambda rowlabeledpointrowlabel  rowfeature     printtype of trainingdata    printtypetrainingdata    printtrainingdatatake2    model  logisticregressionwithlbfgstraintrainingdata  numclasses5   printtypemodel    get the follow error    ltclas  pysparksqldataframedataframegt    rowlabel20  feature  sparsevector2000   51  10  160  10  341  10  417  10  561  10  656  10  863  10  9  39  10  1021  10  1324  10  1433  10  1573  10  1604  10  1720  10     rowlabel30  feature  sparsevector   2000   24  10  51  20  119  10  167  10  182  10  190  10  195  10  285  10  432  10  539  10  571  10   630  10  638  10  656  10  660  20  751  10  785  10  794  10  801  10  823  10  893  10  900  10  915   10  956  10  966  10  1025  10  1029  10  1035  10  1038  10  1093  10  1115  20  1147  10  1206  10  12  52  10  1261  10  1262  10  1268  10  1304  10  1351  10  1378  10  1423  10  1437  10  1441  10  1530  1  0  1534  10  1556  10  1562  10  1604  10  1711  10  1737  10  1750  10  1776  10  1858  10  1865  10  1  923  10  1926  10  1959  10  1999  10      type of trainingdata   ltclass  pysparkrddpipelinedrddgt     labeledpoint20   2000511603414175616568639391021132414331573160417201010101010101  010101010101010       labeledpoint30   20002451119167182190195285432539571630638656   66075178579480182389390091595696610251029103510381093111511471206125212611262126813041351   1378142314371441153015341556156216041711173717501776185818651923192619591999102010101   0101010101010101010201010101010101010101010101010102010101  0101010101010101010101010101010101010101010101010       traceback  most recent call last    file  home  lr  testpy   line 260  in  ltmodulegt   accuracy  trainlrcmodeltraindata  testdata   file  home  lr  testpy   line 230  in trainlrcmodel  model  logisticregressionwithlbfgstraintrainingdata  numclasses5   file  usr  lib  spark  python  lib  pysparkzip  pyspark  mllib  classificationpy   line 381  in train  attributeerror   list  object have no attribute  feature   can someone please suggest where PRON be miss something  PRON want to use the logistic regression in pyspark and classify the multi  class classification   currently PRON be use spark version version 162 and python version python 279 on google cloud   thank PRON in advance for PRON kind help   try omit the   so that PRON do not create python list  trainingdata  trainingdatamaplambda row  labeledpointrowlabel  rowfeature   
__label__python __label__pandas __label__correlation PRON be newbie to data science  PRON be try to understand how to correlate the position of an app in the app store  eg 1  10  to the number of backlink  eg  1  250 link   8  50 link  to the app store listing page in python   PRON have manage to correlate both row entirely  which obviously give PRON a nonsense figure   could someone point PRON in the right direction to allow PRON to fill this knowledge gap  PRON be not entirely sure of the correct terminology for this  make google PRON pretty impossible   sound like PRON be look for the pearson correlation coefficient between the two variable  PRON can compute that by use scipy s pearsonr method   person correlation assume data be come from a normal distribution and there be a linear relationship  an alternative be the spearman correlation or kendall s tau for rank datum   as an edit  here be the link to how PRON would calculate the spearman correlation coefficient and kendall s tau  respectively   httpdocsscipyorgdocscipy0151referencegeneratedscipystatsspearmanrhtml  httpdocsscipyorgdocscipy0151referencegeneratedscipystatskendalltauhtml  good of luck  
__label__linear-solver __label__preconditioning __label__least-squares PRON have see a lot of literature  lecture video  etc  on solver  preconditioner for non  symmetric andor indefinite system  however  now PRON want to solve the mixed poisson  darcy equation use the least  square finite element method     intomegaleftboldsymbolumathbfx    nabla pmathbfx    boldsymbolgmathbfxrightcdot  mathbfaleftboldsymbolvmathbfx    nabla qmathbfxrightleftnablacdotboldsymbolufmathbfxrightleftnablacdotboldsymbolvrightmathrmdomega  where  boldsymbolup and  boldsymbolvq be the velocity  pressure trial and test function respectively   mathbfa be a symmetric and positive definite weighting tensor  and  boldsymbolgf be the specific body  volumetric source term respectively  the above weak form will result in a dicrete first order symmetric and positive definite set of equation   that say  what be the  good  solver  preconditioner for this type of problem  PRON have try thing like the conjugate gradient method with jacobi preconditioning  but PRON seem the number of iteration increase proportionally with problem size  thus  lose some scalability in the strong sense   typically multigrid  since least square fem usually result in a second order system for which multigrid do well  the convergence of least square method be also base on the equivalence of a problem  dependent norm with more standard sobolev norm  and the same equivalence may sometimes be use to construct appropriate smoother for multigrid   edit  for example  google search for fosls and multigrid yield some good result  PRON can also find a good discussion in gunzberger  bochev s  least  squares finite element methods  book  
__label__normalization PRON have train a neural network network that give a minmax normalize input  provide a minmax normalize output   this may be late  but be PRON possible from a minmax normalize output to create the actual output  give PRON know the actual min and max value   so an unormalized output   yes  just rearrange the formula for find the normalise value  where  xi be the original attribute  and  zi be the normalised value      beginalign   zi  amp fracxi  minxmaxx   minx     xi  amp zimaxx   minx    minx   endalign     
__label__bigdata both apache  spark and apache  flink project claim pretty much similar capability   what be the difference between these project  be there any advantage in either spark or flink   thank  flink be the apache renaming of the stratosphere project from several university in berlin  PRON do not have the same industrial foothold and momentum that the spark project have  but PRON seem nice  and more mature than  say  dryad  PRON would say PRON be worth investigate  at least for personal or academic use  but for industrial deployment PRON would still prefer spark  which at this point be battle test  for a more technical discussion  see this quora post by committer on both project  
__label__predictive-modeling __label__time-series __label__regression __label__forecast PRON have hourly temperature and power consumption datum of several day of a month  the pattern be almost similar across day like this   use this datum PRON want to predict the usage of a come day  PRON have feature  1  hour of the day 2  temperature  and the response variable  power  look at the datum  PRON believe PRON should fit three separate model and not a single model  first model for datum from midnight to 10 am as usage remain almost constant during this time  and temperature do not vary too much  second model for datum from 11am till 6 pm  this portion follow a sharp increase and then almost constant usage  third model for datum from 7 pm till midnight  this portion show constant decrease in power  to follow this intuition  PRON use three model accordingly and later combine prediction from these to output sequence of 24 number for come day  formula for each of these model be   lmpower  timehour  temperature  datum  xxx   but each model be train with the datum correspond to specific time duration of the day   instead of divide the datum manually and use three separate model  be there any other exist technique which will take care of PRON intuition and do not need manual division of datum or creation of separate model   during PRON search  PRON find that PRON can use gam  generalized additive models  and PRON come up with follow formula  libraryspline   lmpower  nstimehour  knot   9  18    temperture  datum  xxx   use above formula PRON think that PRON be place knot at 9 am and at 6 pm  right   PRON do not know how should PRON enforce knot in temperature feature exactly at these specific time  so that knot of temperature and timehour will sync   the above plot be plot use follow datum   dframe  lt structurelisttimehour  c1  2  3  4  5  6  7  8  9  10  11   12  13  14  15  16  17  18  19  20  21  22  23  24   temperature  c225   24  235  205  225  225  195  235  23  205  265  285   30  32  335  33  305  30  295  29  28  27  28  285   power  c9704319   957225  8859191  8834882  9017179  8882062  8773833  8936342   8531775  911292  11679035  14958614  17232438  17127931   15953858  16203544  17078468  1640275  15586717  13577197   13301235  11629253  10087483  9784942    names  ctimehour     temperature    power    rownam  cna  24l   class   dataframe    and minimal code use be   parmfrow c12    plotdframetimehour  dframepower  typelxlab   hour of day   ylab   power    plotdframetimehour  dframetemperature  typelxlab   hour of day   ylab   temperature    this be a typical time series problem  the first step be to make sure PRON time series be stationary  see here for a good explanation why this be necessary   so PRON first do   dframepowerdiff  lt cna  diffdframepower    plotdframepowerdiff  type   l    give   next PRON want to consider an arimax model  as part of this exercise PRON want to know whether there be a dependency in the time series  a so call auto regressive term  PRON can research this use   acfnaomitdframepowerdiff    which give   in this plot PRON see that there be a positive auto regressive term at lag 1 at a significance level of 95   PRON conclude this base on the observation that the second line  which be lag 1  be above the blue dotted line   next PRON want to fit a arimax model    convert datum to ts object  powerdata  lt tsdata  dframepowerdiff1   frequency  8766  start  c2016  1  1    tempdata  lt tsdata  dframetempdiff1   frequency  8766  start  c2016  1  1     build the model   m  lt arimapowerdata  order  c1  0  0   xreg  tempdata    let PRON see how PRON fit be   plotpowerdata   linesfittedm   col   blue    this give this result   PRON can play with the arimap  i  q  term  PRON should leave  p1  since PRON find that the ar term be significant at lag 1  if PRON add a ma1  PRON find a model that have a mape26046 versus a mape27262 for PRON initial model  so PRON can create the good model use   m2  lt arimapowerdata  order  c1  1  0   xreg  tempdata   linesfittedm2   col   red    will lead to   of course PRON can automate the above step with autoarima    this function will search for the good arima model   m3  lt autoarimapowerdata  xreg  tempdata   linesfittedm3   col   green    which will give PRON   PRON see that each model have a poor fit in the spike  to improve this PRON could introduce a dummy variable   xdata  lt tsdata  asmatrixcbindtempdata  dframepowerdiff1   gt  20    frequency  8766  start  c201611    mdummy  lt autoarimapowerdata  xreg  xdata   linesfittedmdummy   col   orange    which give PRON the follow result   PRON see that this model give PRON a much good result  so if PRON can relate dummy to a point in time PRON could create a model that have a good fit   accord occam s razor PRON should try to keep PRON model simple  so PRON would suggest to build a single model for all PRON time slot instead of three separate model as PRON suggest in PRON question   PRON can turn this back to the original time series with cumsum  hope this help  
__label__python __label__neural-network __label__classification __label__keras PRON be use keras with theanos backend in python   PRON have 2117 sample and each sample have an individual target  on purpose  ie  2117 output   as oppose to category  the target be rating eg   164714494876  174129353234  174476570289  the entirety of the number be important   PRON be have problem  dont know where to start   1  when i run the nn PRON only output the target as whole integer as oppose to the format of the actual value  eg  16 instead of 16xxxxxx  2  presumably i will only be able to gauge the accuracy of prediction base on how close the output be to the target since there be so many target  do this type of classification problem have a name that i can research   3  in 3 research paper i have read that apply nn to PRON specific classification problem PRON list the output layer as only have 1 neuron but provide no further explanation  how could this be   here be PRON model    fix random seed for reproducibility  seed  7  nprandomseedseed   x  nparraydffeaturesvalues   y  nparraydfmtpsvalue    define baseline model  def baselinemodel      create model  model  sequential    modeladddense10  inputdimlenfeatures    initnormal   activationrelu     modeladddense2117  initnormal   activationsoftmax      compile model  modelcompilelosssparsecategoricalcrossentropy   optimizeradam   metricsaccuracy     return model   build model  estimator  kerasclassifierbuildfn  baselinemodel  nbepoch100  batchsize5  verbose2    cross validation  xtrain  xt  ytrain  yt  traintestsplitx  y  testsize02  randomstate  seed   estimatorfitxtrain  ytrain    print class prediction  print estimatorpredictxtest   print yt  thank for any help   PRON be confused  as PRON have say  the target be rating  PRON be definitely a regression problem to PRON  instead of a classification problem   there be several problem in PRON code   for regression problem  PRON usually use linear as activation function of the last layer  sometimes relu  even sigmoid    and also PRON use mse as metricsometime mae  msle  etc   categoricalcrossentropy be use for classification problem  and sparsecategoricalcrossentropy be use for sparse input classification problem  ref  kerasclassifier be use for classification problem  use kerasregressor instead   metricsaccuracy   be use for classification problem  and PRON be meaningless in regression problem  ref  assuming df be a pandas dataframe  then dfvalu be naturally a ndarrary  there be no need to cast nparray   now answer PRON question   as PRON have never use wrapper for scikit  learn api  PRON be not very sure why PRON have integer output  PRON good guess be because of kerasclassifier and predictin scikit  learn api  predict return integer  basically the predict class and predictproba return float  indicate the probability of each class   try to use predictproba  PRON would help  btw  PRON should really use kerasregressor   as PRON have mention  there be so many class to predict  in fact  PRON be a regression problem   could PRON please add the link of these 3 paper  neural network have only 1 neuron in output layer seem to be a regression nn to PRON  regression nn usually use dense1  activationlinear   as the output layer   here be PRON version of PRON code  PRON may work    fix random seed for reproducibility  seed  7  nprandomseedseed   x  dffeaturesvalu   no need to cast nparray  y  dfmtpsvalu   define baseline model  def baselinemodel      create model  model  sequential    modeladddense10  inputdimlenfeatures    initnormal   activationrelu     modeladddense1  initnormal   activationlinear      one neuron  linear activation   compile model  modelcompilelossmse   optimizeradam     mse loss  return model   build model  estimator  kerasregressorbuildfn  baselinemodel  nbepoch100  batchsize5  verbose2    kerasregressor for regression problem   cross validation  xtrain  xt  ytrain  yt  traintestsplitx  y  testsize02  randomstate  seed   estimatorfitxtrain  ytrain    print class prediction  print estimatorpredictxtest   print yt  reference  httpsgithubcomfcholletkerasblobmasterkeraswrappersscikitlearnpyl174 
__label__r __label__optimization __label__xgboost __label__hyperparameter __label__weighted-data PRON be use xgboost for train a model on a data with extreme class imbalance  after refer from here   after perform grid search and some manual setting  PRON find that the follow parameter work the good for PRON   weight  lt asnumericlabel   nrowt   lengthlabel   upscale  lt sumweight   label   10    xgbparam  list   objective   binary  logistic    eta  01   maxdepth  4   evalmetric   auc    maxdeltastep  10   scaleposweight  upscale    how can the process of set optimal hyperparameter for xgboost be automate for good auc  please note that some of these parameter be not support by the caret implementation of xgboost but be very important for the model PRON have to design   in general  if PRON want to automate fine tune a model s hyper parameter  PRON good to use a well test package such as caret or mlr   PRON have use the caret package extensively  here be a reference of the parameter support by caret for tune a xgboost model   to automatically select parameter use caret  do the follow   first define a range of value of each parameter PRON would want caret to search  define this in the tuning grid   start model training use caret after specify a measure to optimize  eg accuracy or kappa statistic  etc   plot or print the performance comparison for various parameter value  refine and repeat if require   refer to the caret guide here to get step  by  step instruction on use PRON   for handle class imbalance  PRON have find from PRON experience that adjust weight be not as helpful as under  sample majority class and over  sample the minority class  or a combination of the two  however  PRON all depend on the size of datum available and the case at hand   in case PRON need to tune some parameter which be not support by caret  then  PRON could write PRON own iterative loop to train and test the model for different value of that parameter and then choose one that work best  PRON think most of the really relevant parameter have already be include in caret   PRON would need to adjust these parameter in case the population PRON change over time  or  the method to gather datum and PRON accuracy may change which could result in performance deterioration  PRON could run a simple check by compare the performance of PRON model over the current dataset vs a 6 month old dataset  if the performance be similar  then PRON may not need to update the model in the future  
__label__matlab __label__algorithms PRON be try to solve the phase problem from single intensity measurement  PRON implement error reduction algorithm in matlab for PRON problem but PRON do not give any result  be there anybody use this algorithm for phase problem   PRON assume PRON refer to what be become know as the error reduction algorithm for phase retrieval  eg as define in the wikipedia article on phase retrieval   PRON have use er in several context  but since this algorithm rely on a very simple alternate projection scheme between the two space  convergence tend to be slow  PRON also tend to get stick in local minima  where the most direct development  hio be mention in the article  in PRON group PRON have be use raar quite a bit  can perform much good   however  PRON would also look seriously into whether the problem  as PRON specify PRON  be expect to be solvable at all  PRON say very little on the structure of PRON problem  and PRON be very easy to specify something which be seriously underdetermined  at least when PRON take experimental noise into account  that is  there be a plethora of feasible  solution  which be nothing alike a proper recovery  and PRON can even be that the very good global optimum  would one find PRON  be too far remove from the true object to be of relevance  have a proper support constraint  and possibly constraint on positivity  reality  no imaginary part  in the object space can make a significant difference  in practice  
__label__dataset __label__feature-selection PRON be work on a dataset which contain more than 80 feature  and thousand of instance  among those feature there be some nominal one  such as ip source  ip destination  flow id  which do not have any signification for PRON machine learning model  PRON question be  should PRON remove those feature manually  or do PRON have to replace PRON value with numerical one   PRON do not need to remove PRON feature from PRON datum set PRON may just drop the feature which PRON do not want consider as feature or lable   drop a feature or label work by exclude the column while fit the datum into the training set  
__label__beginner __label__career first of all this term sound so obscure   anyways  PRON be a software programmer  one of the language PRON can code be python  speak of data PRON can use sql and can do data scraping  what PRON figure out so far after read soo many article that data science be all about good at   1 stat  2 algebra  3 data analysis  4 visualisation   5 machine learning   what PRON know so far   1 python programming  2 data scrapping in python  can PRON expert guide PRON or suggest a roadmap to brush up both theory and practical  PRON have give around 8 month of time frame to PRON   PRON do like berkeley course on data science  will give a good foundation and taste for data science  after move to udacity and coursera and many more resource  so if PRON have programming skill than will need math and stat and a lot of visualization  also will be great to get use to ipython because be essential to see every stepvisualizehow PRON perform instead write a whole script and test after  anaconda be easy to install and work with   course be list bellow   bcoursesberkeleyeducourses1267848wiki  also the stat i find good free course from sas  statistic 1  introduction to anova  regression  and logistic regression supportsascomeduscheduleshtmlctryusampid1979  start with ml will recommend  wwwkagglecomctitanicdetailsgettingstartedwithpython  on left side be also for excel use pivot table  and r datacamp have release the tutorial on how to use r once PRON complete this step than more competition in gain experience be on kaggle  recently release one for san francisco crime classification  and ultimately amazing video tutorial from wwwdataschoolio  hope PRON help   focus less on gain skill and more on gain experience  try to actually solve some problem and post PRON work on github  PRON will learn more in the process and be able to demonstrate knowledge and experience to employer  which be much more valuable than have a supposedly deep understanding of a topic or theory   data science be a pretty loaded field these day so PRON be not sure what kind of work PRON specifically want to do  but assume that machine learning be a component of PRON then kagglecom be a good place to start  in term of goal  if PRON be able to work with the datum in panda  numpy  scipy  build model in sci  kit learn and make some pretty graph in seaborn  ggplot or even matplotlib then PRON will not have a problem get a job from a skill perspective  especially if PRON have code sample and example to demonstrate PRON ability  if PRON get stick then stackexchange will either have the answer or PRON can post a question and PRON will have an answer shortly  once PRON be do the work for a living then PRON will learn even more  likely from a senior team member who mentor PRON   good of luck   disagree with david  a true data scientist be an applied statistician who cod and know how to use machine learn algorithm for the right reason  statistic be the base of all data science  PRON be the  cake  per se  everything else be just ice   the question be what kind of datum scientist do PRON want to be  do PRON want to be a master of the subject  knowledge of how  why  when and when not to apply an algorithm or technique  or a kaggle script kiddie use scipy and think that PRON be a data scientist   1  stat  2 everything else  if PRON want to be a practical man with true knowledge  start with mathcalculus  probability  stat  lelinear algebra   on every step try to implement everything with programing  python be nice for this  when u get good ground  play with real datum and solve for problem  courses   linear algebra  edx laff or cod the matrix  stat  edx stat 2x barkley  calculus  read  PRON simple  david have a good point  PRON would suggest PRON focus on whatev PRON be that drive PRON interest more  PRON be the only way to succeed in every kind of effort  if PRON want to build something cool start with PRON  if PRON want to read a book that s good too  the starting point do not matter  a few day ahead PRON will have a good understanding on what PRON want and should do next   data science be so broad  there be many different path to get into PRON  PRON be usually split into 4 or 5 different type for example   PRON could see from the other post in this topic people come from an applied statistics background  apply the right algorithm   programming background  participate in kaggle   and other apply PRON to a business background  savvy company could refer to a programming skewed person as a  data engineer   big company also use each type for PRON datum science team  so demonstrate good t  shape skill would be a good thing  
__label__dataset __label__decision-trees in scikit  learn documentation and in decision tree learn wikipedia article there be mention of  there be concept that be hard to learn because decision tree do not express PRON easily  such as xor  parity or multiplexer problem    PRON can not recall know such problem type   what be example  and possibly relate dataset  of such problem   below be an example of xor dataset for classification  as PRON can see  decision tree perform pretty poorly on this dataset  reason be decision tree split space into rectangular region  therefore PRON be not pretty good with this kind of distribution   if PRON really want to use tree in that sort of situation  PRON be interesting to use so  call rotation tree  rotation be about perform pca  principal component analysis  on input feature while learn tree  use PRON  decision tree can then build non  rectangular region   also  here be a playground to test gradient boost algorithm include on xor dataset  PRON be really interesting  PRON can click on  rotate tree  to activate rotation   gradient boost playground 
__label__fortran __label__mpi PRON be loop an array use mpi  the problem be  i think that some process be move onto PRON next iteration before other precess have finish PRON  this be cause PRON problem because datum calculate by each process at each iteration be need in the next  be PRON a way that i can pause the process at the end of the loop until all other have finish PRON current iteration   PRON be use fortran90 if PRON make any difference   thank   an mpibarrier can be use to synchronize all process in a communicator  each of the process have to wait till all other process reach the barrier before all of PRON can proceed further   mpibarriercomm  ierror   here comm be the communicator handle and ierror the error status   note that sometimes there be more clever way to deal with such dependency algorithmically  but this be highly problem  dependent   while christian have give PRON the right answer  PRON be almost certainly not what PRON want  unnecessary synchronization be guarantee to limit the scalability of PRON code  if all processor need datum from all other processor  then use a collective like mpialltoall   may be more appropriate  if one task have receive all the require datum from other task  there be usually no harm in that task continue even if other be still work on the previous iteration  in that case  PRON can probably use point  to  point communication only  
__label__neural-networks __label__machine-learning in PRON attempt at try to learn neural network and machine learn PRON be be try to create a simple neural network which can be train to recognise one word from a give string  which contain only one word   so in effect if one where to feed PRON a string contain the train word but spell wrong the network would be able to still recognise the word  can anybody help PRON with some pseudo code or a start of a code  or a general explanation of how to to this because PRON have read like 6 article and 8 example project and still have no clue how to do this  if PRON be read PRON correctly  this question have nothing to do with optical character recognition  PRON want to create a system that take a digital string of character as input  then find the good match from a predetermined list of word  that sound like a task for if  then  else logic and dictionary lookup  PRON may be possible to use a neural net  but not easy   a neural net take a fix number of input  each of which be a value between zero and one  a major hurdle be that PRON probably want variable  sized input  another hurdle be that PRON will need to code the input some way onto number   these hurdle can be overcome but PRON be tipoff that neural network be not well  suit for the task   an optimal solution for the task as state  would be some alignment algorithm like smith  waterman  with a matrix which encode typical typo frequency   as an exercise in nn  PRON would recommend use a rnn  this circumvent the problem that PRON input will be of variable size  because PRON just feed one letter after another and get an output once PRON feed the delimiter   as trainingsdata PRON will need a list of random word and possibly a list of random string  as negative example and a list of slightly mess up version of PRON target word as positive example   here be a minimal character  level rnn  which consist of only a little more than a hundred line of code  so PRON may be able to get PRON head around PRON or at least get PRON to run  here be the excellent blog post by karpathy to which the code sample belong  
__label__matlab __label__data-visualization __label__plotting PRON have a matrix p of m row and n column  pi  j  contain the price of occupy position i at time j PRON want to plot this in matlab but in a way to get some kind of density plot with different price level correspond to different color   an example of the output would be something like this   a fill 2d contour plot seem to be what PRON be look for   assume PRON have vector  or matrix   position  and  time   the follow snippet should work  modulo an apostrophe or two   contourfposition  time  p    colorbar   PRON just find the in  build function PRON be look for   imagesc    for example    gtgt  imagesceye4    colorbar  give this  what about just use surface plot  something like   surfp   view090    or for more option something like   surfpedgecolornonefacecolorinterp    view090   colormaphot   
__label__visualization __label__paraview PRON be work with vtk legacy file contain vector information  the file have incremental extension represent a file series as define by this link   when apply the filter  plot selection over time   PRON get a nice curve as follow   however the x  axis do not show the realtime  rather PRON show the actual time step  PRON would like to preserve the same datum but have PRON show the x axis from 0 to 1 second rather than from 0 to 100 time step   PRON have search up and down for a further filter that PRON can apply to the plot that can do this but have not find one  also the  bottom axis custom scale  property do not help since PRON truncate the datum after 1 second rather than rescale PRON  PRON feel like there should be a relatively straightforward way to accomplish this on paraview  any idea   as tylerolsen suggest  PRON be probably a good idea to do this particular plot use another tool  but if PRON insist on do PRON use paraview  PRON can  apply a temporal shift scale filter  and  then plot over the shifted  scale time   in PRON example below PRON scale PRON time by 01  turn the maximum to 25  PRON have 250 timestep   
__label__convolutional-neural-networks PRON would love to learn how to create PRON own neural network from scratch so i can understand PRON good  PRON goal PRON be not so much to use PRON perception capability  classify picture  as PRON be to use PRON the other way around   PRON be look for a start place  PRON have not find anything use google   sorry if for some reason this type off request be prohibit here   PRON recommend PRON to take a look at handbook of neuroevolution through erlang  PRON guide PRON through the development of an neuroevolutionary substrate encode system   PRON can read the first few chapter where PRON show PRON how to code a neural network   then show PRON how to add plasticity and other feature   note that PRON do not cover back propagation  thus the math be not complex   erlang or similar functional languae be fun to write nn with  but once PRON grasp the concept PRON can use any language PRON prefer   here be a repository for the  final  system dxnn2 on github  and here be the code use in the book book code on github  for the classic neural network part of the cnn  a great starting place for beginner be the book michael nielsen publish at neural networks and deep learning  PRON use python for PRON example but explain everything in detail  so PRON should not be hard to implement the same concept in any other high programming language  PRON study PRON code in detail and can not think of any concept that be very python specific  PRON may have to look up some function like  zip  if PRON be totally new to python but the documentation of those function be easy to follow for a developer   if PRON have not work with neural network before  the amount of mathematic require may seem threaten at first  especially if PRON be not too familiar with gradient decent and partial derivative  but follow the recommend book  and some look  up in wikipedia  should teach PRON everything PRON need to implement the neural network aspect of PRON project in java or c  the other required layer for PRON cnn  especially the convolution layer  depend more on the actual problem PRON be try to solve  PRON say   PRON goal PRON be not so much to use PRON perception capability  classify picture  as PRON be to use PRON the other way around   PRON do not really understand what PRON be try to achieve with PRON cnn  maybe PRON can elaborate more  PRON convolution and pool layer may look quite different from typical implementation use in image recognition  the basic principle of those layer can be find eg on wikipedia as a starting point  without deep knowledge of PRON goal PRON be hard to recommend anything more specific  
__label__neural-networks __label__natural-language __label__chat-bots some paper say that bleu be not a appropriate evaluating method for chatbot  instead PRON use perplexity to estimate chatbot   so what be perplexity  how to calculate PRON   with perplexity PRON be try to evaluate the similarity between the token  in PRON case probably sentence  distribution generate by the model and the one in the test datum   for instance  assume PRON have m sentence s1  sm  each with probability psi   the perplexity be 2l   where l  1m  sumlogpsi   for i in  1  m    please note that while perplexity may be useful to capture certain aspect of the model PRON be by no mean perfect and even if PRON be able to reach great perplexity score PRON will not necessarily translate to a good or even work chat bot  
__label__matlab __label__statistics PRON have write some code  to produce some datum  in what PRON have show below  the output be just one  dimensional array of number  PRON have an analytical expression for what the autocorrelation function of this datum should look like  PRON problem be  that PRON do not look like that  when PRON use matlab build  in function xcorr  PRON do not know why that be  but PRON be hop some of PRON may be able to tell PRON   PRON have attach two graph  to show PRON what PRON mean  the graph herein be normalize  by the way  the red graph be the expect autocorrelation function  and the blue be the measure one   sorry for the horrible image  PRON do not know why matlab do that    as PRON can see  there seem to be some sort of oscillation  or something like that  be that to be expect   this be actually somewhat of a toy  example  the code PRON really  really  need to get work  produce a 4d array  actually a discretiz 3d vector field  the measure autocorrelation of this output be far ugly than this  PRON will upload some graph in a while   update   here be a graph show the measure autocorrelation of the produce 3d vector field  once again  sorry for the poor quality  the blue graph show the output of xcorrvcoef    the purple graph show the output of xcorrvbiased   and the red be how PRON be actually suppose to look  if PRON move the purple graph  so that PRON maximum be 1  the middle part of the graph be pretty close to the red graph  but be this just a coincidence   PRON question be not very clear  there may always be difference in the compute autocorrelation and the theoretical  expect  autocorrelation due to noise in the datum  be PRON sure PRON be not have any  the more noise PRON data have  the less the chance of PRON get a smooth correlation plot  usually the wiggle in the plot indicate presence of pocket of randomness  which be what noise be  compute the autocorrelation use 2 different software package  if PRON see a difference  then PRON may be a programming issue  else PRON do not see what be wrong with this  the analytical autocorrelation do not always match with what PRON get from the measure datum  which be prone to noise and measurement  computation error  
__label__data-visualization __label__paraview __label__vtk PRON need to write some datum from a computation  that will be read later by paraview  vtu or vtk file    when PRON come to file size  should PRON go for the ascii format or the binary format   if PRON only worry be file size  then PRON want binary file   for an illustrative example  let assume PRON be write 1 double precision float point number to a file   let PRON assume that the file system can handle this perfectly and hold the file  header  and padding be all 0   for a binary file  that number would take the exact size of the number in ram  or 8 byte   in ascii format  PRON would hold   16 digit of the base  1 period for the decimal  1 char to delimit the exponent  1 char for the sign of the exponent  2  3 char for the exponent  assume PRON use only 1 byte for a character  that be 22 byte to hold the same number   this do not count the character require to dilimit between number  usually atleast 1    therefore file size for ascii format will be about 3 time large   PRON can trade in file size for the precision in the stored file  only keep 5  6 digit in the base   but that depend on what PRON be use PRON for   the main advantage of ascii be for debug or produce human readable datum   in practice  PRON rarely need datum in visualization file that be more accurate than  say  3 valid digit  in that case  ascii be  maybe surprisingly  often more compact than binary form  if PRON be think about archive  then bzip  e these ascii file be likely go to yield the small file PRON can get   that say  paraview read vtu format which have a compressed binary form  xml  base  but the data be first libz  compress and then uuencoded again to yield ascii text   on typical file  this save a factor of 4  10  for large file  this be definitely the way to go  
__label__algorithms __label__software PRON be try to generate a 2 tuple use maple  can anyone give PRON the command to generate this  thank PRON very much   what do PRON mean by a 2tuple  if the two value PRON want to put into PRON be call x and y  then PRON can create a  sequence as x  y  list as  x  y   set as  x  y   vector as  ltx  ygt   array as arrayx  y    table as eg table1  x  2  y    or use something else as key  or use x and y as key and something else as value   record as eg recordx  y   mutable set as mutablesetx  y   PRON do not believe this be document  so PRON be on PRON own for forward compatibility   deque as dequeuex  y   same remark as for the mutable set   unevaluated function call as eg fx  y   each of these can be use as a 2tuple  with some fantasy   what do PRON want to do with the tuple  
__label__machine-learning __label__neural-network __label__deep-learning __label__autoencoder __label__bayesian-networks PRON have be study a recommender system which use a collaborative deep learning approach and bayesian learning have the follow nn representation   need to know the working of stack denois autoencoder   here be the link to the paper   httpwwwwanghaoinpaperkdd15cdlpdf  
__label__data-mining __label__statistics __label__pca PRON want to study the impact of 26 parameter on one variable  and therefore determine the 3 or 4 which have to most influence on PRON  for that  PRON have construct a 10 x 26 matrix  26 parameter with 10 observation  and a vector of 10 array contain the variable PRON want to study with 10 observation  from this  how can PRON determine which be the 3 or 4 most influential variable on the parameter  PRON be consider use pca  but PRON be not sure that be the right approach  can anyone give PRON some direction here   update  with this sample size PRON almost can not find any useful insight   one of the way to find one to one relationship be find correlation coefficient of two random variable  correlation be the statistical relationship between two random variable or attribute  in PRON case   this coefficient be a value between 1 and 1  if the value be close to 1 PRON mean that there be a strong positive correlation between the two random variable  in other world as the value of one attribute increase  the value of the other attribute increase too  on the other hand  if the value be close to 1 PRON mean that there be a strong negative correlation between PRON which mean as the value of one attribute increase the value of the other attribute decrease  if the correlation value be close to zero PRON mean that there be no significant correlation between the two random variable   if PRON could use python  PRON can load a csv file into a pandas object and run the corr   method of the panda object to get the correlation coefficient of two or more random variable  see the code below  PRON could not find the source of this code to mention   from string import letter  import numpy as np  import panda as pd  import seaborn as sns  import matplotlibpyplot as plt  snssetstylewhite     generate a large random dataset  rs  nprandomrandomstate33   d  pd  dataframedata  rsnormalsize100  26     column  listletters26      compute the correlation matrix  corr  dcorr     generate a mask for the upper triangle  mask  npzeroslikecorr  dtype  npbool   masknptriuindicesfrommask    true   set up the matplotlib figure  f  ax  pltsubplotsfigsize11  9     generate a custom diverge colormap  cmap  snsdivergingpalette220  10  ascmap  true    draw the heatmap with the mask and correct aspect ratio  snsheatmapcorr  mask  mask  cmap  cmap  vmax3   square  true  xticklabels5  yticklabels5   linewidths5  cbarkwsshrink   5   ax  ax   pltshow    in this plot PRON can see the strong or week correlation between all of the variable and then pick the one PRON find the most significant   there be a number of method for this  here be a list   PRON can build a regression model and observe the p  value of the coefficient of each variable   pearson correlation  spearman correlation  kendall correlation  mutual information  rrelieff algorithm  decision tree  principal component analysis  which PRON have try   etc   PRON can search for other method with these keyword   feature selection  variable importance  variable ranking  parameter selection  PRON be all almost the same  just different terminology in different field   however  the issue that PRON must pay attention here be  10  observation be almost nothing  PRON will be extremely difficult to trust any outcome  regardless of the method PRON use   PRON can try use lasso which do l1 regularization on the weight and set the irrelevant parameter to 0  among the a subset of correlate parameter  there only one will be choose  this work particularly well if PRON suspect that there be multi  collinearity in PRON dataset  
__label__information-retrieval __label__cosine-distance __label__vector-space-models in information retrieval when PRON calculate the cosine similarity between the query feature vector and the document feature vector PRON penalize the unseen word in the query   example if PRON have two document with feature vector  d1   11100   d2   01110   PRON can see that the two document have the second feature so if PRON want to search for the second feature with query vector  q   01000  then the cosine similarity between q and d1d2 will be  1√3   and not 1 because that PRON penalize the other feature that PRON have not mention in the query   from this discussion PRON do not understand why penalize PRON be a good idea   be penalize unseen feature good   be there another similarity measure that do not penalize PRON   of course cosine similarity be not proper for search a specific feature in document  to do this  PRON can exactly use dot product  as PRON will ignore zero feature in query vector from document   cosine similarity  in the current context  can be use to find similarity between two document  so  all feature can be important to find similarity  PRON mean  if there be not a feature in query vector  but there be in  a document  or vice versa  these two be 100 percent similar  hence  PRON make sense   the point of PRON example seem to be more one of length normalization  long story short   rare term be more informative than frequent word and and PRON do not want to weight all term equally   the more frequent a term  the more likely a document carry information about a term   this video provide example demonstrate why consider all the word in the text be important  include example particular to the point PRON make in PRON text  to get an intuitive feel for why PRON search may be a problem  consider the fact that under PRON approach  all query will be extremely similar to an english dictionary 
__label__optimization if PRON want to fit a nonlinear regression model with some parameter like  sigma2where  sigma be the standard deviation  which be positive   how can PRON guarantee that  hatsigma be positive   PRON mean  if PRON use maximum likelihoodand the model only have square term of  sigma   how do the optimization method know  sigma be positive   well  PRON seem ok that the result estimation of  sigma be negative  but PRON look weird  and this be why PRON ask this question    PRON would suggest parametriz with a logarithm of volatility so PRON do not have to care about positivity  run the estimation and then invert back to original scale  alternatively  PRON can consider constrained optimization routine  without know more about the problem  at least the language PRON be use   that be about PRON  
__label__descriptive-statistics PRON be work on 2 sample independent t  test  PRON have conduct analysis on test group vs control group and PRON have to write a report but PRON have few question   1  do PRON have to take out the outlier and then perform t  test   2  once PRON perform t  test can anybody explain the t  test output  the explanation should not be in term of statistical term but in such a way that non business person can also understand  PRON need simple explanation for confidence interval and difference in mean of the two sample   3  what kind of chart can PRON draw for t  test to represent PRON result   1  maybe  remember that PRON be assume a normal distribution  if PRON do not satisfy those assumption PRON be not run a valid test   2you be test whether or not the difference be zero  ie no difference  zero in PRON confidence interval   3bar chart be the easy to understand because PRON can see the difference  box  plot provide more info but be for technical people only   PRON be fine to do a t  test on unequal sample size  however  the power would not be as good as equal sample size   1   yes or no  impossible to say without plot the outlier  what be more important  can PRON assume PRON datum be normally distribute  have PRON check the qq  plot  have PRON check the histogram  do PRON look like close to a normal distribution  while the t  test be robust against non  normal datum as long as the sample size be sufficient large  PRON datum should not behave too far away from a normal   when PRON think about outlier  ask PRON the follow question   how many outlier  if PRON have many  t  test be probably not appropriate   why the outlier  if PRON be a random error  PRON be just unlucky   PRON could include PRON in the t  test  if PRON be a systematic error  stop the test  go back and check PRON datum   how do PRON define the outlier   do those outlier look symmetry  if so  PRON may assume PRON sample come from a normal population  PRON can check the skewness of PRON datum   PRON have to try to understand those outlier to come with up a decision   2   PRON can just explain like  the probability of the difference in mean be  or be not  significant    3   PRON should draw a box  plot for each group  
__label__finite-element __label__numerical-analysis __label__fluid-dynamics PRON be try to solve lid driven square cavity flow problem of stokes equation use finite element method  PRON have boundary condition for velocity as zero on every boundary but u1 on top boundary  can PRON please help PRON with the boundary condition of stream function and how to obtain  especially on top boundary   PRON have go through many literature but why most of PRON consider as zero on every boundary   for the streamline formulation the divergence equation for the velocity  vecvu  vt    mathrmdivvecv0  hold identically if    vecv    u  vt left  fracpartial psipartial yfracpartial psipartial xrightttag  for boundary in which the velocity be  vecvvec0  PRON be easy to see from      that  psi  constant and  vecncdotvecmathrmgradpsi0 without any loss of generality PRON can set this constant to  0  on the other hand  on the top of the cavity in which  vecvu00t the streamline function  psi must fulfill     fracpartialpsipartial yv0qquad fracpartialpsipartial x0    the velocity and stream function be relate by     u  psiy  qquad v   psix     this can be relate to the vorticity     delta psi  omega  vy  ux     on left and right  PRON have   u  v00     psi  const  quad psix  0     on bottom   u  v    00     psiy  0  qquad psi  const     on top   u  vu00     psiy  u0  qquad psi  const     PRON can not enforce two boundary condition  let PRON use dirichlet bc     psi  const     on all boundary  since  psi must be continuous  assume  omega in  l2  atleast   the constant must be same on all side which PRON can set to zero  
__label__finite-element __label__unstructured-mesh PRON want to solve a poisson problem on two domainsinterface problem  let  omega  be a square   omegaomega1 cup omega2  and let  omega1   be a circle inside the square    gamma be the boundary of circle     delta u1f1 in omega1    delta u2f2 in omega2    u2gon partial omega    u2u1w ongamma    fracpartial u2partial n2fracpartial u1partial n1vongamma  what should  PRON do  by the condition on  gamma  PRON be boundary condition   if yes  how to enforce these condition  when the test space be  h1omega  the condition on  gamma be call interface condition  PRON could incorporate the jump condition into PRON weak form  typically  the trial and test space be  h1omega1   time h1omega2 numerically  the interfacial condition be easy to treat if  gamma be align with a mesh of  omega one example of how PRON may treat the embed interface condition can be find in chapter 4 of the phd thesis by chandrasekhar annavarapu  out of john dolbow s group at duke  
__label__machine-learning __label__precision double  precision calculation be significantly slow or more expensive than single  precision calculation  for example  the nvidia tesla which perform well on double be much more expensive then regular gpu   at the same time PRON do not know about machine learn case where double  precision be really need  currently PRON think that 64bit float be need by the model only in case then PRON be over  fit  in other word double be need only by  bad  model with bad generalization   PRON be not an expert in machine learning  but PRON can outline the consideration that be relevant   the numerical calculation in machine learning be generally linear algebra  either solve linear system or linear least square   for both type of problem  there be well  know backward  stable method  so PRON will assume PRON be use a backward  stable algorithm   then PRON should expect an error of roughly   kappa epsilon  where  kappa be the condition number of the problem and  epsilon be unit roundoff   for the linear system  ax  b  PRON have  kappa  a a1  kappaa  the condition number of the matrix  a   for the least square problem  the condition number can fall anywhere in the range   kappaakappa2a  see eg the text of trefethen  amp  bau for detail   thus for linear system  single precision will be sufficient as long as  kappaa be much less than  10  7  for least square  PRON may already be in trouble when  kappaaapprox 10  3  10  4  for large dataset  those be not very large condition number   so PRON certainly seem plausible that PRON may need double precision   PRON think PRON be right that double precision be not need usually   as a proxy  apple s metal framework do not support double at all  and PRON be be bill as a compute framework with apple  provide function for machine learning   info about metal datum type  httpsdeveloperapplecommetalmetalshadinglanguagespecificationpdf 
__label__boundary-conditions PRON be work on develop a dissipative particle dynamics  dpd  model of a colloid in a bulk fluid  PRON essentially want to ensure that the perturbation of PRON meshed colloid be completely uncoupled from the duplicated copy of PRON that be present as a result of use periodic boundary condition  this include any hierarchical interaction  for example  if fluid particle be disturb from interact with  a duplicated version of the colloid  the propagation of these interaction should dissipate out enough such that by the time this propagation reach the colloid of interest  that PRON essentially have a negligible effect on the behavior of the colloid  unfortunately PRON PRON be not certain about what  negligible  mean  but perhaps someone with more experience in the field could offer some advice on that point in particular   as a starting point  PRON would assume the follow    l  d  gt  rc   where  l be the box length   d be the diameter of the colloid and  rc be the large cutoff for the bead  bead interaction force  this ensure that the colloid will not interact with PRON as a direct result of colloid  colloid interaction  but do not ensure the prevention of the hierarchical interaction PRON describe earlier   
__label__linear-algebra __label__linear-solver __label__sparse PRON be interested in solve a sequence of shift linear system   asigma ix  b for various value of  sigma the matrix  a be sparse and not too large  so PRON have PRON lu factorization available  what be the good way to do this   this be for calculate a frequency spectrum   sigma be roughly the frequency so PRON will vary smoothly in an interval  PRON expect that solution to nearby  sigma will be somewhat similar  PRON first thought be to use an iterative solver like gmres with the lu factor as a preconditioner  but PRON be wonder if there be any good method   PRON can use the direct solve with  a  sigma0 i as a preconditioner for  a  sigmai i as long as  sigmai be reasonably close to  sigma0 this tend to work well if the spectrum be away from the origin  PRON can improve the approximation somewhat use the first order correction describe in this answer  if the spectrum cross the origin between the two shift  sigma0  and  sigmai  as occur for helmholtz  PRON do not know a way to utilize the old factorization   the solution  x  and hence any simple response compute from PRON  be an analytic function of  sigma  hence PRON can be well approximate locally by rational function  the response have pole at the eigenvalue of  a  which be why rational interpolation be need  the follow interpolation technique should work quite well with a number of factorization approximately equal to the number of peak in the response   compute factorization for three value of  sigma  namely the endpoint and the midpoint of PRON interval  and then calculate the response of interest and PRON first two  or more  derivative with respect to  sigma note that give the factorization  PRON can easily get the derivative by multiple backsolf   then use piecewise rational hermite interpolation  first a single rational  then two rational piece  one in each subinterval  assume that the response of interest decay to zero for  sigmatoinfty  if not one need to change the degree in the interpolant    d derivative at the endpoint of a subinterval allow one to interpolate by a quotient of a degree  d1  polynomial divide by a degree  d polynomial  for  d2   this capture a single lorentz peak exactly  for  d4  PRON capture a superposition of two   if this be not enough  one can also consider multipoint hermite interpolation  make good use of the compute datum  at the expense of more programming work    test the accuracy of the current interpolant by an additional exact calculation at the point where the last two interpolant differ most  use a new factorization and a new rational fit in the two interval create by the new point  to decide when PRON can quit   unless PRON spectrum be very complicated  PRON will need few factorization only   there be a method call automated multilevel substructuring  amls  which be originally design for a similar problem in vibration analysis  where the solution of the linear system with a particular shift correspond to the frequency response problem at a frequency which be the square  root of the shift  the basic idea be to use nest dissection in order to generate a tree of separator and substructure  and to use extension of the low  frequency mode of substructure  which can be find cheaply  as a means of reduce the global model  PRON have a setup cost proportional to a single factorization  but the cost grow with the high frequency that PRON would like to solve to   the original paper be here  and an extension to maxwell s equation be here   disclaimer  PRON use to work in the lab that invent amls 
__label__matlab __label__computational-physics __label__mathematica __label__electromagnetics PRON have a nonlinear schrodinger equation which read     frac12  fracd2udx2 u2u  vxu  i fracdudz  where  vxcoswx i a sinwx and  w   a be number   how to solve PRON band structure  mean PRON dispersion relation   the plane wave expansion method be use for a real potential  can PRON be apply for this complex potential also   what be other method  if any  in matlab or mathematica or other software   
__label__machine-learning __label__neural-network __label__deep-learning __label__convnet __label__computer-vision PRON be work on recognize object class in image use neuronal net so PRON could make classifier for cat  dog  use imagenet and some conv net famous architecture but PRON problem be if PRON have an image that contain a human and a cat and a car how can PRON detect and recognize 3 of PRON   PRON fall under the multi label object detection problem  there have be lot of advancement do in this field and PRON should definitely look at implementation like   yolonet  httpspjreddiecomdarknetyolo  squeezenet  httpsonghangithubiosqueezenetdeepcompression 
__label__optimization __label__algorithms __label__nonlinear-programming __label__data-sets __label__testing what be PRON preferred test set to test quality of non linear solver   this could be set of datum  model and result obtain with some benchmark solver  or simply a panel of test function that could be use to generate synthetic datum   thank   PRON depend a lot on which nonlinear problem PRON want to solve   httpplatoasuedubenchhtml  be a good place to start for local constrained optimization   httpwwwmatunivieacatdferitestenvhtml  for global constrained optimization  and  httparchimedeschemecmueduqdfocomp  httpcocogforgeinriafrdokuphpidbbob2012  for unconstrained black box optimization   PRON be not aware of special test set for nonlinear least square   if PRON know the kind of fitting problem PRON want to apply PRON to  PRON can generate from any known model simulate datum and then compare the solution obtain with the input for the simulation   but if PRON want to have a general  purpose constrain least square solver   then PRON can take anyway hardly any advantage of the least square structure   thus PRON would be sensible to use  or develop  a general purpose nonlinear program solver  and test PRON on the test set from one of the above site  
__label__algorithms __label__mesh PRON be able to draw to the outer airtight boundary of hole  now PRON would like to implement interior flood  fill algorithm  to get an internal seed  only one method come to PRON mind which be test neighbor of red cell for ray casting  however  PRON would like to know be there a fast method   
__label__finite-element __label__mesh-generation PRON be write a fen solver on tet10 element  PRON right now have a cfd grid gen tool  pointwise  which generate tet4 mesh  from this PRON have the element connectivity list and the node list  how do PRON go about generate a tet10 mesh from this info   be there any particular tool  code to do this   
__label__nlp __label__getting-started __label__node-js PRON be new to ai and PRON be try to convert natural languageenglish  to the sql query for PRON application  PRON have to develop this be node js  PRON have hear about open npl package but PRON do not have any idea on how to develop   
__label__philosophy PRON be read such nonsense about how an ai would turn the world into a supercomputer to solve a problem that PRON think PRON need to solve  that would not be ai  that be procedural programming stick in some loop nonsense  an ai would need to evolve and re  organise PRON neuron  PRON would not be stick to hardcode if PRON become intelligent by re  write PRON code   PRON be not necessarily a nonsense  PRON all depend on the impose criterion  imagine the following  say an advanced ai system be design to control the stability of the local fauna and flora  area enclose in some kind of a dome   PRON can control the pressure under the dome  the amount of light that go through the dome etc   everything that ensure the optimal condition  now  say that the dome be inhabit by various specie  include human  PRON be worth note that simple implementation of such system be be use nowadays already   give that human tend to destroy and abuse the natural resource as well as pollute the environment  the system may decide that lower the population of the give specie  human in this case  may in the long run benefit the entire biome   the same principle may be apply globally  however  this assume that all specie  include human  be treat equally and the utmost goal of the ai be ensure the stability of the biome PRON  take care of   people do such thing nowadays  PRON control the population of some specie in order to keep the balance  wolf  fish  to name but a few   PRON be a possible side effect  any goal  orient agent may  well  simply do thing that achieve PRON goal while disregard side effect that do not matter for these goal   if PRON goal include a tidy live space  PRON may transform PRON yard to a nice  flat lawn or pavement while wipe out the complex ecosystem of life that be there before  because PRON do not particulary care about that   if the goal of a particular powerful ai happen to include do anything on a large scale  and somehow do not particularly care about the current complex ecosystem  then that ecosystem may get wipe out in the process  PRON do not need to want or need to wipe out PRON  if PRON be simply not relevant to PRON goal  then PRON be make of material and occupy space that PRON may want to use for something else   PRON be a threat to most goal  any goal  orient agent may want to ensure that PRON can fulfill PRON goal  any smart agent will try to anticipate the action of other agent that may prevent PRON from achieve those goal  and take step to ensure that PRON succeed anyway  in many case PRON be simple to eliminate those other agent rather than ensure that PRON effort fail   for example  PRON goal may include store a bag of sugar in a country house so that PRON can make pancake when visit without bring all ingredient every time  however  if PRON leave PRON there  PRON be likely to get eat by rat during winter  PRON may take all kind of precaution to store PRON better  but rat be smart and crafty  and there be clearly a nontrivial chance that PRON will still succeed in achieve PRON goal anyway  so an effective extra precaution be kill the rat before PRON get a chance to try   if the goal of a particular powerful ai be to do x  PRON may come to an understanding that  some   human may actually not want x but y instead  PRON can also easily deduce that some of those human may actively do thing that prevent x andor try to turn off the ai  do thing that ensure that the goal get achieve be pretty much what a goal  seek agent do  in this case if existence of human be not strictly necessary for goal x  then eliminate PRON become a solid risk reduction strategy  PRON be not strictly necessary and PRON may take all kind of other precaution as well  but just like in PRON example of rat  human be smart and crafty and there be clearly a nontrivial chance that PRON will still succeed in achieve PRON goal  so that x do not happen as ai intend  so an effective extra precaution could be kill PRON before PRON get a chance to try   ai be already use as weapon  think on the drone   PRON suspect  a  robot take over the world  scenario have the high probability  if PRON have an intermediate step  this intermediate step could be  human take over the world with robot    this can go somewhere into a false direction   PRON suspect  PRON be not surely so far as PRON seem  consider the us have currently 8000 drone  what if PRON would have 8million  a small group capable to control PRON could take over the world  or the small group control different part of the fleet  could fight against eachother  PRON should not be all in the us  at the time the us will have this fleet  other country will develop also PRON   btw  a world takeover seem to PRON unreal  the military leader can maybe switch the human pilot to drone  PRON be not PRON job  but the  high level control   ie to determine  what to do  who be the target  these decision PRON will not ever give out from PRON hand   next to that  the robot do not have a long  term goal  PRON  human  have   thus PRON do not consider a skynet  style takeover very realistic  but a chernobyl  style  mistake  of a misinterpreted command  which result the unstoppable rampage of the fleet  do not seem to PRON impossible   PRON feel like most of the scenario about ai s wipe out the world fall into one of two category   anthropomorphized ai s  or  intelligent but dumb computer run amuck  in the  1  case  people talk about ai s become  evil  and attribute to PRON other such human element   PRON look at this as be mostly sci  fi and do not think PRON merit much serious discussion   that be  PRON see no particular reason to assume that an artificial intelligence  regardless of how intelligent PRON be  will necessarily behave like a human   the  2  case make more sense to PRON   this be the idea that an ai be  for example  put in control of the nuclear missile silo and wind up launch the missile because PRON be just do PRON be job  but miss something a human would have notice via what PRON may call  common sense    hence the  intelligent but dumb  moniker   neither of these strike PRON as terribly alarming  because  1  be probably fiction and  2  do not involve any actual malicious intent by the ai  which mean PRON will not be actively try to deceive PRON  or work around any safety cut  out  etc   now if somebody build an ai and decide to intentionally program PRON so that PRON develop human like characteristic like arrogance  ego  greed  etc  well  all bet be off  
__label__neural-network __label__deep-learning __label__dataset __label__preprocessing PRON have a dataset of shape 105 x 501 x 266 where 105 be the number of data and 501 x 266 be the shape of 1 datum ie the labelsdataset be of shape 105 x 1   each value of the 501 x 266 matrix be a complex number   so PRON essentially become 501  266  2real and imaginary part of the number   and now PRON have to feed this datum to a cnn   PRON  m new to training network  so need to know whether PRON data be in good possible form for the cnn or not   PRON have print out max  min  sd  mean of real part  imaginary part and magnitude of the dataset for more info   max real  0186396  min real  0204375  max imag  0166608  min imag  0159017  max abs  0219019  min abs  233527e10  mean real  401718e10  complex  679294e15  abs  882916e05  std dev real  0000442753  complex  0000400677  abs  0000590573  be this a good form of datum for input to a cnn  what be the option to make PRON more suitable   
__label__neural-network __label__dataset __label__caffe PRON want to build a system that recognise  with a give uncertainty  the make and model of a car from an image  PRON have decide to use convolution neural networks  specifically the caffe framework   PRON next decision be how good to build PRON data set  accord to this book  PRON need around 5000 datum point for each class  so let say 500k image    PRON have do a little bit of read on here and other resource  and PRON seem that the google custom search api be a potential option  but that limit PRON to at most 100 search per day  for free   PRON think about build a script to scrape site like autotrader  but PRON experience with scrap the web be zero   do anyone have experience generate image data set of this size  any pearl of wisdom that PRON could share with PRON  PRON be happy to invest time and effort learn for example beautiful soup or this google api  but PRON do not want to waste time go down the wrong rabbit hole   PRON decide turn PRON comment into an answer   if PRON want to go pro  use a framework such as scrapy   personally  PRON find PRON overly  cumbersome and PRON have be successful use the follow approach  PRON think PRON use case be simple enough for PRON to be of use to PRON as well   assume PRON be use python3 as well  PRON can grab a webpage easily  and then get what PRON want use xpath notation   from lxml import html  import urllibrequest   keep run until there be no  next  page  for page in range999    url   httpblablablacompaged   page  text  urllibrequesturlopenurlread    tree  htmlfromstringtext   image  treexpathimgclasscarhref      type  treexpathdivclasstypetext      if not image   break  for i   cartype  image  in enumerateziptype  image     urllibrequesturlretrieveimage    s  paged  imgdpng    cartype  page  i     purely illustrative example    now adjust as PRON may  xpath be an incredibly powerful notation of access xml node  much more be possible than PRON be write here  take this tutorial to learn the full xpath syntax   some web designer make PRON much hard to access whatev PRON want because PRON do not properly class  ify PRON html object  in those case  PRON may have to access a parent node and ask for PRON child  or access a sibling and then get the sibling  anyway  xpath and python s lxml package make all this incredibly easy   any modern browser like chrome and firefox also let PRON easily explore the dom of any webpage  just right  click and press inspect or go to developer tools in the tools menu or some such   note  some website like scholargooglecom disallow scraper and be very good at detect if that be what PRON be do  PRON can specify an user  agent for urllib  but PRON may be futile  even advanced framework may not be able to help PRON there   edit  PRON have make a blog post where PRON elaborate a little more   have PRON look at the stanford car dataset  PRON have about 16k image of 200 car  while PRON do not have the number of image PRON be look for  PRON do seem sufficient for build a classifier  see reference below   this blog post by justin chien provide a good overview of the approach for build the classifier on this datum use cnn  and this paper also provide an overview of a few different approach   there be many different publicly available dataset out there  and most come with a paper describe how the dataset be acquire  almost nobody take a camera and start take thousand of picture PRON  PRON may find some inspiration by look at those paper and adapt PRON method for find image   a very popular way be to download the image from flickr  this be a photo platform where user share PRON photo and add comment or tag  describe the content of the image   flickr also have an api to find and download image   a couple of test query show that there be thousand of photo available   query  no of matches    vw passat  57702  ford focus  187344  toyota corolla  81529  mitsubishi lancer  126242  however  this be not a clean high  quality dataset  PRON include old model  wrong tag  photo from the interior  and so on  still  PRON may be a good starting point to acquire huge number of image   dataset cleanup  maybe PRON can live with a couple of low  quality image  but PRON guess the good way be to clean the dataset up  there be a lot of different possible step  some may not be need in PRON case  and PRON may need other or additional step   remove non  car photo  PRON probably do not want photo from car interior  or photo that do not show the car at all  PRON could eg classify all PRON image with an imagenet classifier and discard all image that be not recognize as  car    use image retrieval algorithm  eg sift descriptor and match  to build a graph contain all image and PRON similarity  as describe in  1   discard all image that have very little similarity to the rest of the image  or at least review those image    manual labelling  this be the good way to ensure that PRON have a really high  quality dataset  have someone go through all image and make sure that PRON satisfy all condition PRON have and be label correctly   this be very very expensive  but will definitely give PRON the good result  if PRON do not really need to  do not do this   if PRON have to  PRON could rely on the mechanical turk or similar website   licensing  flickr s api description say that   the flickr api be available for non  commercial use by outside developer  commercial use be possible by prior arrangement   important note  all photo be property of PRON respective owner  all image on flickr have specific license condition  which PRON can also query through the api  a list of the available license on flickr be available on PRON website  PRON have to make sure PRON do not infringe the copyright of the respective owner  especially if PRON work be commercial  this complicate thing   reference   1   gordo  a  almazan  j  revaud  j   amp  larlus  d  2016   deep image retrieval  learn global representation for image search  arxiv  160401325  
__label__xgboost __label__feature-engineering PRON be read the material relate to xgboost  PRON seem that this method do not require any variable scaling since PRON be base on tree and this one can capture complex non  linearity pattern  interaction  and PRON can handle both numerical and categorical variable and PRON also seem that redundant variable do not affect this method too much   usually  in predictive modeling  PRON may do some selection among all the feature PRON have and PRON may also create some new feature from the set of feature PRON have  so select a subset of feature mean PRON think there be some redundancy in PRON set of feature  create some new feature from the current feature set mean PRON do some functional transformation on PRON current feature  then  both of these two point should be cover in xgboost  then  do PRON mean that to use xgboost  PRON only need to choose those tun parameter wisely  what be the value of do feature engineering use xgboost    what be the value of do feature engineering use xgboost   performance maybe    note PRON do not use xgboost  but another gradient boost library  though xgboost s performance probably also depend on the dimensionality of the datum in some way    PRON have a dataset where each item consist of 3 signal  each 6000 sample long  that be 18k feature  use these feature directly take age  day   so PRON do some manual feature engineering to reduce the number of feature to about 200  now train  include parameter tuning  be a matter of a few hour   for comparison  a short time ago PRON also start train convnet with the same datum and the whole 18k feature  no feature engineering   PRON reach the same accuracy as the gradient boosting model after only about 2 hour of training   this be probably the good answer to PRON question from the guy who use too much of xgboost and stack  httpblogkagglecom20170317outbrainclickpredictioncompetitionwinnersinterview2ndplaceteambrainafkdarraghmariosmathiasalexey  let PRON define first feature engineering   feature selection  feature extraction  add feature through domain expertise  xgboost do  1  for PRON   xgboost do not do  23  for PRON   so PRON still have to do feature engineer PRON  only a deep learning model could replace feature extraction for PRON    feature selection  xgboost do the feature selection up to a level  in PRON experience  PRON always do feature selection by a round of xgboost with parameter different than what PRON use for the final model  PRON typically use low number for row and feature sampling  and tree that be not deep and only keep the feature that enter to the model  then fine tune with another model  this prevent overfitt for PRON when the number of feature be very high   feature generation  xgboost  classification  booster  gbtree  use tree base method  this mean that the model would have hard time on pick relation such as ab  a  b and ab for feature a and b PRON usually add the interaction between feature by hand or select the right one with some heuristic  depend on the application  this can really boost the performance  
__label__neural-network __label__convnet PRON recently read fully convolutional networks for semantic segmentation by jonathan long  evan shelhamer  trevor darrell  PRON do not understand what  deconvolutional layer  do  how PRON work   the relevant part be  33  upsampling be backwards stride convolution  another way to connect coarse output to dense pixel  be interpolation  for instance  simple bilinear interpolation  compute each output  yij from the near four input by a  linear map that depend only on the relative position of the  input and output cell   in a sense  upsampl with factor  f be convolution with  a fractional input stride of 1f  so long as  f be integral  a  natural way to upsample be therefore backwards convolution   sometimes call deconvolution  with an output stride of   f such an operation be trivial to implement  since PRON simply  reverse the forward and backward pass of convolution   thus upsampling be perform in  network for end  to  end  learn by backpropagation from the pixelwise loss   note that the deconvolution filter in such a layer ne not  be fix  eg  to bilinear upsampling   but can be learn   a stack of deconvolution layer and activation function can  even learn a nonlinear upsampling   in PRON experiment  PRON find that in  network upsampl  be fast and effective for learn dense prediction  PRON good  segmentation architecture use these layer to learn to upsample  for refined prediction in section 42   PRON do not think PRON really understand how convolutional layer be train   what PRON think PRON have understand be that convolutional layer with a kernel size  k learn filter of size  k time k the output of a convolutional layer with kernel size  k  stride  s in mathbbn and  n filter be of dimension  fractextinput dims2  cdot n however  PRON do not know how the learning of convolutional layer work   PRON understand how simple mlp learn with gradient descent  if that help    so if PRON understanding of convolutional layer be correct  PRON have no clue how this can be reverse   could anybody please help PRON to understand deconvolutional layer   this tutorial by andrej karpathy do an excellent job of explain convolutional neural network   read this paper should give PRON a rough idea about deconv network   these slide be great for deconv network   PRON think one way to get a really basic level intuition behind convolution be that PRON be slide k filter that PRON can think of as k stencil over the input image and produce k activation each one represent a degree of match with a particular stencil  the inverse operation of that would be to take k activation and expand PRON into a preimage of the convolution operation  the intuitive explanation of the inverse operation be therefore  roughly  image reconstruction give the stencil  filter  and activation  the degree of the match for each stencil  and therefore at the basic intuitive level PRON want to blow up each activation by the stencil s mask and add PRON up   another way to approach understand deconv would be to examine the deconvolution layer implementation in caffe  see the follow relevant bit of code   deconvolutionlayerltdtypegtforwardgpu  convolutionlayerltdtypegtbackwardgpu  cudnnconvolutionlayerltdtypegtbackwardgpu  baseconvolutionlayerltdtypegtbackwardcpugemm  PRON can see that PRON be implement in caffe exactly as backprop for a regular forward convolutional layer  to PRON PRON be more obvious after i compare the implementation of backprop in cudnn conv layer vs convolutionlayerbackwardgpu implement use gemm   so if PRON work through how backpropagation be do for regular convolution PRON will understand what happen on a mechanical computation level  the way this computation work match the intuition describe in the first paragraph of this blurb   however  PRON do not know how the learning of convolutional layer work   PRON understand how simple mlp learn with gradient descent  if that help    to answer PRON other question inside PRON first question  there be two main difference between mlp backpropagation  fully connect layer  and convolutional net   1  the influence of weight be localize  so first figure out how to do backprop for  say a 3x3 filter convolve with a small 3x3 area of an input image  map to a single point in the result image   2  the weight of convolutional filter be share for spatial invariance  what this mean in practice be that in the forward pass the same 3x3 filter with the same weight be drag through the entire image with the same weight for forward computation to yield the output image  for that particular filter   what this mean for backprop be that the backprop gradient for each point in the source image be sum over the entire range that PRON drag that filter during the forward pass  note that there be also different gradient of loss wrt x  w and bias since dloss  dx need to be backpropagat  and dloss  dw be how PRON update the weight  w and bias be independent input in the computation dag  there be no prior input   so there be no need to do backpropagation on those    PRON notation here assume that convolution be y  xwb where    be the convolution operation   this video lecture explain deconvolution  upsampling   httpsyoutubebyjapdwxkj4t16m59s  deconvolution layer be a very unfortunate name and should rather be call a transpose convolutional layer   visually  for a transpose convolution with stride one and no padding  PRON just pad the original input  blue entry  with zero  white entry   figure 1    in case of stride two and padding  the transpose convolution would look like this  figure 2    PRON can find more  great  visualisation of convolutional arithmetic here   just find a great article from the theaon website on this topic  1    the need for transpose convolution generally arise from the desire to use a transformation go in the opposite direction of a normal convolution     to project feature map to a high  dimensional space       ie  map from a 4dimensional space to a 16dimensional space  while keep the connectivity pattern of the convolution   transposed convolution – also call fractionally stride convolution – work by swap the forward and backward pass of a convolution  one way to put PRON be to note that the kernel define a convolution  but whether PRON ’ a direct convolution or a transpose convolution be determine by how the forward and backward pass be compute   the transpose convolution operation can be think of as the gradient of some convolution with respect to PRON input  which be usually how transpose convolution be implement in practice   finally note that PRON be always possible to implement a transpose convolution with a direct convolution  the disadvantage be that PRON usually involve add many column and row of zero to the input  result in a much less efficient implementation   so in simplespeak  a  transpose convolution  be mathematical operation use matrix  just like convolution  but be more efficient than the normal convolution operation in the case when PRON want to go back from the convolved value to the original  opposite direction   this be why PRON be prefer in implementation to convolution when compute the opposite direction  ie to avoid many unnecessary 0 multiplication cause by the sparse matrix that result from pad the input    image gt  convolution gt  result  result gt  transpose convolution gt   originalish image   sometimes PRON save some value along the convolution path and reuse that information when  go back    result gt  transpose convolution gt  image  that be probably the reason why PRON be wrongly call a  deconvolution   however  PRON do have something to do with the matrix transpose of the convolution  ct   hence the more appropriate name  transpose convolution    so PRON make a lot of sense when consider compute cost  PRON would pay a lot more for amazon gpus if PRON would not use the transpose convolution   read and watch the animation here carefully   httpdeeplearningnetsoftwaretheanoversionsdevtutorialconvarithmetichtmlnozeropaddingunitstridestranspos  some other relevant reading   the transpose  or more generally  the hermitian or conjugate transpose  of a filter be simply the match filter3   this be find by time reverse the kernel and take the conjugate of all the values2    PRON be also new to this and would be grateful for any feedback or correction    1  httpdeeplearningnetsoftwaretheanoversionsdevtutorialconvarithmetichtml   2  httpdeeplearningnetsoftwaretheanoversionsdevtutorialconvarithmetichtmltransposedconvolutionarithmetic   3  httpsenwikipediaorgwikimatchedfilter  the follow paper discuss deconvolutional layer  both from the architectural and training point of view  deconvolutional network  step by step math explain how transpose convolution do 2x upsampl with 3x3 filter and stride of 2   the simple tensorflow snippet to validate the math   import tensorflow as tf  import numpy as np  def testconv2dtranspose      input batch shape   1  2  2  1  gt   batchsize  height  width  channel   2x2x1 image in batch of 1  x  tfconstantnparray       1    2       3    4        tffloat32    shape   3  3  1  1  gt   height  width  inputchannel  outputchannel   3x3x1 filter  f  tfconstantnparray       1      1      1         1      1      1         1      1      1        tffloat32   conv  tfnnconv2dtransposex  f  outputshape1  4  4  1   strides1  2  2  1   paddingsame    with tf  session   as session   result  sessionrunconv   assert  nparray       10    10     30    20       10    10     30    20       40    40    100    60       30    30     70    40        resultall    in addition to david dao s answer  PRON be also possible to think the other way around  instead of focus on which  low resolution  input pixel be use to produce a single output pixel  PRON can also focus on which individual input pixel contribute to which region of output pixel   this be do in this distill publication  include a series of very intuitive and interactive visualization  one advantage of think in this direction be that explain checkerboard artifact become easy   PRON could use pca for analogy   when use conv  the forward pass be to extract the coefficient of principle component from the input image  and the backward pass  that update the input  be to use  the gradient of  the coefficient to reconstruct a new input image  so that the new input image have pc coefficient that better match the desire coefficient   when use deconv  the forward pass and the backward pass be reverse  the forward pass try to reconstruct an image from pc coefficient  and the backward pass update the pc coefficient give  the gradient of  the image   the deconv forward pass do exactly the conv gradient computation give in this post   httpandrewgibianskycomblogmachinelearningconvolutionalneuralnetworks  that be why in the caffe implementation of deconv  refer to andrei pokrovsky s answer   the deconv forward pass call backwardcpugemm    and the backward pass call forwardcpugemm    
__label__text-mining __label__crawling for an upcoming project  PRON be mine textual post from an online forum  use scrapy  what be the good way to store this text datum  PRON be think of simply export PRON into a json file  but be there a good format  or do PRON not matter   in general  use a storage method that allow PRON to quickly query PRON  if PRON collection be huge  PRON may need something lucene  base  like elasticsearch  if PRON be a sql crack and PRON favorite db support PRON  a full  text index may do the trick  for small size like the 5000 document  even linux  locatedbgrep or osx  spotlight could be enough   the important point be to be able to quickly verify assumption about the content of PRON data  how many document contain x and y  do any document contain w but not v  etc   this will be useful at both the whole set level as well as for analyze PRON topic cluster  finally  a few gnu tool or sql mastery can also help PRON profile PRON document set more efficiently  n  gram count  rank  collocation  concordance  etc   edit  that mean  for the above reason and give PRON collection size  good old plain text  in a file system or a database  may be more efficient than any  fancy  format   suppose that a forum post have  on average  2000 character  which be more or less the equivalent of a page of text  than the total memory need be 10 mb if text be ascii  even if the text be unicode encode in an asian language PRON will take 40 mb   this be far too little for modern computer  so a simple text format be the good since PRON can be parse in the fast way  and load into ram all at once   PRON think one have to be very careful when store textual datum  if PRON be user comment then  for security concern PRON be good if PRON be encode in some format before storage   a protobuf object can then be define to resolve the encoding   depend on the query pattern  and accept latency in retrieval of the datum  db should be decide  just a recommendation  if the idea be to store comment over a period of time for each user  consider hbase or cassandra  PRON be optimize for time range query   recommend read  httpinfomaprcomrsmaprimagestimeseriesdatabasespdf  let PRON assume PRON intend to use python library to analyze the datum  since PRON be use scrapy to gather the datum   if this be true  then a factor to consider for storage would be compatibility with other python library   of course  plain text be compatible with anything   but eg pandas have a host of io tool that simplify read from certain format   if PRON intend to use scikit  learn for modeling  then pandas can still read the datum in for PRON  if PRON then cast PRON from a dataframe to a numpy array as an intermediate step   these tool allow PRON to read csv and json  but also hdf5  particularly  PRON would draw PRON attention to the experimental support for msgpack  which seem to be a binary version of json   binary mean here that the store file will be small and therefore faster to read and write   a somewhat similar alternative be bson  which have a python implementation   mdash  no pandas or numpy involve   consider these format only make sense if PRON intend to give at least some formatting to the store text  eg store the post title separately from the post content  or store all post in a thread in order  or store the timestamp  if PRON consider json at all  then PRON suppose this be what PRON intend   if PRON just intend to store the plain post content  then use plain text  
__label__machine-learning __label__neural-network __label__regression PRON be currently use a regression ann to determine the size of an object  in the range  1100   PRON get the output value  sometimes outside of the desire range   but PRON have no idea how confident the network be of the result   PRON be think  if PRON set up an ann with a 100 one  hot vector as the output would PRON be able to determine the most likely result and PRON confidence  PRON understanding be that a one  hot classification network give a number of output value  the sum of which equal 1  so in essence PRON be get the probability of each value and can work out some error bar   be PRON way off here  or be there a good way of determine the confidence of the result on a regression ann   PRON should mention that PRON be currently use python 35 and keras with a tensorflow backend   assume the output size of PRON model be an integer  ie  inmathbb z 0100  a vector  in mathbb r100 should definitely be the output to PRON ann  this be the most common approach to create output classification for neural network   the value of this output vector could be all over the place depend on how PRON weight be initialize  train so in order to interpret the output as a probability of belong to one of the give 100 class  and sum to 1 as PRON say  PRON need to feed the output through a soft max function  PRON can then take the output of this to get PRON size class prediction and train use the cross entropy   if PRON output be continuous the easy approach be just to have one output size  train on mse  and get PRON answer by round between 0 and 100  
__label__finance PRON help would be appreciate   PRON be try to calculate the net return of a series loan  at different stage of completion  PRON have categorise the loan into open loan  repay  amp  late loan  and closed loan  repay  amp  default    for the closed loans  PRON have put the loan into a cashflow and use irr methodology to approximate the net return   for the open loans  PRON have put these loan into a similar cashflow  assume PRON complete as schedule and do not default   default assumption  PRON be try to come up with a sensible default assumption to discount the open loan  base on the default performance of the previous year   try methodology     propose methodology enter image description here this methodology attempt to calculate the impact on net return of defaulted loan across all year  2010  2016   PRON then take the average and compare against the actual default and discount the difference from the overall net return  as the majority of loan be open on in the later year  particularly 2016  the impact of this discounting be great   calculate the  of closed  repaid defaulted  loan  in 2010 all  loan be close so 100   in 2016 only 7  of loan be close  use  this to calculate a weighting for average  hint  the loan in 2010  be more influencial in the average that 2016 because all loan be  closed   the overall net return be calculate by a summation of   a  of repaid loan x repay loan net return  b  of defaulted loan x  default loan net return  negative number   c  of late loan x late loan net return  d  of repay loan x repay loan net return  take the weighted average  use the figure calculate in 1  of the   of defaulted loan x default loan net return  this  single figure give an effective default rate on the overall net  return of the total portfolio   use this new estimate to discount the open loan by difference between the new calculated average and the actual default     PRON can not get PRON head around this calculation and whether PRON be way off  be good to get PRON thought  PRON have attach some datum to play around with   cheers  iain  
__label__data __label__training __label__text __label__classifier PRON be work on detect duplicate text document use a classifier  PRON be look for train datum  a corpus of text document and corresponding metadata which list out pair of duplicate document from the corpus   where can PRON find such datum   instead of train a classifier  detect duplicate may advantageously be do in a direct fashion  use a similarity metric like shingle signature or a hashing  checksum function    a text to representation function f should preserve formal  meta element that PRON already know to be identifier or partial  identifier  and PRON should also incorporate a compact representation of the content  like sparse excerpt  checksum  or any other hash key or canonize form  the comparison function g should then compare these text representation provide by f and define a similarity score    PRON would still need an evaluation corpus  so look for a corpus be still important  PRON do not know of any open one  but job  board and academic database contain lot of duplicate so PRON could start there  
__label__error-estimation __label__fourier-analysis if PRON take the fourier transform of datum x   sigma be there a standard approach to what the error in the output will be  would the good way be direct evaluation of the upper and low bound   
__label__classification __label__clustering __label__visualization __label__data-cleaning PRON be work with a database about internally displace person in colombia  all datum be absolute value  so PRON calculate the rate per 1000 people   PRON start to visualize all datum use qgis  PRON choose std dev  quantile and jenk method to determine the arrangement of value into different class   here be the example   however  PRON get a question in this proccess  what be the good arrangement of value when PRON have big gap between PRON   this be a list of value PRON be work with   value   290 161 154 133 126 126 118 112 112 103 102 102 101 100 96 96 92  87  87  86  85  84  84  80  79  79  76  73  71  70  70  69  65  60  59  58  57  57  56  55  54  53  53  53  53  52  51  51  50  50  50  50  49  49  49  49  49  48  47  47  47  46  45  44  44  44  44  43  42  42  41  41  40  40  40  40  40  39  39  39  38  38  38  38  37  37  37  37  37  37  37  36  36  35  35  35  35  35  34  34  34  32  32  32  32  32  31  31  31  31  31  31  31  31  31  31  30  30  30  30  30  30  30  30  29  29  29  29  29  29  29  29  28  28  28  28  28  27  27  27  27  27  26  26  26  26  26  26  26  26  26  26  25  25  25  25  25  24  24  24  24  24  23  23  23  23  23  23  23  23  23  23  22  22  22  22  22  22  21  21  21  21  21  21  21  21  21  21  21  20  20  20  20  20  20  20  19  19  19  19  19  19  19  19  19  19  19  19  19  18  18  18  18  18  18  18  18  18  18  17  17  17  17  17  17  17  17  17  17  17  17  17  16  16  16  16  16  16  16  16  16  16  16  16  16  16  16  15  15  15  15  15  15  15  15  15  15  15  15  15  15  14  14  14  14  14  14  14  14  14  14  14  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  13  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  12  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  10  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   any suggestion to understand good datum cluster be appreciate   thank   PRON believe that PRON should use logarithmic scale for good clustering result   take a look at the photo on the follow link   wikimedia  logarithmicscale  let PRON know if that help   one solution be bin the datum into group  for example  create three group   high    medium    low   the numerical split group can be handpick to emphasize the signal in the datum   another solution be to threshold the datum and not show datum above or below the threshold  
__label__python __label__machine-learning PRON be work with a high  dimensional dataset  and have find that PRON attempt at dimensionality  reduction hurt the accuracy of downstream classifier  suggest that PRON be effectively lose information in the course of feature  compression   furthermore  when PRON sort the feature by importance  per estimation via random forest  and then train neural network on different number of the most important feature  PRON find that the result classifi accuracy stop increase before PRON have exhaust all of the useful  seem feature   PRON suspect that simultaneous training on many thousand of feature be the problem  and PRON would like to simplify the problem by learn from the feature incrementally  that is   PRON would like to explore training a neural network add a few feature at a time  eg train use the top 100 feature  then add in the next 100  train again  use coef from previous round plus random coef for the new part of the input layer   rinse  repeat   note that this  incrementalness  be across feature  not sample  PRON do not affect whether training be incremental   online   with respect to sample   PRON gather this sort of thing have be try before with some success but PRON have not be able to find implementation in the python ml library PRON have check  nor can PRON find good explanation of the detail of successful algorithm   PRON have about 70 k example  divide over 6 balanced class  for most of PRON work PRON care only about distinction between a total of 4 class  so PRON merge two pair of the initial class and end up with a total of 4  PRON end up with unbalanced training set as a result  but since the two large class be the one whose classification be most important  PRON be fine with that for now   PRON have a total of about 28 k feature  but be find that PRON get good classifi performance when use no more than the top 4 k feature  pre  sort by importance via random forest    do PRON approach seem reasonable   be there accept publish detailed algorithm  eg  be PRON reasonable to add the feature as extension of the original input layer  or be another approach prefer    do any python lib  particularly any compatible with scikit  learn  implement such an algorithm   the opposite of what PRON describe be use more often  PRON start with many feature and then prune the neural network at the end   PRON do not believe any python neural network package do what PRON want   but PRON be very simple to implement   for example  use sklearn  PRON can just pass warmstart  true  so that PRON do not reinitialize weight every time PRON call fit    and then PRON can increase the number of node in PRON feature layer  or any layer PRON wish   from sklearn import dataset  from sklearnneuralnetwork import mlpclassifier  import numpy as np  x  y  datasetsloadiristrue   x   x  npmeanx  0npstdx  0    normalize  nhidden  10  m  mlpclassifiernhidden  maxiter1000  warmstart  truefitx     1   y    every 1000 iteration  throw in a new feature  for nfeature in range2  xshape11    mcoefs0   nprmcoefs0   nprandomrandn1  nhidden    mfitx     nfeature   y    in the end  PRON have a neural network train with all feature  printmscorex  y   
__label__matlab __label__plotting __label__octave a user have this nice graph of fx  on PRON profile and PRON want to learn how to do PRON in matlab or gnu octave   fx   xxx  x  1    00   center  colorx  y   arctan2x  fx    lumx  y   sinhypotx  fxalpha   but when PRON try in matlab PRON fail    gtgt  y xxxx1    gtgt  x  linspace02pi100     gtgt  plotx  y   the matlab code do not make a plot that resemble the nice visual graph  PRON understand that PRON must also accomplish the effect  but can PRON help PRON on the way  be there a program that be more or less suitable to make a plot like the above   use python and mpmath   import matplotlib  import mpmath  f  lambda z  z3z1  mpmathcplotf  points100000   easy peasy  see here for the documentation on mpmathcplot 
__label__data-mining PRON have an excel sheet with n column  these column contain info about the student  for admission PRON have the score of test score in multiple subject area  score from an interview  and score of a write test and comprehension test  there be a column which contain student s academic level  high  mhigh  middle  mlow  low   PRON want to compare the last column with the other variable and see whether there be common feature that pass student have in common   be there software for this  if this can be do with excel  how can PRON do PRON  do spss provide this kind of analysis   a good place to start would be to look at the spearman s rank correlation between PRON dependent variable  academic level  and PRON individual independent variable  other column   this should give PRON a basic indication of whether any of PRON column be correlate with academic level   this should be straightforward to implement in both excel and spss   if PRON want to go a step further PRON could set the problem up as one of multinomial logistic regression  this would allow PRON to build a model to directly attempt to predict academic level from PRON other variable  PRON be certain there be probably some way to do this in excel  but spss can definitely handle PRON   PRON can try some form of correlation or PRON can try a dimensionality reduction in a prediction model  in this case what be leave be the important feature for the prediction problem   see httpswwwknimeorgfilesknimeseventechniquesdatadimreductionpdf   rosaria 
__label__neural-network __label__deep-learning in a neural network  each neuron value be multiply by the weight of the connection  then  each neuron s input be the sum of all those value  on which PRON apply the activation function  sigmoid  relu  tanh  etc     why can not the input be a vector of all those value  so that PRON can decide to multiply PRON if PRON want  or apply another function from the vector space to the real  and then apply the activation function on that real value  any implementation PRON have see do not allow this  and if PRON want to do PRON  PRON seem PRON would have to implement PRON whole neural network architecture by PRON without the possibility of use theano  keras or other library  be there a reason that escape PRON of why this would not work  and if there be not  be there any library out there PRON could use that already do this without have to create PRON from scratch   this be the standard definition of matrix multiplication  sum those value PRON mean  PRON do not have to make PRON work like that  PRON can in fact multiply PRON if PRON like  simply do a series of scalar multiplication instead    input vector  x  tfplaceholderfloat32    1  30    so a 1x30 input vector    layer 1   wl1     for n in range50    wl1appendtf  variabletfrandomnormal1       l1     for n in range50    l1appendx  wl1n    l1out  l10   for n in range1  50    l1out   l1n   be this close to what PRON mean   more generally  PRON think most of what PRON want to achieve can be do with the reduceprod method in tensorflow  and theano have an analogous function  PRON think PRON be not do in practice because PRON do not form a ring over tensors   use a low  level library  such as theano or tensorflow  PRON be likely that PRON can construct new scheme for reduce tensor  maybe via some learnable weight vector etc   in tensorflow  PRON also get automate gradient calculation  so PRON should be able to still define a cost function and use existing optimiser  without need to analyse the new design PRON or re  write back propagation formulae   the universal approximation theorem essentially state that in order to have a network that could learn a specific function  PRON do not need anything more than one hidden layer use the standard matrix multiplying  plus non  linear activation function  so  invention and use of new architecture need some reason  the reason be usually that these addition improve the speed or scope of what can be learn  PRON generalise better from training datum  or PRON model something from a problem domain really well   no doubt there have be exploration of all sort of variation to the standard nn model over time  the one that have become popular have all prove PRON on some task or other  and often have associate paper demonstrate PRON usefulness   for example convolutional neural networks have prove PRON good at image  base task  the design of a 2dimensional cnn layer have a logical match to how pixel in an image relate to each other locally  define edge  texture etc  so the architecture in the model nicely match to some problem domain   if PRON can find a good match from PRON vector multiplication model to a specific problem domain  then that be an indication that PRON may be worth implement in order to test that theory  if PRON just want to explore other nn structure to see whether PRON work  PRON can still do so  but without a specific target PRON will be search for a problem to solve  unless by chance PRON stumble upon something generally useful that have be overlook before    to address the question in the title   why neural network model do not allow for multiplication of input   some low  level one do  eg tensorflow   however  this be not an architecture that have prove PRON useful  so author of high  level library  eg keras  have no reason to implement or support PRON  there be the possibility that this situation be an oversight  and the idea be generally useful  in which case  once someone can show this PRON will find PRON would get support across actively  develop library pretty quickly since PRON seem straightforward to implement  
__label__linear-algebra __label__statistics __label__sorting PRON apologize in advance if this be the wrong place to be ask this question   PRON have consider put PRON on cv first    PRON be study financial  sampp  dow jones  etc  datum  and would like to compute some quantile of an array of number  PRON have be read about quantile  so PRON know what PRON be  but do the computation of quantile always mean that PRON need to sort PRON datum first  or be there method where quantile can be compute without first sort PRON array of number   thank PRON   PRON do not need to sort PRON datum first  if PRON have access to additional temporary storage  in which case  PRON should use a selection algorithm  
__label__data-mining __label__k-means __label__pca PRON start with a dataset that contain many dimension for individual  each PRON would be a separate individual   and extract three feature  attributes column for each individual Idaho  PRON goal be to bucket these individual into two or three distinct group  base on these feature  to see if PRON can identify distinct separation between certain group   PRON be wonder if anyone have any recommendation  or algorithm  preferably in python  that would cluster these individual into distinct group   PRON do not have these individual classify  so PRON be an unsupervised clustering problem   PRON be think that k  means may be a good option  or something similar to pca which could reduce PRON dimensionality and provide insight into the feature that seem to separate the group of individual into distinct group   thank for look   note  the datum show below be artificially generate to illustrate PRON question   appendix  reference data   pattern  feature a  feature b  feature c  6218013800  9931367658  11089183433  13973950159  23051649009  26904342390  27835918340  28304563688  32978652850  43411336183  49957152709  51113998942  53622137707  62879303752  63627729098  71464006655  73134932515  83453385204  84948971323  86380300076  88173064996  sound like PRON be on the right track   there be many way to attack this problem   if PRON want to visualize the clustering  PRON would help to reduce the datum down to two component   this could be do via pca or manifold learning if PRON want to go non  linear  httpscikitlearnorgstablemodulesmanifoldhtml    in term of clustering  there be many different method to do this   here be some comparison of different method  httpscikitlearnorgstablemodulesclusteringhtml  clustering be the process by which PRON create group of similar item so that the difference between the item within group be minimise and the difference between the item in different group be maximise  therefore  PRON be on the right track  as the previous answer already state   PRON could start by something simple  like k  mean  and then keep improve PRON model depend on the result  another simple option be hierarchical clustering   however  there be several thing that PRON should take into account  for instance  PRON may be advisable to normalise  standardise the datum before cluster  so that the relative importance of all the feature be the same  additionally  some method  like for instance k  mean  require a priori knowledge of the number of group in PRON datum  therefore  if PRON do not have such knowledge  PRON will have to decide how to test PRON clustering result for several number of cluster and how will PRON choose the good model   pca be not a clustering algorithm  but a dimensionality reduction algorithm  if PRON datum have only three feature  PRON do not really think that PRON would need pca beforehand  clustering method base on distance metric do not perform well in case of high dimensional datum  however  some trick can still be use   but three feature be not bad in that regard  however  apply pca may be useful if PRON want to visualise the group and PRON datum  once again  since PRON only have three feature  just make a 3d plot may be enough   just start apply some method  explore PRON datum  and let the result help PRON decide what the next step be  
__label__image-classification __label__object-recognition PRON want to detect attribute of part of object  for example classify tire and chassi of car into different category  PRON somewhat build upon this question but with some additional challenge  PRON be wonder what pipeline of processing would be a good set up to solve this  PRON can think of very different approach and wonder how many step would be ideal  for example the follow set up seem reasonable but PRON could be overengineer   try to recognise the object PRON   bounding  box the object and crop the image to remove non object  relate element from the image  process the result image in an attempt to classify object part give the type of object  update  a side note towards complexity  recognise part of object also mean multiple object detection  each party on PRON   PRON wonder if this be better deal with in separate eg to reduce amount of predictable class or in a single one to benefit from learning of the other part   
__label__matlab __label__approximation-algorithms __label__b-spline PRON be look for a good matlab package  library for b  spline approximation  ideally  PRON would take knot  t1  ldot  tn  and data point  gt1ldot  gtn  and produce   vg   sumlimitsj1n bi k  t  gi       where the  bi k s be the b  spline basis of order  k with respect to the give knot  set   remark   really  PRON just need the cubic b  spline  and  simple  knot  ie  without repeat a note twice   PRON follow the definition in pp  160 of c de boor  a practical guide to splines  1978   PRON may use curve fitting toolbox which be provide by matlab   the function PRON need be spcol  
__label__data-formats the icd code scheme have go through many revision  httpsenwikipediaorgwikiinternationalstatisticalclassificationofdiseasesandrelatedhealthproblem  be there any compatibility between the different version   eg can icd9 code be directly use where icd10 code be expect    9 and 10 have different schema  for example  a displace transverse fracture of the right femur would be  82101rt in 9  s72321 in 10  10 be also more specific  in the example  9 do not tell PRON that the fracture be transverse and displace   since icd10 be basically a better organize and more precise superset of the datum represent by icd9  someone have probably create a mapping  however  PRON have not look into that   icd 10 be a vast generalization of icd 9  in that each procedure and diagnostic type now have many more code  in addition  there be new diagnostic and procedure in icd 10 that be not in icd 9  icd 9 be forwards compatible with icd 10  if PRON settle on the approximate icd10 code that be most similar for example here  if PRON try  icd 9  8154  total knee replacement   then PRON will see a list of approximation here  as PRON can see  the specificity of icd 10 be something PRON will have to compromise on   icd 10 will not in general be backwards compatible  except in case similar to the above knee replacement  where PRON would settle on something  one compromise here would be to use clinical classification software  ccs  grouping which collapse icd 9 and 10 code into multilevel group  for example   knee arthroscopy    so PRON can compare procedure and diagnostic by group 
__label__data-mining __label__feature-extraction __label__feature-engineering __label__anomaly-detection __label__outlier PRON want to combine the follow vector in a way that just the red point  number 7  become inconsistent with other point  become an outlier and become distant from other point  and other point become consistent with each other  please note that PRON have test mahalanobis distance and kullback  leibler divergence between two vector but PRON be not so good and detect   a13269  13354  13318  13282  134666 13460  136084 13526  13539  13510  13480  13479  134893   b003520099201570014310163401629010460165501635016420165801666015735   thank in advance   autoencoder solution  PRON could try an autoencoder  the autoencoder would take an input vector and PRON would try to recreate PRON as an output  so PRON take PRON input  and measure the distance between the input and the predict output variable  use PRON metric of choice  euclidean should work but try various   large distance can be think of as more abnormal  so PRON can stack rank PRON observation from weird to most normal   make sure that PRON be only train the autoencoder on normal datum though  this would of course assume that PRON have more than the 13 sample that PRON be look at  if not this probably be not go to work very well  just because of the small sample   kde solution  the idea be to use kernel density estimation to generate a non  parametric joint density of PRON data set  PRON then find what the probability of find a value that extreme would be  here be some code use python s sklearn package   from sklearnneighborskde import kerneldensity  import numpy as np  x  npmatrix13269  13354  13318  13282  134666  13460  136084  13526  13539  13510  13480  13479  13489300352  00992  01570  01431  01634  01629  01046  01655  01635  01642  01658  01666  015735     kde  kerneldensitykernelgaussian   bandwidth45fitxt   score  kdescoresamplesxt   prob  npexpscore   printprob  prob6    this code show that the observation in the low probability density area be observation 12 and 7  of course this would work better with a large sample  and PRON need to fuss with the bandwidth to calibrate PRON  but that this should do PRON   from the second graph PRON seem pretty easy to identify the outlier  PRON could probably just fit a simple polynomial  or some other function  and then flag all point that have a distance great than 2 standard deviation  or whatev seem appropriate  from the fit curve   import numpy as np  import matplotlibpyplot as plt  b  nparray00352009920157001431016340162901046   0165501635016420165801666015735    x  nparange13   p  nppoly1dnppolyfitxb4    y  px   pltplotxbro    pltplotx  yb    pltshow    PRON do not really agree with the idea of  want  a point to be an outlier and then massage the algorithm to make PRON so  PRON have 2 dimension and PRON either an outlier or PRON not   if one standardize the datum and then decipher the mahalanobis distance  point 6 be only one of two point that sit outside of a certain threshold  point 0 be the other point   beyond that there s not much PRON can do beyond some sort of nonlinear transformation which PRON know to be true due to some deterministic knowledge PRON have about a particular phenomenon   anyways  here be the two outlier version in case PRON be not standardize PRON datum first   from pandas import dataframe  readcsv  dfr  readcsvmachinelearn  ipythonnotebook  aboutliercsv    dfdfr  dfrmeandfrstd    import numpy as np  import matplotlibpyplot as plt  from sklearncovariance import empiricalcovariance  mincovdet  empcov  empiricalcovariancefitdf   fig  pltfigure    pltsubplotsadjusthspace1  wspace4  top95  bottom05    show datum set  subfig1  pltsubplot3  1  1   myplot  subfig1scatterdfadfb   subfig1setxlimsubfig1getxlim0   11    subfig1settitlemahalanobis distance     show contour of the distance function  xx  yy  npmeshgridnplinspacepltxlim0   pltxlim1   100    nplinspacepltylim0   pltylim1   100    zz  npcxxravel    yyravel     mahalempcov  empcovmahalanobiszz   mahalempcov  mahalempcovreshapexxshape   empcovcontour  subfig1contourxx  yy  npsqrtmahalempcov    cmap  pltcm  pubur   linestylesdash    subfig1legendempcovcontourcollections1    myplot      mle dist     locupper right   borderaxespad0   pltxtick      pltytick      hope this help  
__label__numerical-analysis __label__fortran PRON have an expression like the following  anyway to input latex       zmleft  lnleftfraczz1rightsumk1m fraczkk  right  for m integer  z complex  PRON seem if PRON code PRON the follow way  PRON give noticeable round  off error in case  z approx 1  z11z  zv1logzz1    zk10q  do k1m   zk  zkz1  zv1zv1zk  k  enddo  zv1zv1zk  any suggestion on a good way to code this expression  thank   add note   PRON have compare with gfortran build  in function log and mathematica nintegrate the follow two equivalent expression     lnleftfraczz1right  and    int01frac1z  x  dx    for  z to be complex  and find that these two can differ as big as  1015 at small  z  PRON push nintegrate to a precisiongoal of  1017   this difference seem to be responsible for the error of the expression PRON be seek help  PRON can upload a figure to support PRON claim make here if possible    do PRON think this be likely the case  which result should PRON trust  from the build  in function or from the numerically converge result  if use taylor series to help improve numerical convergence  how to treat the case with  zsim 1   which do not validate any series expansion   note add   besides the possibility that mathematica do not do a good enough job with log function  PRON could equally be that nintegrate do not reach the prescribed precision goal for some input z value  PRON be try to clarify this issue with mathematica and see how PRON respond   if PRON properly convert the expression to be calculate into math notation  then what PRON be deal with be subtract off the  m lead term  ignore a constant of zero  in the power series expansion of  ln1  z1  correspond to an expansion of  ln1z    this would because subtractive cancellation and thus amplification of round error relative to the magnitude of the difference     ln1  z1    sumk1infty fraczkk     the difference between this power series and PRON truncation at the  k  m index be then scale by  zm  but multiply by  zm for  z approx 1  should not have substantial impact on the relative error of the final result   PRON suggestion be to implement a summation of the tail of the power series  the portion leave after the lead term be subtract off     sumk  m1infty fraczm  kk     PRON will turn PRON hand to implement this for complex  z in a neighborhood of  1   but note that the question ask for something different  a computation valid for  z in a neighborhood of the unit circle   here lie a serious difficulty  in that the natural logarithm be a multi  value function in the complex plane  and any valid branch cut will intersect the unit circle   the principal branch of the natural logarithm be form to agree with the real natural logarithm on the positive real axis   the branch cut be then choose  as a matter of convention  to be the origin and the negative real axis  in the complex plane    therefore some clarification be need of where in the neighborhood of the unit circle  annulus  the computation be expect to be valid  which branch of the natural logarithm be PRON go to compare to for accuracy    PRON could try ask herbie  a piece of software that be design to find more accurate formula  httpherbieuwplseorg 
__label__time-series __label__keras __label__lstm PRON be give a time series datum for 6 year on daily basis  PRON want to train on 5 year and predict for next few day or month  PRON want to know how much ahead i can predict use lstm   also if PRON have to predict say 1 day ahead and also predict 2 day ahead  would PRON have to train two different model for both  if not  how to do that in kera by train only one model   also how many day should PRON look backin the past  ie how big should be PRON input vector  ie value of k in t  k  t2t1  while train for good result   
__label__python __label__nlp __label__gensim PRON some code that identify ngram of interest in a small bit of text  then search for the ngram in a large bit of text to provide a snippet of the term use in context   one ngram may be unordered numerical set   right now  if PRON can not find unordered numerical set  PRON start trim the string from the left to see if numerical set be available in the large text   what PRON would like to do before PRON trim from the left  be to see if the any of the word have an antonym  like order for unordered   this be because PRON ngram may be define in one way  but use in the opposite   be there a know list or other way of find out if there be a highly correlate antonym PRON may try for a set of word   for instance  PRON think PRON could write some regexps that look for prefix  like  un  or  dis  and see if the word that result from remove these prefix be a valid english word   this seem likely to be already solve  so before PRON try to create anything  PRON want to find out what may already exist   PRON be currently use python use gensim  nltk  and word2vec for the rest of PRON processing if PRON matter   PRON can start with small list of antonym like this one   maybe PRON can get a comprehensive list that will take care of most of the case PRON be interested in  for now  let assume that PRON do not have such a list an discuss algorithmic way to do PRON   as PRON write regard prefix like  un  and  dis   PRON can use rule base of morphology too  such rule be likely to have high precision  pair that obey the morphological rule will be antonym  but low recall  PRON will miss many pair    PRON should use these rule in order to increase PRON dataset of antonym   now  PRON should take a dataset of text as as wikipeida or the wall street journal even google ngram datum set  antonyms will tend to appear in the same context but not together   for example  people will write about  order list    unordered list  but not about  order unordered list    a proper association for this purpose be word that appear in distance small than x more than y time and the joint probability be high than the expect by z  PRON can use the dataset of antonym PRON have in order to find a proper value of the parameter above   once PRON calculate the association level among word  the antonyms will be associate to the same word but not associate to each other  note that here PRON be expect to have high recall but low precision since synonyms also tend to have such relation   while that option of list be of the low effort and the low benefit  and the association method take a considerable effort  there be another middle way   many dictionary  like wiktionary  have a section of antonym  PRON can scrape PRON and build such a list   such a list be of great use and PRON be quite surprised that there be not such a common resource  if PRON will build one and will be willing to share PRON  PRON will be very helpful   word2vec can be use to find a word that relate to another word in the same way an example pair do   fi  x be to happy  as bad be to good   PRON could use that to generate candidate of antonym on the fly   PRON may not be as accurate as precompil list  but PRON will cover almost every candidate  in fact word2vec could also help PRON find other  other than antonym  likely candidate for likely word give a context   that be an interesting problem   this accord to PRON be the most comprehensive wayif speed be not a problem  or PRON can just pull all these word and create a dictionary  database of PRON own    PRON can this   httpswordsapiv1pmashapecomwordsloveantonym  more info about this api at this link   however  PRON can restrict result to antonyms with this api   PRON can use request to make api call   import request  import simplejson  response  requestsgeturl   result  simplejsonloadsresponse   then search for the antonym from the result  if PRON be get a big list of antonym  use only the top n result for PRON searching   although  w2v give most commonly use word in the context of the keyword  PRON be hard to guess which be the antonym of the keyword  
__label__interpolation PRON want to do the interpolation on a interval  PRON can do PRON use with one chebyshev grid point  but i want to do PRON use two chebyshev grid on each half of this interval  PRON can do PRON separately for each segment but how can PRON do PRON in the same timei mean i do not want to use a for loop in PRON code for do PRON   follow be a illustration of the problem  PRON want to do PRON for b  
__label__machine-learning __label__r PRON have datum for each vehicle s lateral position over time and lane number as show in these 3 plot in the image and sample datum below    gt  a  frame  id  xcoord lane  1  452 2739400  3  2  453 2738331  3  3  454 2742999  3  4  455 2746512  3  5  456 2749066  3  the lateral position vary over time because a human driver do not have perfect control over vehicle s position  the lane change maneuver start when the lateral position change drastically and end when the variation become  normal  again  this can not be identify from the datum directly  PRON have to manually look at each vehicle s plot to determine the start and end point of lane change maneuver in order to estimate the duration of lane change  but PRON have thousand of vehicle in the data set  could PRON please direct PRON to any relevant image analysis machine learn algorithm that could be train to identify these point  PRON work in r thanks in advance   look like PRON could just look for a few second of high then noise derivative   just calculate the absolute value of the finite difference from each timestep to the last  or one of the former  and wait for a series of high value  that be when a lane change occur   try the changepoint package  PRON use PRON in a similar case   changepoint analysis be the statistical name for method that detect change between two  regime   a car stay in a lane be a line with gradient 0 at the midpoint of a lane  PRON can easily fit a statistical model to car drive in lane  a car change lane be drive along a line with a gradient that be not 0  the model have change  changepoint analysis  and the changepoint package  be exactly what PRON need to determine the point when a model change from y  a   straight and level  toy  abx   go up or down    a first derivative  on the surface  would do PRON   however  the datum PRON show have a great deal of noise in PRON  so PRON need a way to evaluate the first derivative in a somewhat noiseless way  or at least within a frequency domain that eliminate the jitter and preserve the major derivative change   wavelet analysis could achieve this for PRON  especially if PRON use the first derivative of a gaussian as PRON mother wavelet   r have some decent wavelet package  see rprojectorg for starter    if PRON do the wavelet transform at short scale  this will identify the location of the bit of jitter in the steering   if PRON do PRON at large scale  ie low frequency   PRON can likely find just the lane shift and not the little jitter   if PRON train the transform up with a reasonable datum set  PRON should be able to identify a scale or range of scale that correspond to lane change   but note that if PRON do not figure that out  this be something like on2   so try to narrow the scale range down a bit to save compute time  
__label__linear-solver __label__parallel-computing __label__petsc __label__dense-matrix for a bounday element method problem PRON require the solution of a system of linear equation with multiple right  hand side  though this be a dense system  PRON still want to do PRON via petsc in parallel  would the good way to do this be through something such as ksptype preonly pctype lu pcfactormatsolverpackage mump   furthermore and importantly  PRON refine PRON grid in a regular way by add  n1  point between the already present  n point  this lead to a new system of equation of size  nleftn1right  time nleftn1right in  nleftn1right unknown   the upper  left matrix  a11 in this new system be identical to the unrefined matrix  multiply by 05   the other three block  a12   a21 and  a22 be new     beginbmatrixa11ampa12a21ampa22endbmatrix  the question be now whether knowledge about the unrefined matrix  a11 can be use to speed up calculation of the refined system   PRON be think  for example  about use the lu decomposition of  a11 to calculate the lu decomposition of the entire matrix  a use the well  know formula s     leftbeginaligned   l21  u11   amp a21    l11  u12   amp a12    l22  u22   amp a22   l21  u12   endalignedright      where  l21 and  u12t be  n1time n full matrix   l11   l22 be lower triangular  ntime n and  n1 time n1  matrix and  u11   u22 be upper triangular with the same size   be there any way to do this in petsc   or be there a good thing PRON can do   for example  due to the fact that  a21   a12 and  a22 result from mesh refinement of the original system  PRON all look a lot like  a11  definin    v  leftunderbraceleftbeginarraycccc   05  amp  05  amp  0  amp  cdot  amp  0  amp  0   0  amp  05  amp  05  amp  cdot  amp  0  amp  0  vdot  amp  vdot  amp  ddot  amp  ddot  amp  vdot  amp  vdot   0  amp  0  amp  0  amp  cdot  amp  05  amp  05  endarrayrightnright  n1     an  n1 time n matrix  PRON can be see that   a21  approx v a11    ie an average of the row of  a11   a21  approx a11  vt   ie an average of the column of  a11   a22  approx v a11  vt   ie an average of the row and column of  a11  this would therefore mean that a good approximate for  a would be    a approx leftbeginarrayc  1n  v endarrayright  a11  leftbeginarrayc  1n  v endarrayrightt     from which PRON may be possible to construct some preconditioner easily use the lu decomposition from  a11      leftbeginaligned   l21   ampapprox v l11    u12   ampapprox u11  vt  endalignedright       from the equation above  however  this imply that  l22  approx 0  and  u22approx 0   which PRON do not really understand well as PRON lead to non  invertible approximation for  u and  l     l approx leftbeginarraycc  l11   amp  0  v l11   amp  0 endarrayright   textand   u approx leftbeginarraycc  u11   amp  u11  vt  0  amp  0 endarrayright     how would this be use as a preconditioner   
__label__machine-learning __label__computer-vision __label__audio-recognition __label__natural-language-process __label__ai PRON be interested in work on challenging ai problem  and after read this article  httpsdeepmindcomblogdeepmindandblizzardopenstarcraftiiairesearchenvironment  by deepmind and blizzard  PRON think that develop a robust ai capable of learn to play starcraft 2 with superhuman level of performance  without prior knowledge or human hard  cod heuristic  would imply a huge breakthrough in ai research   sure PRON know this be an extremely challenging problem  and by no mean PRON pretend to be the one solve PRON  but PRON think PRON be a challenge worth take on nonetheless because the complexity of the decision make require be much close to the real world and so this force PRON to come up with much more robust  generalizable ai algorithm that could potentially be apply to other domain   for instance  an ai that play starcraft 2 would have to be able to watch the screen  identify object  position  identify unit move and PRON trajectory  update PRON current knowledge of the world  make prediction  make decision  have short term and long term goal  listen to sound  because the game include sound   understand natural language  to read and understand text description appear in the screen as well   PRON should probably be endow also with some sort of attention mechanism to be able to pay attention to certain region of interest of the screen  etc  so PRON become obvious that at least one would need to know about computer vision  object recognition  knowledge bases  short term  long term planning  audio recognition  natural language processing  visual attention models  etc  and obviously PRON would not be enough to just study each area independently  PRON would also be necessary to come up with way to integrate everything into a single system   so  do anybody know good resource with content relevant to this problem  PRON would appreciate any suggestion of paper  book  blog  whatev useful resource out there  ideally state  of  the  art  which would be helpful for somebody interested in this problem   thank in advance   PRON pose an interesting question  the problem that PRON see be that even if PRON develop all the item PRON mention  and PRON be not sure PRON need all of PRON  PRON would not have the computing power need for this   PRON see  most of these item be base on reinforcement learning  which mean that the model be give a relatively small set of datum  roughly speak  the rule of the game  and then be set off to play million  amp  million of game  read the whitepaper that be write by google on beat go  PRON basically admit that there whole point be to write something very  very simple that could learn on PRON be own and then just set PRON free on google s massive power  PRON be nothing for PRON to say   let PRON have this robot play 10 million game   so the level of sophistication from PRON initial model be pretty low  PRON just need to make PRON an exceptional learner   so PRON make an exceptional learner of starcraft  then what  how be PRON go to have the power to make PRON algorithm to play million of game   this answer be base on PRON opinion and other   PRON may not be the answer PRON expect   the problem PRON want to solve be probably in the field of artificial general intelligence  the problem require agi to solve be informally know as ai  complete   ai  complete problem be hypothesise to include computer vision  natural language understanding  and deal with unexpected circumstance while solve any real world problem   currently  PRON be not ready to solve these problem  and if gartner be right  the hype will not come to peak in 10 year   this be the gartner hype curve in 2017  and agi be still in PRON early phase   the problem be  at the moment  PRON state  of  the  art ai model  deep learning  do not have the ability to explain PRON decision or reason about PRON behavior  which make learn highly inefficient   in addition  the reinforcement learning framework be way too simple and PRON ignore the complexity of human environment  PRON may find this answer useful   geoffrey hinton  one of the leader of the field  say in an interview that ai need to start over    PRON view be throw PRON all away and start again     PRON do not think PRON be how the brain work  PRON clearly do not need all the label data    that be PRON  if one day ai can learn how to play starcraft 2  PRON probably learn in a very different way from what PRON learn today  
__label__machine-learning __label__algorithm __label__self-learning PRON want an algorithm  predictive machine learning  mostly  to identify pattern in PRON csv file without the user specify any condition  what can PRON use   PRON could try 3 different approach   first PRON must classify each chunk  of the csv file and label PRON in base  of what situation be in that case  like  1 optimal situation 2 critical    and  then PRON can use the machine  learn algorithm PRON like most   cluster PRON datum with an algorithm  like som or k  means and then PRON  simply classify the class PRON will  get   use some unserpervised learning  approach    identify pattern in PRON csv file without user specify any condition  PRON want to do unsupervised learning here  the wikipedia definition of the same be   unsupervised machine learning be the machine learn task of  infer a function to describe hidden structure from  unlabeled   datum  a classification or categorization be not include in the  observation    as PRON be unclear about the  kind ofdata PRON csv have  PRON shall recommend PRON to go through the list of unsupervised learning algorithm here and use the one which would fit PRON need    if PRON be start out  then PRON would recommend start with learn the k  mean clustering algorithm   as other have already point out  what PRON be basically look for be unsupervised learning   there be a lot of usl technique around  but PRON be not sure PRON will find one that do exactly what PRON want with no user input at all   still  if PRON skim the literature on these approach  PRON may well find something useful   one option be dbscan  a very popular clustering algorithm that do not require the user to input an initial target number of cluster  something that  most cluster algorithm do require    but even then  PRON still have to give the algorithm value for epsilon  a distance use in calculate the cluster  and minpt  the minimum number of point require to constitute a  dense  region    PRON may also look at self  organize maps  an approach to unsupervised learning for neural network   some other search term that may lead PRON in a useful direction include  datum mining  and  knowledge discovery in database   kdd   
__label__neural-network __label__r the goal be to run poisson regression for neural network  multi  layer perceptron  in r  PRON be currently use the neuralnet package in r  PRON have read be there an r package which use neural network to explicitly model count datum  and httpwwwmathstatdalcahguneural20comput20amp20applicpdf  PRON have figure out that PRON should change the error function to errfct  functionx  y   xylogx     for poisson    code   fit  lt neuralnetnclaim  ageph  gender  tarregion  weight  agecar  ptw   datum  df   hide  1   errfct  functionx  y   xylogx      actfct   logistic     PRON know that actfct   logistic  specify the sigmoid function in the hidden layer  but how can PRON change to the exponential function in the output layer   
__label__bayesian-networks PRON be serach for a  while  an example  on how to use  pymc  pymc3 to do  classification task  but have not  find  an concludent  example  regard  on how to do  the  predicton on a  new datum point   PRON be currious if some  could  give  PRON some reference   pymc will not provide PRON pretty sklearn  style predict method for this case  however PRON can do PRON on PRON own  the idea be simple enough  PRON should draw coefficient for the classifier use pymc  and after PRON use PRON for the classifier PRON manually   PRON can see a very basic example at this blogpost or more complicated case at pymc3 documentation  
__label__deep-learning __label__keras __label__dropout PRON input be a 200 dim vector  which be generate by mean of the word2vector of all word of a article  PRON output be a 50 dim vector  which be generate by the lda result of a article PRON want to use mse as the loss function  but the value of the loss always be 0 PRON code as follow   model  sequential    modeladddensecolsfootsize  400init   glorotuniform     modeladdactivationrelu     modeladddropout02    modeladddense400  400init   glorotuniform     modeladdactivationrelu     modeladddropout02    modeladddense400  50init   glorotuniform     modeladdactivationsoftmax     modelcompilelossmse   optimizerrmsprop    and regardless of the number of epoch  the loss be always 0  first  be PRON output a one  hot vector of predict class  ie  class one be  1  0  0    and class two be  0  1  0  0      if so  then use softmax activation at the output layer be acceptable and PRON be do a classification problem  if PRON be do a classification problem  one  hot output  PRON can not use mse as the loss  use categorical cross  entropy   softmax scale the output so that the number give be a predict probability of a certain class  wikipedia here  httpsenwikipediaorgwikisoftmaxfunction  if PRON be expect the output vector to be real number then PRON need to use linear activation on PRON output neuron  
__label__dropout when apply dropout mask  why be PRON acceptable to divide the result state by the percentage of survive neuron   PRON understand that PRON be to prevent signal from die out  but PRON have do the test  and find that PRON disproportionally magnify the result state   assume the original state be   01  01  02  50  and PRON mask be   0  0  1  1  with 50  of neuron that survive    so  the original length be    beginalignsqrt01time 01  01time 01   02time 02  5times 5    amp sqrt2506   approx 5006endalign  as for the masked vector  PRON length be   sqrt02time 02  5times 5   sqrt2504  approx 5004  apply the compensation give  5004  05  10008  this seem incorrect  PRON compensation just blow up the state vector  perhaps PRON should be compensate differently  more carefully  PRON think PRON would even get bad if PRON mask the individual weight  like dropconnect do    in PRON actual test  the state  vector of  192  element have length of   0885  and the masked vector  with compensation have length of  1305   PRON example be cherry  pick  PRON mask out small number and keep a large one   but dropout be apply randomly  each of the follow six mask  and of the correspond value for the vector length  be equally likely to appear      beginalign     amp1  1  0  0    ampsqrt01  2  01  2   ampapprox 01414   amp1  0  1  0    ampsqrt01  2  02  2   ampapprox 02236   amp1  0  0  1    ampsqrt01  2  5  2   ampapprox 50010   amp0  1  1  0    ampsqrt01  2  02  2   ampapprox 02236   amp0  1  0  1    ampsqrt01  2  5  2   ampapprox 50010   amp0  0  1  1    ampsqrt02  2  5  2   ampapprox 50040  endalign       the average vector length be     frac16  01414  02236  50010  02236  50010  50040   25991      which be roughly half of the original vector length  5006 so PRON make sense to divide PRON by the dropout rate of  50 
__label__computational-biology PRON want to compute  delta f  f  as explain in this link  reference 1   but PRON be not able to do integration as PRON have distinct value and no function to generate the continuous time base value   PRON have a  1times n time series datum where  n be the total number of neuron response for one neuron  one for each millisecond  how do PRON calculate  delta f  f for this single neuron  if PRON do   fi1   fifi  PRON give PRON very small value   reference  jia  hongbo  et al   in vivo two  photon imaging of sensory  evoke dendritic calcium signal in cortical neuron   nature protocol 61  2011   28  35  doi101038nprot2010169  
__label__multigrid PRON be try to solve a poisson equation in structured grid with geometric multigrid method   however  when coarsen the fine grid  PRON simply double the grid space at each successive level   that mean that the number of point on the fine grid must be a power of 2  this be an undesirable constraint when do some numerical simulation   be there any strategy to avoid this constraint  ie  to use grid whose size be not power of 2   thank   all PRON need be a hierarchy of mesh that reduce sufficiently fast  PRON can use a 31 coarsen ratio  or PRON could use unstructured mesh  etc  there be no such restriction as the one PRON cite  even though PRON be common to implement the multigrid method with a 21 coarsen ratio   actually there be no restriction that the number of interval in the fine grid should have to be a power of 2  this number can be either odd or even  no big deal  take 1dimension problem as an example  suppose the number of interval in the fine grid be 7thus the number of node be 8  then PRON can use the follow restriction operator  column 1 through 7  05  025  0  0  0  0  0  0  025  05  025  0  0  0  0  0  0  025  05  025  0  0  0  0  0  0  025  05  column 8  0  0  0  025  if the number of interval in the fine grid be 8thus the number of node be 9   then the restriction operator should be  column 1 through 7  05  025  0  0  0  0  0  0  025  05  025  0  0  0  0  0  0  025  05  025  0  0  0  0  0  0  025  05  0  0  0  0  0  0  0  columns 8 through 9  0  0  0  0  0  0  025  0  025  05  so PRON see  whether the number of interval in the fine grid be odd or even  the multigrid technique would work well  
__label__fluid-dynamics __label__simulation __label__computational-physics __label__mesh __label__adaptive-mesh-refinement PRON be use adaptive mesh refinement to solve one dimensional inviscid burgers equation  however PRON be face some difficulty to handle grid interface which be not uniform  coarse  fine grid interface   there have to be conservation of variable at the grid interface which PRON be not able to incorporate in PRON code   if anyone have any idea about PRON or have do the similar work base preferably on finite difference scheme  PRON could be a great help to PRON   reference  httpepubssiamorgdoiabs1011370724063  
__label__software __label__multiphysics __label__particle PRON have a problem for which PRON be try to determine the most suitable software to use  here be a brief description of the problem   PRON have a mixture of inertial particle a and b with different property  PRON want to inject this mixture into a channel fill with fluid  and observe the transport property  PRON expect inter  particle collision and two  way coupling between fluid and particle to be important to the physics PRON want to capture  PRON would also like to use non  newtonian fluid   PRON be look for suggestion for commercial or open  source software that can tackle this problem   do PRON wish to model the two  way couple solid  fluid flow or PRON wish to carry out piv or track on particle image   if PRON wish to model the two  way couple solid  fluid flow  there be numerous type of model be that valid  most notably euler  euler approach or euler  lagrange  cfd  dem unresolved or resolve  approach  euler  euler approach be available in numerous open source  openfoam  or commercial  fluent  star  cmm  software   for cfd  dem model PRON can look at the follow open source project   cfdem project   httpswwwcfdemcom   i be a developer of PRON during PRON thesis  some commercial software be also incredibly powerful for these type of cfd  dem problem   fluent  edem  fluent  rocky 4  star ccm 
__label__finite-volume __label__oscillations PRON be currently solve pde use the finite volume method  the surface integral of the equation that PRON be solve involve compute face gradient  the current algorithm that PRON use to compute face gradient be    nablaphifacefrac12nablaphil  nablaphir   boldsymbolhatdfrac12nablaphil  nablaphircdotboldsymbolhatdphilphirfracboldsymbolhatdboldsymbold  the lhs be the face gradient of some quantity  phi  nablaphil and  nablaphir be the cell gradient at the left and right neighboring cell of the face  respectively   boldsymbolhatd and  boldsymbold be the unit distance and distance vector between the left and right cell center  respectively  the rhs consist of 3 term  the latter 2 term be the  odd  even decouple  correction term that be apply to avoid numerical oscillation that would otherwise arise if the computation of the face gradient be a simple averaging  ie  just the first term on the rhs    PRON have some question regard odd  even decoupling    1  be odd  even decouple also necessary for compute non  gradient value at the face  for example  if instead of compute the temperature gradient at the face  PRON instead want the temperature at that face   2   be there a variant of this equation for non  gradient term  for example  what if PRON just want to compute a face value  this equation can almost be duplicate for a plain  phi  non  gradient  term if PRON be not for the 3rd term on the rhs   
__label__python __label__pandas __label__performance PRON have to filter a pandas data frame by match a complex regular expression on a text index   the data frame be multi level index  and contain more than 2 million record   the way PRON be do be   identifier  selfdataindexgetlevelvaluesidentifier    filt    anarrayofnpboolwiththesamelengthasmydata  pattern    a complex regular expression  as a string  filt  filt  amp  nparrayidentifiersstrcontainspatpattern  case  false  regex  true   dtype  npbool     other filtering  unfortunately  the line begin by filt  filt  amp  be very slow   PRON be wonder if PRON have some idea to make PRON faster  PRON guess that be because of the identifiersstrcontain  thank a lot   edit   thank emre  PRON be not allow to share those datum  but the code below demonstrate the problem   step 0  00000013527  step 1  00000010127  step 2  00004468114  step 3  00002109594  step 4  00000027437  in fact  PRON feeling be that PRON apply the regular expression on all the value of the identifier  while PRON would expect that the filter apply on the possible value of the index  lose of value be reuse many time    import panda as pd  import numpy as np  import datetime  n  2000000  ndvc  10000  def getdata     identifier  nprandomchoicenparray     need    need  foo    need  bar    need  foo  bar    foo  need    bar  need     not  need    not  need  foo    not  need  bar    not  need  foo  bar    foo  need  not    bar  need  not     need ign    need  foo ign    need  bar ign    need  foo  bar ign    foo  need ign    bar  need ign     ign need    need  ign foo    need  ign bar    need  foo  ign bar       n   device  nprandomchoicenparangendvc    timestamp  nprandomchoicepddaterange112016 000000   periods60  60  24  freq s    n   x  nprandomrandn   y  nprandomrandn   datum  pd  dataframeidentifier   identifier   device   device   timestamp   timestamp   x   x   y   y    datasetindexdevice    identifier    timestamp    drop  true  inplace  true   return datum  def filterdatadata     PRON know those regular expression be not perfect for the example    but PRON mimic the real expression PRON have  rexpplus   ssneedssss   rexpminus       snotigns    tic  datetimedatetimenow    identifier  dataindexgetlevelvaluesidentifi    print step 0   s   strdatetimedatetimenow    tic    tic  datetimedatetimenow    filt  nprepeatnpfalse  datashape0    print step 1   s   strdatetimedatetimenow    tic    tic  datetimedatetimenow    filt  filt  nparrayidentifiersstrcontainspatrexpplus  case  false  regex  true   dtype  npbool   print step 2   s   strdatetimedatetimenow    tic    tic  datetimedatetimenow    filt  filt  amp   nparrayidentifiersstrcontainspat  rexpminus  case  false  regex  true   dtype  npbool    print step 3   s   strdatetimedatetimenow    tic    tic  datetimedatetimenow    datum  datalocfilt    print step 4   s   strdatetimedatetimenow    tic    return datum  if   name        main      filterdatagetdata     PRON have find a solution  instead of filter through all the value  PRON compute a hash table  in fact a dataframe  of the possible value and filter on those value   step 0  00000014251  step 1  00000010097  init fast filter  00000353645  step 2  00000027462  step 3  00000002550  step 4  00000027452  globally  00000435839  PRON run faster because there be lot of duplicated value   here be a class which be do that stuff  PRON work for index and normal column   import panda as pd  import numpy as np  class dataframefastfiltertextobject       this class allow fast filtering of text column or index of a data frame  instead of filter directy the value  PRON compute a hash of the value  and apply the filtering to PRON   then  PRON filter the value base on the hash mapping   this be fast than direct filtering if there be lot of duplicate value   please note that miss value will be replace by   and every value will  be convert to string   input datum will not be change  work on a copy        def   initself  datum  column  isindex       constructor   data  pandas data frame   column  name of the column  the column can be an index   isindex  true if the column be an index      selfdata  datacopy     work with a copy because PRON will make some change  selfcolumn  column  selfisindex  isindex  selfhashedvalue  none  selfinithashanddata    def  inithashanddataself       initialize the hash and transform the datum  1  datum  a add an order column  b create the column from the index if need   c rename the column as  value  and  d drop all the other column  e fill na with    f convert to string  2  hash  data frame of  hashid    hashvalue   3  datum  a sort by initial order  b delete order      selfdataorder    nparangeselfdatashape0    if selfisindex   selfdataselfcolumn   selfdataindexgetlevelvaluesselfcolumn   selfdata  selfdataloc     selfcolumn   order     selfdatarenamecolumnsselfcolumn   value    inplace  true   selfdatavalue    selfdatavaluefillna     inplace  false   selfdatavalue    selfdatavalueastypestr   selfhash  pd  dataframevalue   pd  seriesselfdatavalueunique       selfhashhashid    selfhashindex  selfdata  pdmerge   leave  selfdata   right  selfhash   onvalue    howinner      selfdatasortvaluesbyorder   inplace  true   del selfdataorder    def getfilterself   arg    kwarg       return the filter as a boolean array  PRON do not apply PRON    args  positional argument to pass to pd  seriesstrcontain   kwargs  name argument to pass to pd  seriesstrcontain      assert  na  not kwargskey     useless in this context because all the nan value be already replace by empty string  h  selfhashcopy    h  hlochvaluestrcontainsargs    kwarg     copy    d  selfdatacopy    dkeep    dhashidisinhhashid     return nparray   dkeep     dtype  npbool   copy    and the example code be adapt as follow     tic  datetimedatetimenow    fastfilter  dataframefastfiltertextdata   identifi    print init fast filter   s   strdatetimedatetimenow    tic    tic  datetimedatetimenow    filt  filt  fastfiltergetfilterrexpplus   print step 2   s   strdatetimedatetimenow    tic    tic  datetimedatetimenow    filt  filt  amp   fastfiltergetfilterrexpminus    print step 3   s   strdatetimedatetimenow    tic     
__label__machine-learning __label__neural-network __label__classification __label__tensorflow __label__cost-function PRON be train a neural network that  for each of six class  try to predict the probability that a sample belong to PRON  after that  PRON want to use these probability as fraction of the sample belong to that class  PRON network give softmax output and be train use cross  entropy cost  in fact linear output which be then transform to softmax by tfnnsoftmaxcrossentropywithlogit   be this the right cost function to use when PRON want to train the network in get all 6 probability correct instead of just classify each sample as one of the 6 class  PRON start hesitate because the tensorflow documentation say about tfnnsoftmaxcrossentropywithlogit   measure the probability error in discrete classification task in which the class be mutually exclusive  each entry be in exactly one class    update  even though PRON sound strange  the cross  entropy cost function seem to work best here  in PRON understanding this be because the cross  entropy for discrete probability distribution  wikipedia  be   hp  q   sumx px  log qx  this function be minimal when  px   qx for all  x this explain why minimize the cross  entropy error force the output distribution  q to the target distribution  p  even though PRON be abuse tfnnsoftmaxcrossentropywithlogit by instead of feed PRON label PRON feed PRON a discrete probability distribution   why PRON work better than mse probably be because the magnitude of the 6 class probability be very different  class 1 may have a probability of 80  while class 2 have a probability of 05   so mse pay more attention to get the 80  class right   do this mean that ce be the good option still  or be there a way to scale the output or weigh PRON such that the network pay attention to get each class correct   cross entropy be indeed appropriate for multiclass classification   when the tensorflow documentation state about crossentropy that    measure the probability error in discrete classification task in which the class be mutually exclusive  each entry be in exactly one class     PRON be refer to the fact that  at a conceptual level  class be not expect to overlap  an example of non  overlap class be  cat  and  dog   an example of overlap class be  elephant  and  african elephant   this type of problem be call multilabel classification  because instead of class  PRON have label and each individual can be assign many label   update  with the new information about the problem  PRON can tell that the problem be face be not a classification one  as the desire output be specific probability value  this imply that the problem be a regression one  as the output be probability  PRON be appropriate to use softmax to ensure PRON add up to 1  as PRON be regression  PRON would be appropriate to use mse as loss function  
__label__fluid-dynamics __label__stability __label__advection __label__navier-stokes when supg  pspg stabilization be add to momentum equation of flow problem  be need stabilization for energy equation also  PRON would guess that when stabilization for velocity work fine so one get velocity without spurious wiggle and assume that thermal diffusivity be large enough then answer be no  how be these two condition quantify in the language of dimensionless quantite   look at the cell péclet number    mathrmpeh  h v  k  where  h be mesh size   v be the magnitude of velocity  and  k be diffusivity   PRON be analogous to cell reynolds number for the momentum equation and be small when  thermal diffusivity be large compare to advection    PRON be common common in macro  scale fluid dynamic that thermal diffusivity  k be very small   if the cell péclet be much large than 1  PRON need stabilization  
__label__fortran PRON would like to write the most optimize program for array manipulation in fortran  eg matrix multiplication of large matrix  square large matrix  etc    so  use the fortran compiler on the market today  eg gfortran   how do f77 measure up against f9095  or f03   be there a particular compiler PRON should use to optimize speed with a certain version of fortran   furthermore  be there a difference in how PRON code these fortran program to optimize speed that be base distinctly in the version of fortran use  or be this simply change the extension name  eg f90 to f70  and the program will run faster   there be almost no good reason to write PRON own dense matrix manipulation routine until PRON have compare to fast library  mkl  flame  magma  etc   write these library be challenge work that depend highly on the target architecture and require deep concept than naive nest do loop  there be a whole literature on the subject that PRON should engage with  and PRON should note that basically none of the best  perform library in this space be write in fortran  of any flavor  these day  most of PRON be c  c plus target intrinsic or assembly code  compilers just be not magic enough to achieve the full performance of any chip  
__label__profiling PRON know this question be somewhat familiar to PRON previous question find here  however PRON be now come at PRON from a slightly different and more general angle   let PRON say PRON have a code whose runtime be in the day to week range   for one reason or another  the code can not be change to improve performance   PRON goal be to purchase a computer to run this specific code as fast as possible   PRON be currently parallel be some regard  but must be share memory   currently PRON have two computer on which to run PRON   PRON both run the same linux distribution and both run on intel  xeon processor  however the cpu model be different as be the memory speed  and other hardware value  the computer have about 5 year of age difference between PRON    on both computer PRON can easily change the number of core utilize for run the code  and the new of the two support hyperthread   PRON question be  be there a way to profile run of the code on one or both computer that would allow PRON to find where the hardware bottleneck be  and thereby tune a new computer purchase for optimal performance with this code   PRON assume PRON be talk about server  class part here  client part may be a bit different  but most of the logic hold   find a test case that run with one thread in 10 minute that be representative of PRON long problem in that PRON exercise the important part of PRON code in similar ratio  PRON may have to scale down the problem size in order to scale down the initialization time  etc   since PRON only have intel xeons  try turn on and off hyperthread and compare the run time of a version of the code that use the maximum thread count in both case  if PRON code be modestly faster with hyperthread turn on  then PRON should borrow time on an amd bulldozer  base machine to test to see if PRON be share fpu be advantageous for PRON code  PRON be not exactly the same  but if two hyperthread can successfully share the xeon fpu  then PRON may have a chance at some decent performance out of the share fpu on an amd machine   use linux or the bios s ability to set a sequence of fix  but reduce cpu clock rate for run of PRON code  eg 27ghz  26ghz     if the performance scale strongly with clock rate  without plateau at the high end  buy the fast clock rate cpu PRON can afford  if the performance plateaus after some clock speed  then consider not buy the top bin cpu  but back off and buy more or fast memory  if PRON at all can  benchmark PRON code on the actual hardware PRON want to buy  this may be hard if PRON only want to buy one node  but ask around  someone PRON know may have one PRON can borrow   consider PRON computational plan  if the case PRON need to run be finite and enumerate  calculate the cost of rent some ec2 instance from amazon instead of buy a machine  if PRON be only plan on buy one node  PRON may find that even keep PRON fully load  PRON plan run will take long than PRON can afford to wait  and buy 10x as many instance from amazon and run PRON there may be more effective  plus  PRON be actually hard to keep a single workstation or server node busy all the time  PRON require diligence  amazon nod PRON can rent when PRON need and discard when PRON do not  PRON do not have to pay for time PRON be not use   strongly consider apply for supercomputer time with an organization like xsede in the us or prace in the eu  or PRON country s local equivalent   PRON  PRON work for tacc which participate in xsede  literally give away the time  and 100k core  hour can be have for free in a matter of day or week through a start  up allocation  PRON presume the complementary organization have similar policy  
__label__data-mining __label__genetic-algorithms PRON be implement a module which find base on user interaction on an online portal to find which attribute of a certain product and what value for those product influence the buyer to choose the say item   PRON employer have suggest PRON to use genetic algorithm to achieve this  PRON have start implement and find even the initial population generation to be a bit ineffective  before PRON go further with this PRON would like PRON thought whether  genetic algorithm  be the right way or be there a good way to go about this   for PRON each genome be a list of attribute and PRON correspond value   eg  genome 1  color  black  material  wool  thread count5  genome 2  color  white  handwash  yes  dry clean only  no  PRON a mixture of different attribute and PRON value  be PRON go about this the wrong way   
__label__finite-element __label__parallel-computing __label__sparse __label__poisson PRON want to solve poisson equation  with fem  in a parallel manner  PRON hear that paralution be a good library  be that true   be there any good library  in what respect   edit  PRON want to insert space and time dependent property  conductivity  to poisson eq   
__label__theano __label__regularization __label__caffe PRON have a lasgane code  PRON want to create the same network use caffe  PRON could conver the network  but i need help with the hyperparameter in lasagne  the hyperparameter  look like   lr  1e2  weightdecay  1e5  prediction  lasagnelayersgetoutputnetout     loss  tmeanlasagneobjectivessquarederrorprediction  targetvar    weightsl2  lasagneregularizationregularizenetworkparamsnetout    lasagneregularizationl2   loss   weightdecay  weightsl2  how do i perform the l2 regularization part in caffe  relevant part from PRON solverprototxt be as below   baselr  001  lrpolicy   fix   weightdecay  000001  regularizationtype   l2   stepsize  300  gamma  01  maxiter  2000  momentum  09  
__label__machine-learning __label__neural-network __label__beginner __label__matlab __label__octave PRON be a beginner and PRON have develop code in octave to train a neural network  as part of andrew ng s coursera course    now PRON would like to generate some dataset in excel for simple mathematical function and try and train PRON neural network implementation   PRON try y  x2 and y  sinx   but PRON training accuracy be only around 67    PRON have vary the number of hidden layer unit  vary the lambda  but still do not get good result  where could PRON be wrong   without code PRON could have make to many mistake to answer PRON question directly   however  PRON suggest two thing   gradient checking  eg this explanation  but there be many more  pretty sure also in the course   xor problem with a 221 network with only sigmoid activation  plot the error surface  compare the decision surface with that of this interactive demo   PRON should look similar to this   however  please keep in mind that the problem may not be PRON implementation  but rather the network architecture  hyperparameter such as the number of epoch PRON be train or the training datum   also  very important  PRON doubt that PRON will get good result for  y  x2   except if  x be restrict to   11 or something similar simple  please keep the domain of PRON output layer in mind  
__label__machine-learning __label__sampling __label__ensemble-modeling PRON read about compare datum resampl method in this publication  httpsarxivorgabs170703905  in PRON  PRON use the dolan  more curve  PRON have be try to read more about PRON but can not find material  be anyone aware of any implementation   preferably in python  a brief overview of how the curve work with be helpful too  thank  other suggestion on how to do such comparison will be appreciate as well  
__label__machine-learning __label__r __label__text-mining __label__topic-model PRON be learn about probabilistic topic models by read this article by d blei  watch this video  and do this exercise a gentle introduction to topic modeling in r  after the topic in PRON corpus be define  by the algorithm   will the  lda  package in r allow PRON to find the document for a specific topic that be model  put differently  how can PRON check which document contribute to the formation of a certain topic in the final model   any advice on this topic or how to better formulate PRON question be appreciate   in the r code under the link for  a gentle introduction to topic modeling there be a snippet that do just what PRON be look for  ie PRON attribute each document in the corpus to one of 5 theme topic    write out result   doc to topic  ldaouttopic  lt asmatrixtopicsldaout    writecsvldaouttopicsfilepasteldagibbskdocstotopicscsv     this snippet quantifie the probability with which each document be associate with a certain theme    probability associate with each topic assignment  topicprobabilitie  lt asdataframeldaoutgamma   writecsvtopicprobabilitiesfilepasteldagibbsktopicprobabilitiescsv    
__label__dimensionality-reduction PRON be try to understand t  sne better and PRON be hop someone could elaborate on how the  sigma  i s be choose   PRON be also wonder why PRON be not just calculate in the normal way standard deviation be calculate from the datum with  xi   as the expect value for each  sigma  i   
__label__tensorflow __label__rnn PRON be start to experiment with neural network in google s tensorflow framework  and in particular be work on a model that have an lstm layer  PRON be use the rnn tutorial to get PRON start   to train PRON model  PRON have sequence of datum that be several thousand datum point long  in the tutorial  the author be process small sequence of datum  short sentence of paragraph  and concatenate those sequence together  separate by  ltend of streamgt   divide the entire data set equally into batchsize number of chunk and process those chunk in parallel  PRON would like to process each of PRON sequence of datum separately from start to finish  but would like to be able to process several of PRON in parallel  as the rnncell object be set up to do  PRON process batchsize sequence at once    this would be straight forward to do if the sequence be of the same length  but PRON be not  an rnncell  lstmcell expect the datum to come in with a fix dimensionaltiy  batchsize x inputsize  so there be a problem once some of the sequence reach PRON end and PRON batchsize want to decrease   how can PRON work around this  PRON do not think PRON can  left  or  right  justify PRON datum so that PRON line up without affect the training   PRON would suggest pad PRON sequence to all be the same length  obviously this be not ideal as PRON end up waste time process fake datum but PRON do solve the problem  
__label__machine-learning __label__clusters the problem  PRON have several people that do a lot of ml work in PRON lab and have more people ask to use PRON equipment  PRON have several machine that have ubuntu on PRON that people share  this cause a problem for resource contention  persona have all of PRON code  library on machinea or a person be wait on machineb which have 4gpus instead of 1gpu   look for   what PRON be look for be a way of create a cluster with these machine that will allow people to utilize and share the resource  this would allow people to scale out and make good use of the resource that PRON already have  eg wait for a specific machine that be already use   this could be use by interactively run code in PRON ide or submit job to the cluster like some type of scheduler  slurm  pbs   at this point the end user could look at this as a  pool  of resource   eg PRON have 100gpus  persona request 4gpus  PRON have 96gpu available for someone else to request   would also like to avoid the user from have to anything about the cluster  for example usera simply request 4gpus  PRON do not know and do not care which machine PRON get launch on   PRON use a mix of tenserflow and mxnet with kera and some theano   tensorflow  mxnet seem to have clustering option but do not think this would handle the scheduling problem  or would PRON   if PRON have successfully deploy some type of machine learn cluster with gpu node  PRON would really be interested in see PRON architect  tool  and software PRON use   if this be out of scope for this forum please feel free to move this to another channel   PRON depend on how the cluster  ie  compute resource  be be manage  for example  apache mesos be an open  source cluster management tool   one establish option be horovod  PRON be design to manage workload for  tensorflow and keras across multiple core in a single machine or across multiple machine  
__label__petsc __label__c __label__precision PRON be implement a finite element code  translate from a work matlab version  so PRON have result to compare to  and for some odd reason  some of PRON computation be only accurate to around 6 decimal place   PRON be work with double and petscscalars and PRON have double and quadruple check PRON code and be fairly certain there be nothing wrong with PRON implementation  PRON go back and make sure all of PRON multiplication be with double or petscscalars  though PRON be not sure if that change anything   PRON test on an exact solution  where PRON should get machine precision error  on a refinement study  the coarse mesh  two  element triangular mesh on   012  PRON get machine precision error but for every refinement after that  the error bump up to around  107 and completely flatline there  for exact solution that can not be exactly express in term of the function space  PRON be obtain proper convergence rate for basis degree 1 and 2  and then the rate rapidly drop off for any high degree   PRON have be try to hunt down the error for a while now  this be PRON first time programming such a large project in c so PRON be sure there be some nuance that PRON just do not know about  although PRON think PRON be experience enough with c to have not make any error when translate the work code  do PRON have to specify that the petscscalar ’s be double  PRON reconfigur with  precision  double but PRON do not change the result   be PRON solve PRON linear system iteratively  use solver such as cg or gmres  if so  PRON need to set the tolerance within these solver appropriately or PRON overall accuracy will be limit by the accuracy PRON get from the linear solver   for archival purpose  note that the resolution to the problem be find in the comment below the question  datum be be read from a file contain low  precision datum  
__label__data-mining __label__dataset __label__data-cleaning __label__linear-regression __label__supervised-learning PRON have string with list of specification chapter  for example    113  1118  28  341  or  13  25 and all from 3    PRON task be predict software development time base on these chapter  and some other feature    chapters have deep of nest 5  ie 12345  or 4281412 chapter be possible   usually  more chapter lead to long development  but count chapter only be not enough  because some chapter be big and complex a even one of PRON can take a long time to develop  make one feature for each chapter lead to huge number of feature   be there a way to transform PRON into the reasonable count of good numerical feature   
__label__machine-learning __label__neural-network PRON read some blog article recently  one mention that PRON could not imagine high dimensional space as 2d or 3d as distance between any 2 point in high dimensional space tend to be similar  which mean  dense   however in the t  sne paper  PRON say high dimensional space tend to be sparse such that PRON have to employ special dimensionality reduction technique to visualize in 2d or 3d space in meaningful way  so how to reconciliate these 2 different view   datum in a high dimensional space tend to be sparser than in low dimension   there be various way to quantify this  but one way of thinking that may help PRON intuition be to start by imagine point spread uniformly at random in a three dimensional box   now flatten the box into a square  push two opposite side together so all the point lie on a single plane   do PRON see that the average distance between a point and PRON near neighbor be now small   now flatten the square into a line segment   do PRON see that the average distance between a point and PRON neighbor be now small still   there be no conflict between this and say that the average distance between any 2 point in the high dimensional space tend to be similar   the latter statement do not imply density   the real number line be dense  PRON have no gap   and yet the distance between point range from 0 to infinity   the point be that the high the dimension of PRON space  the more likely the point be to lie near the edge of the space rather than the center   again  consider the dimension PRON can actually see   consider a circle with radius1  inscribe in a square with side of length2   the circle occupy  pi  4  of the square s area  about 785    now consider a sphere of radius1 inscribe in a cube with side of length2   the sphere occupy  pi  6  of the cube s volume  about 524    as PRON see in this example  the odd of a randomly place point lie close to the center  with close to the center in this case mean within the circle or the sphere  be low as the dimension increase   point be more likely to be in the corner   this be why in high dimension the distance between the point tend to be similar  because randomly place point tend to be close to the edge of the region   PRON would like to elaborate a bit on the fact that datum in high dimensional space be sparser   usually  PRON think of euclidean space  this mean if PRON have point  p1  p2 in mathbbrn PRON say PRON distance be    dp1  p2   sqrtsumi1n  left  p1i    p2i   right   2  so PRON take the square root of the sum of the component  wise squared difference   now think of two point in a unit hypercube in  mathbbrn  hence   0  1n   the maximum distance two point can have in this hypercube be when PRON be on the diagonal  hence the distance be  sqrtn  mean the high the dimension be  the farer away can point be in the unit hypercube   see also  what be the maximum distance of k point in an n  dimensional hypercube  
__label__linear-algebra __label__algorithms __label__eigensystem __label__sparse PRON be try to implement the pagerank algorithm describe in this paper  fig  1   here be the breakdown of the step   httpwwwlouismulliecomalgopng  where   pt be a probability distribution for the random walk  typically  each element be 1n where n be the total number of element   p be the connectivity matrix where pi  j    1 or 0     outbound link   PRON understand the rationale for the first two step well  in the following equation   step 2 solf for the second term  however  PRON do not understand the rationale behind step 3  4  in particular  PRON do not see how add omega  pt in the description of the algorithm amount to add  1dn in the equation show above  as a result  the output PRON be get with the two method differ  can anybody enlighten PRON   the pagerank method be basically the power iteration for find the eigenvector correspond to the large eigenvalue of the transition matrix   the algorithm PRON quote be come directly from equation  4  and  5  of the paper PRON reference  and this be just a way of implement the power iteration for a matrix with a particular structure   PRON have no idea where PRON equation involve  pra    be come from or what PRON mean or how PRON relate to the rest of PRON question   as page 5 of the paper PRON link to explain  the random surfer model of the pagerank algorithm interpret one step in the power iteration as a surfer look at a give page follow one of the link with probability  c  which of these be determine by PRON relative weight  ie  the component of the current iterate  xk  and jump to an unrelated page  eg  by use a bookmark  with probability  1c  which if these be determine by the component of the vector  p   as explain  write this as a matrix  vector product  pa with an explicit matrix  a be horrendously expensive since jump to an arbitrary page lead to a fully populated matrix  so one write  a  cp1ce and calculate  xa  cxp   1cxe separately  in the algorithm above  step 2 correspond to the first term and step 3 and 4 correspond to the second term  the specific form be derive in detail in the paper  equation  4  and  5     note that the wikipedia article only include a uniform jump probability  confusingly call damp factor   while the paper use a non  uniform probability  give by the personalization vector  p  and include an additional term  d to prevent get stick on page without outgoing link    PRON should also remark that the wikipedia article be not a terribly good explanation of what be go on  especially mathematically  a good source be this siam review article   
__label__pde __label__finite-element __label__reference-request after read three book about finite element method  with two of PRON cover also finite volume and grid generation  PRON find PRON lose when PRON have to discuss these topic with library developer scientist that use an advanced mathematical language  for example  PRON usually encompass the discussion with sobolev and hilbert space    so  PRON question be this  what reference book  beyond the fundamental and standard finite element book  be recommend   for a little more mathematical rigor  PRON like some section in  theoretical numerical analysis  a functional analysis framework  by atkinson and han   assume that PRON already have a background in partial differential equation and basic numerical analysis  functional analysis be helpful to understand fem theory   from personal experience  PRON be a tedious topic and be a bit difficult to really master autodidactically   having say this  PRON have gain some insight from book like  understanding and implement the finite element method  introductory functional analysis  applied calculus of variations for engineers  applied functional analysis and variational methods in engineering  each book help PRON in some way  shape  or form to understand the mathematic behind the finite element method   however  none of PRON really provide a  perfect  or  complete  understanding to PRON   this be in no way an exhaustive list of  good reference  and PRON do not think there be any  canonical book  on the subject either   but at least  these be some book that help PRON and PRON hope PRON can help PRON as well   PRON have to disagree with paul  functional analysis be a beautiful  elegant topic with an enormous range of application  but de gustibus     but in any case  PRON do not need to know a lot of  pure  functional analysis to understand finite element method  normed vector space   strong  convergence  completeness  inner product and PRON induce norm  orthogonality  dual space and adjoint operator  except for the last two  these topic should be already familiar if PRON already know some multivariate analysis and linear algebra  modulo some technicality   PRON be all treat in  the first four chapter of  kreyszig s introductory functional analysis which be already recommend  and which PRON find to be a very pleasant read    but much more than the behavior of abstract vector space  PRON need to understand the behavior of lebesgue and sobolev space  as well as the theory of  weak solution  of partial differential equation   from PRON previous question  PRON would say that this be precisely what PRON have problem with   these topic  which some would classify as  apply  functional analysis  be usually summarize  but not treat in detail  in what PRON  as a mathematician  would call the fundamental and standard finite element book   braess  finite elements  cambridge university press  2007  brenner  scott  mathematical theory of finite elements  springer  2008  ern  guermond  theory and practice of finite elements  springer  2004  these be list roughly in order of mathematical abstractness  and should be enough to be on the same page as the scientist PRON be talk to  if PRON want more detail  PRON need to look at book on the theory of partial differential equation  the standard reference would be  again in order of abstractness   evans  partial differential equations  ams  2010  renardy  rogers  an introduction to partial differential equations  springer  2004  wloka  partial differential equations  cambridge university press  1987  in fact  chapter 5 and 6  as well as appendix d and e  of evans  book should cover most of the theoretical background PRON need for the  mathematical  theory of finite element method  
__label__classification __label__scoring PRON have three classifier for language identification   a  en  de  ru  fr  ij  kl  b  en  de  ru  fr  xy  c  en  de  ru  fr  no  pq  rs  and PRON have a balance dataset which match the class of a  what be a fair way to compare those classifier   PRON thought  accuracy on class of a  not fair  because b can not possible recognize ij  kl and will likely make mistake due to the fact that PRON know language xy   similar for c   smallest common subset  possible  but not so interesting as those be the  easy  class   precision  give a language the classifier know  how often do PRON actually recognize PRON   unfair for c  as PRON have more possibility to make mistake  probably this could be somehow compensate  eg if PRON recognize a class which be not in a  just the next good be take  until the correct class be predict or a wrong class from a be preditect   there could also be a possibility to use the classifier a in combination with another classifier  the weight  importance  however that be measure  of the contribution of the other classifier be PRON score   be there any publication which do something similar   
__label__optimization __label__algorithms __label__linear-programming consider a node  s let PRON assume that there be three outgoing arc from node  s namely   s  is  j and   s  k corresponding to each of these arc  there be a flow proportion value  tsjin  01 PRON mean that the amount of flow along the arc   s  j should be  tsj   fps in which  fps be flow over the single arc go to node  s now in PRON lp PRON have the follow constraint regard node  s    fps     fsifsjfsk   fsi    tsi   fps   fsj    tsj   fps   fsk    tsk   fps  in a special case  just assume that  sumj  tsj1 for example assume that  tsj be probability value   now  PRON question be that how PRON can get rid of these proportional equality constraint  ie constraint  2  4 in PRON give example  PRON be look for a way to modify the structure of the network in such a way to include these constraint implicitly   
__label__classification __label__predictive-modeling __label__random-forest __label__missing-data PRON have the problem  predict   method return na  PRON plan be   read datum from file and separate datum to 2 set  test and train  remove column with na fraction over 95   replace na value with mean value in each column  ifcolumn have more than 50 level  then read comment to changefactor   method  feature selection  here PRON choose 5 feature with great importance rank   build the model and perform predict   as a result na value accure in final vector  could anybody explain to PRON what PRON be do wrong   sorry for the code quality  PRON know PRON be horrible  setseed123    read datum from file and separate PRON to 2 set  train and test  datum  lt readcsvtraintxt   header  t  sep    datum  lt traincncoldata   1ncoldata   1     train  lt dataid    testset  lt dataid     select column with na fraction over 95   stat  lt applytrain  2  functioncol  meanisnacol     stat  lt asdataframestat   statname  lt  rownamesstat   colswithoutnacolumn  lt statstatstat  lt  095   name     replace na value with mean value in each column  train  lt train   colswithoutnacolumn   trainy  lt asfactortrainy   fori in 1ncoltrain     col  lt traini   ifisfactorcol     colisnacol    lt getmodecol     else   colisnacol    lt meancol  narmtrue     traini   lt col     feature selection  train  lt changefactorstrain   rfselectorlt  randomforestfactory  train  ntrees1000    varimpplotrfselector    choose five variable with the great importance rank  predictorscols  lt ca    b    e    r    k     PRON be PRON weight due to unbalanced datum  weight  lt c00726251 0072625   rfclassificatorlt  randomforestfactory  traincy   predictorscol     ntrees1000   classwt  weight   testset  lt changefactorstesttestset  train   testset  lt testset   colnamestrain     httpsstackoverflowcom  questions24829674r  random  forest  error  type  of  predictor  in  new  data  do  not  match  testset  lt rbindtrain11   testset   testset  lt testset1     here be the problem  na return as predict value in some case  predictrfclassificator  testset1   typeprob      additional function   getmode  lt functionv    v  lt visnav    uniqv  lt uniquev   uniqvwhichmaxtabulatematchv  uniqv         because randomforest work only with factor contain less then 53 level   PRON choose 49 the most popular level   and other level change to one level   other   changefactorslt  functiontrain    factorcol  lt namestrain   gt filterf  isfactor    newfactornameltother   fori in 1lengthfactorcols     fcol  lt factorcolsi    iflengthlevelstrainfcol     gt  50    subtrainset  lt subsettrain  select  cy   fcol    namessubtrainset   lt cy    fcol     result  lt subtrainset   gt groupbyfcol    gt summarisecount    n      gt arrangedesccount    factorstoreplace  lt result   gt slice50nrowresult     gt selectfcol    gt asdataframe    factorstoreplace  lt pastefactorstoreplacefcol    replace the least popular level to one common new level  train   fcol   lt ascharactertrain   fcol    traintrain   fcol   in factorstoreplace  fcol   lt newfactorname  train   fcol   lt asfactortrain   fcol        train     function perform on test test to unificate factor level of test set to   be equal to train set  changefactorstestlt  functiontestset  trainset     factorcol  lt namestestset   gt filterf  isfactor    newfactorname  lt  other    fori in 1lengthfactorcols     fcol  lt factorcolsi    iflengthlevelstestsetfcol     gt  50    printfcol   testset   fcol   lt ascharacter  testset   fcol    testsettestsetfcol   in uniquetrainsetfcol     fcol   lt  newfactorname  testset   fcollt asfactortestset   fcol        testset    
__label__machine-learning __label__sports PRON have a variety of nfl dataset that PRON think may make a good side  project  but PRON have not do anything with PRON just yet   come to this site make PRON think of machine learn algorithm and PRON wonder how good PRON may be at either predict the outcome of football game or even the next play   PRON seem to PRON that there would be some trend that could be identify  on 3rd down and 1  a team with a strong running back theoretically should have a tendency to run the ball in that situation   scoring may be more difficult to predict  but the win team may be   PRON question be whether these be good question to throw at a machine learn algorithm   PRON could be that a thousand people have try PRON before  but the nature of sport make PRON an unreliable topic   definitely PRON can   PRON can target PRON to a nice paper  once PRON use PRON for soccer league result prediction algorithm implementation  primarily aim at have some value against bookmaker   from paper s abstract   a bayesian dynamic generalize model to estimate the time dependent skill of all team in a league  and to predict next weekend s soccer match   keywords   dynamic models  generalized linear models  graphical models  markov  chain monte carlo methods  prediction of soccer matches  citation   rue  havard  and oyvind salvesen   prediction and retrospective analysis of soccer match in a league   journal of the royal statistical society  series d  the statistician  493  2000   399  418   machine learning and statistical technique can improve the forecast  but nobody can predict the real result   there be a kaggle competition a few month ago about predict the 2014 ncaa tournament  PRON can read the competition forum to get a good idea on what people do and what result do PRON achieve   PRON have be show before that machine learning technique can be apply for predict sport result  simple google search should give PRON a bunch of result   however  PRON have also be show  for nfl btw  that very complex predictive model  simple predictive model  question people  or crowd knowledge by utilise bet info  PRON all perform more or less the same  source   everything be obvious once PRON know the answer  how common sense fail   chapter 7  by duncan watts   yes  why not    with so much of datum be record in each sport in each game  smart use of datum could lead PRON to obtain important insight regard player performance   some example   baseball  in the movie moneyball  which be an adaptation of the moneyball book   brad pitt play a character who analyse player statistic to come up with a team that perform tremendously well  PRON be a depiction of the real  life story of oakland athletics baseball team  for more info  httpwwwtheatlanticcomentertainmentarchive201309forget2002thisyearsoaklandasaretherealemmoneyballemteam279927  cricket  sap lab have come up with an auction analytic tool that have give insight about impact player to buy in the 2014 indian premier league auction for the kolkata knight riders team  which eventually go on to win the 2014 ipl championship  for more info  httpscnsapcomcommunityhanainmemoryblog20140610saphanaacademycricketdemohowsaphanapoweredthekolkataknightriderstoiplchampionship  so  yes  statistical analysis of the player record can give PRON insight about which player be more likely to perform but not which player will perform  so  machine learning  a close cousin of statistical analysis will be prove to be a game changer   there be a lot of good question about football  and sport  in general  that would be awesome to throw to an algorithm and see what come out  the tricky part be to know what to throw to the algorithm   a team with a good rb could just pass on 3rd  and  short just because the opponent would probably expect run  for instance  so  in order to actually produce some worthy result  PRON would break the problem in small piece and analyse PRON statistically while throw PRON to the machine   there be a few  good  website that try to do the same  PRON should checkem out and use whatev PRON find to help PRON out   football outsiders  advanced football analytics  and if PRON truly want to explore sports data analysis  PRON should definitely check the sloan sports conference video  there be a lot of PRON spread on youtube   PRON have read some about PRON and PRON have the follow blog in mind   httpfellgernontumblrcompost46117939292predictingwhowillwinanflmatchathalftimeutehm7twtqg  this blog deal with the prediction of a nfl match after the half time be already over  the prediction be 80  accurate with simple glm model   PRON do not know if that be suitable for soccer   PRON have do some research in this area  PRON have find first order markov chain work well for predict within game scoring dynamic across a variety of sport   PRON can read in more detail here   httpwwwepjdatasciencecomcontent314  michael maouboussin  in PRON book   the success equation   look at differentiate luck from skill in various endeavor  include sport   PRON actually rank sport by the amount of luck that contribute to performance in the different sport  p 23  and about 23 of performance in football be attributable to skill   by contrast  PRON use mm s technique to analyze performance in formula 1 racing  and find that 60  be attributable to skill  less than PRON be expect    that say  PRON seem this kind of analysis would imply that a sufficiently detailed and craft feature set would allow ml algorithm to predict performance of nfl team  perhaps even to the play level  with the caveat that significant variance will still exist because of the influence of luck in the game   PRON can not predict  but PRON can tell PRON the most likely result  there be an study about this kind of approach from etienne  predicting who will win the world cup with wolfram language  this be a very detailed study  so PRON can check all the methodology use to get the prediction   interesting enough  11 from 15 match be correct   as one may expect  brazil be the favorite  with a probability to win of 425   this striking result be due to the fact that brazil have both the high elo ranking and play at home    let PRON go brazil    a lot of people have stress about what be the thing that can be predict in PRON answer  now  with the fascination for deep learning  PRON could  for example  use rnnssay lstm  to predict outcome for sport problem that be base on time  these be state of the art and beat traditional model hand down  
__label__statistics __label__beginner __label__ab-test PRON recently have a phone interview with a consumer tech company for a quant position  the question be basically   imagine a facebook style social network site  six month ago a new feature call  mention  be add which allow PRON to tag PRON friend with an  sign  how would determine whether this feature be a success    PRON be a bit take aback by how broad the question be  PRON first ask if the feature be give to everyone in the network or a sample  to which the interviewer respond  PRON decide   mean PRON could approach the analysis either way  PRON talk in general term about calculate week over week usage of the feature as well as month over month  PRON also discuss compute a baseline metric for product interaction and then compare the usage of the new mention feature relative the baseline statistic  overall PRON leave the interview feel quite dumb  as PRON have a pretty solid command of stat  but come away look like an idiot   be there specific statistical procedure to test for something like this  al la a  b testing  or some kind of hypothesis test  and secondly  be there a good framework for approach these type of open end case study style question in general   personally   PRON do not think this question be reasonable  the first thing PRON need to do be determine from the stakeholder what  success  be  this could be increase traffic  increase revenue  etc  without know how the stakeholder view success PRON can find all kind of interesting thing in whatev datum PRON have and never satisfy PRON client  PRON be very common for datum scientist to look for a needle in the haystack only to find the wrong needle if PRON can even find one   this question  something PRON have ask variant of several time in interview  have absolutely nothing to do with statistical or other quantitative procedure  what be be ask here be for an understanding of the overall data mining process PRON   the first thing to determine be what the definition of success  so PRON have to ask  the stakeholder usually will not volunteer this unless ask anyway  then  depend on the answer describe the overall process for datum mining base on this end goal   PRON believe open end question like this one have the goal to see what be PRON thought process  the interviewer want to know how would PRON tackle the problem  what do PRON do first  what hypothesis do PRON consider  and  most important  how do PRON defend PRON decision  ask question and  sometimes  even think out loud can be helpful as PRON show the interviewer PRON thought process   PRON would probably employ a similar approach and assume that this feature be roll out for a subset of user  then PRON would check metric such as user engagement  here PRON can define that as number of comment  number of like comment  number of comment reply  or a function of all of those  and perform an a  b test  base on the result a conclusion can be reach if this new feature be indeed a success  base on the aforementioned metric  or not  
__label__neural-network __label__backpropagation PRON question would be about backpropagation and understand the term feedforward nn vs backpropagation  PRON have two question really   if PRON understand correctly  even a feedforward network update PRON weight  via a delta rule for example   be not this also backpropagation  PRON have some random weight  run the datum through the network  then cross  validate PRON with the desire output  then update the rule  this be backpropagation  right  so what be the difference between ffw nn and rnn  if PRON can not backpropagate on the other hand  how do PRON update the weight in a ffw nn   PRON have see nn architecture look like this   basically all the neuron be be feed the same datum  right  ok  the weight be randomize and thus different in the beginning  but how do PRON make sure temp  value  1 will be different from temp  value  2  if PRON use the same update rule   thank PRON   PRON be not an expert on the backpropagation algorithm  however PRON can explain something  every neural network can update PRON be weight  PRON may do this in different way  but PRON can  this be call backpropagation  regardless of the network architecture   a feed forward network be a regular network  as see in PRON picture  a value be receive by a neuron  then pass on to the next one   a recurrent neural network be almost the same as a ffn  the difference be that the rnn have some connection point  backward   eg a neuron be connected to a neuron that have already do PRON  job  during backpropagation  because of this  the activation of the previous output have an effect on the new output   on question  2  interesting question  this have to do with weight initialization  yes  PRON be right  each neuron in the hidden layer accept the same connection  however  during the initalization process  PRON have receive a random weight  depend on PRON nn libary  the neuron may also have be initialize with a random bias   so even though the same rule be apply  each neuron have different outcome as all PRON be connection have different weight than the other neuron weight   on PRON comment  just because all the neuron happen to have the same backpropagation function  do not mean PRON will end up with the same weight   as PRON be initialize with random weight  each neuron error be different  thus PRON have a different gradient  and will get new weight   PRON also have to keep in mind that for a certain output to be reach  there be multiple solution  due to non  linearity   so due to initialize random weight  one neuron may be close to a certain solution while another neuron be close to the other   additionally  as be state in the comment  a network work as a whole  the output neuron be also non  linear  and for most test case  the output should be non  linear and the output neuron most likely require that the hide neuron activate at different input value  
__label__self-learning __label__agi __label__ultraintelligent-machine let PRON say PRON have the basic scenario where two agi of about the same intelligence  but not same origin  code  model  have to communicate as efficiently as possible to achieve a common goal  now PRON could have 2 start point for that   1  either all PRON have be a common communication bus  eg sound  light  radio  etc   and instrument  eg transceiver  to support PRON  and PRON have to figure out the rest   2  or PRON be some kind of advanced chatbot  but since the human language be lack a lot to be use as a highly efficient protocol  PRON will have to communicate with what PRON have  to build a proper one   would PRON be possible to somehow induce PRON to communicate  and try to figure out what each other  say   how could this be do   and a more abstract question be how could this protocol  look  like   this be a purely theoretical question  currently in the realm of philosophy and speculative fiction   nevertheless  PRON be an interesting question  and may be instructive   if PRON use the standard definition of artificial general intelligence as automaton with human level intelligence  then PRON could certainly devise PRON own communication protocol  just as human have   1  these automaton be agi so PRON be creative and resilient  just like human  and  where there be a will there be  sometimes  a way   absent robotic capacity PRON would not be able to build anything physical and would have to rely on exist communication infrastructure  if PRON have access to 3d printing and versatile robot  PRON could probably build something new  but this would be infeasible for anything that require extensive capital outlay  unless the automaton first acquire a major communication infrastructure firm or two   2  PRON be always interesting  and usually quite entertaining  to observe chatbot converse with each other  but if PRON be agi  nlp be just one of many function  and PRON doubt PRON would bother converse with each other in human language  since all data be ultimately reduce to a string  if PRON be smart enough to be deem agi  PRON would certainly communicate in  specie with the most efficient protocol available  and probably optimize PRON further  if possible  or create a unique protocol for purpose of exclusivity   if PRON be truly agis  PRON would not ostensibly have to induce PRON to communicate  because PRON would be smart enough to understand the benefit of communication and cooperation  and would likely seek to form coalition as a natural survival function   game theory provide a mathematical basis for this    multi  agent system can self organize  even where the intelligence of the give agent be low  and in PRON scenario  the automaton be smart    PRON believe that some work by randall beer in the 1990 demonstrate that even simple agent could learn a share communication protocol   computational and dynamical language for autonomous agent  viewed in general term  a sentence utter by an agent be just another  feature  of the environment  from the perspective of an observe agent  learn to extract and operate upon a subset of feature be what modern machine learning be mostly concerned with   while braitenberg s book  vehicles   do not deal specifically with language acquisition  PRON do nicely illustrate some general principle for self  organisation of more complex recogniser and behaviour  
__label__scikit-learn __label__xgboost __label__gbm PRON be try to train a gradient boost model over 50k example with 100 numeric feature  xgbclassifier handle 500 tree within 43 second on PRON machine  while gradientboostingclassifi handle only 10 tree    in 1 minute and 2 second  PRON do not bother try to grow 500 tree as PRON will take hour  PRON be use the same learningrate and maxdepth setting  see below   what make xgboost so much faster  do PRON use some novel implementation for gradient boost that sklearn guy do not know  or be PRON  cut corner  and grow shallow tree   ps  PRON be aware of this discussion  httpswwwkagglecomchiggsbosonforumst10335xgboostpostcompetitionsurvey but could not get the answer there   xgbclassifierbasescore05  colsamplebylevel1  colsamplebytree1   gamma0  learningrate005  maxdeltastep0  maxdepth10   minchildweight1  miss  none  nestimators500  nthread1   objectivebinary  logistic   regalpha0  reglambda1   scaleposweight1  seed0  silent  true  subsample1   gradientboostingclassifierinit  none  learningrate005  lossdeviance    maxdepth10  maxfeatur  none  maxleafnod  none   minsamplesleaf1  minsamplessplit2   minweightfractionleaf00  nestimators10   presortauto   randomstate  none  subsample10  verbose0   warmstart  false   since PRON mention  numeric  feature  PRON guess PRON feature be not categorical and have a high arity  PRON can take a lot of different value  and thus there be a lot of possible split point   in such a case  grow tree be difficult since there be  a lot of feature  times a lot of split point  to evaluate   PRON guess be that the big effect come from the fact that xgboost use an approximation on the split point  if PRON have a continuous feature with 10000 possible split  xgboost consider only  the good  300 split by default  this be a simplification   this behavior be control by the sketchep parameter  and PRON can read more about PRON in the doc  PRON can try lower PRON and check the difference PRON make  since there be no mention of PRON in the scikit  learn documentation  PRON guess PRON be not available  PRON can learn what xgboost method be in the PRON paper  arxiv    xgboost also use an approximation on the evaluation of such split point  PRON do not know by which criterion scikit learn be evaluate the split  but PRON could explain the rest of the time difference   adressing comment  regard the evaluation of split point  however  what do PRON mean by  xgboost also use an approximation on the evaluation of such split point   as far as PRON understand  for the evaluation PRON be use the exact reduction in the optimal objective function  as PRON appear in eq  7  in the paper   in order to evaluate the split point  PRON would have to compute  ly  hi1hi where  l be the cost function   y the target   hi1 the model build until now  and  hi the current addition  notice that this be not what xgboost be do  PRON be simplify the cost function  l by a taylor expansion  which lead to a very simple function to compute  PRON have to compute the gradient and the hessian of  l with respect to  hi1  and PRON can reuse those number for all potential split at stage  i  make the overral computation fast  PRON can check loss function approximation with taylor expansion  crossvalidated q  a  for more detail  or the derivation in PRON paper   the point be that PRON have find a way to approximate  ly  hi1   hi efficiently  if PRON be to evaluate  l fully  without insider knowledge allow optimisation or avoidance or redundant computation  PRON would take more time per split  PRON this regard  PRON be an approximation  however  other gradient boost implementation also use a proxy cost function to evaluate the split  and PRON do not know whether xgboost approximation be quick in this regard than the other  
__label__pde __label__adaptive-mesh-refinement assume a solid mechanic problem  linear elasticity  with a domain split in two by an interface that be not align with the mesh  the mesh be a quadtree with square  there be different material property at each side of the interface  now PRON use amr and the algorithm will refine the mesh along the boundary  but will never be able to align the mesh with the interface because of the quadtree  how will the stress converge for this case  do the a priori error estimation still hold in the stress near the interface   what if the interface be align with the mesh  make PRON jaggedin this case PRON will have many re  entrant corner and PRON assume the stress will not converge  but somehow  make the mesh fine and fine  that jagged interface will become somehow smooth  be there any type of convergence possible here   edit  PRON forget to add that the method be finite element with linear quadrangular lagrange element   
__label__matrices __label__c __label__data-structures in PRON c language program  PRON have to store multiple dense  mtime m matrix correspond to gridpoint  xi with  i1  n  PRON decide to create a three dimensional array  ain rntimes mtime m as follow   double     a   adoublemallocsizeofdoublen    for  i0  iltn  i    aidoublemallocsizeofdoublem    for  j0  jltm  j    aijdoublemallocsizeofdoublem        in this way  PRON be index PRON matrix as aijk  such that at each index  i  correspond to a unique  mtime m matrix   however  PRON now have to input these 2 dimensional   mtim mmatrice into a solver which assume PRON be store as only two dimensional array   if i be write the program in matlab  PRON could extract the 2d matrix by simply cod   b  ai        but PRON be not sure how to do this in c  there must be some clever trick use pointer   as aron note in PRON comment  rarely be matrix store in nest pointer datum structure in practice  multiple level of indirection  PRON be tell  because considerable performance penalty  and require multiple allocation and free  more performance issue  plus more potential to screw up and segfault   PRON have see nest pointer use to store matrix only a few time  in chemkin  and PRON think also in cantera   far more common practice be to store the datum in memory associate with a single pointer  the c interface to blas and lapack use this practice   PRON probably want to do something more like  double  a   a   double   mallocsizeofdouble   m  m  n     ai  1   m  m   j  1   m  k    aijk   where i  j  k refer to traditional one  base mathematical index  this sort of arrangement of datum be call row  major ordering   then  to do the slicing PRON would like  PRON will need to copy the datum in a to another pointer   double  b   b   double   mallocsizeofdouble   m  m     here  zero  base indexing be use instead of the one  base indexing in the   last code snippet   for  j  0  j  j  lt  m    for  k  0  k  k  lt  m    bj  m  k   ai  m  m  j  m  k    b   ai          there may be slicker way of do this operation  use something like memcpy  but PRON will still need to copy the datum to another data structure   let PRON suppose that PRON proceed with PRON current code  and suppose further that PRON continue with PRON strategy of store a matrix in nest pointer  to slice a  PRON would do something like   double   b   b   double    mallocsizeofdouble    m     zero  base indexing also use in this code snippet   for  j  0  j  j  lt  m    bj    double   mallocsizeofdouble    m    for  k  0  k  k  lt  m    bjk   aijk        again  there may be slicker way of do this copying  such as use memcpy   in addition to matlab  language such as fortran 909520032008 and python support array slice syntax natively   if PRON do not want to litter PRON program with this kind of copying loop then maybe c be not PRON language  assume this be a common operation  c provide far nice way to do this sort of thing  by make the array a class and  possibly  make the sub  array a view of PRON   even fortran have a nice syntax for this kind of operation  what this illustrate be that PRON can express everything in every turing  complete language but that PRON be not true that everything can be express with equal ease in each language  
__label__stochastic __label__stochastic-ode PRON be try to familiarise PRON with sde and have be read a few review paper on the topic  PRON leave the impression that a great deal of work have be put into solver that be derivative  free  to PRON understanding this mean that for a dde like    newcommanddiffmathopmathrmd    diff x  fxdiff t  gx  diff w      the derivative of  f and  g be not require for the method  correct PRON if PRON be wrong    PRON can understand that this property be useful in some application where the derivative be difficult or computationally infeasible to obtain or do not exist  however  PRON would not expect such problem to be very relevant in application   this suggest to PRON that at least one of the follow applie   there be some further relevant advantage to derivative  free solver that PRON be miss   problems where derivative  free solver be require  due to the above reason  be more relevant than PRON think PRON be   the demand for derivative  free solver be low than the  supply   ie  the attention give to PRON by those who develop solver   which be PRON   PRON be not an expert in specifically stochastic differential equation  but PRON would assume that PRON answer will still be of some value   computation of the derivative can be challenging  as PRON mention in PRON question  however  this would be even more pronounce in a multidimensional case  as one would have to calculate jacobian matrix   n2  entry   so  non  derivative  free solver will suffer from the curse of dimensionality  the situation become even bad when high  order derivative be require for a scheme   computation of a derivative by PRON generally amplify numerical noise  so  for example  if the underlying function   f or  g  be not analytical  the error in the derivative may completely distort the solution    PRON can understand that this property be useful in some application where the derivative be difficult or computationally infeasible to obtain or do not exist  however  PRON would not expect such problem to be very relevant in application   if an analytical solution for the derivative be not know  PRON be very costly and error prone  calculate the jacobian be  n2  entry  but numerical differentiation technique will need to do multiple function call per entry  then  to get this right  numerical differentiation technique have to divide by a small number when compute the derivative  which cause a lot of numerical issue   with autodifferentiation tool this cost be reduce  but PRON can still be significant  so when analytical jacobians be not prescribe  PRON be usually good to stay away from method which require derivative   however  PRON would not expect such problem to be very relevant in application   for most thing like nonlinear spde or large system of sde  1000 s  come from biology  get the jacobian write out can be nearly impossible and error prone  PRON would say that PRON be the other way around  expect an analytical jacobian to be provide be not a good idea   there be some further advantage as well  runge  kutta method be derivative  free method  and PRON can do a lot of coefficient optimization   the demand for derivative  free solver be low than the  supply   ie  the attention give to PRON by those who develop solver   that be not the case  in differentialequationsjl derivative  free method be implement before the kps stochastic taylor series method because  for most user  PRON will lead to ease  of  use and increase performance  that say  in the field of differential equation  PRON can always find a counter example where that be not the case  so PRON do plan on implement some method which explicitly use derivative  however  PRON be sure most user will probably just default to the derivative  free method because the cognitive load on PRON end be much low  
__label__convergence PRON be attempt to calculate temperature of section of rock in the earth as a function of vertical position in the rock and time   along with PRON PRON be calculate the heat flow through the rock as a function of vertical position in the rock and time   note that this be a one  dimensional examination of the problem imagine that the rock be uniform in every direction but up  down   please be patient with PRON  PRON currently do not have the vocabulary nor the background to talk at the level of familiarity with fem and differential equation PRON see in other question on this site   PRON be a software developer with some physics background  which have allow PRON to get this far   description of methodology  PRON be currently splitting up the section of rock into horizontal slice  depth step   and then calculate the temperature and heat flow of each depth step at specific time  step   PRON be assume that heat only flow upward from below the section of rock   as PRON understand PRON  the temperature difference between the top and bottom of a depth step would be express as      delta t  fracqdelta zlambda      where  q be the heat flow at the middle of the depth step in  mw  m2    delta z be the thickness of a depth step at this time  and  lambda be the thermal conductivity of the rock  constant    additionally  the change in the heat flow between the bottom and top of a depth step could be express as      delta q   cp rho delta z fracdelta tdelta t      where  cp be the heat capacity of the rock  constant    rho be the density  constant   and  fracdelta tdelta t would be the change in temperature of the depth step over the course of this time  step   as a starting point for the calculation  PRON know the temperature at the top of the rock layer as a whole  but not the temperature at the bottom of the rock layer   PRON also know the heat flow value just below the rock layer  but not the heat flow value at the top    the key piece of complexity to this be that the rock layer be increase in thickness through time  aka deposition   so that PRON be add to the  delta z of the top  most depth step and then add more depth step to the top of the section of rock as time progress   as a consequence  individual depth step get more and more bury  and thus hot through time  which thus cause the heat flow to decrease as PRON move upwards in the rock layer  as per equation 2 above    PRON method of calculate the temperature and heat flow for each time  step be as follow   increase the thickness of the rock layer  add depth step to the top as necessary   calculate the temperature at the top and bottom of each depth step go down the layer  since PRON know the surface temperature  and thus the starting point   use the heat flow value from the previous time  step   calculate the heat flow value at the bottom and top of each depth step go up the layer  since PRON only know the heat flow at the very bottom of the rock layer   use the temperature of each depth step calculate in step  2  along with the temperature in the previous time  step   repeat step 2  3 until the solution converge  note that in step  2 PRON can now use the heat flow calculate in step  3   the problem   this solution do not always converge   as the rock layer get thick and thick through time  each time  step take long and long to converge  until PRON reach a tipping point and start to diverge   what can PRON do to force this to converge   PRON be suspicious that the reason for divergence be because PRON be force to calculate temperature down the depth step and heat flow up the depth step   this mean that the bottom  most depth step s temperature be sensitive to all the change in temperature above PRON as per the first equation above   but this then cause the heat flow at the bottom to change from the previous iteration by a fair amount  which then propagate all the way to the top of the rock layer via the second equation above  which then affect the temperature at the top strongly  etc   warning  the above problem be actually a major simplification of a large problem PRON be try to solve the constant in the equation above be actually temperature dependent and depth dependent in a complicated  non  continuous way   however  the problem PRON be face still exist in the simplification as describe above   
__label__computational-geometry __label__data-sets in the course of try to implement algorithm for voronoi and laguerre diagram  PRON realize PRON need to verify if PRON implementation be work correctly use a point  or circle  configuration with a know voronoi  or power  diagram   PRON have not be successful in look for dataset with know diagram  do anyone know a good source for these diagram   the most trivial pointset be regular grid of point  PRON look innocent  but be the most difficult to handle in voronoi  laguerre code because the delaunay triangulation be non  unique  so PRON can be use to detect bug in PRON code  PRON can be handle by adapt geometric predicate  see PRON publication  6  on geometric predicate and the reference herein    to detect bug in PRON voronoi  laguerre diagram code in 2d and 3d  1   PRON use the following approach   compute the voronoi  laguerre diagram for a collection of pointset   random  and test that the output of PRON code satisfie the empty  ball property  for voronoi  this mean that the circumscribed circle2d  or sphere3d   of each delaunay triangle do not contain any point  for laguerre  there be a similar condition   compute the voronoi  laguerre diagram of random pointset with a code  that PRON trust  1   if PRON trust PRON    23   PRON trust PRON  and compare with the output of PRON code   add to the list of pointset difficult one  the difficult dataset  be those that have cocyclic  2d   cospheric  3d  point such as regular grid that PRON mention before  note that when PRON get cocyclic  cospheric point  the delaunay triangulation be non  unique  then the output of PRON code may differ as compare to other code    when all the weight be set to zero  the laguerre diagram be a voronoi diagram  then PRON can compare the output of PRON algorithm  if PRON mismatch  then both be probably wrong  like PRON happen to PRON    PRON strongly recommend to use a continuous integration platform  4  to do automatic non  regression testing  voronoi  laguerre code be not very complicated  approx  300 line of code   but there be some gotchas  subtlety  in particular in the predicate for handle cocyclic point  automatic daily test  do 1   2   3   4   valgrind  5  memory test  allow to detect several bug in PRON own code    1  geogram  httpaliceloriafrsoftwaregeogramdochtmlindexhtml   2  cgal  httpwwwcgalorg   3  tetgen  httpwias  berlinde  software  tetgen   4  httpsjenkinsio   5  httpvalgrindorg   6  httpshalinriafrhal01225202 
__label__math __label__fuzzy-logic a search on fuzzy logic lead PRON to a zadeh function in the context of  synthesis of fuzzy logic function give in tabular form    the following expression be define as a row of a choice table of a two argument function   x₁ ≤ x₂ ≤ x̄₂ ≤ x̄₁  x₂  wolfram tell PRON that the macron may connote the mean of a set  a complex conjugate  the complement of a set  the order type of a set  or absolute value   similarly  a colon have multiple meaning   q  what be the meaning of the macron and colon in this expression   PRON initially ask this on mathematic  but no luck   this be just a formula PRON stumble across and PRON look useful in relation to what PRON be work on  which involve abstraction in relation to optimal decision making in a condition of intractability   PRON be really just try to get a sense of what the expression  say  and PRON figure PRON may have more luck on ai  where folk likely have more context  experience on this particular field   
__label__neural-network the cost vs iteration graph while use the relu activation function have number of spike  the accuracy on mnist dataset  both train and test  be around 95   also on use sigmoid as an activation function PRON get a smooth downward sloping curve so PRON think the implementation be correct  be these spike expect for relu  how would PRON explain this property   when PRON run PRON implementation of mnist digit recognition PRON even get those spike for a sigmoid transfer function  PRON have to question PRON  how bad be those spike  PRON presume PRON use stochastic gradient descent right  for every iteration PRON consider a batch  a subspace of the sample  and train on that  some of those batch contain very difficult picture and PRON network will fail at PRON  result in a high cost  this in turn will lead to a temporary high adaptation of PRON weight and bias  PRON see that PRON drop again quite quickly because the other sample will again send PRON in the right direction   PRON think PRON have more spike because PRON relu be just good at generalization and thus after a while PRON will get more peak  but low  because the network get good and good  and in the end the most important criterion be PRON accuracy and not the cost   this have less problem with the spike because PRON take into account all the sample instead of a batch   concluding  stochastic gradient descent will go to a low point  but because of PRON random walk PRON may bump into some peak  but in the end PRON will find a nice low cost   if PRON still have some question  feel free to ask  PRON be still quite new here and still have to learn how to give good answer  
__label__finite-element __label__adaptive-mesh-refinement __label__spectral-method the context  PRON be work with a spectral fe  high order interpolation at gll node  code on conform hexahedral mesh  and PRON pi be interested in improve mesh quality  possibly with adaptive refinement  however  the only local refinement scheme PRON know of for hex involve nonconform mesh  hanging node  which do not tend to work so well for spectral fe method   the conform mesh refinement scheme PRON be familiar with involve essentially splitting a 2x2 set of quad into a 3x3 set  or equivalently  split the middle node of the 2x2 set of quad into 9 new node in 2d   as below   PRON only know of conform adaptivity use some mesh metric  be there a way to use instead local error indicator to drive adaptivity in a conform manner   in practice  everyone who use quad or hex use non  conform mesh refinement because the conform method be just too unwieldy  the only alternative PRON can think of would be that PRON start with a triangular  tetrahedral  mesh and get PRON quad  hex  mesh by subdivide the triangle  tet   then  when PRON know which quad PRON want to refine  PRON go back to the triangular mesh  refine PRON in some conform way into another triangular mesh  and get PRON next quad mesh by subdivide the refined triangle  same for tet    for the how part refer to in the previous answer  conform quad or hex mesh refinement be most likely go to use an algorithm base on the work of r schneiders  2 and 3 refinement algorithm  these method be use in mesh generation  two paper that PRON happen to have that do adaptive conform quad refinement be    a new fast hybrid adaptive grid generation technique for arbitrary two  dimensional domain   by ebeida et al  int  j num  meth  eng  84305  329  2010  who use 2refinement algorithm and show time dependent example   and a preprint   conformal refinement of unstructured quadrilateral meshes  by garimella  who use 3refinement and whose source PRON do not remember   both of these be 2d  but schneider present 3d algorithm  too    the question refer to apply a conform algorithm use some local error estimate  the paper PRON cite show adaptive  local  hrefinement  but do not say much about the decision of when to subdivide and coalesce  for the spectral approximation  one approach would be to use an element  local interpolation estimate like cathy mavriplis do in PRON dissertation  or an adjoint base estimate like that use by matt willyard in PRON dissertation  these would indicate that an element need to be refine  PRON could then apply the conform h  adaptation as in the paper above to satisfy the prediction that the estimate provide   the spectral algorithm  however  will prefer to increase the order over subdivide element if the solution be smooth  that will lead to  functionally  non  conform mesh locally  if that be the question that be really be ask  then no  PRON do not see a way of do conform order refinement   PRON can always choose which element to refine by compute a local error estimator  indicator for each cell  pick a cutoff value  and refine every cell beyond the cutoff  this be independent of whether the refinement be conform or non  conform  if PRON be use a conform refinement strategy that work on block of cell  2x2 to a 3x3 as PRON mention   then PRON can choose to refine a 2x2 block of cell if any of the individual cell need refinement  if all of the cell need refinement  or some combination in between  PRON can also try project the cell value onto the node and refine node instead   the bookkeeping be more complicated  but PRON can also make a conform refinement in 3d use basically the same strategy  ie  PRON can make a 2x2x2 block of cell into a 3x3x3 block   the real trouble with conform refinement strategy be that PRON have these topologically odd element insert with potentially lousy shape property  the question then be what to do when refine one of those element  do PRON just follow the same rule  or do PRON try to do some trick to help maintain the shape regularity  non  conform refinement do not have these problem   finally  PRON may be commonplace to refer to PRON as non  conform refinement  but most hang node refinement that PRON know of  use constraint along the element interface to force the refined element into a conform space  PRON look like non  conform element  but for continuous galerkin scheme  the underlie element do conform  
__label__open-source introduction  let PRON first state some conflicting assertion of the matter to illustrate what be the issue   personally PRON would like to have PRON code open at every stage of development  since  other shall see and take advantage of what PRON be do  PRON also like to reuse exist code  third party can contribute  the public be fund PRON  so the public have the right to see  but PRON boss say  PRON need to approve what become public under PRON name  or the institute s  and PRON can not approve every single step  there be guideline refer to intellectual right property of the institute  and PRON colleague say  other will come and steal PRON unpublished idea  PRON experimental code be of little use for other  questions  to come up with a blueprint for code publication and open source development in PRON lab  PRON want to raise the follow question   be there already such a guideline  cover the important issue of open software in academia   which issue have to include into such a guideline   what do PRON think be the right way to implement and use such a guideline   remarks  the issue of crediting  reproducability  code documentation  and where to publish  PRON want to address in a separate guideline   PRON have collect the legal and personal concern of colleague and the head of the department and compile a form that  define the form  content  and scope of software publication  name the developer  address compete interest within the lab  can be sign by the head of the department  to grant a general but well define approval for software publication both in final version or as open source development   please see this gitlab page for a download of the document and the tex source  
__label__deep-network __label__ai-design __label__strong-ai __label__ethics __label__intelligent-agent ethic be define be define to be the set of moral principle that govern a person s or group s behavior  innately  should not any system devise ethic for PRON  most of PRON articulation  in either direct or indirect manner  talk about intelligent systems in context of human presence  why  in first place  be PRON study agi ethics   give roseau s reasoning for a society   human being can improve only when PRON leave the state of nature and  enter a civil society   do not that automatically apply to any intelligent system  also   if PRON do  then can not PRON imply that any inorganic intelligent system should  perhaps  come as an offset of a network of intelligent computer  a society per se    who be PRON to constitute ethic and rule  morale for another society which  perhaps  could be much more intelligent than PRON  in fact  human have prove PRON to be idiotic time and again for a task such as this   nonhuman ethic can be totally arbitrary  regard PRON citation  perhaps PRON be useful to consider why should anything that philosopher have say about human ethic apply for arbitrary nonhuman system  PRON all share a common biological background  PRON behavior  emotion and especially positive  negative feeling arise from the particular way how PRON brain be build  man be by nature a social animal  and much of PRON behavior and feeling of empathy  compassion  fairness  greed  social status  etc be form and express in society  PRON mind be adjust for this  as be the mind of other mammal  a human that form an ethical system when join a civil society do so in the basis of all this share context  and within the limitation of PRON   however  all this be orthogonal to intelligence  all the variety of mankind encompass a tiny island in the whole space of possible intelligence and PRON goal  any system of ethic be feasible  a vast majority of PRON have no relationship whatsoever to what PRON could consider ethical  a human can be expect to join a society and learn ethic because PRON brain be hardwir to respond to social stimulus  an ai would learn ethic from a society only if PRON previous ethic already happen to value that highly  and that be not likely to happen without careful design   there be a thought experiment that be commonly use as an example  the  paperclip maximizer   an artificial agent with an ethical system that define  good  proportionally to the number of paperclip that exist in the universe and only that  and that be a perfectly reasonable example  because a completely random ethical system be overwhelmingly likely to reduce to something stupid like that  the space of  reasonable   to PRON  ethical system be very tiny compare to all ethical system possible  and expect that a randomly select ethical system will just happen to land there be like rely on win a high  stake lottery   PRON want PRON ethic to contain certain thing  so the main reason why PRON be study agi ethic be because PRON  or some of PRON  really  really  want the agi ethical system to include certain thing  for example  PRON would like that ethical system to include PRON survival  PRON would prefer that ethical system to be consistent with PRON happiness and wellbeing  and the very existence of a very powerful entity that simply happen to not care about PRON at all be not compatible with PRON survival and wellbeing  so PRON would really prefer for PRON to care about PRON  or alternatively never become very powerful   one could certainly argue that PRON have no right to constitute ethic  rule and morale for another society which could be much more intelligent and good than PRON  that another society may be  good  in some manner  but good for whom  if PRON be good for  PRON  but bad for PRON  and humanity   then PRON have strong motivation to simply claim that right and try PRON hard to ensure that this hypothetical future society include PRON  PRON desire and PRON ethic  
__label__python __label__confusion-matrix PRON use two different classifier to predict a binary target  random forests and decision trees   now PRON want to evaluate PRON model create a confusion matrix  for example  for predict the binary value use random forest PRON have   trainingfeature  testfeatur  trainingtarget  testtarget   traintestsplitdfdropscoregoal    axis1    dfscoregoal     testsize  3   randomstate12   clfrf  randomforestclassifiernestimators25  randomstate12   clfrffittrainingfeature  trainingtarget   printaccuracy use random forest classifier be   clfrfscoretestfeature  testtarget100   PRON be confusing because PRON do not know how PRON can compare the predict value to identify how many false positives  etc  PRON have   anyone know how can PRON build that function   thank   look like PRON be use scikit  learn  so why not explore a bit more  scikit have a metric module  that can be of use for PRON problem  essentially  what PRON need be to have two separate array  one with real label and another with predict label  and then PRON be good to go  PRON could call metricsclassificationreport or metricsconfusionmatrix or metricsaccuracyscore  all of PRON use the real label and the predict label   there be nothing wrong with use clfrfscoretestfeature  testtarget   but PRON will only give PRON a single value  if PRON look at the source code  what happen be that the score method call a predict method with testfeature for prediction of label  which occur behind the scene   PRON be good to actually capture those predict label  so that PRON can reuse PRON   clfrffittrainingfeature  trainingtarget   predictedtarget  clfrfpredicttestfeature   accuracy  sklearnmetricsaccuracyscoretesttarget  predictedtarget   cnfmatrix  sklearnmetricsconfusionmatrixtesttarget  predictedtarget   classreport  sklearnmetricsclassificationreporttesttarget  predictedtarget   and then PRON could do whichev PRON like with the calculated metric  print  plot  etc   have a look at the example that be include for each model  metric PRON be use 1  2  3  4  etc  
__label__python __label__dataset __label__image-classification __label__mnist PRON come across this statement from cntk 103a   gzfname  h  urlretrievesrc   deleteme    PRON understand all the rest but that gzfname  h    what exactly be the purpose of h  be PRON a filehead thing   code   def loadordownloadmnistfilesfilename  numsample  localdatadir    gzfname  h  urlretrievesrc   deleteme    if  localdatadir    localpath  ospathjoinlocaldatadir  filename   else   localpath  ospathjoinosgetcwd    filename   if ospathexistslocalpath    gzfname  localpath  else   localdatadir  ospathdirnamelocalpath   if not ospathexistslocaldatadir    osmakedirslocaldatadir   filename   httpyannlecuncomexdbmnist   filename  print   download from   filename  end    gzfname  h  urlretrievefilename  localpath   print    done     return gzfname  gzfname  h be do tuple unpacking from the return of urlretrieve   from the doc  urlretrieve return   return a tuple  filename  header  where filename be the local file name under which the object can be find  and header be whatev the info   method of the object return by urlopen   return  for a remote object   exception be the same as for urlopen     and the info   method of the urlopen object return   info    return the meta  information of the page  such as header  in the form of an emailmessagefromstring   instance  see quick reference to http headers   so  h be meta information about the retrieve page  
__label__research __label__multi-agent-systems __label__real-time PRON have get curious about this topic and be wonder what the stack exchange community have to say about PRON  also  do anyone know of any professor  researcher who have publish paper pertain to this   a couple of thought   human can not reliably predict trend in the stock market  so expect ai s to do so be probably unreasonable   the above would be more true if PRON be prove that the movement of stock price be really a random walk  but PRON understanding be that the current thinking be that stock movement be not completely random  but just really close to random   if there be some trend there that represent a useful signal  and if somebody have find that  PRON be very unlikely to share that information  as the market would then immediately price that information in and PRON would lose PRON edge   this be a highly relevant question as market trend have become more emphasize over the fundamental of individual company  and algorithmic trading have prove to be quite effective  particularly in area such as high  frequency micro  trading   this 2013 forbes article estimate nearly 80  of stock trading volume in the us be conduct by automate trading system   more recently  bloomberg publish an article on the subject  the us stock market belongs to bots   the massive adoption of algorithmic trading in a fairly short span be a pretty good indicator that these system be effective   however   PRON feeling with any predictive system be that PRON work until PRON do not  imperfect and incomplete information be persistent in real  world scenario   the implication be that there be always unknown factor that can diminish quality of analysis  and even lead to disastrous outcome   no matter how complex an algorithm become  reality be more complex   at least for the foreseeable  pre  singularity future    add to that  system in general tend to be imperfect  and when engage in massively repetitive function  small error can grow exponentially and produce catastrophic effect    flash crash  be a recent phenomenon  and a result of algorithmic  high  frequency  black  box trading   there be quite some research do by hans  georg zimmermann  who have program neural networks for siemens since some 20 year in order to predict stock market  PRON write some book on PRON  too  though PRON do not know if PRON be any good in english   this article get to the point a bit faster than the video  PRON hope PRON help   edit  PRON think this interview give a good short introduction to PRON work  
__label__python __label__parallel-computing __label__opencl __label__memory-management PRON have write some multithread code use pyopencl  which work fine under the following condition   gpu 32bit float point value  intel iris gpu can only handle single precision value   cpu 32bit float point value   all datum buffer be define  and the program execute and output datum enqueu  all inside a function which be call within a loop  when use 64bit double  precision value  the loop run twice before throw the error  segmentation fault  11    therefore  PRON do not believe PRON be due to the string  manipulation involve in prepare the opencl kernel  more likely seem to be something to do with allocate memory   interestingly  the cpu maxworkgroup be 1024  and gpu maxworkgroup be 512   what possible cause could there be for the success use single  precision value  but failure when use double  precision value   PRON be impossible to tell without know the code PRON be use  but fortunately  segmentation fault be easy to debug  basically  a segfault mean that PRON be access memory PRON should not access  and the operating system stop PRON program at the point where this be happen  this mean  that if PRON run PRON program under a debugger  then PRON will see exactly which line be cause the problem  PRON can inspect the line where PRON happen  the local variable and PRON value at this point  and the backtrace  ie  which function call the one where the problem happen  which function call that function  etc    so  run PRON program in a debugger and see what concretely happen  everything else be essentially just speculation that may or may not be correct  
__label__text-mining PRON have dataset of post from blog and for each post PRON have the number of view   PRON want to extract the topic  or phrase  that make the post with more view   PRON be plan divide all post in two set base on number of view  one set with low number of view and the other with high number   then extract topic use lda from each set and compare how PRON differ   PRON be wonder if this be right approach and if there be other approach that can be good or similar   seem right  however  establish causality will not be as simple as extract keyword and notice the difference  and PRON would suggest not to divide the post  instead club PRON together run lda  extract the keyword  then analyse the difference  by separate PRON be introduce quite a huge bias into PRON model   instead of diving into lda directly  PRON would be rather start with simple one like tf  idf and see whether PRON can extract keyword from each class  blog  recently PRON get into this kind of problem where PRON need to extract topic out of tweet and PRON get fruitful result with tf  idf be a part of PRON method   PRON would treat each blog as individual data point rather than merge PRON  so that document can be club base on similarity of word obtain and then extract the topic out of PRON  at the end PRON can use view to see the average view each topic have get   well PRON get tool like lsa which construct a matrix base on word count  this matrix be reduce by svd which can be computationally time take on matrix of huge size   so before try any of the big method  do try simple one and if the result be unsatisfactory  approach other method   hope PRON help  
__label__pde __label__eigenvalues __label__elliptic-pde do PRON know any method  solver for pde or eigenvalue problem like   begincas  delta u 0  text  or  lambda u   amp  text  in  omega  u  0  amp  text  on  partial omega endcases  dirichlet    begincas  delta u 0  text  or  lambda u   amp  text  in  omega  fracpartial upartial n   0  amp  text  on  partial omega endcases   neumann    begincas  delta u 0  text  or  lambda u   amp  text  in  omega  fracpartial upartial nbeta u  0  amp  text  on  partial omega endcases   robin   in 2d and 3d that do not involve the finite element method   in 2d there be at least an option  for dirichlet and neumann   mpspack which use a basis for the space of harmonic function and work only with coefficient in this basis  in 3d PRON do not know anything that do not use finite element method   PRON be interested in a different method than fem because in shape optimization domain change at every iteration and so do the mesh  the discretization  etc   
__label__python __label__nlp __label__text-mining __label__data-cleaning background  PRON work for a major airline in the united states and have be give a pretty significant text mining and machine learn task  since there be several unique term  acronym  and abbreviation in aviation  PRON have write a program that read through the document  the program be use 4 group of term  word  stops  airports  and synonyms  word  stops  and airports be all set where the first two be initialize from nltk and the last be initialize with a manually build list of airport that PRON employer service  synonyms be a nest dictionary  trie  style structure where the branch be misspelling and the leaf be the term that PRON represent  PRON be initialize as an empty dictionary  PRON have also create a command prompt ui that inform PRON when PRON find a word that PRON do not know and allow PRON to correct the word or add PRON to one of the aforementioned group  the document be be tokeniz by spacy and feed into scikit  learn machine learn algorithm   problem  PRON have start notice that some of the word that PRON previously accept in word or synonyms be show up with alternate meaning and could have a drastic impact on how well the algorithm operate  for example the word act can be the verb act  or PRON can be an abbreviation for actuator or active  or PRON can be an acronym for auxiliary central tank or a couple other thing  at present  PRON be use the ui to manually correct or ignore these occurrence  which be time consume  cumbersome  and prone to error   desire result  what PRON be look for be a method of determine whether  lt  mean leave or light depend on context  lt wing vs warn lt  with minimal manual entry  at a minimum PRON would like some high level idea  discussion on how this may be accomplish in python  optimally  PRON would like some example code  preferably use the same tool PRON already have  spacy  nltk  and sklearn but PRON be flexible and willing to learn   note  PRON have consider a couple method of do this but PRON have not be able to come up with any idea strong enough to try  for example  PRON have consider mark the term with the pos inside the set by concatenate PRON with the word before add PRON like  rat  noun   the issue with this be that rat  noun could be an animal or PRON could be an abbreviation for ram air turbine  which be also a noun   after rubber duck with a friend of mine  PRON decide to take a two stage approach   first  whenever PRON see a word PRON have recognize as a multiple use word  PRON flag PRON in the system and give PRON the appropriate correction  once PRON evaluate that part  PRON store the corrected word and enough word such that there be sufficient context in a tuple  inside a list  inside a dictionary  PRON all look something like usecase    abbr     five word abbr context string    correction     secondly  PRON have create a dictionary of scikit  learn linearsvcs that look like svmdict    abbr   linearsvc  once these thing be establish and the program be run again  when PRON find one of the multiple usage word PRON first check usecase to see if that exact set of 5 word surround this case  if PRON do PRON swap PRON out for the original correction  otherwise PRON try to predict the usage with the appropriate svc and ask the user if the prediction be correct   PRON know PRON be not fully automate  but neither be the program in which PRON run  PRON figure the occasional yes  no question to the user be easy than leave PRON off the list and rewrite PRON every  single  time   either way  PRON have PRON all in place but have not get to the point where PRON be be use yet  so  PRON will either be edit  delete  or accept this answer sometime next week when PRON have have some time to testr out  
__label__hyperbolic-pde __label__parabolic-pde the theorem state that if a difference scheme converge then PRON necessarily satisfy the cfl condition  how can this be prove   
__label__machine-learning __label__predictive-modeling __label__marketing what would be appropriate model  algorithm  strategy for predict good individual send time for marketing campaign base on past response timestamp   datum   give for example                                                              customer campaign  campaigntime  responsetime    1  100  a  2017  01  01 065001 2017  01  01 080221  2  101  a  2017  01  01 065001 2017  01  01 164531  3  101  a  2017  01  01 065001 2017  01  02 072000  4  100  b  2017  01  07 063021 2017  01  08 081521  5  101  b  2017  01  07 063021 2017  01  07 170012  6  100  c  2017  01  14 064355 2017  01  14 075944  7  101  d  2017  01  21 140201 2017  01  21 165001    two customer 100amp101   four past campaign a  d   with each campaign have different time of dispatch   and multiple  one or no response times   eg buy a product  for customer and campaign  goal   assume that  campaigntime can vary for 100 and 101  personalize time  of dispatch   and  past response time be an indicator for when  customer be most receptive for a campaign  PRON would like to predict the good next campaigntime  2017  01  28          for each customer base on past responsetime  so that the number of respondent per campaign be maximize   anyone have any experience with something similar or any idea where to start  PRON would be happy to hear some idea   to simplify thing  PRON would consider the first responsetime the most valuable one   should be predict  and PRON would also abstract from weekday   PRON be about predict time 000  2359  mark by the  above   however PRON would be nice to have a continous prediction instead of a discretiz one  like suggest here    PRON may suggest a change to PRON theoretical setup   PRON sound like PRON be try to maximize an equation ft   where t be the time of dispatch  and ft  be the risk adjust chance of purchasing  as PRON note PRON can be 0many    in that case  PRON be likely good to use a setup like this                                                               customer campaign  campaigntime  totalresponseswithin1week    1  100  a  2017  01  01 065001  0  2  101  a  2017  01  01 065001  3  3  101  a  2017  01  01 065001  2  4  100  b  2017  01  07 063021  1  5  101  b  2017  01  07 063021  1  6  100  c  2017  01  14 064355  1  7  101  d  2017  01  21 140201  1    unless PRON have specific internal datum or journal research that directly indicate a relationship between speed of response and likelihood  number of purchase  PRON may avoid include PRON   now  if PRON need to estimate time of response for issue like labor or inventory  PRON could use a probabilistic model  but PRON will avoid make specific recommendation there  as PRON relative strength be in regression model  PRON would want the function to output a probabilistic curve over a give time period  and then sum all of those curve together  eg to estimate that during weds morning  PRON will have 80  109 order  so need 5 customer service rep    if PRON goal be purely response frequency  within a relevant time   time of response do not need to be model  however   as there be important question to the scenario and datum  PRON be share some thought together with assume answer to some question rather than a complete solution   first of all  in sample datum there be not clear how case when user have not respond be model andor whether each campaign have be send to all user  which in real live be never the case as user base be change over the time   but PRON can safely assume that there be this information give  either all campaign have be send to all user  for simplicity or PRON now which user receive with campaign    then PRON can be easily imagine that some campaign can be better suit to be send earlier in the day  eg b2b service  during work hour  and some  rather in the evening  eg trip last minute offer  this can heavily influence the response rate for many user and PRON do not have any information about the campaign PRON  no feature to try to model the content of the campaign and PRON be impact on responsiveness  if PRON now that the same type of campaign have be send always but at various time to various user  that could create a global  audience  wide  trend in responsive rate in different time of the day   second aspect that influence responsiveness globally be the quality of the campaign  how PRON be appeal to the user  that could be partially infer observe responsiveness of user when PRON receive various campaign at the same time of the day   that pose a question what be the distribution of the time of the day each campaign separately and in comparison between PRON  whether there be enough datum to try to infer that information   assume for starter that these be similar campaign regard the quality and PRON be time  of  the  day agnostic  PRON can focus on each user separately   if PRON look to a distribution of number of response over the time of the day  eg average by the number of campaign send in the give point of time   PRON can spot the maximum there  which be the good one  shot candidate for the next campaign   this approach however have follow caveat   for the long term  PRON will create a  time point bubble  for the user  try to always send PRON on this time point  that would not reflect the change of the user preference   here PRON could apply move average  eg over fix time window  like  last 3 month   andor try other datum point from time to time and probably other technique to gain more diverse and future  proof strategy    the maximum time point may have be an exceptional behaviour while there be much more evidence for a bit small number of response but support by more time point in other time range   to take into account various quality of campaign  PRON rating could be create by compare responsiveness of user to which more than one campaign have be send in the same time point  then weigh number of response by inverted rating of the campaign on the distribution of number of response for the give user   datum   give for example                                                              customer campaign  campaigntime  responsetime    1  100  a  2017  01  01 065001 2017  01  01 080221  2  101  a  2017  01  01 065001 2017  01  01 164531  3  101  a  2017  01  01 065001 2017  01  02 072000  4  100  b  2017  01  07 063021 2017  01  08 081521  5  101  b  2017  01  07 063021 2017  01  07 170012  6  100  c  2017  01  14 064355 2017  01  14 075944  7  101  d  2017  01  21 140201 2017  01  21 165001    as other have already answer  i will like to add to PRON few point   what about feature engineering  PRON can add a lot of other feature to the give data  set   week differences   day  month etc   time differences as well   and then create group by view of the data  set as well  after that PRON can use a rf to help use see what be the feature importance of different attribute  column etc   after that PRON can also go for dendogramshierarchial clustering  to see what column be irrelevant   and the process continue with different model  do eda s etc  
__label__convex-optimization PRON have an optimization problem with the follow objective function    maxaln  k  bln  k    sumn1overlinenl   bk  n  fracc1c2  log2 bigg1  fracak  n  hk  n    bn  k  c3bigg    the constraint be linear   the objective be concave  if PRON keep all the constant as 1  for simplicity the objective function be    f   b cdot log21a  b  which be concave right  or do PRON depend on the actual value of the constant   also if PRON add another parameter in the denominator of the log term as    maxaln  k  bln  k    sumn1overlinenl   bk  n  fracc1c2  log2 bigg1  fracak  n  hk  n    bn  k  c3  xbigg   do PRON still remain concave    f b⋅ log  1fracab1 be not concave right  or do PRON depend on the value of x and other constantswhich PRON keep one     blog1fracab be concave  PRON be the perspective of the concave function  log1a the function  blog1fraca1b be not concave  PRON can compute hessian to prove if PRON want  or pick two suitable point and check if the point in between have a low function value than the average of the function value at the end point  test the two end point   a  b001001 and   a  b    11 for instance   PRON just plot the function and see that PRON be neither convex nor concave  try simple thing first   
__label__finite-element PRON know that long back dr leszek demkowicz finite element codes1dhp2dhp3dhp  be available in PRON website  PRON be find PRON difficult to locate PRON now  be there any alternative available to these code in fortran7790  only   PRON think the code be also distribute with PRON book  but PRON may just want to write PRON an email   dr demkowicz s code be pretty unique  so PRON be not go to find a complete alternative  in any language   if PRON be just look for a fortran code  PRON can get feappv  PRON put this wikipedia page together a long time ago  and someone liste elmer   PRON have no experience use PRON   PRON have these code on PRON computer  file  tar1dtgz  tar2dtgz  and tar3dtgz   PRON download PRON from PRON website a year ago  or so   PRON just like collect code  PRON be also look for some  if someone could help PRON find orszag s centikube PRON would be great    PRON may contact PRON and PRON will forward PRON to PRON   prof  demkowicz have ask PRON to post the following reply  PRON have slightly edit PRON so that fit good with the site s format    1d and 2d code come with the first volume of PRON book  PRON can e  mail both code follow a personal request but PRON be impossible to read PRON without PRON book   the  old  3d code have be describe in the second volume of PRON book  PRON be still in use by PRON colleague in cracow include dr rachowicz  please  contact PRON directly for a work copy of that code  again  PRON be impossible to use that code without read the book first   PRON have a new  very powerful 3dhp code  develop by current student  that PRON be accessible to PRON collaborator  interested party should get in touch with PRON directly  
__label__r __label__xgboost __label__kaggle accord to a benchmark of gbm vs xgboost vs lightgbm  httpswwwkagglecomnschneidergbmvsxgboostvslightgbm  PRON be possible to implenet xgboost with the argument  treemethod   hist   in r  however do so give PRON always an error   error in xgbiterupdatebsthandle  dtrain  iteration  1  obj    invalid input   hist   valid value be    approx    auto    exact    what be PRON miss   the fast hist mode be available in new version of xgboost  PRON can find infos on httpsgithubcomdmlcxgboost  for small datum set  100k line   PRON will not feel much gain in training speed  
__label__molecular-dynamics __label__computational-biology __label__biophysics __label__electromagnetics __label__pdb PRON have a pdb file and PRON would like to make a pqr one  in gromacs  PRON use editconf mead to produce the pqr file  with topoltop take from pdb2gmx and use PRON to produce pdb file  then use pdb2pqr  which one be the good   first 10 line of pqr make from gromacs  atom  1 n  pro a  1  063000  3963000  1725000 02020  16250  atom  2 h1  pro a  1  137000  3993000  1789000  03120  05345  atom  3 h2  pro a  1  024000  4048000  1688000  03120  05345  atom  4 cd  pro a  1  121000  3879000  1618000 00120  16998  atom  5 hd1  pro a  1  230000  3876000  1629000  01000  09800  atom  6 hd2  pro a  1  096000  3921000  1520000  01000  09800  atom  7 cg  pro a  1  063000  3739000  1630000 01210  16998  atom  8 hg1  pro a  1  128000  3680000  1695000  01000  13248  atom  9 hg2  pro a  1  052000  3690000  1533000  01000  13248  atom  10 cb  pro a  1  072000  3764000  1699000 01150  16998  first 10 line of pqr make from pqd2pqr   atom  1  n  pro  1  0630  39630  17250 02020 18240  atom  2  h  pro  1  1370  39930  17890  03120 06000  atom  3  h2  pro  1  0240  40480  16880  03120 06000  atom  4  cd  pro  1  1210  38790  16180 00120 19080  atom  5  hd3 pro  1  2300  38760  16290  01000 11000  atom  6  hd2 pro  1  0960  39210  15200  01000 11000  atom  7  cg  pro  1  0630  37390  16300 01210 19080  atom  8  hg3 pro  1  1280  36800  16950  01000 14870  atom  9  hg2 pro  1  0520  36900  15330  01000 14870  atom  10  cb  pro  1  0720  37640  16990 01150 19080  these be from 3b7v  a hiv1 protease  PRON have be minimize energy   the van der walls radius  last column in the output   be calculate from the force field  probably pdb2pqr and editconf use different force field  hence different radius  PRON do not use pdb2pqr  but PRON seem  1  amber99 be the default force field  though charmm  parse and tyl06 be support  gromacs  editconf  read force field from topology file generate by pdb2gmx  2   in PRON system pdb2gmx default to amber03  try  pdb2gmx ff select  which one be good depend on PRON problem  be PRON generate input to apbs  if so  PRON would use pdb2pqr   ref    1  httpwwwpoissonboltzmannorgpdb2pqruserguideusingpdb2pqr   2  httpmanualgromacsorgonlinepdb2gmxhtml 
__label__ai-design  would ai be a self  propogat iteration in which the previous ai be  destroy by a more optimise ai child   would the ai have branch of PRON be own ai warn not to create the new ai   a common concept in ai be  recursive self  improvement   that is  the ai 10 would build a version 101  which would build a version 102  and so on   this be probably not go to be think of as the new version  destroy  the old version  if an ai can self  modify  PRON be probably go to be more like go to sleep and wake up smarter  or learn a new mental technique  or so on   one important point be that even if the ai be not allow to self  modify  maybe because of a block put in by PRON programmer  that will not necessarily prevent PRON from construct another ai out in the wild  and so an important problem be to figure out how to best generalize the concept of  do not improve PRON  so that PRON can make ai that have bound scope and impact   honestly  nobody know   any talk of sentient ai s be still basically sci  fi and PRON can not really offer anything more than informed speculation   but think about PRON this way   sentience  in and of PRON  do not necessarily involve any  goal  or  desire  or  objective  beyond what the ai creator program in   be careful not to over anthropomorphize and assume that any  sentient ai  be go behave like a human   in other word  there be no particular reason to say that any give ai must be  a self  propogat iteration in which the previous ai be destroy by a more optimise ai child    so all of that say  PRON answer to  would a sentient ai try to create a more optimise ai which would eventually overtake ai 10  be    if the creator of the ai program PRON to do that  then yes  otherwise  probably not    so would a hypothetical ai creator program the ai to try and improve PRON   who know  PRON be the kind of thing that seem like PRON may be a good idea   and PRON suppose such a motive could  in principle  even slip in by accident   now in most case PRON still have clear distinction between program and datum  but when an ai become sentient  PRON datum would be as powerful as what PRON currently call program  and PRON program may be as irrelevant as what PRON currently call hardware  then PRON would be difficult to distinguish create an ai from learn new thing  or buy new hardware with improved instruction set   for example  if some ai invent new algorithm that PRON creator finally put that on PRON  buy PRON some new computer  write a new efficient compiler that recompil PRON own code and put that to the new computer  fill the new computer with all the knowledge PRON learn  and cut off the communication for reason such as mission on the mars  do PRON create a more optimize ai   in contrast  if some ai create something completely new  but share some code with PRON  in fact  that be because PRON run in the same operating system and share the same standard c library  be the new ai consider evolve from PRON and not a separate entity  maybe the core ai algorithm and even some basic knowledge would be as common as the standard c library in the future  and what PRON think be base on the same system be consider completely new in the future   anyway  human have limit and nonextensible resource  nontransferrable knowledge  and limit throughput interact with the world  these problem could probably be overcome within a few ai generation  with the same hardware  PRON doubt that the ai relate algorithm could be indefinitely good and good  and there be a physical bind on the hardware  PRON will not last long even if that happen   in the unlikely case that there could be that many generation and ai be that violent  as long as there be competitor  the warning do not make much sense consider how evolution work  
__label__optimization PRON be search for a strategy to optimize such a function   PRON have a function which need several minute to be evaluate  and this function depend on 20 parameter that need to be optimize   since PRON need to rerun the optimization process every few day  PRON define the function as costly   more  small change  say  less than 5   in the parameter can have no result in the function output  that be why PRON define PRON as  step    the good part be that PRON have an initial set of parameter  and PRON can estimate the boundary where PRON can move   PRON google around  read some paper  but find nothing appropriate for PRON case   do PRON have any suggestion   thank PRON   
__label__machine-learning __label__python __label__scikit-learn __label__pandas __label__parameter many scikit  learn and pandas object  function use randomstate  none as a default parameter  how can PRON be overridden to randomstate100 by default for all object without manually edit the randomstate for each object   makeblobsnsamples100  nfeatures2  centers3  clusterstd10   centerbox100  100   shuffle  true  randomstate  none   for scikit  learn can set nprandomseed1   for example  and as long as nothing in PRON script be modify the seed nondeterministically then PRON should get reproducible result   this be describe in the scikit  learn faq under how do PRON set a randomstate for an entire execution   however  PRON do not believe PRON be possible to do the same thing for panda  see here for discussion  
__label__neural-networks __label__image-recognition __label__convolutional-neural-networks do anyone know  or can PRON deduce or infer with high probability from PRON characteristic  whether the neural network use on this site  httpsquickdrawwithgooglecom  be a type of convolutional neural network  cnn    PRON believe PRON do not use cnn  the most important reason why PRON be because PRON have more information than a regular image  time  the input PRON receive be a sequence of  x  y  t  as PRON draw on the screen  which PRON refer as  ink   this give PRON the construction of the image for free  which a cnn would have to deduce by PRON   PRON try two approach  PRON currently most successful approach do the follow   detect part of the ink that be candidate of be a character  use a feedforward neural network to do character recognition on those candidate  use beam search and a language model to find most the most likely combination of result that result into a word  PRON second approach be use an lstm  a type of recurrent neural network  end  to  end  in PRON paper PRON say this be good in a couple language   source  PRON be an intern in google s handwriting team in summer 2015  on which PRON believe quickdraw be base   but the technique PRON explain can be find in this paper  
__label__pde __label__finite-difference __label__numerical-analysis __label__nonlinear-equations __label__stability PRON be try to solve a nonlinear partial differential equation  beginequation   luxxttuxxuttuxt2uxtutt0  endequation   use finite difference method  in order to remove the second derivative with respect to time  PRON substitute  beginequation   v   ut  endequation   such that PRON become  beginequation   lvxxtuxxvtvx2vxvt    0  endequation   so in PRON algorithm  PRON first implicitly calculate  v at the next time step use central difference approximation  in order to ensure that local truncation error be second order both in time and space   ie  beginalign    vimxx   amp    vmi1   2 vim  vmi1delta x2     vimx  amp    vmi1vmi1    2delta x    vimt  amp    vim1   vim12 delta t  endalign   once PRON have calculate  vim1  PRON calculate  uim1 as follow  beginequation   vim   uimt  fracum1i  uim12 delta t  implie um1   2 delta t vim  uim1  tag1   endequation   PRON would like to determine the stability of this program  and any tip on how to do that be greatly appreciate   PRON first want to determine the stability use von neumann s method  but PRON do not think this be possible  but please correct PRON if PRON be wrong   let PRON explain   base on the answer give to PRON previous question  PRON have decide to  freeze   uxx and  vx in order to linearize the pde  beginequation   lvxxtvxvt    0  endequation   however  if PRON substitute equation   1 into PRON finite difference operator  then PRON will get term involve  um2   um1   um  um1   um2 thus  if PRON now substitute in the following  as per usual for the von neumann stability analysis   beginequation   uim  am exphati  k i delta x       hati sqrt1   endequation   then PRON will end up with a fifth order polynomial in  a  beginequation   aa4  ba3  ca2  da  e  0  endequation   this mean PRON will not be possible to solve for  a and thus PRON can not determine the amplification factor  hence  unless PRON have miss something  PRON be not possible to use the von neumann analysis to determine stability of the scheme   PRON can always try to find the root numerically   but in term of alternate way of analysis  if PRON be still refer to the same paper  then PRON show that there be an analytic solution to the equation  what PRON can try to do be substitute this solution in PRON numerical scheme and see whether PRON can do something with PRON  
__label__visualization __label__symbolic-computation look to introduce a mathematic program in PRON work  PRON would have 2 us   in a classroom  high school level   provide good visualisation of physics idea  and make PRON lecture more interactive  an example be the mathematica demonstration project  or simple interactive plot   symbolic computation  PRON work with noncommutative geometry  algebra  etc   PRON have very limited experience with mathematic program and zero with programming  PRON use ubuntu at work and a mac at home  PRON avoid the terminal most of the time  but PRON use PRON when PRON seem the only solution  with a lot of copy pasting   PRON use latex to write mathematic   mathematica seem amazing and easy  but if the school license seem a bit expensive with the current exchange rate  the one for research be out of the question   the jupyter notebook and the sagemath alternative seem nice  but PRON do not know python  not sure how complicated PRON be go to be implement PRON  and how much time consuming could be produce simple presentation   a few question then   be those PRON good alternative   be PRON good for PRON classroom like PRON explain   how difficult would be implement PRON compare with mathematica   sagemath seem to have a bunch of different language  should PRON focus in a specific one  like maxima  or r    sage would be perfect for classroom use  PRON once assist a professor with classroom demonstration of physics  and PRON work out pretty well   work with sage do not require advanced programming knowledge  student would be able to do PRON without break PRON head like in a usual programming class  PRON can get start with this tutorial or this book  the only thing in PRON opinion that make PRON  harder  than mathematica be the text  base input  in mathematica PRON have a toolbox from which PRON can click integral sign and so on  and PRON can pretty much give input the way write equation look  in sage  everything have to be do with text   intlimitsinftyinfty ex2  dx may be more intuitive than integralex2   x  oo  oo   otherwise program with sage be no more difficult than do PRON in mathematica   instead of focus on maxima or r or the other possible extension  PRON suggest that PRON work with the sage language PRON  these extension can only do one thing each  and PRON will only need to go for these extension in advanced case   finally  PRON do not know how good sage be go to be for research level work  PRON make figure for PRON first paper in sage  but have find that there be integral that mathematica can do but sage can not   have use mathematica  then try sage  and now sympy  symengine in julia  the clear winner be symenginejl  sage be hard to get work  be very slow  and PRON find PRON very hard to develop PRON own algorithm  which would get decent performance  that be the key  PRON can write algorithm for PRON  but PRON will not necessary be speedy    the syntax can be really hard to learn too   while PRON still enjoy mathematica s notebook  symenginejl have a lot go for PRON  even sympyjl use from julia be a very good experience  good than use in python    symenginejl be the c re  write of sympy   first of all  PRON be free  but most of all  PRON work as part of julia  and work naturally through dispatch  if PRON have expression a and b  then to make the expression ab  the code be  ab  everything just work via proper dispatch  PRON mean  everything  PRON do not even need to really know much about the package PRON  julia provide a generic fallback for inva  which take an inverse of a matrix  if PRON have a matrix of symbolic expression  then because symbolic expression have  and  define  by extension inva  just work  PRON can even do absurd thing with PRON like throw PRON in a numerical ode solver and PRON will just work like PRON be a number  and give out the symbolic expression for the solution after n step  so if PRON know julia and PRON base library  then PRON already know all of the command for symenginejl  sympyjl  also  every type  stable julia function PRON write compile down to something with c performance  so PRON do not have to worry there either  so PRON tend to just use PRON like normal julia number with a few special function  like differentiate  and everything be fast  make PRON really easy to use   the only thing that be miss be the beautiful notebook  PRON can use a jupyter notebook  but PRON be still not the same because the mathematica one where PRON code actually have fraction and all of that  but the performance and productivity increase definitely make up for the lack of notebook  
__label__python __label__logistic-regression __label__tensorflow PRON be a beginner in machine learning  PRON have build a logistic classifier in python use tensorflow to train on notmnist dataset  PRON code be as such   weight  tf  variabletftruncatednormalshape   784  10     bias  tf  variabletfzerosshape   10     logit  tfmatmulfeature  weight   bias  prediction  tfnnsoftmaxlogit   crossentropy  tfreducesumlabels  tflogprediction   reductionindices1   loss  tfreducemeancrossentropy   trainfeeddict   feature  trainfeature  label  trainlabel   validfeeddict   feature  validfeature  label  validlabel   testfeeddict   feature  testfeature  label  testlabel   iscorrectprediction  tfequaltfargmaxprediction  1   tfargmaxlabel  1    accuracy  tfreducemeantfcastiscorrectprediction  tffloat32    epoch  5  batchsize  50  learningrate  01  optimizer  tftraingradientdescentoptimizerlearningrateminimizeloss   validationaccuracy  00  with tf  session   as session   sessionruntfglobalvariablesinitializer     batchcount  intmathceillentrainfeaturesbatchsize    for epochi in rangeepoch    for i in rangebatchcount    sessionrunoptimizer  feeddict  trainfeeddict   printsessionrunaccuracy  feeddict  trainfeeddict    however  the problem be that while the training loss be decrease continuously  the accuracy waver initially  and then finally stagnate  at around 0062   PRON be not able to understand what be wrong with the code  any help would be appreciate  thank   use tfsoftmaxcrossentropywithlogit function instead of write PRON by PRON because tfsoftmaxcrossentropywithlogit be more computationally stable   try this   loss  tfreducemeantfsoftmaxcrossentropywithlogitslogit    refrence  httpswwwtensorfloworgapidocspythontfnnsoftmaxcrossentropywithlogit 
__label__finite-element __label__ode __label__time-integration give the functional      fmathbfxfrac12mathbfxtextt    dmathbfxfrac12mathbfxtextt    mathbfaxfrac12mathbfxtextt0mathbfxt      where  mathbfa be symmetric   mathbfx0 be the initial condition   mathbfxt be continuous  and       mathbfftextt    mathbfgintt0 mathbfftexttttaumathbfgtautextdtau     and      dleftcdotrightfractextdtextdtleftcdotright      PRON take the variation as      delta fleftmathbfxrightleftfracpartialpartial varepsilon  fleftmathbfxvarepsilondeltamathbfxrightrightvarepsilon0      functional at the varied function be then      fleftmathbfxvarepsilondeltamathbfxrightfrac12mathbfxtexttvarepsilondeltamathbfxtextt    dmathbfxvarepsilondeltamathbfxfrac12mathbfxtexttvarepsilondeltamathbfxtextt    mathbfaleftmathbfxvarepsilondeltamathbfxrightfrac12leftmathbfxtextt0varepsilondeltamathbfxtextt0rightleftmathbfxtvarepsilondeltamathbfxtright      the individual term be      frac12mathbfxtexttvarepsilondeltamathbfxtextt    mathbfaleftmathbfxvarepsilondeltamathbfxrightfrac12mathbfxtextt    dmathbfxfrac12varepsilonmathbfxtextt    ddeltamathbfxfrac12varepsilondeltamathbfxtextt    dmathbfxfrac12varepsilon2deltamathbfxtextt    ddeltamathbfx           frac12mathbfxtexttvarepsilondeltamathbfxtextt    mathbfaleftmathbfxvarepsilondeltamathbfxrightfrac12mathbfxtextt    mathbfamathbfxfrac12varepsilonmathbfxtextt    mathbfadeltamathbfxfrac12varepsilondeltamathbfxtextt    mathbfamathbfxfrac12varepsilon2deltamathbfxtextt    mathbfadeltamathbfx          frac12leftmathbfxtextt0varepsilondeltamathbfxtextt0rightleftmathbfxtvarepsilondeltamathbfxtrightfrac12mathbfxtextt0mathbfxtfrac12varepsilonmathbfxtextt0deltamathbfxtfrac12varepsilondeltamathbfxtextt0mathbfxtfrac12varepsilon2deltamathbfxtextt0deltamathbfxt      PRON can differentiate these with respect to  varepsilon and then set PRON to zero  add PRON up lead to      delta fleftmathbfxrightfrac12mathbfxtextt    ddeltamathbfxfrac12deltamathbfxtextt    dmathbfxfrac12mathbfxtextt    mathbfadeltamathbfxfrac12deltamathbfxtextt    mathbfamathbfxfrac12mathbfxtextt0deltamathbfxtfrac12deltamathbfxtextt0mathbfxt      PRON also can say      frac12mathbfxtextt    mathbfadeltamathbfxfrac12deltamathbfxtextt    mathbfatexttmathbfx       since  mathbfa be symmetric  this just mean that      frac12mathbfxtextt    mathbfadeltamathbfxfrac12deltamathbfxtextt    mathbfamathbfx       substitute this back into the varied functional yield      delta fleftmathbfxrightfrac12mathbfxtextt    ddeltamathbfxfrac12deltamathbfxtextt    dmathbfxfrac12deltamathbfxtextt    mathbfamathbfxfrac12deltamathbfxtextt    mathbfamathbfxfrac12mathbfxtextt0deltamathbfxtfrac12deltamathbfxtextt0mathbfxt      notice  now PRON have two similar term which can be group  do so yield      delta fleftmathbfxrightfrac12mathbfxtextt    ddeltamathbfxfrac12deltamathbfxtextt    dmathbfxdeltamathbfxtextt    mathbfamathbfxfrac12mathbfxtextt0deltamathbfxtfrac12deltamathbfxtextt0mathbfxt      PRON can lump this into the other term which a variation      delta fleftmathbfxrightfrac12mathbfxtextt    ddeltamathbfxfrac12deltamathbfxtextt    leftdmathbfx2mathbfamathbfxrightfrac12mathbfxtextt0deltamathbfxtfrac12deltamathbfxtextt0mathbfxt      now  if PRON say that  deltamathbfx00   PRON get      delta fleftmathbfxrightfrac12mathbfxtextt    ddeltamathbfxfrac12deltamathbfxtextt    leftdmathbfx2mathbfamathbfxrightfrac12mathbfxtextt0deltamathbfxt      this be the form PRON would use for discretizing  but PRON can show which system PRON be stationary with respect to  take integration by part on the first term yield      frac12mathbfxtextt    ddeltamathbfxfrac12mathbfxtextt0deltamathbfxtfrac12mathbfxtextttdeltamathbfx0frac12deltamathbfxtextt    dmathbfx        substitute this back in yield  with  deltamathbfx00        delta fleftmathbfxrightdeltamathbfxtextt    leftdmathbfxmathbfamathbfxright       which imply      dmathbfxmathbfamathbfx0     or      fracdmathbfxtdt mathbfaxt      again  the first variation of this functional  with no ibp  be      delta fmathbfxfrac12mathbfxtextt    ddeltamathbfxfrac12deltamathbfxtextt    leftdmathbfx2mathbfaxrightfrac12mathbfxtextt0deltamathbfxt      PRON now discretize this over the interval  left0deltaright  use the approximation      mathbfxtmathbfx0 n0tmathbfx1 n1t         deltamathbfxtdeltamathbfx0 n0tdeltamathbfx1 n1t      where      n0t1fractdelta      and     n1tfractdelta      the result be      delta fmathbfx   approx deltamathbfx1texttleftleftfrac12mathbfifracdelta6mathbfarightmathbfx1fracdelta3mathbfamathbfx0frac12mathbfx0right      give that  deltamathbfx1  be some arbitrary discrete variation  PRON can say that PRON be coefficient must be zero      leftfrac12mathbfifracdelta6mathbfarightmathbfx1fracdelta3mathbfamathbfx0frac12mathbfx00     PRON question be  be the follow also true       leftfrac12mathbfifracdelta6mathbfarightmathbfxi1fracdelta3mathbfamathbfxifrac12mathbfx00      too many comment  so PRON will continue here   PRON assume PRON be refer to equation  440  in httpwwwdicunitsitperspagetontidepositoinitialpdf  in this case the answer be yes  PRON be possible  due to the specific integral formulation base on convolution  the first variation of  440  lead to the initial value problem    xt   axt  with  x0 give  PRON can then discretize this equation with a standard galerkin method  the one PRON be try to get be equivalent to a crank  nicolson time step method   in principle  PRON can also go the other way around as PRON be propose  discretize the functional  and then take discrete variation  depend on the discretisation  PRON end up with different time step method  but often there be one choice that lead to a standard time step method such as crank  nicolson  whether that be possible in this case  PRON be not sure  the crank  nicolson method be equivalent to a petrov  galerkin method  ie  trial and test function come from different space  continuous piecewise linear and discontinuous piecewise constant  in this case   which mean there can be no variational formulation  since the test function come from variation of the trial function  which mean PRON have to live in the same space    as regard PRON specific question  no  this be unlikely to be true  since the initial condition should not appear there  to get the next time step  PRON need to explicitly compute variation with respect to  delta x2   and so forth     note that in the definition of PRON functional   t should really be  t  the end time   otherwise the functional make no sense   this mean that  ftg be not really the convolution of  f and  g  which would be a function   but a related bilinear form  and hence a real number    that mean PRON need to be careful when take variation not to mix up  t and  t also  the  x0 in PRON formulation be a fix initial condition  better denote by  x0 since  x0x0  be fix  there can be no variation at  t0   ie   delta x0   0  
__label__machine-learning __label__deep-learning for some reason  alphago zero be not get as much publicity as the original alphago  despite PRON incredible result  start from scratch  PRON be already beat alphago master and have pass numerous other benchmark  even more incredibly  PRON be do this in 40 day  google name PRON as  arguably the good go player in the world    deepmind claim this be a  novel form of reinforcement learning   be this technique truly novel  or have there be other time when this technique be used and if so  what be PRON result  PRON think the requirement PRON be talk about be 1  no human intervention and 2  no historical play  but these be flexible   this appear to be a similar question  but all the answer seem to start from the assumption that alphago zero be the first of PRON kind   the alphago zero article from nature   master the game of go without human knowledge   claim four major difference from the early version   self  learn only  not train on human game   use only the board and stone as input  no hand  write feature    use a single neural network for policy and value  a new tree  search algorithm that use this combine policy  value network to guide where to search for good move   point  1  and  2  be not new in reinforcement learning  but improve on the previous alphago software as state in the comment to PRON question  PRON just mean PRON be now use pure reinforcement learning start from randomly initialize weight  this be enable by good  faster learn algorithm   PRON claim here be  PRON primary contribution be to demonstrate that superhuman performance can be achieve without human domain knowledge    p 22    point  3  and  4  be novel in the sense that PRON algorithm be simple and more general than PRON previous approach  PRON also mention that be be an improvement on previous work by guo et al   unify the policy  value network  3  enable PRON to implement a more efficient variant of monte  carlo tree search to search for good move and simultaneous use the search tree to train the network faster  4   this be very powerful   furthermore  PRON describe a number of interesting implementation detail like batch and reuse data  structure to optimize the search for new move   the effect be that PRON need less compute power  run on 4 tpu rather than 176 gpu and 48 tpu for previous version of PRON software   this definitely make PRON  novel  in the context of go software  PRON believe that  3  and  4  be also  novel  in a broad context and will be applicable in other reinforcement learning domain such as eg robotic  
__label__computational-chemistry __label__hartree-fock __label__basis-set how do PRON pick a basis set for an ab  initio hartree  fock evaluation  in other word  what be the important characteristic of a basis set so that a proper choice can be make   this be a pretty big topic  but the basic important quality of an atom  center basis set be   the number of separate  unlinked function PRON use at the valence level  the so  call  zeta  count   which allow the modeling of that many different electronic environment  practically speak   additional high angular momentum so  call  polarisation  function to allow for more angular complexity   whether PRON have diffuse function to model more spacially extensive orbital  so called  augment  function   generally  though  for hartree  fock   double  zeta for very quick  very rough structural parameter   double  zeta  polarisation for slightly less rough structural information  large system  100  atom   or simple bonding environment like alkane   triple  zeta for good accuracy and useful energy   augmented function if PRON be work with anion   go to a good basis set than this for hartree  fock probably will not gain PRON much  in any case  unless PRON be move on to a more complex method  PRON should probably be use density  functional theory  dft  instead   for more information on basis set include how different basis set be label  PRON recommend chapter 52 in jensen s  introduction to computational chemistry    here be some reference that may be useful   httppubsacsorgdoiabs101021cr00074a002  httpwwwsciencedirectcomsciencearticlepii0167797785900036  httpbooksgooglecombooksidgt4pnpufhucamplpgpa853ampvqhelgakeramppgpa725vsnippetampqhelgakerampffalse  if PRON see nothing  click next then prev    see httpwwwworldscientificcomdoiabs10114297898128321150001 to find the analog version in PRON library   httpvergilchemistrygatecheducourseschem6485pdfbasissetspdf 
__label__clustering __label__algorithms __label__k-means k  mean be a well know algorithm for clustering  but there be also an online variation of such algorithm  online k  mean   what be the pro and con of these approach  and when should each be prefer   online k  mean  more commonly know as sequential k  mean  and traditional k  mean be very similar   the difference be that online k  means allow PRON to update the model as new datum be receive   online k  mean should be use when PRON expect the datum to be receive one by one  or maybe in chunk    this allow PRON to update PRON model as PRON get more information about PRON   the drawback of this method be that PRON be dependent on the order in which the data be receive  ref    the original macqueen k  means publication  the first to use the name  kmean   be an online algorithm   macqueen  j b  1967    some methods for classification and analysis of multivariate observations   proceeding of 5th berkeley symposium on mathematical statistics and probability 1  university of california press  pp  281–297  after assign each point  the mean be incrementally update use a simple weight  average formula  old mean be weight with n  the new observation be weight with 1  if the mean have n observation before    as far as PRON can tell  PRON be also mean to be a single pass over the datum only  although PRON can be trivially repeat multiple time to reassign point until convergence   macqueen usually take few iteration than lloyds to converge if PRON data be shuffle  because PRON update the mean faster    on order datum  PRON can have problem  on the downside  PRON require more computation for each object  so each iteration take slightly long  additional math operation  obviously   
__label__optimization __label__python __label__constrained-optimization __label__quadratic-programming __label__constrained PRON be minimise a diagonal quadratic matrix use cplex  all off diagonal element be zero   PRON have 500 variable and 20 linear constraint plus each variable be constrain to be within 0 and 1  all of the element on the diagonal be great than zero   cplex complain that PRON be either not  optimal or unbounded depend on the value of the matrix   PRON can not see how this problem can be unbounded as PRON be a minimsation of a convex function  for some value cplex say that the solution be not optimal   PRON have post the lp file if that help at   httpspeedyshug76kquadraticfaillp  here be the cplex log  try aggregator 1 time   qp presolve eliminate 15 row and 0 column   reduce qp have 5 row  500 column  and 1000 nonzero   reduced qp objective q matrix have 500 nonzero   presolve time  000 sec   029 tick   parallel mode  use up to 8 thread for barrier   number of nonzero in low triangle of aa   10  use approximate minimum degree ordering  total time for automatic ordering  000 sec   000 tick   summary statistic for cholesky factor   thread   8  rows in factor   5  integer space require   5  total non  zero in factor  15  total fp op to factor   55  itn  primal obj  dual obj  prim inf upper inf  dual inf  0  27831848e014 27831848e014 665e001 907e002 557e014  1  18149043e012 18149043e012 537e000 732e001 449e013  2  15035906e012 15035906e012 489e000 667e001 409e013  3  11322578e012 11322578e012 424e000 579e001 355e013  4  79610680e011 79610680e011 356e000 485e001 298e013  5  55016492e011 55016491e011 296e000 403e001 247e013  barrier time  000 sec   096 tick   total time on 8 thread  000 sec   096 tick   status   2  error statu be  unbounded  printing lp  can anyone reproduce PRON problem use cplex   update  PRON have force cplex to use the primal algorithm rather than the default  the optimiser now run but PRON get a lot of  markovitz threshold set to xxx  style warning   PRON problem seem quite easy indeed  PRON try mosek  another convex optimizer  instead of cplex and PRON solve in almost no time  see the log at the end    the problem be well scale and thee be no evident reason for cplex to get fool  PRON guess be that PRON be not able to certify optimality  unboundness should not be the issue   mosek version 700114  build date  2014  4  27 193042   copyright  c  1998  2014 mosek aps  denmark  www  httpmosekcom  open file  quadraticfaillp   read summary  type   qo  quadratic optimization problem   objective sense   min  constraints   20  scalar variable  500  matrix variable  0  time   00  computer  platform   linux64x86  cores   2  problem  name    objective sense   min  type   qo  quadratic optimization problem   constraint   20  cones   0  scalar variable   500  matrix variable   0  integer variable   0  optimizer start   interior  point optimizer start   presolve start   linear dependency checker start   linear dependency checker terminate   eliminator  try   0  time   000  eliminator  elim s   0  lin  dep    try   1  time   000  lin  dep    number   0  presolve terminate  time  000  matrix reordering start   local matrix reordering start   local matrix reordering terminate   matrix reordering terminate   optimizer   thread   2  optimizer   solve problem   the primal  optimizer   constraints   5  optimizer   cones   0  optimizer   scalar variable   505  conic   0  optimizer   semi  definite variable  0  scalariz   0  factor   setup time   000  dense det  time   000  factor   ml order time   000  gp order time   000  factor   nonzero before factor  15  after factor   15  factor   dense dim    0  flop   000e00  ite pfea  dfeas  gfeas  prstatu  pobj  dobj  mu  time  0  10e02  11e03  20e04  000e00  9762500000e03  9762500000e03  10e00  000  1  25e00  27e01  13e03  824e01  1461547953e03  1063368559e04  11e01  001  2  21e00  23e01  12e03  590e01  1926066470e03  1046829917e04  10e01  001  3  13e00  13e01  70e02  510e01  2077327854e03  6495728296e03  60e02  001  4  52e01  56e00  34e02  424e01  3416151322e03  2006514499e03  28e02  001  5  66e02  70e01  87e01  641e01  4786227760e03  3147891392e03  59e03  001  6  63e03  67e02  14e01  960e01  4744795814e03  4485405619e03  83e04  001  7  83e06  88e05  76e01  998e01  4726819747e03  4712348560e03  39e05  001  8  80e07  84e06  13e01  100e00  4723211688e03  4720790746e03  66e06  001  9  28e09  30e08  87e03  100e00  4722531001e03  4722365208e03  45e07  001  10  17e12  19e11  55e04  100e00  4722481461e03  4722470859e03  29e08  001  11  35e15  59e14  33e05  100e00  4722478119e03  4722477487e03  17e09  001  12  57e16  11e13  17e06  100e00  4722477945e03  4722477912e03  89e11  001  interior  point optimizer terminate  time  001   optimizer terminate  time  003  interior  point solution summary  problem status   primalanddualfeasible  solution status  optimal  primal   obj  47224779451e03  viol   con  2e16  var  0e00  dual   obj  47224779122e03  viol   con  4e12  var  4e13 
__label__linear-algebra __label__matrices PRON need to compute the  moore  penrose  pseudoinverse of fix  size 3x3 matrix  PRON would prefer to have a simple method without bring in the industrial strength machinery of lapack  be there any simple method to do this   edit  PRON should add two note  first  PRON be prioritize robustness and simplicity over speed  second  PRON really only need the action of the pseudo  inverse on a 3x1 vector   the book generalized inverses  theory and applications  second edition  by ben  israel and greville state the follow  chapter 1  theorem 5  page 48    theorem  if  a be an  m by  n complex matrix of rank  r  gt  0   and have the full rank factorization  beginalign   a  xy   endalign   then PRON moore  penrose   1234generalized inverse   adagger be give by  beginalign   adagger   yxay1x      endalign   where the asterisk denote hermitian transpose   like many theoretical result  this result be amenable to proof  but not immediately useful for computation   in numerical analysis  there be no routine for  full rank factorization   rather  PRON get that information use the svd or a qr factorization  consequently  PRON be much good off use lapack to calculate one of those two well  know factorization   suppose that the svd of  a be  a  usigma v suppose that  sigma  mathrmdiagsigma1   ldot  sigman define  sigmavarepsilon   mathrmdiagsigma1   ldot  sigman  where  beginalign   sigmai   leftbeginarrayccsigmai1    amp  textrmif  sigmai geq varepsilon    0   amp  textrmif  sigmai  lt  varepsilonendarrayright   endalign   where singular value with magnitude less than  varepsilon  gt  0  be treat as  numerically zero    then  beginalign   adagger   vsigmavarepsilonu  endalign   this method be not simple  but PRON will be more robust  and PRON will be correct   edit  two common formula for the moore  penrose generalize inverse be give on wikipedia   if  a be an  m by  n matrix with linearly independent column  so  m geq n   then  beginalign   adagger    aa1a  endalign   if  a be an  m by  n matrix with linearly independent row  so  m leq n   then  beginalign   adagger   aaa1  endalign   a problem with both of these formula be the requirement of linear independence  in the square case  if all column be linearly independent  then all row be also linearly independent  and the matrix be invertible  the inverse of a matrix be also PRON moore  penrose generalize inverse  and if the matrix inverse be truly need for a computation  often time  but not always  PRON be not  and can be replace by solve an appropriate linear system  PRON should be calculate directly use lu  qr  or svd  rather than use one of the two formula for moore  penrose generalize inverse   another problem with both of the above formula in the square case  assume that the condition be satisfied  be that PRON will be less accurate numerically than an svd or qr  base approach  because the condition number of  aa or  aa will be the square of the condition number of  a  all condition number mention in this explanation be for linear system   in the case where  a be not square  but one of the two linear independence condition be satisfied  the product of  a with PRON hermitian transpose  in the appropriate order  will have a condition number equal to the square of the ratio of the large singular value of  a to the small nonzero singular value of  a  again  this condition number could be large  such a matrix product be best avoid unless there be compelling reason for use PRON in an application  speed  for instance    for 3x3 matrix this should be easy  here be a least  square routine in fortran that form moore  penrose pseudoinverse in the process of solution  PRON may help   function solveleastsqa  b  m  n  resultx       solve system with m x n system matrix in least square sense  minimize euclidean norm      system be overdetermin so PRON solve aa  x  ab  where a  be transpose of a    the solution be give by x   aa1ab      result  realdp   dimensionn    x    input  realdp   dimensionm  n   intentin    a  realdp   dimensionm   intentin    b    local  realdp   dimensionn  m     at  realdp   dimensionn  n     ata  invata  realdp   dimensionn     atb  at  transposea    transpose of a  a   ata  matmulat  a    left  hand side multiplication of a by a   aa  atb  matmulat  b    ab  invata  invata    an inverse of aa  x  matmulinvata  atb   x   aa1   ab  end function  for this to work PRON need these routine to find an inverse of a 3x3 matrix   function det3x3a     result  realdp    det3x3    input  realdp   dimension33   intentin    a  det3x3   a11a22a33a12a23a31a13a21a32   amp   a13a22a31a12a21a33a11a23a32   end function                                                                           function adja       adjugate of a 3x3 matrix  the transpose of the cofactor matrix        result  realdp   dimension33    adj    input  realdp   dimension33   intentin    a  adj11   a22a33   a23a32   adj12   a13a32   a12a33   adj13   a12a23   a13a22   adj21   a23a31   a21a33   adj22   a11a33   a13a31   adj23   a13a21   a11a23   adj31   a21a32   a22a31   adj32   a12a31   a11a32   adj33   a11a22   a12a21   end function                                                                           function inva       an inverse of a 3x3 matrix       result  realdp   dimension33    inv    input  realdp   dimension33   intentin    a    local  realdp    detr  detr  1det3x3a   inv  adjadetr  end function  PRON use double precision for real   integer  parameter   dp  kind10d0  
__label__finite-difference __label__nonlinear-equations PRON have a very basic question  and PRON hope some of PRON may be able to help PRON  in fluid dynamics  a common equation turn up time and time again be the so  call continuity equation     fracpartialpartial xtfracpartial hpartial x    r  sfracpartial hpartial t  where  t be the transmissivity of the soil matrix  define by the product  kh  where  h be the hydraulic head and  k be the hydraulic conductivity  a constant    r a recharge constant  and  s the storage coefficient  another constant  in fd solution for the unconfined case  where the transmissivity vary with  h  this equation be often linearize by assume that  t be constant  if PRON ignore this linearization and continue  PRON get     fracpartialpartial xkhfracpartial hpartial x    r  sfracpartial hpartial t  now PRON do not see a problem with solve this  could not one just apply the chain rule of differentiation to get     fracpartial khpartial xfracpartial hpartial x   khfracpartial2 hpartial x2 r  sfracpartial hpartial t   and then solve each of these term by conventional center difference and second order central difference  PRON can see that this kind of solution may be problematic for implicit scheme  where PRON will get a squared  h in the first term  but why be this not do in explicit scheme   for explicit scheme  this indeed make sense  there be no reason to linearize the explicit term  however  as other have already point out  PRON get into trouble with explicit scheme for the diffusion equation because the time step must be choose proportional to something like     delta t propto minx fracsx  delta x2tx   propto minx fracsx  delta x2kh       ie  PRON need to choose the time step very small if either  s become small somewhere in the domain  or  h become large  in addition  of course  reduce the grid size by a factor of 2 require PRON to reduce the time step size by an undesirable factor of 4   for these reason  explicit time step method be not commonly use for the diffusion equation  rather  one want to use implicit method to avoid the awkward cfl condition and take large time step  in particular if one expect that after some time the solution find some kind of equilibrium and PRON want to take large time step   of course  if PRON use an implicit method  PRON have to linearize somehow because otherwise PRON end up with a system of nonlinear system that may be very large  and PRON do not know how to solve these other than through a linearization procedure that reduce the large nonlinear problem to a sequence of large linear problem  
__label__linear-algebra __label__graph-theory be there an efficient storage format for general  non  symmetric sparse matrix for which one can find all non  zero entry in a give row or column in  od time    d be the max number of non  zero entry in any row or column    if PRON store an  m time n sparse matrix  a in the compressed row or ellpack format  PRON can find all non  zero entry in a give row in  od time  but find all non  zero entry in a give column take  om of course  PRON could store  a in compressed column format  in which case the column access would take  od but the row access would take  on  a blunt approach would be to store both format  compress row and column  for the same matrix  which of course entail double the memory  can one do much good   best PRON can come up with  for square matrix  one can store PRON as a structurally symmetric matrix  if PRON want to find all entry in column j  PRON instead find all entry in row j and then remove the redundant one   PRON can store the matrix as usual  in row  compress sparse format  then for each column  PRON store which entry exist in the sparsity pattern  if PRON need the entry of a column  PRON can find out from this second data structure which entry exist  and PRON can then get the value from the row  wise  normal  data structure that store the matrix   this be essentially one integer per entry of the matrix for the column  wise storage  in addition to one integer per entry for the row  wise storage plus one float point number for each entry   with 32bit integer for the column or row entry  and 64bit double precision for the number  this give   12 byte per entry for the regular row  wise datum storage scheme  16 byte per entry if PRON also want to keep the column  wise information   access all entry in a column require traverse a column in the column  wise datum structure  which be   cal od  and for each entry PRON find  PRON need to look PRON up in the row  wise format to get at the value  which will cost PRON   cal olog d for each entry  in total  that make   cal od log d to access all element of a column  
__label__finite-element bathe s finite element procedures show the  nonlinear strain stiffness matrix  for a 2d truss element as     frac  tp   l0  delta l   left  beginarrayccc   1  amp  0  amp  1  amp  0   0  amp  1  amp  0  amp  1   1  amp  0  amp  1  amp  0   0  amp  1  amp  0  amp  1   endarray  right      but other source  such as this httppeopledukeeduhpgavincee421trussfinitedefpdf and the ansys theory manual  omit row 1 and 3  axial direction       frac  tp   l0   left  beginarrayccc   0  amp  0  amp  0  amp  0   0  amp  1  amp  0  amp  1   0  amp  0  amp  0  amp  0   0  amp  1  amp  0  amp  1   endarray  right      the extra 1 s do not seem to be physically correct accord to PRON intuition  PRON say the element become stiff when PRON be under tension  that suggest the force  displacement relationship be not linear in a seemingly arbitrary way   bathe do say  note that if the material stress  strain relationship be such that  tp be constant with change in  delta l   then something that seem to lead to the extra 1 s be omit    perhaps PRON be use a generalization of hooke s law which be nonlinear but do not have any extra parameter   PRON experiment with nonlinear solid element in calculix show the same behavior as these truss element with the extra 1 s present   edit  use the full diagonal matrix with pre  stressed frequency analysis lead to what seem like an incorrect result  a 1d spring  mass system s natural frequency increase with tension on the spring  in other word  this common physics concept of gravity not influence the motion describe here be violate  httpsshareehsuenorgsitesdefaultfilesunit02lesson2pdf the same problem occur with solid element in calculix  be PRON really wrong   edit 2  example show that the full  diagonal matrix do not work for pre  stressed frequency analysis  PRON be a 1d spring with length 1  spring constant k  mass m concentrated at each node and no constraint   the natural frequency should be independent of p PRON can see this be true when use the 2nd geometric stiffness matrix above by solve the eigenvalue equation      textbf ke  textbf kg  omega2 textbf m  textbf u  0     PRON form the 1d matrix by take on the 1st and 3rd row and column from the 2d matrix above     textbf ke   left  beginarrayrr   k  amp  k   k  amp  k   endarray  right       textbf kg   left  beginarrayrr   0  amp  0   0  amp  0   endarray  right       textbf m   left  beginarrayrr   m  amp  0   0  amp  m   endarray  right     which have the non  zero eigenvalue  omega2frac2km    this be consistent with a simple spring  mass oscillator  the factor of 2 appear because PRON be really two 1dof oscillator with half the spring each   now  if PRON use the first geometric stiffness matrix above  with  delta l  approximately 0  PRON get the wrong answer       textbf ke  textbf kg  omega2 textbf m  textbf u  0       textbf ke   left  beginarrayrr   k  amp  k   k  amp  k   endarray  right       textbf kg   left  beginarrayrr   p  amp  p   p  amp  p   endarray  right       textbf m   left  beginarrayrr   m  amp  0   0  amp  m   endarray  right     this have a non  zero eigenvalue of  omega2frac2kpm    PRON be apparently wrong because PRON be a function of p but PRON should be independent of p PRON show that a spring be stiffen by increase tension  conversely  if p be negative  PRON also show that compression soften PRON and PRON even become unstable when p  k  this indicate axial buckling  not euler column buckling  under compression  which seem to be unphysical  even if  delta l be not zero  PRON can not make  textbf kg zero  so there will always be some discrepancy   so PRON be wonder if the matrix with the full diagonal above be really correct  at least PRON appear to be wrong when use for frequency or linear buckle analysis  solid element in calculix follow this  incorrect  behavior for frequency analysis  which add to the confusion   if PRON objective be to perform a geometrically nonlinear  analysis of truss structure where the element be allow  to undergo arbitrarily large rotation  then PRON first form  of the geometric stiffness matrix with the  extra  one be the correct one   to see this  PRON be useful to step back to the internal force vector for this  truss element  there be several way to derive this vector but a simple way  be to just write the equilibrium equation for the member in PRON deform  position  that give    f   tp  co theta  sin theta  cos theta  sin thetat  where  theta be the angle of rotation between the undeformed and deform  truss element   this equation be on page 547 of bathe  in a nonlinear analysis  the need for  a  stiffness matrix  be the result of solve the nonlinear equation by  the newton  raphson method  the derivative of the residual vector with respect to  to the  unknown be need  simple differentiation of the internal force vector  above with respect to the nodal displacement yield a tangent  k that include the  first form of the  kg matrix   that be what bathe do on page 547  the nonlinear mechanic be all contain  in the internal force vector and the tangent  k be just a straightforward  differentiation  the first form of  kg be correct in that sense   in the note PRON refer to by gavin  PRON describe the approximation PRON  make in derive the simple matrix  these approximation yield a  kg  matrix that be fine for most engineering linear  buckling and pre  stressed vibration analysis but not large  rotation nonlinear analysis   if PRON want to see a step  by  step  detailed derivation of the first form   take a look at these note   httpwwwcoloradoeduengineeringcascoursesdnfemdnfemch08dnfemch08pdf  this note by felippa have an interesting comment on exactly the question pose in this post so PRON be include part of that here    most of the early work  as well as the confusion allude to by martin  pertain to what be now call the  geometric stiffness matrix  the early name for PRON be initial stress matrix  a bar geometric stiffness be  first present in  767   if this be compare to  826  one may observe that half of the nonzero entry be  miss  consequently that inaugural kg be not invariant with respect to the choice of coordinate frame   and have the wrong rank    felippa s equation 826 be the first version of  kg show above and the version present in reference 767 be the second   in both the bathe and felippa derivation  PRON be important to note the  distinction between cauchy and second piola  kirchoff stress tensor   that be the reason for the  delta l in the denominator of the first   kg matrix  that distinction be  ignore in the classical  ie gavin  derivation  specifically  the stress  strain law be assume to be  s11e epsilon11  where  s11 be the axial component of the second piola  kirchoff stress  tensor and  epsilon11 be the axial component of the green  lagrange strain  tensor  this stress  strain law agree with the engineering stress and strain  version of hooke s law only for infinitesimal strain and this difference affect  both the material and geometric element matrix   here be a few comment on PRON example problem  first  the classical  linear spring be a very simplistic model of an elastic solid so PRON be  hardly surprising that a more sophisticated continuum mechanic model  give different result  a similar comment can be make about the   classical   kg matrix derivation  PRON be base on an analysis of  thin  pre  stress member where only transverse deflection be consider   axial deflection be explicitly ignore  so PRON be hardly surprising that  the two  kg matrix give different result in problem where only  axial effect be consider  
__label__statistics PRON be currently fit above mention function to PRON datum and PRON can observe  that both lognormal and weibull be good fit than powerlaw  in literature  PRON be often suggest  that PRON be hard to distinguish these function  but PRON can in PRON datum   PRON do not completely understand the difference between PRON though  what may be an explanation for the underlying process that generate PRON datum that PRON can infer from these result   hope someone can help PRON   see these lecture note from the complex system course teach by prof  peter dodds   PRON have some great link to literature  etc   in a nutshell  random multiplicative growth can lead to lognormal distribution and there be certain thing one can enforce within the dataset  minimum number of occurrence among other thing  see some of this wonderful literature in the reference   that can lead to a power  law distribution   as mention by ajk  this can be hairy business and as PRON probably know there be entire research group  generally categorize as complex system group  that be dedicate to determine what give rise to different distribution and how to classify PRON   there be a rich history here  
__label__r __label__python __label__xgboost xgboost have be do a great job  when PRON come to deal with both categorical and continuous dependant variable  but  how do PRON select the optimize parameter for an xgboost problem   this be how PRON apply the parameter for a recent kaggle problem   param  lt list   objective    reg  linear    booster   gbtree    eta   002   006   001   maxdepth   10   change from default of 8  subsample   05   07  colsamplebytree   07   07  numparalleltree   5   alpha  00001    lambda  1    clf  lt xgbtrain   param   param   data   dtrain   nround   3000   300   280   125   250   change from 300  verbose   0   earlystopround   100   watchlist   watchlist   maximize   false   feval  rmpse    all PRON do to experiment be randomly select  with intuition  another set of parameter for improve on the result   be there anyway PRON automate the selection of optimizedb  set of parameter    answer can be in any language  PRON be just look for the technique   PRON could use the caret package to do hyperparameter space search  either through a grid search  or through random search   whenever PRON work with xgboost PRON often make PRON own homebrew parameter search but PRON can do PRON with the caret package as well like krisp just mention   caret  see this answer on cross validated for a thorough explanation on how to use the caret package for hyperparameter search on xgboost   how to tune hyperparameter of xgboost tree   custom grid search  PRON often begin with a few assumption base on owen zhang s slide on tip for datum science p 14  here PRON can see that PRON will mostly need to tune row sampling  column sampling and maybe maximum tree depth  this be how PRON do a custom row sampling and column sample search for a problem PRON be work on at the moment   searchgridsubcol  lt expandgridsubsample  c05  075  1    colsamplebytree  c06  08  1    ntree  lt 100   build a xgb  dmatrix object  dmmatrixtrain  lt xgb  dmatrixdata  yourmatrix  label  yourtarget   rmseerrorshyperparameter  lt applysearchgridsubcol  1  functionparameterlist     extract parameters to test  currentsubsamplerate  lt parameterlistsubsample     currentcolsamplerate  lt parameterlistcolsamplebytree     xgboostmodelcv  lt xgbcvdata   dmmatrixtrain  nround  ntree  nfold  5  showsd  true   metric   rmse   verbose  true   evalmetric    rmse     objective    reg  linear    maxdepth   15   eta   2ntrees    subsample   currentsubsamplerate   colsamplebytree   currentcolsamplerate   xvalidationscore  lt asdataframexgboostmodelcv    save rmse of the last iteration  rmse  lt tailxvalidationscorestestrmsemean  1   returncrmse  currentsubsamplerate  currentcolsamplerate       and combine with some ggplot2 magic use the result of that apply function PRON can plot a graphical representation of the search   in this plot light color represent low error and each block represent a unique combination of column sampling and row sampling  so if PRON want to perform an additional search of say eta  or tree depth  PRON will end up with one of these plot for each eta parameter test   PRON see PRON have a different evaluation metric  rmpse   just plug that in the cross validation function and PRON will get the desire result  besides that PRON would not worry too much about fine tune the other parameter because do so will not improve performance too much  at least not so much compare to spend more time engineering feature or clean the datum   other  random search and bayesian parameter selection be also possible but PRON have not make  find an implementation of PRON yet   here be a good primer on bayesian optimization of hyperparameter by max kuhn creator of caret   httpblogrevolutionanalyticscom201606bayesianoptimizationofmachinelearningmodelshtml 
__label__machine-learning __label__neural-network __label__deep-learning __label__image-classification __label__cnn PRON be currently try to understand the architecture of a cnn  PRON understand the convolution  the relu layer  pool layer  and fully connect layer  however  PRON be still confused about the weight   in a normal neural network  each neuron have PRON own weight  in the fully connect layer  each neuron would also have PRON own weight  but what PRON do not know be if each filter have PRON own weight  do PRON just have to update the weight in the fully connected layer during back propagation  or do the filter all have a separate weight that PRON need to update   during back propagation both dense layer and convolution layer get update but max  pool layer do not have any weight to be update  dense layer be update to help the net to classify  convolution layer get update to let the network learn the feature PRON  because PRON have not ask in the question PRON just add a link for PRON if PRON want to know more  there be a very good explanation for back propagation here which can be useful for PRON    in a normal neural network  each neuron have PRON own weight   this be not correct  every connection between neuron have PRON own weight  in a fully connected network each neuron will be associate with many different weight  if there be n0 input  ie n0 neuron in the previous layer  to a layer with n1 neuron in a fully connected network  that layer will have n0n1 weight  not count any bias term   PRON should be able to see this clearly in this diagram of a fully connected network from cs231n  every edge PRON see represent a different trainable weight   convolutional layer be different in that PRON have a fix number of weight govern by the choice of filter size and number of filter  but independent of the input size   each filter have a separate weight in each position of PRON shape  so if PRON use two 3x3x3 filter then PRON will have 54 weight  again not count bias  this be illustrate in a second diagram from cs231n   the filter weight absolutely must be update in backpropagation  since this be how PRON learn to recognize feature of the input   if PRON read the section title  visualizing neural networks  here PRON will see how layer of a cnn learn more and more complex feature of the input image as PRON get deep in the network  these be all learn by adjust the filter weight through backpropagation  
__label__r __label__integral-equations PRON want to compute the expect value of a multivariate function fx  wrt to dirichlet distribution  PRON problem be  penta  nomial   ie 5 variable  so calculate the explicit form of the expect value seem unreasonable  be there a way to numerically integrate PRON  be there some package  eg in r  that can let PRON do that  PRON realize PRON can solve PRON use first principle but PRON want the computation to be really fast   fx   sum04qilogn  qi    
__label__time-series can someone explain some question PRON have about the pseudocode in the wikipedia article for dynamic time warping   PRON be talk about the implementation with the window constraint parameter w  on the line dtw0  0    0  why be PRON set the first element to zero after set the entire matrix to infinity  PRON be sure there be some reason  but PRON can not figure out why   on the line initialize the second set of for loop   for i   1 to n  for j   max1  i  w  to minm  iw   why do PRON skip the first row  column  PRON get the bound idea in the inner loop with the window constraint  but from what PRON understand  in dtw  not use the first column and first row be essential  why   within the for loop   cost   dsi   tj    dtwi  j    cost  minimumdtwi1  j      insertion  dtwi   j1     deletion  dtwi1  j1     match  PRON understand the cost be the current distance between two element in the respective time series  but what be PRON do when PRON set dtwi  j  to  cost  minimumdtwi1  j      insertion  dtwi   j1     deletion  dtwi1  j1     match  PRON be not the good at dynamic programming  so PRON do not understand this line as well  why be each case consider an insertion  deletion  and a match   finally  for the last line   return dtwn  m   why be PRON return the last element  PRON think dtw be suppose to return the align time series which PRON figure would be represent by the warp path in the matrix   
__label__methods PRON be try to stitch together multiple package and tool from multiple language  r  python  c etc   in a single analysis workflow   be there any standard way to do PRON  preferably  but not necessarily  in python   luigi be an open source python package by spotify that do exactly what PRON describe   luigi be a python  27  33  34  35  package that help PRON build complex pipeline of batch job  PRON handle dependency resolution  workflow management  visualization  handle failure  command line integration  and much more   PRON philosophy be similar to gnu make  let PRON define task and PRON dependency   there be also apache airflow  originally from airbnb   another python solution   airflow be a platform to programmatically author  schedule and monitor workflow   use airflow to author workflow as direct acyclic graph  dag  of task  the airflow scheduler execute PRON task on an array of worker while follow the specify dependency   PRON may find a complete comparative table here  
__label__machine-learning __label__training __label__geospatial new york city provide ten of gig of datum of taxi route all over the city  what PRON would like to do  be use this data  or some other method   to come up with an algorithm that can take a person gps datum over a short span of time  say an hour   and answer the question  be this person drive a cab   the algorithm should work in any location  not just nyc  the idea be that PRON would like to be able to determine pattern  that signal that a route be drive by a person be the type of route a person drive a cab would take   ideally  PRON would like to write this in ruby  but PRON be open to other suggestion  approach  and implementation  link to project PRON should research  suggestion on language to use  approach to take  etc be all appreciate   take the taxi route and combine PRON with civilian car route to form a data set for classification  use a map  say  from google   break down each route into a sequence of road segment  from intersection to intersection  if PRON only have gp trace this will involve spatio  temporal segmentation   intersection  terminus be place where car go but stop at  and the road segment be the place car move through to get to from one intersection to another   model these as categorical variable  road segment 1  2  3  etc   abstract out the physical location  then train a classifier than accept a sequence as input  eg  a recurrent neural network   use the time of departure as another feature  model as two real variable   cos2pit24   sin2pit24  where  0lttlt24 if PRON have really accurate gps information  PRON would also try to estimate the rate of lane crossing  taxi driver be well know for PRON aggressive driving  and the rate of lane crossing would capture this well  finally  use a pair of boolean variable to record whether the point of departure and destination be parking area  if PRON can obtain this information  PRON imagine taxi will be more likely to start and stop at prohibited place   the rest be PRON usual hyper  parameter optimization black magic   if PRON want to be able to do anything besides classify a given route as taxi  drive or not  please say so  
__label__classification __label__clustering __label__statistics __label__missing-data PRON have a set of datum with many sample and many feature  but where half of the data be miss one variable  call PRON a   which be compose of four category  base on the half of datum which have a  PRON want to know what category the sample without a would most likely be in if PRON do have a  PRON could build a classifier base on the datum with a  and predict the datum without a  this be the good route imho    but PRON be wonder  out of curiosity  if this method could also be a very  very  very rough way of do something similar   cluster the datum which have a into the same number of cluster as category in a  in this case four    check for an an association between the cluster and the category in a  use a frequency table and chi  square test    if there be an association  run the datum without a through the clustering model to figure out what category of a PRON be most likely associate with  base on what cluster PRON be in    thought   PRON try to stay away from bounty problem as PRON seem to lead to one upmanship  but PRON can not resist this one as PRON an interesting question   the upper classification method that PRON describe sound spot on  use that one   the bottom method have a number of issue   how would PRON know that the cluster PRON get out would correspond to the category of a  the answer be that PRON would not  there be other datum feature and the cluster would be a linear superposition of the maximum separation between the various variable   PRON guess PRON could standardize PRON datum and then coax the cluster to conform to the separation along a by multiply the a feature by a factor great than 1  2  10   but this seem like more of a bastardization of cluster than a good process   the main issue that PRON see with PRON method be that PRON be essentially employ a semi  supervised learning methodology  but PRON do not need to because PRON have the datum to solve this as a supervised learning problem  PRON would be a good methodology if PRON only have a handful of label a value  but since PRON have lot more datum  PRON should harness the power of that datum that PRON have  the supervised learning problem will fully harness the variance of PRON datum and much of that will be throw out in the semi  supervised learning methodology   hope this help and good luck   clustering technique at high dimensionality tend to be unstable  inaccurate   as such  if PRON be able to accurately classify one column base on other  then PRON be well suited with pursue a dimensionality reduction technique such as non  negative matrix factorization   in this way  PRON can simply remove the column from all datum  as the other column hold the same information   and then cluster on the reduce space   PRON think the question PRON be wrestle with be essentially this  be there a way to use information that may be present in the datum without a as part of PRON strategy for predict a   there be actually a name for the set of method that do exactly that  semi  supervised learning   while there be multiple technique  a method analogous to what PRON suggest in PRON question would be something like the following   here PRON refer to the set of datum with a as label and the rest as unlabeled   apply an unsupervised learning technique  such as cluster  to the full datum set  both label and unlabelled    transform the label datum base on the result of PRON unsupervised learning technique   apply a supervised learning technique with the transform set  possibly combine with the original set  of label datum   generally PRON would want to apply a cross  validation strategy as well to detect overfitting  
__label__scikit-learn PRON have a large sparse data matrix  bag of word  over large number of entry   PRON can easily treat PRON as a sparse matrix in sklearn model such as randomforest  but  if PRON want to use catboost  PRON need to turn PRON into a dense matrix  PRON be wonder if there be any efficient method to work with catboost that do not because this  for example  any internal build  in feature such as tfrecord of tensorflow  to load bacthe   
__label__machine-learning __label__deep-learning __label__algorithms PRON know that the algorithmic complexity of cnn and other method of deep learning can not be fully express in simple term  like big oh complexity  that say  how would one go about compare the efficiency  complexity of cnn to standard machine learning method  say decision tree  lda  naive bayes  etc    deep learning method be expensive  but how do PRON know how expensive PRON be comparatively   one deep learning method can apply to multiple problem  for example computer vision  natural language processing etc   the performance of the method may be different depend on the problem that the method handle  but there be  popular dataset  that usually use by deep learning researcher to benchmark PRON method  let say PRON have deep learning method  if PRON want to know how efficient PRON method for handwritten classification PRON can benchmark PRON method by classify mnist dataset and compute PRON accuracy  if PRON want to compare the performance of PRON method with another method PRON can search on the internet another method that use the same dataset for benchmark  here be an example 
__label__optimization __label__nonlinear-programming __label__conjugate-gradient for the optimization problem  undersetmathbfxin mathbbrnoperatornameargmin   fmathbfx  PRON can use the following standard nonlinear conjugate gradient method to find the solution    mathbfd0   nabla fmathbfx0   mathbfxi1mathbfxialphaimathbfdi where  alphai be find by line search    mathbfgi1   nabla fmathbfxi1    mathbfdi1 mathbfgi1   betaimathbfdi  PRON wonder for the problem  undersetmathbfxin mathbbrn    mathbfyin mathbbrnoperatornameargmin   fmathbfxmathbfy  where  f be a function of 2 variable  whether the correspond nonlinear conjugate gradient method be    mathbfd0x  nablamathbfx   fmathbfx0  mathbfy0   mathbfd0y  nablamathbfy   fmathbfx0   mathbfy0  where  nablamathbfxf and  nablamathbfyf be the derivative of  f with respect to  mathbfx and  mathbfy    mathbfxi1mathbfxialphaimathbfdix   mathbfyi1mathbfyialphaimathbfdiy    mathbfgi1x  nablamathbfx   fmathbfxi1mathbfyi1   mathbfgi1y  nablamathbfy   fmathbfxi1mathbfyi1    mathbfdi1x  mathbfgi1x  betaixmathbfdix   mathbfdi1y  mathbfgi1y  betaiymathbfdiy  also  PRON be not sure how to compute  betaix   betaiy if  betaixbetaiy be compute by the fletcher – reeves method  be  betaixbetaiyfracmathbfgi12mathbfgi2  where  mathbfgi    mathbfgix  mathbfgiy   or  betaixfracmathbfgi1x2mathbfgix2   betaiyfracmathbfgi1y2mathbfgiy2   if PRON introduce a variable  mathbf zmathbf xt  mathbf ytt in mathbbr2n  then PRON can write  fmathbf x  mathbf yfmathbf z and PRON fit the exact format PRON write in PRON outline of the first method and PRON will all work as describe  PRON just have to match thing like  nablaz fmathbf z    nablax fmathbf xmathbf yt  nablay fmathbf xmathbf ytt etc   for PRON two variable case   betai should be same for both  betaix and  betaiy  PRON be compute as  betaifracmathbfgi1tmathbfgi1mathbfgitmathbfgi  where  mathbfgi    mathbfgix  mathbfgiy   the  mathbfg be a 2x1 vector  thus  mathbfgtmathbfg give a single value  PRON may help to check subrat pathak s master thesis  a comparative study of non linear conjugate gradient method  where PRON numerically test the two variable nonlinear cg in several nonlinear function  
__label__eigensystem __label__numpy __label__polynomials PRON write a simple script to generate random polynoimal  displaystyle fz sumk0n ak fraczksqrtk     of high degree and find PRON root   for more discussion on random polyomial see here   n  500  a  nprandomnormal01n   n  nparangen11  n  npinsertn01   n  npsqrtn   n  npcumproductn   z  nprootsan1npsqrtn   pltxlim11    pltylim11    pltplotzreal  zimag       for small value of  here n150  PRON do get a the picture of the unit disk fill with zero   for large value of n there be an error message   PRON be sure numpy can compute eigenvalue of large matrix than this   right   linalgerror  eigenvalue do not converge  use the inversion operation PRON be possible to circumvent some of the exponential growth issue of the coefficient   write the coefficient backwards n1  instead of n1  PRON get the reciprocal of the root   n  500  a  nprandomnormal01n   n   10  nparangen1n  n  npinsertn01   n  npsqrtn   n  10n  n  npcumproductn   z  nprootsan1    z  1z  l  15  pltxlimll    pltylimll    pltplotzreal  zimag       t  nparange01001   z  1npexp2jnppit   pltplotzreal  zimag       then use the function  displaystyle z mapsto frac1z PRON can flip the root back   once the degree n be too large  this trick no longer work  
__label__optimization __label__efficiency __label__mixed-integer-programming __label__combinatorics in combinatorial optimization  there be many problem that can be formulate as either network flow model or mixed integer programming  mip   eg supply chain  transportation  and graph  base problem  some solver make use of logical andor graph  base syntax to efficiently solve network problem  and then the network simplex method be apply   also  as state in bazaraa  ms  jarvis  jj  and sherali  hd  linear programming and network flow  4th edition  hoboken  new jersey  wiley  amp  sons  inc  2010  page 453   PRON discuss appropriate datum structure that facilitate the implementation of such a graph  theoretic procedure on the computer  the overall efficiency with which such a procedure operate enable one to solve problem 200  300 time faster than with a standard simplex approach that ignore any inherent special structure other than sparsity   practically speak  from the aspect of time efficiency  be there any significant difference between model as a mixed integer programming and modeling as a network problem  and why  other than sparsity    which optimization solver be computationally fast at solve network problem    practically speak  from the aspect of time efficiency  be there any significant difference between model as a mixed integer programming and modeling as a network problem  and why  other than sparsity    yes  the reason network simplex be faster primarily have to do with exploit the total unimodularity of network matrix  basically  network matrix be always structure such that PRON submatrice have integral inverse  this property mean that for network problem  instead of have to enumerate a branch  and  bind tree to obtain an integral solution  PRON can invert the column of the coefficient matrix correspond to variable in the simplex basis and obtain an integral solution  these inversion operation be cheap than solve the branch  and  bind tree  in that PRON run in bad  case polynomial time  even good than ordinary simplex  which run in bad  case exponential time  but typically in linear time   branch  and  bind run in bad  case exponential time  and also typically run in exponential time for general mixed  integer program   sure  sparsity be nice too  and a typical property of network problem  but the main speed gain come from be able to use regular linear algebra operation to obtain integral solution instead of branch  and  bind  which be a glorified guess  and  check scheme  
__label__machine-learning __label__nlp PRON be work on a problem where PRON have access to a database with news article  PRON publication date and the number of view PRON get 24hrs PRON get publish   the objective be to be able to predict the number of view for any new article   PRON want to add a feature that  for a give article  give an indication of the number of view  publication of all article with a similar theme that happen a short time before the publication of this article  this would capture the  buzz  phenomenon   if there have be a lot of article on election  then PRON be quite likely that a new article on election will get lot of view    here be PRON strategy   PRON classify all article among x theme   PRON segment the timeline into y timestep   for each timestep   for each theme   PRON count the number of view of article of this theme publish in this timestep  ditto for the number of publication  PRON add these two value as feature for all article publish in the next timestep  so  PRON be wonder if this violate the iid  assumption   because  if PRON randomly separate the datum into train  test  this inject information from the target value of the test set as a feature of the training set   be this  allow    be PRON miss something  do PRON see another strategy   
__label__nlp __label__text-mining __label__similarity __label__cosine-distance PRON have be work on a small  personal project which take a user s job skill and suggest the most ideal career for PRON base on those skill  PRON use a database of job listing to achieve this  at the moment  the code work as follow   1  process the text of each job list to extract skill that be mention in the listing  2  for each career  eg  data analyst    combine the process text of the job listing for that career into one document  3  calculate the tf  idf of each skill within the career document  after this  PRON be not sure which method PRON should use to rank career base on a list of a user s skill  the most popular method that PRON have see would be to treat the user s skill as a document as well  then to calculate the tf  idf for the skill document  and use something like cosine similarity to calculate the similarity between the skill document and each career document   this do not seem like the ideal solution to PRON  since cosine similarity be best use when compare two document of the same format  for that matter  tf  idf do not seem like the appropriate metric to apply to the user s skill list at all  for instance  if a user add additional skill to PRON list  the tf for each skill will drop  in reality  PRON do not care what the frequency of the skill be in the user s skill list  PRON just care that PRON have those skill  and maybe how well PRON know those skill    PRON seem like a good metric would be to do the following   1  for each skill that the user have  calculate the tf  idf of that skill in the career document  2  for each career  sum the tf  idf result for all of the user s skill  3  rank career base on the above sum  be PRON think along the right line here  if so  be there any algorithm that work along these line  but be more sophisticated than a simple sum  thank for the help   use jaccard index  wikipedia  this will very much serve PRON purpose   perhaps PRON could use word embedding to better represent the distance between certain skill  for instance   python  and  r  should be close together than  python  and  time management  since PRON be both programming language   the whole idea be that word that appear in the same context should be close   once PRON have these embedding  PRON would have a set of skill for the candidate  and set of skill of various size for the job  PRON could then use earth mover s distance to calculate the distance between the set  this distance measure be rather slow  quadratic time  so PRON may not scale well if PRON have many job to go through   to deal with the scalability issue  PRON could perhaps rank the job base on how many skill the candidate have in common in the first place  and favor these job  
__label__r __label__visualization in the past few year there have be an explosion in the quantity and quality of available datum visualization tool in r PRON have long ago stop use plot base graphic in favor of the wonderful ggplot2 package  and PRON be consider go for interactivity now  perhaps use one of the js  integration package  and output html   however  PRON be often in the business of create long html report page with dozen of graphic in there  and PRON care about a number of factor   load  time for the graphic generate can not be too long   should work decently with shiny and rmarkdown  ideally flexdashboard    should have a non  insane syntax for tweaking option  ie no 20 line js code insert to change legend location    should be relatively well  document   should be actively maintain   what be PRON go  to package for interactive datum viz   PRON current go  to be some graphic from rstudio s flexdashboard and a set of relatively well  maintain package   leaflet  a library for create dynamic map that support pan and  zooming  with various annotation like marker  polygon  and popup   dygraph  which provide rich facility for chart time  series datum  and include support for many interactive feature include  series  point highlighting  zooming  and pan   plotly  which via PRON ggplotly interface allow PRON to easily  translate PRON ggplot2 graphic to an interactive web  base version   rbokeh  an interface to bokeh  a powerful declarative bokeh framework  for create web  base plot   highcharter  a rich r interface to the popular highcharts javascript  graphic library   source  flexdashboard components 
__label__matlab __label__computational-physics __label__octave __label__maple how to define a variable which be an integral involve cauchy principal value inside in any computer programming language  PRON want to know how to break down the procedure step by step from a computational science viewpoint   this be the integral PRON encounter    somega    int0infty d omega frac1omega  left  mathcalp  left  frac1     omega   omega  right   mathcalp  left   frac1omega   omega  right  right     the lhs be a function of w   the rhs be a function of w   for example  xt   2t1 define the variable x in term of t t  1 mean x  3   for example in matlab   t  1   x  2 t  1   now PRON want to define the variable s in term of w   for example   w   1   s  2w   1    lt    but what be s in term of w  in PRON question   then s  3 here  but PRON be not such a simple expression here  PRON need the expression of s or set up the integral on the rhs so PRON can get the expression of s in term of w  like in line     for example in matlab  be PRON need to do PRON use sym  symbolic  or do PRON have to do PRON by fourier transform fourier   note that the rhs be a definite integral of w but since PRON will disappear  w  be the only variable of s here   or to cut off the question into part of just define cauchy principal value  PRON browse in matlab documentation httpwwwmathworkscomhelpsymbolicinthtml  int1x  1   x  0  2   principalvalue   true  be what PRON encounter  but now PRON be not calculate improper integral  PRON want to define the cauchy principal value of   frac1x as in complex analysis and distribution  ie   pleftfrac1xright   limvarepsilonrightarrow 0  leftintabvarepsilon  1xmathrmdxintbvarepsilonc 1xmathrmdxright  related question   sokhotski – plemelj theorem  PRON can do PRON in maple like this   first PRON need to specifiy the low and upper limit  a  b  and c  for normal  primitive integration  PRON use  int1x  x  a  c    now to find the cauchy principal value  there be build  in option in maple int   int1x  x  a  c   cauchyprincipalvalue     but if PRON actually want type in PRON then  limitint1x  x  a  b  epsilonint1x  x  bepsilon  c   epsilon  0  right   
__label__matlab __label__pde PRON want to solve a 1d heat conduction pde use matlab which look like     rho cp dfracpartial tpartial t   dfracpartialpartial zleft  lambda dfracpartial tpartial z  right       where the initial condition be t1  in kelvin  and the final condition be t2  also in kelvin   the difference be t1 take different value from t0 hour to t24 hour  t be time  and t2 be constant  say 200 k   PRON check the pdepe toolbox in matlab and PRON feel the condition that PRON want to implement may not be possible through the toolbox   can anyone suggest a way to create a loop through which t1 can read value from an array so that a diurnal plot can be create   this problem can be solve use the pdepe solver in matlab   if PRON understand well  t1 and t2 represent PRON left and right boundary condition for the spatial domain      tz0t   t1  t    tz  lt   const   t2      initial  and  final  term be usualy relate to the time domaine   the initial  ie at time t0  temperature field could be  for instance  uniform in  z   tz  t0   const    specify use the icfun function    and there be no final condition in time   spatial boundary condition be specify by the bcfun function  which return value of the  p and  q coefficient for the left and right boundary  see eq  1  6 in the matlab doc    these coefficient can be function of time  in PRON case PRON will be something like   function  pl  ql  pr  qr   bcfunxl  ul  xr  ur  t   pl  t1  t    pr  t2   ql  0   qr  0   function t  t1  t   t   
__label__svd this should be easy  but   PRON would like to express the singular value decomposition of a 2 x 2 complex matrix  a as function of PRON coefficient  aij in  close form   no intermediate value  straight up   what PRON mean be that if PRON express the svd in term of eigenvalue  say  then PRON have to write those eigenvalue in term of the  aij s and substitute  in other word  PRON do not want to write the solution to svd in term of the solution of something else   possibly useful background   in the wikipedia page on svd  if PRON write  a as a linear combination of the pauli matrix  and identity   the singular value be express in term of the correspond coefficient  but then PRON do not know how PRON find those coefficient  so PRON can not tell if PRON PRON be in  close form    httpenwikipediaorgwikisingularvaluedecomposition  thank   the pauli matrix and the identity matrix form an orthogonal basis of the space of  2times 2  matrix  so find the expansion coefficient amount to just a projection onto this basis  ie  PRON need to form the inner product of PRON matrix onto each element of this basis    but  for  2times 2  matrix  the answer be simple enough to write thing down right away  the eigenvalue be those value  lambda for which     det  alambda PRON   det beginpmatrix  a11lambda  amp  a12   a21   amp  a22lambda endpmatrix   0      this be a quadratic equation in  lambda       a11lambdaa22lambdaa12a21    0     for which PRON can write down the root immediately as a function of the element of  a  
__label__pde __label__nonlinear-equations __label__time-integration __label__operator-splitting there be lot of high order time splitting method as show by the list  with real and complex coefficient  ai  bi  ci       ecs delta t hat c   ebs delta t hat b  eas delta t hat a     ec1 delta t hat c   eb1 delta t hat b  ea1 delta t hat a  u     PRON be not clear which one should PRON choose  so be there any advantage of complex coefficient over real coefficient  also  which method be good in practice  intuitively  the least step  s and real coefficient seem an easy choice   the design of the  good  splitting scheme be discuss and investigate at length in this recent paper  disclosure  PRON be one of PRON author    in short  the most commonly use criterion be the size of the lead truncation error term coefficient   this can generally be make small by use a large number of stage  and the tradeoff can be worthwhile if PRON step size be limit by accuracy consideration in practice   PRON do not know of any inherent advantage to method with complex coefficient  generally that be a disadvantage since PRON have to do complex arithmetic which require more operation  
__label__testing __label__programming-paradigms what be the good practice for name output file from script that PRON be experiment with  PRON be try different parameter in PRON data generation script and PRON would like to keep track of which file correspond to which parameter  if PRON use the standard way of name with timestamp PRON soon lose track of how the file be create   this question may get close for be opinion  base  but PRON think PRON be an important question  so here be PRON opinion   as far as PRON know  there be no real industry good practice for this kind of thing  a lot of  donts  but not a lot of  dos  for example  stanford have a web page dedicate to file naming   what be work well for PRON be to create a series of folder whose name be numerical key  eg  7001    7002    7003      as well as a text file which give a description of each folder  eg date  numerical parameter  motivation  brief summary of result  etc     PRON like this scheme because the folder name be short which make PRON easy to find the one PRON want  have a description of each folder in one text file make PRON nice for find result month later  and remember why PRON do what PRON do   the good approach may depend on the problem at hand  just make sure that  1  PRON be well automate and 2  every detail be record  PRON have be name file with date and most important parameter  or the parameter that be be change    the problem of record how a particular piece of scientific datum be generate have come to be know as  provenance tracking   here be an article on PRON which also include the description of a software tool  sumatra  httprrcnsreadthedocsioenlatestprovenancetrackinghtml   another  simple python tool to do something similar be recipy  
__label__machine-learning how can PRON pass from fp  tree generate from fp  growth to the bayesian network   
__label__finite-element __label__pde __label__parallel-computing __label__spectral-method __label__dense-matrix PRON be read  use mpi  3rd edition   from william gropp  where in chapter 4 application section 413  PRON introduce an mpi application nek5000nekcem which be base on spectral element method  sem  couple with high  order timestepping to solve the govern ivm  bvm problem   there be one statement about sem that PRON do not quite understand  yes  PRON be not relevent to mpi     a central idea in the sem be never to form local elemental matrix  which would in general have   n16  nonzero per element  but to instead use precondition iterative solver that requrie only the action of operator apply to function    PRON have some limit backgroud in fem and sem  and PRON know that sem elemental matrix be usually much large than fem due to high polynomial order  but PRON still can not get what the author be try to convey with this statement   could someone please explain PRON with more concrete description  any help would be greatly appreciate   in sem there be usually an explicit formula for the action of the local matrix on local vector  the local matrix can be think of as numerically precomput operator that evaluate  interpolation  differentiation  and integration  people precompute PRON in tabulate form as matrix because that be the only way to produce a sparse matrix  which be the format direct solver like lu  cholesky   expect  for sem though these local matrix require a lot of memory  and iterative solver do not require a sparse matrix input  so PRON can avoid precomput these operator   one way to avoid precomput these be to compute interpolant  integral  and derivative by way of three  term  recurrence from underlie orthogonal polynomial basis  in sem most basis have such a recurrence relation  or be lightly modify version of a basis which do   in many case also PRON only need to store  reference  matrix by precomput these operator on a reference element  and then compute the desire action of these by geometric transformation alone  PRON can store geometric transformation factor for each element of the mesh  or even recompute that information at need to save even more memory   this only do the local action though  PRON be still up to PRON to get inter  element detail right  PRON will need to store some kind of connectivity graph  for PRON mesh and use that information to piece together all the local matrix  vector product into a full action of the operator   that full operator evaluation can be feed into a krylov solver as a black  box function  and that be enough information for PRON to try and solve PRON result system  
__label__finite-element __label__fluid-dynamics PRON have to model the boussinesq thermal equation on a 2d semicircular canal of ear with ansys software  PRON be not so familar with boussinesq eqation and PRON have trouble with find the parameter to expose to PRON draw model   be there anyone to show PRON the solution    
__label__nonlinear-equations __label__diffusion consider a thin film with a perpendicular apply magnetic field   ha  a  m  in z  axis  in fact  by increase   ha the magnetic field penetrate gradually the film  the equation for the time evolution of the local magnetization   m  a  m   scalar variable  be     frac  dmr  tdt   2mathcalf1    k1  mathcalf   nabla   nabla mr  tnabla mr  t    fracdhat    dt     where  r    xy    mathcalf and  mathcalf1 be fourier and inverse fourier transform  respectively  and  k1 be the wave vector   the calculation of the temporal evolution of   m be base on discrete integration forward in time     frac  dmr  tdt   fracmr  tdelta t mr  tdelta t     at  t  0   m  0   how can PRON solve this equation    any type of suggestion or comment will be very appreciate   thank PRON   
__label__algorithms __label__c++ __label__interpolation __label__image-processing PRON be transform cartesian image into polar image    x  y     angle  radius   PRON fill the polar image by iterate on each of PRON pixel and fill PRON by do the reverse polar transform  for a give  angle  radius  pair  PRON be interpolate in the source cartesian image at the follow location   xcartesianpoint  cos  angle   radius  centerx   ycartesianpoint  sin  angle   radius  centery    PRON do some computation with the polar image   then PRON transform back the polar image in cartesian coordinate  again  PRON be fill the destination image by do the inverse transformation  for a give  x  y  point  PRON be interpolate the value from the polar image point locate at   angle   atan2ycartesianpxl  xcartesianpxl   mpi   radius  getradiusxcartesianpxl  ycartesianpxl    xpolarpoint  angle anglestep   give the index in the polar image correspond to the angle  ypolarpoint   polarimageheight  maxradius   radius   the point index be float point value that PRON use to interpolate in the source image of the give transformation   this code work well but give artifact like those  first image be the source  second be the transform source in polar transform back in cartesian  the artifact be more apparent on the pixel far from the center    source image httpimageshackusaimg5393131932528png  transform image httpimageshackusaimg63154593b3365png  edit  what PRON do with PRON polar image   PRON be polar transform the source image with the goal of blur PRON  this be do to get zoom  spiral like blur  the blurring be do by do an fft of the polar image which be transform back  the artifact would not be a problem if the whole image be blur  but PRON be possible for the user to specify a gain matte that modulate the blur radius for each pixel  so  some pixel may be fully blur and other not at all   since the underlie grid for PRON polar representation become coarser in the circumference as PRON go further from the center  PRON be not surprising PRON get artifact there  PRON simple suggestion then would be to just use many more pixel in the polar representation   PRON be not clear to PRON how PRON be  interpolate  from the code PRON provide  be PRON simply take the value from the near pixel  a least  square interpolation would likely be good  for a give pixel PRON want to find the color for  find the 4 surround pixel of the previous image and compute an average weight by the inverse of PRON distance   PRON can think of three option   significantly increase the resolution of PRON polar representation   instead of a linear interpolation  create spectral representation of the image  eg by fft   this will be considerably more accurate   apply a blur  gaussian filter to the final image or anti  alias  this will not actually improve the quality of the transformation  but PRON will get rid of the jagged border edge  
__label__r __label__classification __label__feature-selection __label__categorical-data PRON have a dataset with 261 predictor scrap from a large set of survey question  224 have value which be in a range of scale  some 1  10  some 1  4  some simply binary  all use 0 where no value be give   and the rest be unordered category   PRON be try to perform classification use these predictor and identify the top n predictor  be think of the follow approach   convert the 224 order predictor into numeric  center  and scale   run separate modeling  PRON use caret from r   one for use the numeric predictor  another use the  remain 37 categorical predictor  both cross  validate within each modeling exercise    choose the respective best  fit model modeln and modelc for the numeric and categorical predictor   choose top n  say 10  predictor from model n and model c  combine PRON in an ensemble model that can handle both numeric and categorical datum  say  random forest    choose top n predictor in the ensemble model   PRON be go through this a roundabout way rather than directly fit all predictor into an ensemble model to try and reduce the complexity of the problem first  and because in r  PRON be have a problem with too many level from the predictor    would this be a valid approach to identify the n most salient predictor   any possible issue to mitigate   just some random thought  do PRON have a mathematical model to base on  for example  PRON want to predict how pressure be change against temperature  PRON will not drop any predictor that be relate to  temperature   no matter how remote PRON be  if so  that should govern PRON choice of predictor and PRON should start with that as that would lend more credibility to PRON final model   if not and PRON just want an algorithm to pick the good predictor  have PRON think of run a regression model with l1 norm on  this will drive out insignificant factor and PRON can start with that set as a basis   ricky   loose thought   depend on the algorithm PRON intend to use  center may not be a good idea  eg if PRON go for svm  center will destroy sparsity   PRON would suggest not to handle order  unordered separately  as PRON be likely to miss interaction that way  if the categorical one do not have too many possible value  randomfor in r can handle factor   if that be an issue  as PRON seem to hint   PRON think PRON have two possibility  binary indicator or response rate  if PRON be feasible in term of computational cost  i would convert all factor to binary  use sparse matrix if necessary  and then try a greedy feature selection  caret  if memory serve  have rfe or somesuch   if that be too much trouble  try calculate response rate  average value per factor level  PRON do not see any info whether PRON problem be classification or regression   PRON split PRON set into fold  and then for each fold fit a mixed effect model  eg via lme4  on the remainder  use the factor of interest as the main variable  PRON be a bit of a pain to setup all the cv correctly  but PRON be the only way to avoid leak information   hope this help   k 
__label__finite-element __label__fortran __label__adaptive-mesh-refinement PRON be look for a program to obtain mesh to finite element code 2d and 3d as complete as possible  preferably in fortran 90 or c  c   for example  software  triangle  or   tetgen  generate mesh but PRON would like a software that create mesh with more information  for example  normal vector    may PRON suggest to PRON some software   PRON would say gmsh  PRON use PRON for a few finite element project  and PRON be mostly easy to work with  the mesh output format be very parseable  and there be at least one third  party parser  meshpy  that can parse the output  PRON also have a c api  and the mailing list get enough traffic  probably 10  20 message a week  that PRON question may be answer  in PRON experience  PRON be something like a 1in2 chance   
__label__machine-learning __label__data-mining __label__r __label__classification PRON be try to classify set of text file whether PRON belong to category a or category b use knn algorithm  here be the error that i get   gt  knnpred  lt knntdmstacknl  tdmstacknltest  tdmcand   error in knntdmstacknl  tdmstacknlt  tdmcand    dim of   test  and  train  differ   gt  dimtdmstacknl    1   184 1599   gt  dimtdmstacknlt    1   1 992  as obvious the number of word use by train dataset be different from train  how can PRON avoid this   PRON want to mention that this error i get when i divide the training and test dataset manually to  7030  ratio  however i do not get this error if i divide the training and test dataset use samlpl method    gt   create hold  out   gt  setseed500    gt  trainidx  lt samplenrowtdmstack   ceilingnrowtdmstack   07     gt  testidx  lt  1nrowtdmstacktrainidx    gt    gt   create model  knn clustering   gt  tdmcand  lt tdmstack    targetcandidate     gt  tdmstacknl  lt tdmstack    colnamestdmstack   in  targetcandidate     gt    gt   set up model   gt  knnpred  lt knntdmstacknltrainidx    tdmstacknltestidx     tdmcandtrainidx     gt  dimtdmstacknltrainidx      1   129 1599   gt  dimtdmstacknltestidx      1   55 1599  however this method will not work for PRON as PRON want to classify a document in a real time use the model that be build before   please help  kindly let PRON know if PRON need any other info   arun  PRON be avoid the error with sample purely by chance  the word be common   simplest way would be to merge training and test  construct a tdm on a joined set  separate into training and test again and then purge some column  the constant one in the train set  as PRON correspond to word occur in test only   useless for training   
__label__dataset __label__apache-hadoop PRON have a job that result in a directory of part file  PRON would like to read PRON as if PRON be one file  specifically  PRON would like to read PRON that way over a web interface   how can PRON do either one of these thing  be there a hadoop component which make this easy   hadoop fs getmerge  lthdfs  output  directorygt   ltlocal  filegt   this command can be use to concatenate the hdfs file into a single local file   ref  httphadoopapacheorgdocsr270hadoopprojectdisthadoopcommonfilesystemshellhtmlgetmerge 
__label__machine-learning __label__genetic-algorithms PRON have be look recently into what use ai  specifically machine learning  may have in automate engineering design   for a long time there have be algorithm that solve constraint satisfaction problem  and to PRON PRON make sense to consider engineering problem as a superset of constraint satisfaction problem   in spite of this  PRON have not be able to find any case of engineering design be automate other than a couple of case of genetic algorithm be use to optimise structural member   so PRON question be  why can not PRON find any example   the first thing that spring to mind be that PRON just have not be look hard enough  if this be the case  could anyone point PRON in the right direction   the other obvious answer be that PRON be not a widely research area  if so  why not   be PRON just due to lack of interest or be there technical hurdle  abstraction  complex logic  amp  reasoning  etc   that make this a much more difficult problem than computer vision  game  and so on   example of ga implementation   nasa ’s spacecraft evolve antenna  solution for tsp with big number of connection  PRON can find more detail here in PRON another answer about tsp  here be an example with python and here with c   from PRON experience  ga can be even use for a training of small artificial neural network  but with less efficiency than with traditional deep learning approach  also  PRON hear about successful project with use ga for scheduling  packing and  logistic  therefore  everything  that can be represent in a form of digital chromosome  can be serialize to and deserializ  back from a sequence of bit  and PRON can define a function to evaluate PRON efficiency – fitness or reward function can be optimize by use of ga 
__label__bigdata __label__software-recommendation PRON be on a project deal with a lot of datum in the form of image and video  datum relate to wind engineering   PRON requirement be to build a predictive algorithm base on the datum PRON have  PRON have find many tool with which PRON can analyse the datum where each tool have PRON own advantage and disadvantage  big datum be really new to PRON  PRON find PRON very difficult to choose a platform to start with  there should be other people here who should have deal with similar situation   what criterion should PRON mainly take into account before select a  tool for analyse big datum   some of the criteria that PRON have take into account  visualization  interaction  security  data access and integration  speed of response  integrated data mining  pattern matching  ease of use etc  as PRON can see the list that PRON have make for the criterion come from the extensive reading of different article on the topic  but PRON can not narrow down the list nor find the individual contribution of these criterion in the various tool available for analysis   let PRON also list some of the tool that PRON find after google  knime  statistica 2  rapidminer  orange  weka  keel  r and rattle   on what basis could PRON choose a tool from a list of tool that perform similar task   update base on comment  aim  to develop a software that analyse the datum come from the wind mill and generate report  the software should be able to predict when a wind mill can fail base on the analysis   the project be still in the phase of gather user requirements  maybe i be so early to come into conclusion about what tool should be use   someone else suggest that PRON should be finalise the requirement and then think about a tool that can help PRON get thing do  so be PRON possible that PRON find what and how thing should be analyse before find a tool  and be PRON also possible that PRON find an algorithm for predictive analysis without know what would be the result of the tool after analysis   syncfusion provide a big data platform which be an easy to use hadoop distribution for windows  PRON can help PRON get start quickly  syncfusion also provide a pmml processing library use which PRON can execute predictive analytic model  there be a dashboard platform also available that can help visualize the datum   all of the above be available for free through the community license if PRON qualify   note  PRON work for syncfusion    what criterion should PRON mainly take into account before select a  tool for analyse big datum   there be a lot of criterion which be to be take into account when the tool selection be concern  the can be   structure of the datum   the data model ex  hierarchical  tabular  etc   type of datum and what be the problem statement    time series  or classification  etc   speed  security  aim  to develop a software that analyse the datum come from the  wind mill and generate report  the software should be able to  predict when a wind mill can fail base on the analysis   almost all the exist analytic tool like python  julia  r  etc can do this   and be PRON also possible that PRON find an algorithm for predictive  analysis without know what would be the result of the tool after  analysis   yes   the predictive algorithm or technique can be infer by look at the datum and the content of the datum  PRON be not dependant on the tool   some point which PRON would like to include which PRON believe would be useful to PRON   select the database depend on PRON datum  and PRON be type   accord to PRON datum  a nosql database would be more relevant and suitable   select the algorithm and technique only after PRON have a clear knowledge about the problem statement and takeaway and also after clearly look at the datum for an exploratory analysis   if PRON want more flexibility  then use a tool  programming language like python  r and julia  else  PRON can use a tool like knime  orange  PRON have a python library too    rapidminer  etc  
__label__machine-learning __label__neural-network __label__keras __label__dropout just to summarize  understanding dropout and gradient descent  and  httpsstatsstackexchangecomquestions207481dropoutbackpropagationimplementation  suppose PRON need to implement invert dropout in PRON cnn  all the neuron output in dropout layer during feedforward phase be multiply by mask  p  where mask be 0 or 1  p be retain rate  but should PRON apply the same operation  include division by p  at the backpropagation phase  PRON suppose positive answer  see the second link above   but PRON need to know for sure   as give in the link  the answer be yes  note that PRON divide the mask by p so that PRON will not need to multiply by p in the test time and since this be a coefficient for the new activation  PRON will come out of the derivative in chain rule in backprop  
__label__logistic-regression in the follow example  the prediction obtain for  x00 after fit the model be  1   but expect be  0  x  nparray     0  0     1  0     0  1      y  nparray011    model  logisticregression    modelfitxy   x   00   y  modelpredictx0   PRON only work if PRON duplicate the training row of   x00   y0   but why   because PRON have a class imbalance and very little datum  PRON model be essentially work off of a prior probability that disproprtionately favor the positive class  and PRON do not give the model enough datum to escape this prior in the learning process  consider the follow detailed output from PRON model   modelscorexy   06666  modelcoef   array    036566712   036566712     modelintercept  array   018517658    modelpredictprobax    array    045383769   054616231       036566869   063433131       036566869   063433131     PRON model correctly identify that  00  be more likely to be negative than either of the other observation  but the model be not able to escape PRON bias towards the positive class  if PRON duplicate PRON dataset a couple of time  the model will eventually have enough  evidence  to escape this bias  geometrically  two thing will happen when PRON give the model more datum   the discrimination boundary will shift towards the negative class  this will manifest as an increasingly negative intercept term   the slope of the logistic curve will steepen  have the effect that a unit change in the input will have a large impact on the outcome  if PRON give the model enough perfectly separable datum  the curve will approach a step function and the model will output probability of 0 and 1  this manifest in the magnitude of the coefficient  with just three observation  the curve be fairly flat and change the value of the input do not have much effect on the outcome probability  so the result class assignment be essentially determine by the intercept alone   contrast the result from PRON original model with the what happen when PRON replicate PRON dataset 100 time   model2  logisticregression    x2  npmatlibrepmatx1001   y2  npmatlibrepmaty1001ravel    model2fitx2  y2   model2scorexy   10  model2coef   array    495489068   495489068     model2intercept  array200091433    model2predictprobax    array    088089304   011910696       004954891   095045109       004954891   095045109     this be because PRON have an imbalanced dataset towards class 0  PRON have take a look on the logistic regression coefficient PRON get  on the below chart 1  PRON have plot the decision boundary PRON get with PRON logistic regression  on the second chart  PRON have plot what PRON have expect   so why this difference and why do PRON logistic regression yield chart 1  the reason be even if PRON be a machine learn algorithm  PRON be not really smart  the logistic regression algorithm  want to minimize PRON cost fucntion  cross  entropy   cross  entropy can be define in a really simple way as the distance between PRON point and the decision boundary   but  in PRON training case  PRON have two class1 observation for only one class0 observation  thus the machine learn algorithm will see that in order to reach the low cost function as possible  the good choice be to promote the two class1 observation than the only one class0 observation  the cost function value be large on chart 2 than on chart 1  this be why PRON have yield chart 1 result  majority win   to avoid this problem  here be three idea PRON can do   undersample  oversample PRON dataset in order to have a similar number of observation in each class of PRON training set   change PRON observation class weight   use an other cost function like hinge loss that be the cost function use in svm and where the goal be to maximize the margin between class   in the simple language  PRON model  see  more number of class 1 example than class 0 example  because of very little datum   so the probability of have class 1 as output be high  as other have point out  this be a class imbalance problem  so  even if PRON give  00  in prediction  PRON will return class 1 as output   the current ratio of class 1 and 0 in training set be 21  when PRON duplicate the datum  ratio become 11  so the model now correctly predict PRON example  
__label__time-series an example of some time series input PRON have in mind  suppose PRON have multiple user work on a 20minute search task  use a commercial search engine like google  yahoo  or bing  PRON be the same task  but user may finish after 10 minute if PRON feel PRON have complete the task to the good of PRON ability  the y  axis be a  performance metric   like normalize discount cumulative gain   and measure be take either   at each query  at each minute  both the  of query and time be obviously not constant across user   the goal here be for visualization  visual comparison  but if there be method that be more suited to recommendation  prediction  PRON be happy to hear those solution  too   
__label__image-recognition __label__deep-learning __label__classification wolfram language image identification project launch an image identify site demo which return the top predict tag for the photo   how do PRON work  briefly  PRON mean what type of learn vision technology be use to analyze  recognize and understand the content of an image   the imageidentify project use the highly automate  superfunction  and as part of wolfram language api integration  PRON rely on a complex collection of meta  algorithm and build  in  knowledge   PRON have a build  in classifier train from a large dataset use wolfram data framework  wdf   however the main classifier be base on the deep neural network   source  how the wolfram language image identification project works  the algorithm be not perfect and misidentification be more likely to be cause by  irrelevant object repeatedly be in training image for a particular type of object    PRON can read more at wolfram language artificial intelligence  the image identification project  
__label__optimization suppose that  ax  in mathbbrm time n be a nonlinear matrix function of  x in mathbbrd PRON may assume that  ax be continuously differentiable  be there any good way to estimate  mathrmargminx  ax2   also assume that due to some growth property of the function  ax  this minimizer can only be achieve within a known compact set  there be well know method when  ax be linear with respect to  x  but PRON do not have that linearity here   PRON have try to implement a few quasi  newton method with vary success  and PRON have also try matlab s fminunc command with reasonable success  stabilize around local minima be a problem   unfortunately PRON be relatively unfamiliar with the literature on these thing  and the literature be quite vast   be there any general approach anybody could suggest   this seem to be exactly the problem address in the follow paper  just recast the problem of minimize  a2  into a problem of minimize the maximum eigenvalue of  at a   overton  michael l  and robert s womersley   second derivative for optimize eigenvalue of symmetric matrix   siam journal on matrix analysis and applications 163  1995   697  718   httpftpcsnyueducsfacultyovertonpaperspdffileseighesspdf 
__label__machine-learning __label__classification PRON look to classify semi  harmonic datum  an example datum set would be two variable the first being elapse time and the other be the measure value which follow a harmonic value with noise   time  c000  001  002  003  004  005  006  007  008  009  010  011  012  013  014  015  016  017 018  019  020  021   m  c4  5  6  7  10  6  4  2  1  1  3  5  7  9  13  13  13  14  9  8  4  2    the length of the data be about 600 value and have about 100 datum set of 4 different class  PRON have generate feature base on the relative peak and trough of the measure and PRON be run classification method on those feature but PRON be wonder what method exist for classification of the raw datum and if there be example andor key feature that PRON should generate   thank   
__label__pytorch __label__recurrent-neural-net __label__sequence-to-sequence __label__glorot-initialization in an rnn sequence  to  sequence model  the encode input hide state and the output s hidden state need to be initialize before training   what value should PRON initialize PRON with  how should PRON initialize PRON   from the pytorch tutorial  PRON simply initialize zero to the hidden state   be initialize zero the usual way of initialize hidden state in rnn seq2seq network   how about glorot initialization   for a single  layer vanilla rnn would not the fan  in and fan  out be equal to   1  1 which give a variance of  1  and the gaussian distribution with  mean0  give PRON a uniform distribution of  0s   for  each input  hide weight  variance  20   fan  in  fan  out   stddev  sqrtvariance   weight  gaussianmean00  stddev   end  for  for single layer encoder  decoder architecture with attention  if PRON use glorot  PRON will get a very very small variance when initialize the decoder hide state since the fan  in would include the attention which be map to all possible vocabulary from the encoder output  so PRON result in a gaussian mean of  0 too since stdev be really really small   what other initialization method be there  esp  for the use on rnn seq2seq model   
__label__r PRON be plan on run a poission mean glm in r  use biglm  glm package   however  the original dataset be very large  the datum that PRON be try to analyse be a set of 10 year worth of observation  with each year contain 15million observation  would PRON be possible to integrate into the glm 10 x yearly file  or do the original file PRON call in r have to be as one file   
__label__data-cleaning __label__databases __label__sql PRON have  fft frequency feed  from a particular node  to make the time difference linear  PRON have a day into four group6 hour each  and now i would like to insert the miss row  for example see below   PRON have try generate time series like  date and time  group  2012  02  24  0  2012  02  24  1  2012  02  24  2  2012  02  24  3  2012  02  25  0  2012  02  25  1  2012  02  25  2  2012  02  25  3  and PRON have a time series like  date and time group  2422012  1  2422012  2  2422012  3  2522012  0  2522012  1  2522012  2  2522012  3  may PRON know how to merge PRON in knime to achieve the below   date and time group  2012  02  24  null  2012  02  24  1  2012  02  24  2  2012  02  24  3  2012  02  25  0  2012  02  25  1  2012  02  25  2  2012  02  25  3  
__label__computational-physics __label__software before embark on this task PRON think PRON wise to ask if anyone know of an exist library that do this   PRON would be cod PRON in python and at least for starter work on the assumption that there be only a core and clad layer  PRON would ignore the  air layer    this be PRON first question on the the physics stackexchange so please let PRON know if PRON want additional information   the equation PRON essentially need to solve be with respect to  neff     nu neff   2 left  dfracvu w  right4  left  dfracnco2 jvuu jvu    dfrac  ncl2 kvww kvw   right   left  dfrac  jvuu jvu    dfrac  kvww kvw   right  where    nu  azimuthal mode number  this can take integer value  and PRON need to solve for each case    ncl refractive index of the cladding  constant number   nco refractive index of the core  constant number   u  k r sqrtnco2  neff2   w  k r sqrtneff2  ncl2     v2  u2  w2   constant    jv  bessel function of the first kind   kv  modified bessel function of the second kind  edit   PRON have be ask to add some info to make PRON more appropriate for scicomp   physics   this be the eigenvalue equation from solve maxwell s equation in the case where the refractive index be constant in a cylindrical geometry  there be two layer  the inner layer  core  and the outer layer  cladding   this be what be mean by step  index fiber  httpwwwrpiedudeptphysscitinformationtransferreflrefrrrcontentfibers20html  PRON have try to add some more info about each variable  hopefully that clarify thing   
__label__r __label__decision-trees __label__data-stream-mining how can PRON implement the cvfdt algorithm  concept adaptinfg very fast decision tree learner  for data stream  in r  by use hoeffding adaptive tree for function as cvfdt   
__label__machine-learning __label__r __label__naive-bayes-classifier PRON have build a naive baye model for text classification  PRON be predict correctly  but PRON be return  na  in prediction result if i put  type  rawi have see some result in stackoverflow to add some noisewhen i do that i be get all a category as 0 s and all b category as 1s  how can i get correct probability in naive baye   librarytm     librarye1071     librarysparsem     sampledata  lt readcsvproductscsv     traindata  lt asdataframesampledata160c12      testdata  lt asdataframesampledata6180c12      trainvector  lt asvectortraindatadescription    testvector  lt asvectortestdatadescription    trainsource  lt vectorsourcetrainvector    testsource  lt vectorsourcetestvector    traincorpus  lt corpustrainsource    testcorpus  lt corpustestsource    traincorpus  lt tmmaptraincorpus  stripwhitespace    traincorpus  lt tmmaptraincorpus  tolower    traincorpus  lt tmmaptraincorpus  removeword  stopwordsenglish      traincorpuslt tmmaptraincorpus  removepunctuation    testcorpus  lt tmmaptestcorpus  stripwhitespace    testcorpus  lt tmmaptestcorpus  tolower    testcorpus  lt tmmaptestcorpus  removeword  stopwordsenglish      testcorpuslt tmmaptestcorpus  removepunctuation    trainmatrix  lt ttermdocumentmatrixtraincorpus     testmatrix  lt ttermdocumentmatrixtestcorpus     model  lt naivebayesasmatrixtrainmatrixasfactortraindatagroup     result  lt predictmodel  asmatrixtestmatrix    PRON be assume that PRON be refer to this stackoverflow post that mention to add noise to the datum since the error seem to be come when there be one  or small  instance of a class in the dataset  be that the case with the training datum  if what PRON be try to predict be a rare  event  then a suggestion may be to balance the training datum by oversampl the rare class  hence add noise    provide the above be not work  another suggestion be to remove infrequent term in PRON term  document  matrix use the function removesparseterm   go beyond  give the amount of training datum PRON have  PRON would be good to evaluate if the term document matrix with the word PRON contain or frequency of specific word be sufficient to differentiate the class  if not  PRON should consider add new feature to describe the dataset   few suggestion   count of positive  negative word or a sentiment index that range from 1 to 1  if relevant for PRON data  type of word in dataset  index or count of adjective or noun or verb   again depend on PRON problem  amp  datum  rather than use term  document  matrix  try noun  phrase  finally  PRON be assume that PRON test data contain record for both class  if not  PRON be difficult to evaluate the model   hope that help  if PRON could formulate PRON question more clearly with the data problem and provide some example of the datum  that would help  
__label__pde __label__fluid-dynamics __label__fourier-analysis __label__spectral-method one very efficient way to solve stokes equation with periodic boundary condition  beginequation   eta nabla2  bfv   nabla p  f   nabla cdot bfv   0  endequation   be use the transverse projection operator  in fourier space   beginequation   tildebfv     frac1eta bfk2  left  bfi   frack kleft  k right 2  right  cdot tildebff    endequation   PRON believe a similar principle be behind the fractional step  ie splitting or projection operator  technique for solve the navier  stokes equation  eg doi  1010160021  99918990151  4    can PRON use a similar trick to help PRON out when the viscosity be not a constant function of space  in other word  what if PRON stokes equation look like this   beginequation    nabla cdot left  etabfr   nabla bfv  right   nabla p  bff    nabla cdot bfv   0  endequation   be there an efficient  non  iterative  method to solve such a system of equation   chapter five of the book boundary integral and singularity method for linearize viscous flow  c pozrikidis  may be useful  PRON write boundary integral equation for stoke flow in term of the green function for homogeneous medium  for the case where one fluid region lie within another  this allow to handle a piece  wise constant viscosity  PRON do not know if this be useful for the method PRON be use   PRON should mention  the projection in the projection method  fractional  step method in solve the navier  stokes equation be completely unrelated to this   PRON suspicion be that there be not a good non  iterative way to do PRON since the linear operator be not diagonal in fourier space   however  PRON be possible to solve equation like these without total disaster by iterate  since PRON can choose fairly large iterative step if PRON use semi  implicit method   to do this  PRON have to break up the variable viscosity term into two piece   here be a paper that do this for a cahn  hilliard problem with a variable mobility  the theory be the same  httpjournalsapsorgpreabstract101103physreve603564  PRON would think that multigrid  precondition iterative method would be most competitive  see  for instance  parallel scalable adjoint  base adaptive solution of variable  viscosity  stokes flow problem  burstedde  et al   cmame  2009 and efficient variable  coefficient finite  volume stokes solver  cai  et al   communication in computational physics  2014 for a few example  along with some reference  both approach be highly scalable  but fem approach seem to have more literature associate with effectively precondition solver than fvm approach  
__label__machine-learning __label__predictive-modeling PRON have an algorithm which have as an input about 20  25 number  then in every step PRON use some of these number with a random function to calculate the local result which will lead to the final output of a  b or c  since every step have a random function  the formula be not deterministic  this mean that with the same input  PRON could have either a  b or c  PRON first thought be to take step by step the algorithm and calculate mathematically the probability of each output  however  PRON be really difficult due to the size of the core   PRON next thought be to use machine learn with supervised algorithm  PRON can have as many label entry as PRON want   so  PRON have the follow question   how many label input should PRON need for a decent approach of the probability  yes  PRON can have as many as PRON want  but PRON need time to run the algorithm and PRON want to estimate the cost of the simulation to gather the label datum   which technique do PRON suggest that work with so many input that can give the probability of the three possible output   as an extra question  the algorithm run in 10 step and there be a possibility that some of the input will change in one of the step  PRON simple approach be to not include this option on the prediction formula  since PRON have to set different input for some of the step  if PRON try the advanced method  be there any other technique PRON could use   PRON be not sure if PRON understand PRON question  probably better to plot a scheme at least  but accord to what PRON guess from PRON question   q2 PRON probably need a simple mlp  multilayer perceptron   PRON be a traditional architecture for neural networks where PRON have  n input neuron  here 20  25   one or more hidden layer with several neuron and 3 neuron as output layer  if PRON use a sigmoid activation function range from 0 to 1  the output for each class will be  py1x  x  q1 so PRON question probably be  how many training datum PRON need for learn a model  and to the good of PRON knowledge the answer be as many as possible   and about the last question  PRON really could not figure out what PRON mean  PRON apparently have a very specific task so PRON suggest to share more insight for sake of clarification   PRON hope PRON could help a little  
__label__pandas __label__data-wrangling PRON be struggle with a pandas problem  PRON have the follow datum       symbol  side  status   origqty  executedqty   qty   availableqty   price   boughtvalue  soldvalue   dcalevel       dgdbtc  buy   filled    0125   012500000   012500000   012500000  002000700   000250088        dgdbtc  buy   filled    0125   012500000   012500000   025000000  001960100   000245013        dgdbtc  sell  filled    025   025000000  025000000   000000000  002005900    000501475      dgdbtc  buy   filled    0113   011300000   011300000   011300000  002203000   000248939        dgdbtc  buy   filled    0113   011300000   011300000   022600000  002160300   000244114        dgdbtc  buy   expired   0226   000000000   000000000    002125500          dgdbtc  buy   partial   0226   015800000   015800000   038400000  002126100          dgdbtc  sell  expired   0384   000000000   000000000    002196600          dgdbtc  sell  expired   0384   000000000   000000000    002214300          dgdbtc  sell  expired   0384   000000000   000000000    002189900          dgdbtc  buy   filled    0384   038400000   038400000   076800000  002082900   000799834        dgdbtc  buy   filled    0768   076800000   076800000   153600000  001984300   001523942        dgdbtc  sell  expired   1536   000000000   000000000    002074400          dgdbtc  sell  expired   1536   000000000   000000000    002094100          dgdbtc  sell  expired   1536   000000000   000000000    002076800          dgdbtc  sell  partial   1536   030300000  030300000   123300000  002065000          dgdbtc  sell  filled    1233   123300000  123300000   000000000  002070000    002552310        this a subset of the data group by symbol  for each symbol PRON want to populate the last column with a value that comply with the follow rule   each buy order  side  buy  in a series have the value zero  0    for each consecutive buy order the value be increase by  one  1    when a sell order  side  sell  be reach PRON mark a new buy order serie   rows with status expired be skip   example       symbol  side  status   origqty  executedqty   qty   availableqty   price   boughtvalue  soldvalue   dcalevel       dgdbtc  buy   filled    0125   012500000   012500000   012500000  002000700   000250088     0    dgdbtc  buy   filled    0125   012500000   012500000   025000000  001960100   000245013     1    dgdbtc  sell  filled    025   025000000  025000000   000000000  002005900    000501475      dgdbtc  buy   filled    0113   011300000   011300000   011300000  002203000   000248939     0    dgdbtc  buy   filled    0113   011300000   011300000   022600000  002160300   000244114     1    dgdbtc  buy   expired   0226   000000000   000000000    002125500          dgdbtc  buy   partial   0226   015800000   015800000   038400000  002126100       2    dgdbtc  sell  expired   0384   000000000   000000000    002196600          dgdbtc  sell  expired   0384   000000000   000000000    002214300          dgdbtc  sell  expired   0384   000000000   000000000    002189900          dgdbtc  buy   filled    0384   038400000   038400000   076800000  002082900   000799834     3    dgdbtc  buy   filled    0768   076800000   076800000   153600000  001984300   001523942     4    dgdbtc  sell  expired   1536   000000000   000000000    002074400          dgdbtc  sell  expired   1536   000000000   000000000    002094100          dgdbtc  sell  expired   1536   000000000   000000000    002076800          dgdbtc  sell  partial   1536   030300000  030300000   123300000  002065000          dgdbtc  sell  filled    1233   123300000  123300000   000000000  002070000    002552310        PRON have try the follow two way   mergeddfdcalevel    mergeddfmergeddfside      buy    amp   mergeddfstatusisinfill    partialgroupbysymboldcalevelapplylambda x  xshift1   1   this way throw an error   mergeddfdcalevel    mergeddfmergeddfside      buy    amp   mergeddfstatusisinfill    partialgroupbysymboldcalevelapplylambda x  0 if xshift1  else xshift1   1   PRON try the following alternative approach   symboldf  mergeddflocmergeddfsymbol      dgdbtc    tmpdf  symboldfsymboldfside      buy    amp   symboldfstatusisinfill    partial       tmpdfdcalevel    npwheretmpdfavailableqty    lt  tmpdfavailableqtyshift1   0  tmpdfdcalevelshift1   1   PRON work on some row and not on other and the first buy order in a serie remain nan  PRON have cod the follow which do work  but PRON be sure there be an easy way to do this with pandas   mergeddfdcalevel    np  nan  group  mergeddfmergeddfstatusisinfill    partialgroupbysymbol     colidx  mergeddfcolumnsgetlocdcalevel    for name  group in group   first  true  for index  row in groupiterrow     if rowside      sell    first  true  dcalevel  np  nan  else   if first   first  false  dcalevel  0  else   dcalevel  dcalevel  1  mergeddfilocindex  colidx   dcalevel  mergeddfmergeddfsymbol      dgdbtc    PRON hope someone can help solve this problem   a simple approach could be the follow   import numpy as np  import panda as pd  counter  0  def conditionalcumulativesumx    global counter  if xstatus      expired    return npnan  elif xside      buy    temporal  counter  counter   1  return temporal  elif xside      sell    counter  0  return npnan  frame  pdreadcsvsamplecsv    framedcalevel    frameapplyconditionalcumulativesum  axis1   assume samplecsv contain the datum provide in the example and that the empty value in the column dcalevel correspond to nan value  
__label__nlp __label__java PRON be inform of 5 java nlp library   apache ctakes ™  metamap  lexevs  httplexsrv3nlmnihgovlexsysgroupprojectslvgcurrentwebdownloadhtml   apache opennlp  PRON also plan to parallelize an nlp library via map  reduce with hadoop   however  PRON be new to natural language processing  so PRON do not know how to approach the problem   the goal be to download a set of clinical trial on cancer from wwwclinicaltrialsgov and parse eligibility criterion  both inclusion and exclusion criterion   identify the ecog score and annotate each cta with the allow ecog score   for example  in the follow document  if ecog be specify in inclusion criterion  PRON be not negate  if ecog be specify in exclusion criterion  PRON be negate  if both exclusion criterion and inclusion criterion be not mention  then ecog be not negate   document   eligibility criterion of cta  nct01572038   inclusion criteria   patients must have histological proof of a primary non  small cell lung cancer   bronchoalveolar carcinoma present as a discrete solitary radiological mass or  nodule be eligible   patients must be classify post  operatively as stage ib  ii or iiia on the basis of  pathologic criterion  at the time of resection a complete mediastinal lymph node resection or at least  lymph node sampling should have be attempt  if a complete mediastinal lymph node  resection or lymph node sampling be not undertake  any mediastinal lymph node which  measure 15 cm or more on the pre  surgical compute tomography  ctmagnetic  resonance imaging  mri  scan or any area of increase uptake in the mediastinum on a  pre  surgical positron emission tomography  pet  scan must have be biopsi  note  a  pre  surgical pet scan be not mandatory  the nodal tissue must be label accord to the recommendation of the  american thoracic society  surgeon be encourage to dissect or sample all  accessible nodal level  the desirable level for biopsy be   right upper lobe  4  7  10  right middle lobe  4  7  10  right low lobe  4  7  9  10  left upper lobe  5  6  7  10  leave low lobe  7  9  10  surgery may consist of lobectomy  sleeve resection  bilobectomy or pneumonectomy as  determine by the attend surgeon base on the intraoperative finding  patient  who have have only segmentectomie or wedge resection be not eligible for this  study  all gross disease must have be remove at the end of surgery  all surgical  margin of resection must be negative for tumor  no more than 16 week may have elapse between surgery and randomization  for  patient who receive post  operative adjuvant platinum  base chemotherapy  no more  than 26 week may have elapse between surgery and randomization  patient must consent to provision of and investigators  must agree to submit a  representative formalin fix paraffin block of tumor tissue at the request of the  central tumor bank in order that the specific egfr correlative marker assay may be  conduct  the patient must have an eastern cooperative oncology group  ecog  performance status  of 0  1 or 2  leukocytes  gt 30 x 10  9l or  gt 3000ul  absolute granulocyte count  gt 15 x 10  9l or  gt 1500ul  platelets  gt 100 x 10  9l or  gt 100000ul  total bilirubin within normal institutional limit  alkaline phosphatase   lt  25 x institutional upper limit of normal  if alkaline  phosphatase be great than the institutional upper limit of normal  unl  but less  than the maximum allow  an abdominal  include liver  ultrasound  ct or mri scan  and a radionuclide bone scan must be perform prior to randomization to rule out  metastatic disease  if the value be great than the maximum allow  patient will  not be consider eligible regardless of finding on any supplementary imaging  aspartate aminotransferase  ast   serum glutamic oxaloacetic transaminase  sgot    andor alanine aminotransferase  alt   serum glutamate pyruvate transaminase  sgpt      lt  25 x institutional upper limit of normal  if ast  sgot  or alt  sgpt  be great  than the institutional upper limit of normal  unl  but less than the maximum allow   an abdominal  include liver  ultrasound  ct or mri scan must be perform prior to  randomization to rule out metastatic disease  if the value be great than the  maximum allow  patient will not be consider eligible regardless of finding on  any supplementary imaging  patient must have a chest x  ray do within 14 day prior to randomization  patient  must have a ct or mri scan of the chest do within 90 day prior to surgical  resection if at least one of the follow be undertake   a complete mediastinal lymph node resection  or  biopsy of all desire level of lymph node  as specify above  or  a pre  surgical pet scan within 60 day prior to surgical resection if none of  the above be undertake then the ct or mri scan of the chest must have be  perform within 60 day prior to surgical resection note  a pre  surgical pet  scan be not mandatory  patient must have an electrocardiogram  ekg  do within 14 day prior to  randomization  women of childbearing age and man must agree to use adequate contraception  hormonal  or barrier method of birth control  prior to study entry and while take study  medication and for a period of three month after final dose  should a woman become  pregnant or suspect PRON be pregnant while PRON or PRON male partner be participate  in this study  PRON should inform PRON treat physician immediately  patients may receive post  operative radiation therapy  patient must have complete  radiation at least 3 week prior to randomization and have recover from all  radiation  induce toxicity  patient who have receive radiation therapy should also  be randomize within 16 week of surgery  patient consent must be obtain accord to local institutional andor university  human experimentation committee requirement  PRON will be the responsibility of the  local participate investigator to obtain the necessary local clearance  and to  indicate in write to either the national cancer institute of canada  ncic  clinical  trials group  ctg  study coordinator  for ncic ctg center  or the cancer trials  support unit  ctsu   for all other investigator   that such clearance have be  obtain  before the trial can commence in that center  a standard consent form for  the trial will not be provide  but a sample form be give  this sample consent form  have be approve by the national cancer institute  nci  central institutional review  board  irb  and must be use unaltered by those ctsi center which operate under cirb  authority  for ncic ctg center  a copy of the initial full board research ethics  board  reb  approval and approve consent form must be send to the ncic ctg central  office  please note that the consent form for this study must contain a statement  which give permission for the government agency  nci  ncic ctg and monitor  agency to review patient record  ncic  ctg centers  the patient must have the ability to understand and the  willingness to sign a write informed consent document  the patient must sign  the consent form prior to randomization  ctsu centers  the patient  or in the case of a mentally incompetent patient PRON  or PRON legally authorized and qualified representative  must have the ability to  understand and the willingness to sign a write informed consent document  the  consent form must be sign prior to randomization  patients must be accessible for treatment and follow  up  investigator must assure  PRON that patient register on this trial will be available for complete  documentation of the treatment administer  toxicity and follow  up  initiation of protocol treatment must begin within 10 work day of patient  randomization  patients may have receive post  operative adjuvant platinum  base chemotherapy   patient must have complete chemotherapy at least 3 week prior to randomization and  have recover from all chemotherapy  induce toxicity  patient who have receive  adjuvant chemotherapy should also be randomize within 26 week of surgery  exclusion criteria   prior or concurrent malignancy  patient who have have a previous diagnosis of  cancer  if PRON remain free of recurrence and metastas five year or more following  the end of treatment and  in the opinion of the treat physician do not have a  substantial risk of recurrence of the prior malignancy  be eligible for the study   patient who have be adequately treat for non  melanomatous skin cancer or  carcinoma in situ of the cervix be eligible irrespective of when that treatment be  give  a combination of small cell and non  small cell carcinoma or a pulmonary carcinoid  tumor  more than one discrete area of apparent primary cancer  even if within the same lobe   t4  iiib   clinically significant or untreated ophthalmologic  eg sjogren s etc   or  gastrointestinal condition  eg crohn s disease  ulcerative colitis   any active pathological condition that would render the protocol treatment dangerous  such as  uncontrolled congestive heart failure  angina  or arrhythmia  active  uncontrolled infection  or other  a history of psychiatric or neurological disorder that would make the obtainment of  informed consent problematic or that would limit compliance with study requirement  patient  if female  be pregnant or breast  feed  neoadjuvant chemotherapy or immunotherapy for nsclc  however  patient may have  receive pre  operative limited field  low dose  less than 1000 cgy  external beam  radiation therapy or endobronchial brachytherapy or laser therapy for short term  control of hemoptysis or lobar obstruction  full dose pre  operative radiotherapy of  curative intent be a because for exclusion  patient may have receive post  operative  adjuvant platinum  base chemotherapy however non  platinum  base chemotherapy be a  because for exclusion  history of allergic reaction attribute to compound of similar chemical or biologic  composition to the agent use on this trial  patient with ongoing use of phenytoin   carbamazepine  barbiturate  rifampicin  or st john s wort be exclude  incomplete healing from previous oncologic or other major surgery  PRON want to find document with a query as below   input   patient with eastern cooperative oncology group  ecog  performance status  2  will be exclude  output  cta  nct01572038  labels  ecog 0  ecog 1  ecog 2  what be the simple way to approach this problem   
__label__finite-element __label__fluid-dynamics __label__finite-volume __label__discretization PRON be wonder if anyone could help with understand the geometric conservation law for move domain  PRON come across link1  and have try to understood the paper by farhat et al link2   so far PRON understanding be that the two thing to consider be 1  the time step to which the cell area use for integration of flux term correspond to  2  the time step at which the mesh velocity   dotx  be be evaluate  finally  the ale equation must also be satisfied for a uniform flow case   the paper state a gcl law as     aixn1aixninttntn1intpartial cixdotxoverrightarrowndsigma dt  where  a refer to the area of cell   c refer to the cell area sweep by the  flux  t be time  x be the spatial coordinate at the current configuration  and PRON think  sigma refer to the surface area of integration correspond  c   PRON have a few question that PRON would really appreciate some help with   1   what be  c in 1d be c refer to the length of the cell or the area  which be set to 1 in 1d  2  in the paper  just before equation 16   for a uniform flow  PRON set the variable be solve for at different time step equal to each other  be that not the definition of steady flow   3  finally  be there a way to determine whether a set of numerical result in 1d  obey gcl or not  without actually go through the equation  for instance  PRON understanding be that the result should stay independent of the move domain  so if PRON compare the result obtain use   dotx  0 and  dotx  neq 0  and show that PRON be not the same   if there be any simple paper or example on gcl  please let PRON know   thank PRON in advance   1  in 3d  c be a sweep volume  in 2d a sweep area and in 1d a sweep length  ie the change in length of a cell between  tn and  tn1  2  uniform flow be steady  the fact that the grid deform imply that the simulation be unsteady  but if the gcl be satisfied then the flow PRON should not change   3  yes  that would be enough   as far as PRON know  the first  and easy  paper to introduce the idea of sweep volume be demirdzic  amp  peric  1988 for incompressible flow and finite volume  PRON seem to have be reinvent later in the context of compressible flow and finite element   in etienne ea  2009  PRON will find a nice overview  PRON introduce 3 level of gcl compliance  exact solution of no  flow  exact solution of uniform flow and identical grid  timestep convergence rate on fix and deform grid  manufacture solution be use to verify that a code comply  
__label__statistics __label__algorithms __label__graphs __label__social-network-analysis PRON want to generate an  ntimes n matrix  a so as to target an  n vector of row sum and simultaneously all column sum should sum to 1  in addition to this  PRON have a prefixed number of element which be set to zero   for example  begin with      leftbeginarrayrrr   0  amp  1  amp  1   1  amp  0  amp  0   1  amp  1  amp  0  endarrayright      and the row sum vector   15  025  1t  PRON want to end up with     leftbeginarrayrrr   0  amp  a12   amp  a13    a21   amp  0  amp  0   a31   amp  a32   amp  0  endarrayright      under the following condition    a12   a13    15    a21   025    a31a32   1    a21a31   1    a12a32   1    a13   1   while this be simplistic  in general  PRON have  2n equation in  n2z unknown  where  z be the number of element fix to zero  so  this system of equation could be overdetermin or underdetermined  but PRON would like to be able to generate matrix like this such that all nonzero element lie in   01  this problem  be more of a combinatorial rather than statistical nature  can be though of as a max  flow problem in a bipartite network  each column of PRON matrix correspond to a  source node  in the left part of the bipartite graph  bpl  r  each row of the matrix correspond to a  destination node  in the right part  r capacity of each source node be the correspond column s sum  similarly  capacity be define for the destination via row sum  value  aij in the matrix describe flow of mass from  jth source to  ith destination along an exist edge   j to i of  bp PRON initially give 0  1 matrix describe which edge be present  eg  the matrix in PRON example prescribe the edge   3  2 to be absent from the network  along with all self  loop   if PRON denote  f to be the sum of all column sum  or  alternatively  the total capacity of all source   then PRON problem narrow down to find a max  flow from  l to  r in  bp and check whether the volume of the discover flow equal  f  note 1  feasibility   before plunge into find good flow  PRON may want to check PRON row sum  a necessary condition for PRON problem to have a solution be that the sum of all column sum  which be  n in PRON particular case of a column  stochastic matrix  should be equal the sum of row sum  as both number describe the same  the sum of all element of the target matrix   for example  for  n  3  and the vector of row sum from PRON example  the problem be unfeasible    note 2  upper bound   since  aij describe the flow from  jth source  and the capacity of all source be 1 in PRON setting  then each source can not send more than 1 unit of mass to a single destination  thus  each  aij will not exceed 1   note 3  low bound   to make  aij for each exist edge   j to i strictly positive  PRON can augment the max  flow problem with low bound on edge capacity  in a general setting  edge capacity bound be  0  and   infty PRON may want to replace the low bind with some positive number  small enough not to interfere with the discovery of the good flow and large enough to satisfy PRON edge saturation need  
__label__software __label__molecular-dynamics PRON have just start learn molecular dynamics use dlpolyclassic and analysing trajectory use nmoldyn  for this purpose  PRON want to construct a micellar system of  say of single micelle  in PRON case c12 tab  PRON wish to know how to construct the initial configuration for a such a case  ie to say what be available package for this purpose   
__label__numerical-analysis __label__convergence __label__error-estimation question  consider the iterative improvement algorithm below   start with  azi  ri and   a  ehatzi  ri derive a formula show how the absolute error in the   i  1st iterate  verthatxi1   xvert be bind by a multiple of the absolute error in the  ith iterate  verthatxi   xvert argue that the magnitude of this multiple be dependent on the condition of a  and be less than one  hence convergence  if  a be well  condition   PRON work   PRON know the following     frac1kappaa   fracvert rivertvert bvert  le fracverthatxi   xvertvert xvert  le kappaa  fracvert rivertvert bvert  be  kappaa the condition number of  a   now do      verthatxi1   xvert  verthatxi   zi   xvert le verthatxi   xvert  vert zivert le fracverthatxi   xvertvert xvert  le kappaa  fracvert rivert  vert xvertvert bvert   vert a1rivert     vert avert  vert a1vert fracvert rivert  vert xvertvert bvert   vert a1rivert le vert avert vert a1vert fracvert rivert  vert xvertvert bvert   vert a1vert  vert rivert     vert a1vert  vert rivert leftvert avert fracvert xvertvert bvertright  what PRON do be not correct because PRON need to bound the absolute error of  hatxi1   x by a multiple of   hatxi   x which PRON be not sure how to do   
__label__machine-learning __label__classification __label__deep-learning __label__similarity __label__distance PRON be work on a face recognition application use deep learning  to plot the roc curve and do performance evaluation  PRON extract the feature from the last layer of the deep neural network and PRON have try euclidian and manhattan distance between high dimensional feature representation  but this will not give PRON accurate measure in term of comparison as face have intra  personal and interpersonal difference and other challenge such as pose  light condition  and expression  PRON come across technique such as joint bayesian  sparse projection  kl  divergence base regularizer  priorem algorithm  etc  be there any other approach that PRON be miss here  what would be the efficient approach to generate similarity score   a common method be define  landmark   a set of specific point that exist on every face    for example  the outside edge of each eye or the bottom of the chin be landmark on most face  train the deep learning network to learn the relationship between these landmark which will form the embed space  then define a distance metric in this embed space  finally  create a threshold for similarity between set of point in this embed space  that threshold can be learn through  one  class  classification technique   one gotcha be pose  face turn different direction look totally different to a computer  assume PRON have isolate the face in an image  PRON be good practice to warp each face so that the eye and lip be always in the place in the image  this  and other normalization step  should be do early in the processing pipeline to make subsequent analysis much easy  
__label__finite-difference the finite difference matrix for the first derivative be   beginbmatrix  1  amp  1  amp  0  0  amp  1  amp  1  0  amp  0  amp  1 endbmatrix  the finite difference matrix for the second derivative be   beginbmatrix  2  amp  1  amp  0  1  amp  2  amp  1  0  amp  1  amp  2 endbmatrix   beginbmatrix  1  amp  1  amp  0  0  amp  1  amp  1  0  amp  0  amp  1 endbmatrix   beginbmatrix  1  amp  1  amp  0  0  amp  1  amp  1  0  amp  0  amp  1 endbmatrix    beginbmatrix  1  amp  2  amp  1  0  amp  1  amp  2  0  amp  0  amp  1 endbmatrix  why do multiply together two first derivative matrix not give the second derivative matrix   PRON matrix be not well define  the second derivative operator for example  take three consecutive datum value and output one second derivative value  that be not express by write a square matrix   there be a well  recognise calculus of finite differences  PRON can find book with that phrase in the title  and PRON will find everything to be consistent and useful  look at the operator   d2 u  fracun2   2 un1   undelta x2  if PRON taylor expand this for small  delta x PRON arrive at    d2 u  uxx   delta x uxxx   odelta x2  thus  d2  d2  in the limit as  delta x to 0   as PRON should   but note the error be first order in  delta x and so PRON be not a great approximation   consider instead  the one PRON be think of    d d u  fracun1   2 un  un1delta x2  again take a taylor expansion PRON have    d d u  uxx   fracdelta x212  uxxxx   odelta x4  thus this approximation be actually good since the error go down quadratically rather than linearly   however in the limit PRON be equivalent  
__label__convex-optimization __label__cvx in cvx  how do PRON return the value of the parameter over which the problem be minimize at the optimal value   by this  PRON mean  how do PRON obtain    x   argminx fx when solve the problem with cvx   PRON have go through the user s guide and do some google search  the only thing PRON could find be cvxoptval which only give the optimal value of  fx  PRON be just store in the regular matlab variable x  or whatev PRON be use as the variable in PRON problem  
__label__optimization be PRON pointless to use gradient base optimization algorithm if PRON can only provide a numerical gradient  if not  why provide a numerical gradient in the first place if PRON be trivial to perform finite differentiation for the optimization library PRON    edit   just to clarify  PRON question indeed be in a more general sense than a specific application  although PRON field of application happen to be likelihood optimization under various statistical framework   PRON issue with automatic differentiation be that there always seem to be a catch  either the ad library can not propagate to external library call  like blas  or PRON have to rework PRON workflow so drastically that PRON make PRON a pain to deal with  especially if PRON be work with type sensitive language  PRON gripe with ad be a separate issue altogether  but PRON want to believe   PRON guess PRON need to better formulate PRON question but PRON be do a poor job of PRON  if have an option to either use a derivative  free optimization algorithm or a derivative base optimization algorithm with the caveat that PRON can only give PRON a numerical gradient  which one on average will be superior   if PRON objective be smooth  then use finite difference approximation to the derivative be often more effective than use a derivative free optimization algorithm   if PRON have code that compute the derivative exactly then PRON be normally good to use that code rather than to use finite difference approximation   although some optimization library will compute finite difference approximation for PRON automatically use heuristic to determine the step size parameter  PRON can be good to use PRON own routine to compute the finite difference approximation either because PRON have good knowledge of the appropriate step size or because of special structure in the function that PRON code can exploit   another option that be often worth while be to use automatic differentiation technique to produce a subroutine that compute the analytical derivative from the source code for compute the objective function PRON   to complement brian s excellent answer  let PRON give a bit of  editorial  background  derivative  free optimization method be define as method that only make use of function evaluation  and be basically all variation of  sample the admissible set more or less systematically and save the good function value   that be all PRON can do give the information  these method can be roughly subdivide in  stochastic method  where the selection of sample be fundamentally random  mean that randomness be a crucial component  there can be other  deterministic component   these method be often motivate by physical or biological process and have correspond name like  simulate annealing    genetic algorithm   or  particle swarm  firefly  anthill method   there be rarely any convergence theory beyond  if PRON try long enough  PRON will hit all point  include the minimizer  with probability  1    whether that will happen  with any probability  before the heat death of the universe be another matter   as a mathematician  PRON would consider these method as a last resort  if PRON do not know anything about PRON function  this be all PRON can do  and PRON may get lucky   deterministic method  where the selection of sample be not random  ie  base purely on previous function evaluation  the most famous example be probably the nelder  mead simplex method  other be generate set search method  PRON be important to realize that this can only work if there be any  exploitable  relation between the value of the function at different point  ie  some smoothness of the function  in fact  the convergence theory for  eg  the nelder  mead method be base on construct a non  uniform finite  difference approximation of the gradient base on the function value at the vertex of the simplex and show that PRON convergenc to both the exact gradient and zero as the simplex contract to a point   the variant base on a standard finite  difference approximation be call compass search    model  base method  where the function value be use to build a local model of the function  eg  by interpolation   which be then minimize use standard  gradienthessian  base  method  since a finite difference approximation be equivalent to the exact derivative of a polynomial interpolant  the classical  numerical gradient  approach fall into this class as well   as PRON can see  the boundary between these class be fluid  and often just a matter of interpretation  but the moral should be clear  make sure PRON use all available information about the function PRON be minimize  to quote cornelius lanczos   a lack of information can not be remedi by any mathematical trickery   after all  if PRON do not know anything about PRON function  PRON may as well be completely random  and minimize a random value be a fool s errand   PRON question ask about gradient  base optimizer  so PRON think brian be right on   PRON would only share  since PRON be currently struggle with that PRON  some of the issue   the problem with finite difference be 1  performance  because PRON have to re  evaluate the function again for each dimension  and 2  PRON can be tricky to choose a good step size  if the step be too large  the assumption of linearity of the function may not hold  if the step be too small  PRON may run into the noise in the function PRON  because derivative amplify noise  the latter can be a real problem if the function involve solve differential equation   if PRON be possible to calculate the gradient analytically  or use sensitivity equation  PRON will certainly be more accurate and maybe faster   there be another approach that PRON can try if PRON have not invest too much time already in the software  and that be to run PRON with complex arithmetic   PRON be call complex step differentiation   the basic idea be when PRON evaluate the function  if PRON want PRON gradient with respect to parameter x  PRON set the imaginary part of x to a very small number ep   after PRON do the calculation  the imaginary part of the function s value  divide by ep  be the gradient with respect to x  when PRON want the gradient with respect to y  PRON have to do PRON all again  of course   what be interesting about PRON be that ep can be make very small   the reason PRON work be that the normal rule of differential calculus be precisely mirror in the rule of complex arithmetic   that say  PRON regard PRON as not a panacea  because PRON be not always easy to do a complicated function in complex arithmetic  PRON be not worth PRON if the gradient can be calculate analytically  and in the case of differential equation PRON be exactly equivalent to sensitivity equation  which PRON be do as necessary  
__label__clustering __label__scikit-learn __label__sampling PRON have a biased set of sample go into a binary classification sklearn pipeline  white and black sample   PRON be easy for PRON to fetch as many black sample as require  while white be a bit harder to get   PRON suspect the black set be unbalanced and there for PRON have an incentive to preprocess as many black sample as PRON hardware permit  and pick the most representative sample   the feature set  after drop completely invariant feature  be about 28k feature   the good approach PRON come up with be to cluster PRON black sample to a percentage of PRON white sample  for example say PRON have 500k white sample  PRON would then cluster the black sample to 100k cluster and pick five sample  randomly   out of each cluster   PRON question be the following   how can PRON have a transformer that will cluster the sample and mask out the unneeded sample in sklearn  be there something ready for such task or should PRON build PRON own transformer base on the clustering algorithm   can PRON point at any potential issue with what PRON plan to do  PRON think one potential risk here be not cluster the white sample  but since PRON be not get enough of those anyway   what cluster algorithm may be a good fit  PRON be look a different cluster algorithm that accept the number of cluster as parameter right now PRON plan to experiment with most and see which one work best  but a direction will be appreciate   additionally  PRON would like to read suggestion about other approach to sample subsampling in PRON case   clustering will be much too expensive for PRON purpose  most be on2   and the good one like hac may even be on3   PRON will not be able to run PRON on 300k instance   also beware of the prerequisite of the various algorithm  PRON may not be applicable to PRON datum  or the result may be as bad as random  also  cluster algorithm may behave rather unpredictable  and will profuce unbalanced data set  PRON would not be unusual if 99  of PRON data end up in the same cluster  while other cluster be almost empty  or even empty    PRON would consider first to remove all duplicate and maybe near  duplicate record  ignore identifier  of course   these algorithm be much more scalable and tolerate more datum type  that may already be enough to achieve PRON goal of datum reduction  
__label__finite-difference PRON be follow the article at httpwwwpaykininfoirinaproject2jsp  finite difference method   how to interpret this one  how to convert this to pseudo code     ui  j1   2ui  j   leftfrackhright2ui1j   2ui  j   ui1j    ui  j1  assume the ff give value  PRON be not sure if value be possible    initial condition   ui0sini   dui0dt0  boundary condition   u0j0  u95j0  dt1  dx5  what be the result when t5   the term on lhs be the variable s value at next time step ie  j1  which depend on the value of variable u at previous time step j and  j1    pseudo code could be like this   initialize ui  j  and let PRON equal to ui  j1    for  all grid point except those on boundary   use the discretized equation to advance in time   for boundary node  use the boundary condition   if PRON have to write in c with i vary from 0 to n  PRON can write   for  i  0  i  lt n  i     if  i  gt  0  ampamp  i  lt  n     unewi   2ui    dt  dxdt  dxui1   2ui   ui1    uoldi     else    unewi   ui       hope this help  if still get some query  feel free to ask  
__label__scikit-learn __label__random-forest __label__pandas PRON be new to sklearn and have trouble format the datum to predict and evaluate a confusion matrix   PRON be use this random forest tutorial   here be PRON code  from sklearnensemble import randomforestclassifier  import numpy as np  import panda as pd  dataframe  pdreadcsvoutputtxt   sept    df  pd  dataframedataframe   dfistrain    nprandomuniform0  1  lendf    lt 75  train  test  dfdfistraintrue   dfdfistrainfalse   feature  dfcolumns15   clf  randomforestclassifiernjobs2   y    pdfactorizetraineventcount     clffittrainfeature   y   PRON be this line for PRON prediction which be give PRON the error   pred  df6clfpredicttestfeatures     indexerror  only integer  slice       ellipsis       numpynewaxis   none   and integer or boolean array be valid index  PRON be not possible to write numpy style slice in dataframe  like   observation  data6   try dataframeix  dfix6    cf panda indexing  panda indexing 
__label__python __label__c++ PRON be a phd student in scientific computing and over the past few month  PRON spend a good amount of time learn python and c the right way  PRON feel that PRON have learn c well and PRON can use python to do what PRON want if PRON keep a good reference book around   PRON also know matlab enough to prototype PRON idea and get solution   if PRON be too bored to code python which be PRON first choice    PRON have read multiple time here that one should club c and c into one  c  c  because PRON be extremely different language with different motif and PRON completely agree with that viewpoint   although PRON can not claim to  know  c since PRON be always learn but PRON think PRON understand most of how PRON should use PRON and how PRON should use PRON not  the first language that PRON learn be c but PRON have be very long since PRON last use PRON  PRON question be essentially this   give that PRON know matlab  c and python  should PRON invest time in learn c  will PRON knowledge of the mentioned 3 language be enough for PRON to code   PRON research be more on the numerical linear algebra side but PRON also do some discrete event simulation  stochastic process consulting  PRON intention be to work in industry  PRON advisor suggest PRON learn c so that PRON stay employable though PRON have no personal preference of language    PRON be good to know c  and PRON can actually implement many of the oo feature of c that be relevant to scientific computation in c without much trouble  PRON be very instructive to take a look at the source code for petsc to see how PRON do PRON    that say  there be no one  size  fit  all answer to this question  there be many factor in choose a language  runtime be one of PRON  how long PRON spend write  debug  profile code be another important factor  in the end PRON want to be as productive as possible  know c will be great if PRON intend to get into the high performance side of research  or if PRON anticipate that PRON code will take an extremely long time to run  ie runtime trump PRON cod workflow time   otherwise if PRON know c and these thing be not an issue  PRON should be able to understand c code well enough to communicate with those who be use PRON   PRON answer a similar question once on stack overflow   the obvious answer be to learn everything PRON can  right   so let PRON look at the negative point of view   PRON already be more proficient in three language that PRON would consider to be more valuable than c for scientific programming   in PRON consulting work write scientific software PRON have use matlab  python  fortran  and java   PRON have never have a need to use c or c  but note that all of PRON work have be on new project   PRON would not advise a client to start a large project in c  c  but would recommend java  or even scala  unless there be some compelling reason to use c  c   for a small project PRON would go with python  probably   in PRON case  PRON look like PRON could deal with c on an as  need basis   PRON would expect that learn more about the domain in which PRON will work will pay off more than be a language jockey   in the end  PRON be the problem solution  not the the tool  that matter   knuth s saw about  premature optimization  apply to prepare for a career  as well   PRON be probably not worth PRON   c and c tend to occupy the same niche in computational science  maybe embed system be an exception   PRON use to be the case that c compiler be good  more feature complete  good at optimize  than c compiler  but from what PRON understand now  that be no longer the case   sure  c be a wonderful language  and PRON prefer PRON to c   and if PRON have the time and desire  PRON would recommend learn PRON  but if PRON be strap for time  PRON see no compelling reason other than  PRON need to work on a project that be be write in c   and even then  a great deal of PRON c knowledge will carry over    PRON think some c knowledge be useful  mainly to learn about how to compile thing with  extern c  linkage so PRON can pull legacy c  fortran library  eg lapack  into new c project  PRON would not invest much time into learn the legacy c library  that stuff can be pick up  google as need  or sometimes replace with equivalent functionality in the c stdlib   rather than  learn c   PRON would suggest that PRON be at a point now where PRON have do enough programming and work with enough programming language that PRON should focus instead on improve PRON programming technique and broad knowledge of computer science   then learn c if and when PRON need PRON for a particular project   experienced programmer quickly pick up new programming language as need for particular project   PRON can do this because PRON understand important concept that translate from language to language the syntax be not much of a problem if PRON have master those high level concept   when PRON watch student try to learn PRON second programming language  PRON will often see PRON struggle with understand some new concept that be not part of PRON first language   for example  student who start with fortran often struggle with pointer in c  student who know c often struggle with the object orient feature of c  and so on   the answer to this question really depend on what PRON really do   as a computer scientist who do high  performance computing  PRON would say PRON be essential to know  understand programming in c  but if PRON be just do stuff on computer  then PRON knowledge of c should be enough to get PRON through   more specifically  if PRON be really interested in computational aspect  ie write algorithm that get the most out of modern computer  PRON will not be able to avoid have to deal with low  level aspect such as memory management  data layout  simd vectorization  instruction  level parallelism  share  memory parallelism  or gpu computing  cuda and opencl be base on c    now  most of these thing can be take care of by compiler  eg for memory management  data layout  vectorization  andor high  level abstraction  eg openmp  openacc  optimize library   but only to a certain extent  do thing by hand usually involve more work  but the payoff can be an order of magnitude in performance   basically  whenever PRON want to control the detail PRON  PRON usually end up use c since PRON be  close to the metal   the question be  however  be that what PRON really want   the value of c  such as PRON be these day  be that PRON force PRON to become familiar with the functioning of the computer at a low level of abstraction  compare to  say python   now  c have all the low  level facility of c and could stand in just as well  but most people be encourage when lean c to avoid the low level abstraction and rely on the  well test and debug  high level one  thing like the container library  smart pointer  polymorphism and so on   PRON make sense for most programmer to spend most of PRON time play in realm of high abstraction  because PRON can spend more mental time and energy write what PRON mean than handle the picky little detail of how PRON accomplish PRON   the cost of that attitude be some risk of leaky abstraction bite PRON when PRON least expect PRON  but PRON almost certainly add to overall programming efficiency   so  if PRON be happy with what PRON know about the low level functioning of PRON computer PRON can safely put c off  and even if PRON be not  PRON could use a restricted set of c to accomplish the same thing   PRON will address only the comparison of c to c  while PRON be true that anything write in c can be port to c with a few syntactic touch  up  the community have different value   the c library community  more than almost any other  value binary stability   binary stability be critical for low  level library to avoid inflict constant pain on those layer above  especially when use with a binary distribution model and share library   c be the overwhelming preference for such library that need to just work  with the ability to ship new release without recompil layer above   PRON be possible to ship c library that operate at this level  but PRON end up need to  write c in c    for example  PRON would never place a structure definition with private or virtual member in a public header  PRON would never use template in a public interface  and PRON would never have an api base on inheritance from class that have any datum member   these constraint be necessary to precisely contain dependency  so that PRON can modify PRON implementation without change the binary interface   c tend to be much easy to bind from different language  due to PRON simple object model and well  define abi   if PRON be write code at the application level rather than the library level  then PRON binary interface be unimportant  so many of these concern go away   use of c language feature like inheritance and template still tend to produce more tightly couple code  lead to more time  consume recompilation as the project grow   in addition to more sprawl compile  time dependency  simply compile c code with a c compiler significantly increase compilation time  about 2x with most toolchain    if these concern interest PRON  or if PRON plan to work on low  level library  then spend time on c may be worthwhile   if PRON like use c language feature and be not too bother by binary interface and coupling tightness  then PRON may not be a good use of time   but keep c in mind if these thing thing start bother PRON in the future   there be a bit of copy  paste from PRON answer to this question  with a few extra detail   PRON work in industry  surveying and machine control equipment manufacturer  use weak embed processor  think mobile phone processor   where about 50  of the computational grunt be spend do numeric computation  mostly sensor fusion work   PRON have at one mathematician work for PRON who use to work in a hpc group  so this be one of odd place where PRON may eventually find a job   in this environment  where PRON will almost certainly have an operating system  eg  linux  qnx  wince   c be king  PRON use c only for kernel work  ie device driver  and for deeply embed work without an operating system  8bit micro    PRON do not use c for any numeric work  indeed  PRON do not have a fortran compiler for PRON platform   high performance matter to PRON because PRON only have a 1 watt cpu for processing   whilst PRON do not have some of the  glamourous  problem of parallelism  PRON have to be cache and memory aware  and increasingly need to be aware of simd  think neon   what be more  unlike hpc  PRON have to acutely aware of latency  this be machine control   and other operating system aspect like scheduling and context switching   memory allocation be especially nasty in this environment as PRON almost certainly mean a context switch    latency and  on a 400mhz cpu  expensive in term of time    these issue be independent of choice of language  so PRON be go to disagree with pedro s answer that c be necessary  after all  compiler intrinsic for simd be available when use c  PRON do  however  mean that PRON have to very careful about which feature of c PRON use and what PRON cost   so  to complete the answer  no  PRON do not need c PRON use matlab for analysis work and python for script so PRON two language other than c be good choice  at least in PRON industry  
__label__time-series __label__correlation normalized euclidean distance and normalized cross  correlation can both be use as a metric of distance between vector   what be the difference between these metric   PRON seem to PRON that PRON be the same  although PRON have not see this explicitly state in any textbook or literature   thank PRON   these two metric be not the same   the normalized euclidean distance be the distance between two normalized vector that have be normalize to length one   if the vector be identical then the distance be 0  if the vector point in opposite direction the distance be 2  and if the vector be orthogonal  perpendicular  the distance be sqrt2    PRON be a positive definite scalar value between 0 and 2   the normalized cross  correlation be the dot product between the two normalized vector   if the vector be identical  then the correlation be 1  if the vector point in opposite direction the correlation be 1  and if the vector be orthogonal  perpendicular  the correlation be 0   PRON be a scalar value between 1 and 1   this all come with the understanding that in time  series analysis the cross  correlation be a measure of similarity of two series as a function of the lag of one relative to the other  
__label__machine-learning __label__logistic-regression in case of bagged logistic regression  people suggest more the bag better will be result   there should be some threshold for more  be there any technique available which can suggest no  of bag for the type of datum available   for ex   details about datum  no  of feature  6  no  of record  1 m  bag size  20   no  of bags    for above mention detail  be there any way to devise no  of bag   from the start PRON have to state that PRON be not aware about any paper regard number of bag for bag logistic regression  therefore PRON assertion apply generally  for any bag ensemble   the short answer be no  and PRON do not think that if possible to be construct  there be a couple of reason that PRON will describe above   the first reason be that PRON depend on the complexity of the joint probability PRON want to estimate  technically any model PRON build for prediction purpose aim to estimate a conditional probability space on output variable give the joint input variable  do bagging put the problem if the estimation from select sample do cover the that relation in all place  to exemplify  PRON can have one input categorical variable with 2 level or with 10 level  PRON think more estimation be need for the later case   second reason if the sample PRON be representative  often in practice the sample be not purely a random sample  PRON contain various interdependency which can lead to problem  for example if PRON collect datum from multiple country and PRON have more sample from some country simply because of cost  but not relate with the phenomenon PRON study   third reason be that 20   a typical bootstrap sample have the same size as original  reduce the size be interesting because of performance gain or because of induce artificially variance which make the sample  more independent   however estimate what be the effect of decrease the sample size be again hard to estimate   fourth factor could be also the signal to noise ratio  in other word if PRON datum be noisy enough PRON need more sample to surface to interesting signal in a stable way   what PRON can do   since find the answer to PRON question be hard  and know PRON be close to similar with know the true model  there be thing which can be follow in order to find a proper number of bag  what PRON recommend would be to build repeatedly model with increase number of bag and see the error estimate  usually when those estimate have a low variance and add bag do improve the performance significantly than PRON can assume PRON need no more bag and bag with PRON choose model be not able to capture any other structure in PRON datum   PRON think PRON can put something together   generally speak about component classifier  overall classification be good when the decision rule of the component classifier differ and provide complementary information   having say that  one methodology PRON may try for suggest the number of bag be to create an arbitrary statistic that indicate which part of PRON feature space be cover by a particular bag   PRON could stop add bag when PRON reach a satisfactory level of coverage of PRON entire feature space  when PRON have expert classifier for each region of PRON feature space   PRON bag size could be find heuristically by start with a roughly equal number of pattern in each bag  n1   increase n1 to a value half way between PRON current value and the maximum if PRON find that n1 be to low   conversely decrease n1 to between the current value and the minimum if PRON find PRON be to high  
__label__finite-difference __label__nonlinear-equations __label__mathematica __label__differential-equations PRON have to find and plot a numerical solution for the following equation  PRON have to write a solver      ut    u2 uxx with the following condition  u0t   0  u1t   sqrtfrac2c2t    ux0   0   where  c be a speed of the wave  PRON try to use the following method     fracumn1   umntau   fracfm  12n  fm12nh  where   fm12n   um12n2fracum1n  um1nh  where   um12n  fracum1n  umn2  tt  01   tnn  10000   xnn  20   tau  tt  tnn   h  1xnn   u0x     x   dovm  0   u0hm    m  0  xnn    dov0  n   0  vxnn  n   0   dovm  n  1    vm  n    tau  h      vm  1  n   vm  n  hvm  1  n2   vm  n2  2     vm  n   vm  1  n  hvm  n2   vm  1  n22     m  1   xnn  1     n  0  tnn  1    u  listinterpolation   tablevi  j    i  0  xnn    j  0  tnn      0  1    0  tt      plot3dux  t    x  0  1    t  0  tt   plotrange gt  all   PRON have change the code and have get the correct solvation  but for the u0tu1t   0  ux0   fx   PRON do not actually understand how should PRON write the condition like u1t   ft    PRON get good advice in comment  but as PRON be only 1d problem  PRON can manage PRON use fully explicit method with very small time step  PRON check PRON code in mathematica and there be several misunderstanding there  after correct PRON  the code work for PRON   similarly  PRON description in the question contain  PRON suppose  some mistake  as PRON guess that this be some student project  so let PRON only indicate what shall be correct    PRON method in the question can be view as finite volume method for computational cell   xm12xm12 the quantity  fm  12n approximate the flux at the right point  the approximation in PRON question be inappropriate  check PRON  PRON shall use so  call central difference at point  xm12 analogously the nonlinear conductivity coefficient  um12n at  xm12 be approximate inproperly   PRON code anyway do not implement what PRON describe in PRON question  so  check the scheme and implement PRON exactly and PRON suppose PRON code can work   be careful about a correct input datum for PRON model  in PRON implementation PRON use different boundary and initial condition as PRON describe in the question   PRON hope after these indication PRON manage to finish the task  good luck  
__label__python __label__data PRON need to extract field like the document number  date  and invoice amount from a bunch of csv file  which PRON believe be refer to as  unstructured text   PRON have some label input file and will use the nltk and python to design a data extraction algorithm   for the first round of classification  PRON plan to use tf  idf weighting with a classifier to identify the document type  there be multiple file that use the same format   at this point  PRON need PRON way to extract the field from the document  give that PRON be x type of document  PRON think about use feature like the  most common number  or  large number with a comma  to find the invoice amount  for example  but since the invoice amount can any numerical value PRON believe the sample size would be small than the number of possible feature   PRON have no training here  bear with PRON    be there a good way to do the second part  PRON think the first part should be okay  but PRON be not sure that second part will work or if PRON even really understand the problem  how be PRON approach in general  PRON be new to this kind of thing and this be the good PRON could come up with   PRON be not sure if use a classifier be the good way to approach this problem   if PRON be something which can be easily extract use regex  then that be the good way to do PRON  if however  PRON want to use classifier  here be two question PRON need to ask PRON   one  what do the unlabell datum look like and can PRON design good feature from PRON  depend on the kind of feature vector PRON design  the complexity of the classification task may range from very easy  to impossible   a perceptron can not solve xor usually  except when PRON provide PRON with specific linear combination of the input variable    two  what do the label datum look like  be PRON representative of the entire dataset or do PRON only contain very specific type of format  if PRON be the former  then PRON classifier will not work well on file which be not represent in the label datum   if PRON just want to test run a classifier first  PRON can solve the problem of have more feature than train sample by use regularization  regularization force the training algorithm of the classifier to accept the simple possible solution  think occam s razor    almost all machine learning related package in python will have regularization option PRON can use  so enjoy  
__label__optimization __label__sparse __label__matrix __label__complexity __label__scipy PRON have a 7000x7000 sparse matrix  scipy   which PRON want to exponentiate  PRON have try use scipysparselinalgexpm  which work quite well for small matrix  take a few second for a 1000x1000 matrix  but PRON take too long time to compute for the matrix in question  be there a way to optimise this or work around the time complexity of this algorithm  apparently on2     for background  PRON have try the same with r package  expm  with similar result  so PRON be apparently more about the algorithmic approach to the problem rather than the specific software use  do PRON have any idea   
__label__feature-engineering there be one behavior of labelbinarizer  import numpy as np  from sklearn import preprocess  lb  preprocess  labelbinarizer    lbfitnparray0  1  1    1  0  0      lbclasses  the output be array0  1  2    why there be a 2 there   PRON think the documentation be kind of self  explanatory here  fit take in array of size nsample in which each element be the class of the datum or if the data point belong to multiple class  the input would be obviously of size nsample x nclass  that be what PRON give in as input in PRON example  each point can belong to any of the three class  that be why PRON have  0  1  2  as number of class  so as mention in the documentation if PRON try   gtgt  lbtransform0  1  2  0      1 0 0    0 1 0    0 0 1    1 0 0    and if PRON try a class that be non  existent after fit like   gtgt  lbtransform0  1  2  1000      1 0 0    0 1 0    0 0 1    0 0 0    no class name 1000 exist  so multi  target conversion for 1000 class case be plainly  0  0  0   hope this help  
__label__fluid-dynamics __label__simulation __label__hyperbolic-pde __label__fluid  defpdpartial   deflleftdefrright   defmdotdotm   defepsvarepsilon  consider a tube with longitudinal coordinate  x from  0  to  l and  vary cross  section  ax  derek s bale  a former student of  prof  randall j leveque  give in PRON phd  httpfacultywashingtonedurjlstudentsdbalethesispsgz  the euler equation for density   rho  impulse density   rho u and total energy density  e in such a tube in the form     beginarrayccccl   pdt rho  ampamp  frac1apdxarho u    ampamp  0  pdt  rho u   ampamp  frac1apdxlalfracrho u2rhoprr   ampamp  fracaap  pdt e  ampamp  frac1apdxlaepfracrho urhorampamp  0  endarray      as an example for a spatially vary flux function   thereby  the pressure be determine by the gamma  law  p    gamma1lefrac12 fracrho u2rhor  note  that PRON  have slightly reformulate the system such that beside the pressure  p only conserve  quantity be use   but  be PRON not good to change the conserve variable to the mass per length   mrho a  the mass flow  mdotarho u and the energy per length   eae   as an intermediate variable one could introduce a pressure force  pa pgamma1lefrac12fracmdot2mr  euler s equation reformulate with these new conserved quantity be      beginarrayccccl   pdt m   ampamp  pdxmdot  ampamp  0  pdt mdot  ampamp  pdxlfracmdot2mpr   ampamp  fracaap  pdt e   ampamp  pdxlepfracmdotmrampamp  0  endarray      in this way one obtain a system with the structure of the original euler  equation  include a source term  where the flux function do  not depend on the spatial variable   therefore  the numeric should work fine  PRON could use roe s method on this problem   do PRON miss here something   edit   bale use the first form to calculate supersonic wave through a tube with a narrow halfway  the speed  up in the narrowing cause a supersonic wave and the correspond shock wave   the variation of  ax be smooth      ax   begincas   1amp  text  for  xlt1  1fraceps2biglcospix11bigramptextfor  1leq x leq 3  1amptextfor  xgt3  endcases      PRON be aware of the fact that smooth transformation of the conserved quantity  such as   mmdot  earho  arho ua e have the potential to change the structure of discontinusous solution   but  be this really the case here    defrmcmathrm c   defvrvec r   defvavec a   defvevec e   deflleftdefrright  in the follow PRON try to show that bale s form be correct  if PRON complain  leave a comment   the equation PRON can trust be the integral equation   mass balance      lint0x rhoxt   ax   dxrt0t  intt0t lrho uxtaxrx0x d t   0     with the density  rhox  t and the momentum flow   rho ux  t as average over  ax  momentum balance      lint0x  rho uxt   axdxrt0t  vexcdotintt0tl  intpartial vrho vecvvecvcdot dveca   intpartial v  p dvecardt  0     with the volume  vbigcupxin0xax  there be no flow out of complementary part  armcpd vsetminus  bigl0time a0cup xtime axbigr of the tube  cross  section  0time a0 and  xtime ax furthermore  PRON approximate  intax    rho uvr  t  vecvvr  tcdot dva approx  rho ux  t  ux  t  ax  and remark that for integration in x  direction PRON have  vexcdot dva  d axx  that give      beginarrayl   lint0x  rho uxt   axdxrt0t  intt0t lrho uxt   uxt   ax    paxrx0x  dt     quadintt0t intx0x pxt    d ax   dt   endarray      thereby  intx0x pxt    d ax can be interpret as riemann  stieltjes integral which can be re  write as  intx0x pxt    ax   dx in the case that  ax depend smoothly on  x  the riemann  stieltjes integral suggest to use     pkfracakfrac12akfrac12ak      as a 1st  order update for the source term to capture discontinuity of  ax the 1st  order update of the flux function in the fvm  method be already design such that discontinuity of  ax be capture   energy balance   with a similar approximation for the average over the cross  section as for the momentum balance and with the zero flux out of  armc PRON get      lint0x extaxdxrt0t  intt0t bigglext    pxtuxaxbiggrx0x dt  0     interpretation  leveque describe in chapter 616  capacity  form differencing  of PRON fvm  book that conservation law of the form     fracddtintx1x2  kappax  qx  t  dx  fqx1tfqx2t       with a capacity function  kappax can be treat with the help of  capacity  form differencing   the more general integral form be     lintx1x2  kappax  qx  t  dxrt1t2   intt1t2fqx1tfqx2t    dt      in the above conservation law  ax can be see as capacity function   PRON see that the equation in bale s form be correct even for shock  wave with capacity  form differencing   the other form PRON have give in PRON question be also correct  for that case the conserved quantity stand right under the spatial integral on the left hand side  everything else be just combination of term without differentiation   now PRON believe that bale use PRON form of the equation to have a  simple example for a system with spatially vary flux function  
__label__text-mining __label__similarity PRON be work on evaluate when a pair of string object can be consider equal  eg give that PRON be talk about journal  be  international journal of air and water pollution  the same of  air and water pollution    and PRON be wonder what be the proper corpus to use to build a tf idf vectorizer   PRON be currently use as corpus all the distinct value of these string belong to the same type  in this example all the string refer to journal   be this a valid approach  why  what could be other valid approach   word mover ’s distance  wmd  be an algorithm for find the distance between pair of string  PRON be base on word embedding  eg  word2vec  which encode the semantic meaning of word into dense vector   the wmd distance measure the dissimilarity between two text document as the minimum amount of distance that the embed word of one document need to  travel  to reach the embed word of another document   for example   source   from word embeddings to document distances  paper 
__label__optimization __label__python PRON be have an issue with the implementation of nlopt in python  PRON objective be to minimize a somewhat complicated maximum likelihood function   PRON function be call mle and there be 6 parameter to estimate   find the gradient to this mle be not trivial  so PRON decide to turn to a numerical gradient function   def numgradf  x  step1e6       numgradf  function  x  num array  step  num  gt  num array  numerically estimate the gradient of a function f which take an array as  PRON argument       ary  lenx   curr  x  sponesary  ary    next  curr  spidentityary   step  delta  spapplyalongaxisf  1  next   spapplyalongaxisf  1  curr   return delta  step  then PRON implementation of nlopt go like this   def myfuncx  grad    if gradsize  gt  0   grad  numgradmle   x0   x1   x2   x3   x4   x5    step1e14   return mlex0   x1   x2   x3   x4   x5     opt  nloptoptnloptldslsqp  6   optsetlowerboundsmmin  smin  ming  bmin  vmin  pmin    min bind for each of the param   optsetupperboundsmmax  smax  maxg  bmax  vmax  pmax    optsetminobjectivemyfunc   optsetxtolrel1e15   optmaxeval10000  x  optoptimizex1  x2  x3  x4  x5  x6    minf  optlastoptimumvalue    print  optimum at   x0   x1   x2   x3   x4   x5   print  minimum value    minf  print  result code    optlastoptimizeresult    now the issue be this  the minimization process go wayyy tooo fast  in matlab  PRON take approx 1 hour and here in python 12 second  PRON do not get the same result in matlab use fmincon   PRON feeling be that the code do not recognize the optsetxtolrel1e15  and optmaxeval10000 statement because even if PRON increase the number  no change in the time process   or the problem be elsewhere  what be PRON do wrong   PRON should essentially never estimate the gradient numerically   PRON say the gradient be difficult to estimate  in general  if PRON be at all possible to get the gradient exactly PRON should do so  and use an appropriate algorithm  nlopt have several  the one PRON be use should be fine    however  if PRON can not get the gradient exactly  nlopt feature several derivative free algorithm  which PRON could use instead and expect to get good result  PRON think this be probably the easy solution to PRON problem  and would give PRON good result   the speed difference can easily be real however  difference in optimization algorithm and the fact that python be generally fast than matlab could explain the difference easily   tldr  change from nlopt  ldslsqp to nlopt  lnbobyqa   hope this help   to pass gradient from python to nlopt  PRON have to  grad     gradient     or npcopyto  not  grad  gradient     wrong  a new pointer  the nlopt python doc  have a big warning on this  unfortunately PRON be not in the tutorial example PRON use   by the way  on random startpoint  see nlopt  mlsl   a sequence of local optimization  use some other local optimization algorithm   from random startpoint  cluster heuristic     this theme have many many variation   also  fwiw  this plot  show 3 nlopt method from 10 random startpoint on a dozen academic test function in 8d  
__label__computational-geometry __label__reference-request what be some good resource  book  article  site  about polygon intersection and union algorithm   PRON be a big fan of joseph orourke s work   PRON highly recommend PRON book computational geometry in c  2nd edition  because PRON have a particularly good balance of theory and implementation   chapter 7 contain direct information pertain to polygon intersection   paul s suggestion be great  PRON would just like to add two more    geometric tools for computer graphics   schneider   computational geometry  mark deberg et al   on this note  PRON 2cent  come from experience   if PRON be consider cod such algorithm  PRON advise PRON kindly to take a look at boostgeometry andor cgal libray first  there be no need  hopefully  to re  invent the wheel  if PRON be cod in c  that is   gpc  general polygon clipper be a good implementation for boolean operation on polygon base on vatti s clipping algorithm  the page also contain link to other solution   one strategy be to look for modern algorithm for related problem  like collision detection  etc   often there be good strategy in slightly different application if PRON have a particular problem PRON be try to solve   as for implementation  PRON could check out the boost polygon library   a couple general book for computational geometry that be on PRON shelf be   computational geometry  an introduction by franco preparata and michael shamos be yet another good introductory book on computational geometry algorithm   computational geometry  an introduction through randomized algorithms by ketan mulmuley  be an excellently construct book good algorithmic coverage of a wide variety of algorithm for geometric problem  all do through randomize method  
__label__python __label__r __label__bigdata __label__sas __label__etl PRON want to use r or python to query big structured sql  type datum  but PRON be very slow compare to sas   PRON try use r and python to return a 13 million record oracle odbc passthrough query  the query take 8  15 second in sas  20  30 second in python  and 50  70 second in r do anyone know why   r package use   first PRON use the rodbc package in r to query to the oracle database  then PRON try the roracle package  but both package be much slow than sas   python packages use   for python  PRON use oracle s cxoracle package for the query   thank a lot   sean  this answer some of PRON question from a python perspective   be python any faster   this question be a bit tricky to answer  PRON will depend on PRON usage of python  but python be not a fast language per se  however  the pandas library in python have be report to handle table of 33m100 m row  see this  PRON PRON have use to handle around 10 m row from a postgre table  for a detailed experimentation use panda  see this  in the link PRON apply some operation on dataset of 88 m row and 74 column   do PRON need hadoop or parallel processing or something else to make r  python as fast as sas   before try use hadoop or spark  PRON recommend PRON to follow some optimization trick  tip     1  a beginner ’s guide to optimize pandas code for speed   2  use panda with large datum  this link  do not use hadoop  PRON data be not that big  can be useful too   also for a comparison of sas and panda PRON can read the pandas documentation on such comparison or this   be PRON because r and python be matrix  base language and sas be more  traditional  database  orient   python be not a matrix  base language  as far as PRON know the language do not offer by default any capability for handle matrix  PRON think PRON referring to the numpy  scipy stack  this be a separate library  
__label__machine-learning __label__neural-network in PRON text book PRON have read that whenever PRON reduce the mean of each feature from correspond feature in the training datum and divide each feature by PRON standard deviation  this process be call normalizing input datum   the bias term be not significant  PRON can not understand this  why be that   if PRON want to provide an extra clarification PRON would provide the follow image  the left one be bad condition and the right one be the one which have be normalize   
__label__machine-learning __label__neural-network __label__deep-learning __label__rnn PRON be go through some lecture from the deep learning course that geoffrey hinton teach on coursera and PRON come across this statement    rnn could potentially learn to implement lot of small program that each capture a nugget of knowledge and run in parallel  interact to produce very complicated effect    PRON have no idea what PRON mean nor see  figure out any example of this idea  and PRON be sure other people be wonder too   so if somebody would care to demonstrate  PRON would be much appreciate   create a model be have the computer write a small program  if PRON have a multi  layer network  then multiple nugget may be interact to produce complicated result   that PRON pretty much how thing like face detection work  PRON start with a series of really dumb  detector  that look like basic rectangle and PRON combine in way so that at the top  PRON be recognize brad pitt  
__label__tensorflow __label__keras __label__theano __label__parallel PRON be work on seq2seq model use lstm from keras  use theano background  and PRON would like to parallelize the process  because even few mb of datum need several hour for training   PRON be clear that gpu be far much good in parallelization than cpu  at the moment  PRON only have cpu to work with  PRON could access 16 cpus2 thread per core x 4 core per socket x 2 socket   from the doc of multi  core support in theano  PRON manage to use all the four core of a single socket  so  basically the cpu be at 400  usage with 4cpus use and the remain 12 cpu remain unused  how do PRON make use of PRON too  tensorflow could also be use instead of theano background  if PRON work   in order to set the number of thread use in theano  and  therefore  the number of cpu core   PRON will need to set a few parameter in the environment   import os  osenvironmklnumthreads     16   osenvirongotonumthreads     16   osenvironompnumthreads     16   osevironopenmp     true   this should allow PRON to use all core of all cpu   this can  of course  also be do in tensorflow   import tensorflow as tf  from kerasbackend import tensorflowbackend as k  with tf  sessionconfig  tf  configproto   intraopparallelismthreads16   as sess   ksetsessionsess    ltyour keras codegt  
__label__convnet __label__convolution __label__cnn  why would PRON use two convolution layer in a row with same spec  eg in vgg16 first two layer  PRON use 3 x 3 filter with 64 depth twice  what be the difference if PRON use 3 x 3 filter with 128 depth once   each layer have a limit amount that PRON can transform the layer below PRON  there be one linear component  weight sum of output of layer beneath PRON   and one non  linear component  typically relu    PRON be in theory possible to approximate any function with a large enough single layer in a fully  connect network  however  a stack of similar small layer be more expressive use less resource  that mean for the same number of parameter PRON have access to a more flexible function approximator  at some level of complexity for PRON target function  the cost  in term of cpu time  datum require and effort in training  of make a single layer wider be high than the cost of stack more  similar layer   in addition  for a cnn  PRON have to worry about receptive field  any feature map can only express value that the filter can  see  due to width of the kernel  as PRON add more layer  each kernel apply extend the width and height of the base image that the feature in the last layer effectively calculate over  if PRON also have a fully  connect layer after the convolutional layer  then PRON can in theory compensate for a poor receptive field with a very large fully  connect layer  but then PRON be back to the first problem of wide network with more parameter than strictly necessary to learn the function  
__label__linear-algebra __label__matrices PRON be try to figure out at exactly what dimension PRON be good to use strassen algorithm rather than the standard multiplication  PRON know that there be 18 addition and subtraction in strassen algorithm but PRON do not know how PRON must calculate PRON  this be a homework so PRON do not want direct answer without any explanation  any help be appreciate   the answer to this question depend heavily on the particular detail of the computer that PRON be use   in modern implementation  conventional matrix multiplication implementation  in the form of highly optimize version of the blas xgemm function  use block algorithm that be carefully tune to match the cache size of the processor   in comparison  strassen s algorithm be extremely cache unfriendly  and this make PRON difficult to get good performance on contemporary processor   one recent arxiv preprint claim that a strassen like algorithm can give modest performance improvement  up to about 25  faster  than intel s mkl for matrix of size  n3000  or large on an intel processor base system   austin r benson and grey ballard   a framework for practical parallel fast matrix multiplication  httparxivorgpdf14092908pdf  there be another important issue to consider here strassen like algorithm be less numerically accurate than conventional block matrix  matrix multiplication   in some application that inaccuracy can because problem at the application level   because of this  and because the performance improvement be not that large for reasonably sized matrix  few people make use of strassen like algorithm in practice  
__label__monte-carlo __label__data-analysis for monte carlo simulation  or any other numerical method that rely heavily on the quality of the pseudo  random number generate  ie even  desire distribution on a certain domain  for that matter  why be not evenly  space and perfectly  accurately distribute number  not pseudo  random  use as oppose to pseudo  random number   the reason PRON ask this be because PRON would imagine the main purpose of sample ton of pseudo  random datum as oppose to non  random datum would be to speed up the time take by the program that be sample that datum  much like how throw a bunch of paintball at a canvas  wall would  more easily and quickly  cover up that canvas  wall than carefully paint PRON with a paintbrush would  however  many pseudo  random number generator algorithm that PRON have see look more complicated and time  consume than just use non  pseudo  random  perfectly distribute datum   clearly PRON have a misunderstanding of this topic  could anybody help clear this issue up   the main purpose of sample ton of pseudo  random datum as oppose to non  random datum be relate to runge s phenomenon for polynomial interpolation  uniform spacing of interpolation point be often a bad idea  but choose good interpolation point require knowledge of the function PRON want to interpolate  or integrate etc    if PRON do not have that knowledge  or if compute the point be infeasible due to the high dimensionality of the function  PRON good bet  literally  be to draw point at random  PRON may not hit the  good  point often  but at least PRON be not systematically miss all of PRON    in PRON example  if PRON do not know where the wall be  throw brush at random mean PRON will likely    hit PRON earlier than if PRON be start in one corner and methodically work PRON way around  remember  PRON do not have to cover all the wall  just enough of PRON to see the shape    of course  if PRON point be truly random  PRON can happen with positive  albeit small  probability that there be a large part of space where none of the point land  or a wall PRON never hit  because all of PRON brush by chance go in the other direction   this be the point of quasi  monte carlo method  the point be still random  mean there be no possibly bad structure   but there be some guarantee of uniform coverage on a large scale  
__label__monte-carlo PRON be try to use matlab to simulate an atom decay process by use monte carlo approach  the process be as follow   suppose that atom 1 decay to atom 2  which  in turn  decay to stable  atom of type 3  the decay constant of 1 and 2 be  lambda1 and  lambda2  respectively   assume that at  t0    n1   n0 and  n2   n3   0   lambda100001lambda2000005    tmax for the process be 150000s   PRON be not really familiar with monte carlo method  so PRON have only manage to simulate the single decay process  when PRON come to consecutive decay PRON get completely lose   PRON attach PRON script for single decay below  hope to get help here   thank PRON   n0inputenter total number of nuclide      determine n0  lambda1  00001   decay  s  dt  100   time step  m  1500   total number of time step  t  0dtm1dt    time series  p  lambda1dt    probability of have a decay in the time dt   define the array that will be populate with the number of atom at each time step  n  zerosm1    n1   n0   for j2m   loop over time  nd  0   number of decay  for i1nj1    generate a single random number  r  rand      binary value  1 if the condition be true gt  atom have decay  rb  r  lt  dtlambda1   if rb   1  nd  nd1   end  end  nj   nj1nd    number of atom leave at the step 2  end  plott  n   
__label__bigdata be there any market for datum science as service which provide service without sell software  just consult  model and develop algorithm   yes  if there be not a market  the large consulting   big 3   and audit   big 4   firm would not offer and continue to be expand PRON advisory service in datum  amp  analytic  the same could be say for the thousand of independent consultant who offer such service  such as those who can be find on wwwupworkcom   
__label__research __label__ai-community currently  many different organization do cut  edge ai research  and some innovation be share freely  at a time lag  while other be keep private  PRON be refer to this state of affair as  multipolar   where instead of there be one world leader that be far ahead of everyone else  there be many competitor who can be mention in the same breath   there be not only one academic center of ai research worth mention  there may be particularly hot company but there be not only one worth mention  and so on    but PRON could imagine instead there be one institution that matter when PRON come to ai  be PRON a company  a university  a research group  or a non  profit   this be what PRON be refer to as  monolithic   maybe PRON have access to tool and resource no one else have access to  maybe PRON attract the good and bright in a way that give PRON an unsurmountable competitive edge  maybe return to research compound in a way that mean early edge can not be overcome  maybe PRON have some sort of government coercion prevent competitor from pop up   for other industry  network or first  mover effect may be other good example of why PRON would expect that industry to be monolithic instead of multipolar    PRON seem like PRON should be able to use insight from social science like economic or organizational design or history of science in order to figure out  if not which path seem more likely  how PRON would know which path seem more likely    for example  PRON may be able to measure how much return to research compound  in the sense of one organization come up with an insight meaning that organization be likely to come up with the next relevant insight  and know this number make PRON easy to figure out where the boundary between the two trajectory be locate    oligopoly vs monopoly  the term in the question  multipolar and monolithic  appear to be refer to the micro  economic concept of oligopoly and monopoly respectively   although these concept be not ai specific  PRON certainly apply to such development in the way the question suggest   lead ai rampd be occur in  a relatively small number of corporate  governmental  and university research lab   a simple search for oligopoly vs monopoly will cover the difference between these two scenario sufficiently for all sector of product and service  and those concept and observation apply to a large degree in the ai case   those who have not study economic or political science may incorrectly conclude that oligopoly be necessarily relate to capitalistic competition and monopoly be relate to communism  however  rampd monopoly frequently exist within capitalistic economy and oligopoly frequently exist in a plan economy  such as communist state   strength of each  research oligopoly strength   lead to diversity of approach and target product and service  lower the probability of ethical  environmental  or social technology abuse because multiple independent team  even with corporate confidentiality be in place  exert the force of at least a modicum of accountability  research monopoly strength   efficient in term of return on research investment for two reason  1  the division of a fix amount of available research funding between several lab can increase the rampd require to produce usable result  and  2  unfruitful approach be more quickly abandon when PRON worker can be shift to other team within the same organization  attract all those who have the most talent and experience in the field need by the specific rampd into one place to potentially collaborate  predicting transitions between monopolistic and  oligopolistic  form  the leading edge of any rampd tend to begin as a monopoly or a  biopoly    three or more unit begin the same research at the same time or be fund by government at the same time  as in plan economy  be unusual   over time  employee tend to leave and start PRON own organization  more so in capitalistic society  but also in plan economy  where government may hear case to branch off and grant permission or the scientist may  in one way or another  seek relocation to a free economy   PRON can see these trend throughout technological history  back to early form of writing  crop irrigation and plowing  chariot development  and ship building   modern example include the application of the westinghouse  tesla model of electrical energy distribution and sale  genetic engineering  speech recognition  and http client  browser start with mosaic  then netscape  then ie  then opera  chrome  and other    unknowns  how far ahead one global ai leader be than another can not be so easily predict   some company choose to monetize rampd result sooner in the research cycle than other  depend on PRON strategy and the diversity of PRON revenue stream   for instance  as of this writing  apple and facebook can wait a decade to release to the public manifestation of rampd without jeopardize PRON financial status   furthermore  government may declassify material  another form of what the question call time lag  several decade after discovery   in the case of the von neumann model for achieve critical temperature and pressure of fissile or fusion material  the government may never declassify PRON   what the nsa or equivalent organization outside the united states may decide must fall under an equivalent strategic wall of secrecy can not be know outside of treasonous disclosure   the insight in an organization be not sustainable in practice   most people do not even know where first insight occur and two whom PRON occur first because the current icon in business relate to the product and service stem from the insight be otherwise completely unrelated to origin   few would buy a new car and thank isaac newton or even henry ford   absolutely no one buy a cell phone and thank alonso church  claude shannon  or gene roddenberry when the first call or sms message go through   PRON do not thank the water wheel researcher of fourth century alexandria when PRON turn on a light  or even nicola tesla   PRON do in some way worship computer technology entrepreneur when PRON buy an iphone or require a document in docx format and vaguely understand that PRON be a ms office document type   few know that not a single innovation PRON attribute to those entrepreneur come from within PRON respective organization   everything from personal computing  desktop publishing  touch screen  and window interface be develop in the rampd facility of other corporation   in some way  the company to come up with the next insight be less likely to be the last one that do   innovators usually find good pay after PRON have a choice item on PRON resume like   invent first adaptive cell tower switching protocol for mobile device communication    that person be not only no longer work for the company that file the correspond patent  but be retire  and the company be now struggle and without recent productized innovation   insurmountable competitive edge  thus far competitive edge have always become surmountable   the proliferation of nuclear weaponry  now span at least seven country  be a current and certainly important example of this   german and japanese aeronautic rampd edge be another  
__label__eigenvalues __label__constrained-optimization __label__nonlinear-programming __label__stochastic __label__nonconvex PRON be try to develop an inferential procedure for a multivariate dependent  markov process  basically  the procedure could be consider as a non linear regression  with a know dependence structure among observation belong to the same time  point and independent from the other  the non linear trend correspond to a the solution of a matrix differential equation  a more detailed description follow   let define a matrix  theta of parameter  thetai  beginarrayccc   theta1  amp  theta4  amp  theta7   theta2  amp  theta5  amp  theta8   theta3  amp  theta6  amp  theta9  endarray   each entry  thetai could be unconstrained or linearly constrain  both equality and inequality    this matrix  multiply by a  give  fix matrix  v and a vector of previous time point  vecx observation  govern the derivative of a stochastic process   as a consequence  in PRON objective function  sumvecyfthetavecx2   PRON need to calculate the solution of the matrix differential equation   fthetavecx to calculate the solution  PRON need to calculate a matrix  a  combine the entry of  theta accord to  v  calculate eigen decomposition of  a and finally the solution  hat y  fthetavecx  exponential    the objective function be non  convex and have many local minimum  runnig some simulation study  PRON verify that the global minimum be locate at the true parameter value  and the objective be convex in the closed neighbourhood   until now  PRON use a constrained gauss  newton method to estimate parameter  and  give good initial value  PRON work  relax some condition  the method to calculate initial value be not good enough to guarantee the gauss  newton method to converge to global minimum  PRON get stuck in a closed local minimum   so  PRON be now look for a global optimizer  all PRON code be in r and rcplex and PRON have some experience with c  be cplex able to solve non  convex  linearly constrain problem and flexible enough to allow PRON for solve the matrix differential equation  be there any alternative program  any suggestion   thank   
__label__testing __label__numerical when PRON have a numerical model that represent a real physical system  and that exhibit chaos  eg fluid dynamic model  climate model   how can PRON know that the model be perform as PRON should  PRON can not compare two set of model output directly  because even small change in initial condition will dramatically change the output of individual simulation  PRON can not compare the model output directly to observation  either because PRON can never know with enough detail the initial condition of the observation  and numerical approximation would anyway because minor difference that would propagate through the system   this question be partly inspire by david ketcheson s question on unit testing scientific code  PRON be particularly interested in how regression test for such model could be implement  if a minor initial condition change can lead to major output change  which may well still be adequate representation of reality   then how can PRON separate those change from change cause by modify parameter  or implement new numerical routine   first  PRON be go to focus on PRON last sentence  as PRON touch on a few different thing in PRON question  but PRON feel PRON adequately capture what PRON be ask   if PRON be change numerical routine  PRON should not be change initial condition or system parameter until PRON have validate the new routine from the old one   on the weak level PRON see this as compare some time average value over PRON solution  and PRON be in agreement  even if the transient behavior diverge from each other within the chaos    on the strong level  PRON would expect the two routine to reproduce the full transient behavior   which of these PRON want  and which be acceptable depend on what question PRON be ask and what conclusion PRON be draw from the solution   as far as tell whether a model be  perform as PRON should   that be an entirely different question   this have nothing to do with the numerical routine PRON choose   how PRON build PRON model  from PRON simplify assumption to PRON measurement  calculation of parameter  PRON should be base all of PRON decision on the physics of the problem  and hopefully prior work do on similar case   PRON may be able to validate a model with a simple case reproduce in a lab setting  but there be time when even that be non  trivial   if PRON can not determine an important system parameter to within an order of magnitude  PRON can not expect anyone to trust the small detail PRON be calculate in the transient local behavior   all PRON can compare in such case be the statistic of PRON solution  average  high moment  heat flux across the boundary  and other integral quantity  take a look at one of the many paper discuss turbulence model for the navier  stokes equation  for example  PRON be full to the brim with plot of power spectra  entalpie  entropy  enstrophie  and other word PRON have never before hear of  all be some integral quantity of the flow and PRON be compare against the same integral quantity compute from other simulation andor experiment   if PRON code can run in non  chaotic regime of PRON underlying problem  especially non  chaotic regime where PRON can use the method of manufactured solution  PRON should write regression test that run in these regime even if PRON be not otherwise interesting to PRON  if these test fail  then PRON immediately know that something have go wrong in PRON late code change  then PRON can move on to more physically relevant problem  
__label__machine-learning __label__data-mining __label__bigdata __label__statistics __label__apache-hadoop PRON work at a startup  medium sized company and PRON be concerned that PRON may be over  engineer one of PRON product   in essence  PRON will be consume real  time coordinate from vehicle and user and perform analytic and machine learn on this incoming datum  this processing can be rather intensive as PRON try predict the eta of this entity match to historical datum and static path   the approach PRON want to take be use the late and most powerful technology stack  that be hadoop  storm etc to process these coordinate  problem be that no  one in the team have implement such a system and only have have the last month or so to skill up on PRON   PRON belief be that a safe approach would be to use nosql storage such as  azure table storage  in an event base system to achieve the same result in less time  to PRON PRON be the agile approach  as this be a system that PRON be familiar with  then if the demand warrant PRON  PRON can look at implement hadoop in the future   PRON have not do a significant amount of research in this field  so would appreciate PRON input   question   how many tracking entity  send coordinate every 10 second  would warrant hadoop   would PRON be easy to initially start off with a simple approach such as  azure table storage  then onto hadoop at a later point   if PRON have to estimate  how long would PRON say a team of 3 developer would take to implement a basic hadoop  storm system   be hadoop necessary to invest from the get go as PRON will quickly incur major cost   PRON know these be vague question  but PRON want to make sure PRON be not go to invest unnecessary resource with a deadline come up   yes  this be a how  long  be  a  piece  of  string question  PRON think PRON be good to beware of over  engineering  while also make sure PRON engineer for where PRON think PRON will be in a year   first PRON would suggest PRON distinguish between processing and storage  storm be a  stream  processing framework  nosql database be a storage paradigm  these be not alternative  the hadoop ecosystem have hbase for nosql  PRON suspect azure have some kind of stream processing story   the big difference in PRON two alternative be consume a cloud provider s ecosystem vs hadoop  the upside to azure  or aws  or gce  be that these service optimize for integrate with each other  with billing  machine management  etc  the downside be be lock in to the cloud provider  PRON can not run azure stuff anywhere but azure  hadoop take more work to integrate since PRON be really a confederation of sometimes loosely  relate project  PRON be invest in both a distribution  and a place to run that distribution  but  PRON get a lot less lock  in  and probably more easy access to talent  and a broad choice of tool   the azure road be also a  big datum  solution in that PRON have a lot of the scalability property PRON want for big datum  and the complexity as well  PRON do not strike PRON as an easy route  do PRON need to invest in distribute  cloud anything at this scale  give PRON iot  theme use case  PRON believe PRON will need to soon  if not now  so yes  PRON be not talk about gigabyte  but many terabyte in just the first year   PRON would give a fresh team 6  12 month to fully productionize something base on either of these platform  that can certainly be stag as a poc  follow by more elaborate engineering   first  understand and solve the problem  on manageable datum  gather experience on how to organize the datum  and where the difficulty be  try to identify point where parallelism be possible   second  parrallelize and scale up as necessary   do not do PRON backwards  popular mistake  solve the wrong problem with the wrong tool will fail big  with big datum  
__label__quantum-mechanics PRON be compare the performance of various numerical method that can be use to solve the schrodinger s equation for the hydrogen atom interact with a strong laser pulse  too strong to use perturbation method   when use discretization scheme for the radial part  PRON seem like most  all  people put the atom in a box  just chop the radius off at some large value and solve for those basis set  how do this compare with map the radial variable to a finite domain  and then discretiz that domain  in the process  throw out most of the basis set available    be there a reason nobody seem to do that   baker et al  propose such a mapping for a radial grid for atomic  amp  molecular electronic structure computation in 1994  PRON be still use in modern electronic structure code  eg fhi  aims use PRON  as describe in a recent paper   even with such a mapping  the same problem still remain  if something interesting should happen beyound the outermost grid point  PRON will miss PRON  however  these mapping do have the advantage that the grid can be systematically improve toward the inclusion of distant grid point   this be explain in section 41 of the recent fhi  aims paper   
__label__eigenvalues __label__non-hermtitian PRON have some non  hermitian matrix  a  that PRON have the left and right eigenvector    calculate use slepc  by find the eigenvector of  a and  ah    PRON be not sure how to orthogonalize PRON however   PRON know that PRON must obey the relation     lhr  1  but PRON be not sure how to enforce this   the normalization of the vector be not clear to PRON either  since  leftltlrrightgt   1   do  leftltllrightgt   1    and similarly  leftltr  rrightgt     grahm  schmidt  tild represent non  orthogonaliz quantity      tildeqi   fractilderileftlttilderi  tilderi rightgt    ri  tildeqi  sumjneq ileftlttildeljtildeqirightgt  tildeljh  seem like PRON may work  but unlike grahm  schmidt for self  orthogonaliz  the first step of normalization do not feel right   and what about  li   be  li find by do the same procedure   ie     li  tildeqi  sumjneq ileftlttildeqjrirightgth rjh  be there some resource that can give PRON more information about non  hermitian eigenvalue problem   regard normalisation   leftltlrrightgt   1  be the only normalisation require for the matrix to be decompose correctly as    a  r lambda lh  where  lambda have the eigenvalue on the main diagonal  this leave PRON with one  complex  degree of freedom in the mutual definition of  l and  r  but as long as only a product of the two vector be use  then these factor will cancel out   for convenience PRON may wish to remove this degree of freedom eg by enforce  leftltrrrightgt   1   but in this case PRON will almost certainly get  leftltllrightgt  neq 1 
__label__deep-learning __label__keras __label__prediction __label__probability __label__accuracy say PRON have build a  completely unrealistic  classification model in keras that give PRON 100 accuracy   and next  PRON would like to use PRON model on some new  unseen datum  and use modelpredictproba to get a probability that the observation belong to class  a   say this return to PRON a 075   be PRON interpret this correctly in english    100 percent of the time  the model be confident that this new observation be 75 percent likely to be class a    if this be correct  then let PRON consider if PRON model be not totally perfect  like in real life  and instead PRON give PRON a 040 accuracy  say PRON predictproba be still 075   then  be this correct    40 percent of the time  the model be confident that this new observation be 75 percent likely to be class a    if so  this make PRON seem like predictproba   be not tell a complete story   PRON could mislead someone  say a journalist  or a judge  whoever  by say   there be a 75 percent chance this unseen observation belong to class a  and that may sound great  if PRON fail to reveal that this statment be base on a model that have a low accuracy like  040   be PRON state this correctly  and do PRON apprehension have validity   accuracy be measure in classification model by compare the predict label to the actual know label   the predict label be a function of both the predict probability for each class and a predefined thresholdbinary classification usually be 05   so if sample a get predictproba of  0  02  1  08  PRON will be label as 1since 08  05    accuracy be measure of classification correctness and predictproba be a direct result of the model underlining function  
__label__tensorflow PRON have a number of list  such as  1234    234    12    2346810   whose length be obviously unsame   how can PRON use this as input of placeholder in tensorflow   as PRON have try  the following setting will raise error   tfconstant12123dtypetfint32   so PRON guess placeholder can not be set by the upper input of list   be there any solution   edit   the following be PRON example  how to make PRON run without error   when PRON create a numpy array like this   xdata  nparray    12456123456     the internal numpy dtype be  object    array1  2    4  5  6    1  2  3  4  5  6    dtype  object   and this can not be use as a tensor in tensorflow  in any case  tensor must have same size in each dimension  PRON can not be  ragged  and must have a shape define by a single number in each dimension  tensorflow basically assume this about all PRON datum type  although the designer of tensorflow could write PRON in theory make PRON accept ragged array and include a conversion function  that kind of auto  casting be not always a good idea  because PRON may hide a problem in the input code   so PRON need to pad the input datum to make PRON a usable shape  on a quick search  PRON find this approach in stack overflow  replicate as a change to PRON code   import tensorflow as tf  import numpy as np  x  tfplaceholder  tfint32   3none    y  x  2  with tf  session   as session   xdata  nparray    12456123456      get length of each row of datum  lens  nparraylenxdatai   for i in rangelenxdata       mask of valid place in each row  mask  nparangelensmax     lt  lensnone    setup output array and put element from datum into masked position  padded  npzerosmaskshape   paddedmask   nphstackxdata        call tensorflow  result  sessionruny  feeddictx  padded     remove the padding  the list function ensure PRON   create same datatype as input  PRON be not necessary in the case   where PRON be happy with a list of numpy array instead  resultwithoutpadd  nparray    listresulti0lensi    for i in rangelenssize      print  resultwithoutpadd   output be     2  4   8  10  12   2  4  6  8  10  12    PRON do not have to remove the padding at the end  only do this if PRON require to show PRON output in the same ragged array format  also note that when PRON feed the result padded datum to more complex routine  the zero  or other padding datum if PRON change PRON  may get use by whatev algorithm PRON have implement   if PRON have many short array and just one or two very long one  then PRON may want to consider use a sparse tensor representation to save memory and speed up calculation   as an alternative to use padded array  PRON can just feed all of PRON datum as one big spaghetti string and then do origami inside the tensorflow graph  example   import tensorflow as tf  import numpy as np  sess  tf  interactivesession    noodle  tfplaceholdertffloat32   none    chopindice  tfplaceholdertfint32   none2    doorigami  lambda listidx  tfgathernoodle  tfrangechopindiceslistidx0   chopindiceslistidx1     print   doorigamilistidx  ievalnoodle123236   chopindices022336tolist   for i in range3     result     10  20    30    20  30  60    if PRON have a variable number of inner list  though  then good luck  PRON can not return a list from tfwhileloop and PRON can not just use a list comprehension as above so PRON would have to do the computation separately for each inner list  
__label__performance __label__accuracy with all the hoopla around data science  machine learning  and all the success stori around  there be a lot of both justify  as well as overinflat  expectation from data scientists and PRON predictive model   PRON question to practice statisticians  machine learning expert  and data scientists be  how do PRON manage expectation from the businesspeople in PRON company  particularly with regard to predictive accuracy of model  to put PRON trivially  if PRON good model can only achieve 90  accuracy  and upper management expect nothing less than 99   how do PRON handle situation like these   gather competitive counterpart  try and determine a state  of  the  art and see how PRON model compare with that  PRON also heavily depend on how long PRON team have be work on PRON  science  drive model be not create statically  PRON develop dynamically because a good scientist will always try to find way to improve PRON   upper management personnel should know that a data scientist explore new method  sometimes  often without know PRON quality  PRON should know that machine learning technique do not produce perfect model right away  if PRON do  PRON would not be challenge anyway   a data scientist ought to be evaluate by how PRON justify and discuss PRON result and how PRON plan the future  a way for the management personnel to handle with PRON expectation be to not have unrealistically high one   still  if reasonable result be expect in the field of context  think about these question   do  will result get good over time   be future expectation positive   how well be result compare to similar system  from competitor    PRON like this question because PRON get at the politic that exist in every organization  in PRON view and to a significant degree  expectation about model performance be a function of the org culture and degree to which an organization be  technically literate   one way to make clear what PRON mean be to consider the difference between the 4 big  data science  entity  google  fb  amazon and yahoo  versus the 4 big agency hold entity  wpp  omnicon  interbrand and publicis  google  et al  be very technically literate  the agency  on the other hand  be know to lean towards tech phobia  what be the evidence for this  first off  the technically literate group be found or be run by engineer  computer scientist  geek and people with strong tech background  who run the tech illiterate company  marketer who have rise to prominence by virtue of PRON soft communication and people skill  and not only that  have work in some of these shop in nyc  PRON can testify that these organization systematically punish andor push out the highly technically literate type as not a  fit  with the culture   next  consider PRON aggregate  stock  market cap  the tech literate group add up to about 800 billion dollar while the tech illiterate group amount to 80 billion  tech literate entity be 10x big than the other in market cap  this be a clear statement of the market s expectation and PRON be not high for the illiterate  so  by extrapolation  what kind of hope can PRON have for challenge the  predictive accuracy  expectation of bozo like these   so  give that cultural breakout and depend on where PRON fall  PRON should have more or less realistic expectation  of course  different  tech illiterate  entity will have manager who know what PRON be do  but for the most part  these entity be dominate by the idiocy of the low common denominator in tech skill  ie  people who be at good technical semi  literate  and dangerous  or  more commonly  totally innumerate but do not know PRON  case in point  PRON work for a guy who want word like  correlation  scrub from c  suite deck  this be an extreme case  after all  every secretary know what a  correlation  be   this raise the issue of how one deal with the maddeningly naive and innumerate when PRON ask a really dumb question like   why be not PRON get 99  predictive accuracy   one good response be to reply with a question like   why would PRON assume such an unrealistically high pa be even possible   another may be   because if PRON actually get 99  pa  PRON would have assume that PRON be do something wrong   which be highly likely to be true  even with 90  pa   the there be the more fundamental question of the insistence on pa as the sole criterion for model value  the late leo breiman leave many footprint on the statistical and predictive modeling community of which pa be one  PRON primary concern with pa be to address the many criticism be make in the 90 regard the instability and error inherent in run a single cart tree  PRON solution be to motivate  random forest  as an approximate and provisional method that would maximize accuracy and reduce instability by eliminate tree structure   PRON benchmark the low mse from 1000 iterative rf  mini  model  against the error from a single logistic regression model  the only problem be that PRON never bother to mention the glare apple to orange comparison  what may the logistic regression mse have be if PRON have also be perform 1000 time   the 2008 netflix prize offer a sizeable monetary reward to any statistician or team able to improve upon the mse of PRON recommender system  at the time  netflix be spend  150 million a year on this system  convince that the cost be more than recover in customer loyalty and purchase of movie that would otherwise never have be choose  the eventual winner use a complex ensemble of 107 different model   as netflix learn however  the real problem be that  from a fully load cost perspective  the actual improvement in error over PRON current model be a mere 0005  reduction in the 5 point rating  not to mention that the it cost in time  heavy  lifting and maintenance of the win ensemble of 107 model more than nullify any gain from the error reduction  give this  netflix eventually abandon the pursuit of mse and no more netflix prizes have be award  and this be the point  minimize predictive error can be easily game or p  hack and be prone to analyst fraud  ie  find a solution that glorify the analyst ’s modeling skill  positively impact PRON potential end  of  year bonus   moreover  PRON be a completely statistical solution and goal set in an economic and business vacuum  the metric provide little or no consideration of ancillary  collateral cost  the very real operational consequence evaluate from a to z that should be an integral part of any fully  load  trade  off base decision  make process   this have become one of those issue that be embed in organization and be very  very difficult to change  in other word  PRON be fully aware that PRON be tilt at windmill with this rant about the caveat with the use of pa   rational business people do not pay for accuracy  PRON pay to either  save money on a profitable process  thereby make PRON more profitable   or by  create new money  create new profitable process    so any project that be undertake have to be couch in term that  reflect this  the first step be always understand which of the two  process PRON be work on  and PRON should have a clear idea of how  that may be achieve  while keep in mind that as PRON make  progress the detail of how PRON do that may change   if PRON can improve the accuracy of a process  PRON can probably make money for the firm and the business people will invest in PRON progress  the only rational reason a business person could have for insist on 99 percent accuracy and reject 90 percent be if PRON already have a way of do PRON that be good than the 90 percent   if that be the case PRON be of course justify in PRON position   understanding and present the business case of the project PRON be work on in term that the business people understand be part of the mature process of any engineer  PRON be not unique to data science at all  though data science have some unique aspect  like low maturity but high probability of fortuous serendipity discovery  at least in today s environment   a relevant process that be close to data science that make this step explicit can be find here  httpsenwikipediaorg  wiki  crossindustrystandardprocessfordatamin  but most enterprise architecture frameworks be similarly applicable  
__label__linear-algebra how to estimate   maxi mathrmarglambdai where   lambdai be eigenvalue of a large sparse matrix  a all lie in the left complex half  plane   PRON could multiply a random vector by the matrix repeatedly  the magnitude of a give eigenvalue will tend to because the vector s magnitude to grow  and the argument will tend to because the vector to oscillate   if PRON store the vector after each step  and then take the componentwise fft  the high frequency component should correspond to the large argument of eigenvalue of the matrix   PRON can show this as follow   let each eigenvalue  lambdak  rkeithetak then  for a give eigenvector  xk  the effect of the matrix apply  n time would be  rkneinthetakxk  which clearly have a peak in the fourier transform with respect to  n around  thetak if one have a vector which be a linear combination of eigenvector  there will be one peak per eigenvector   this method have some problem   if the magnitude of the eigenvector with the great argument be small  the oscillation may be so suppress that PRON be unable to detect PRON   PRON be not a standard method as far as PRON know  just something PRON come up with   so PRON probably have thorny bit PRON be not aware of  also  PRON have not actually try PRON   the  rn term will mix in some low frequency component  so if the large argument be small enough PRON could be hide by this term   if PRON just want an upper bind  another way of estimate PRON be to use gershgorin s circle theorem  PRON could simply determine the maximum argument of any point on the union of the gershgorin disk  and this would be guarantee to be large than the argument of any of PRON eigenvalue   this method have a serious problem  though  if the gershgorin region contain some neighborhood of the origin  no restriction on the large argument of any of PRON eigenvalue can be derive  
__label__machine-learning __label__python __label__neural-network __label__classification __label__deep-learning PRON have implement a neural network with 1 hide layer use sigmoid activation unit but after watch a video on how relu activation function can be much faster a try implement PRON but the cost function be either nan or inf in python  PRON have find that many people have the same problem but PRON could not find any solution on the internet   PRON have add   changed  comment to the line that PRON have change from the previous version in which PRON use sigmoid activation function   related python code snippet   def reluarg     PRON have try both relu and leaky relu  return 1arglt000001arg   arggt0arg  def relugradarg    for i in rangeargshape0     for j in rangeargshape1     if argijgt0   argij1  else   argij0  return arg  def softmaxx    x  xtranspose    ex  npexpx  npmaxx    return  ex  exsumaxis0transpose     forward prop   a1  npinsertdata0nponeslendata1astypenpfloat64   z2  a1dottheta1   a2  reluz2   change  a2  npinserta20nponeslena21   z3  a2dottheta2   a3  softmaxz3   change   compute the cost   cost  outputnploga31outputnplog1a3sum    cost   1lendatacost   lamb2lendatanpdeletetheta1002sum     npdeletetheta2002sum     backprop   sigma3  a3output  sigma2   sigma3dotnptransposetheta2     relugradnpinsertz20nponeslenz21    change  sigma2  npdeletesigma201   delta2   nptransposea2dotsigma3   delta1   nptransposea1dotsigma2   grad1  delta1lendata    lamb  lendatanpinsertnpdeletetheta1000npzeroslentheta100   grad2  delta2lendata    lamb  lendatanpinsertnpdeletetheta2000npzeroslentheta200    update theta  theta1  theta1  alphagrad1  theta2  theta2  alphagrad2  what be cause this problem  how can PRON be fix   if a3 be 0 or 1  nploga3  or nplog1a3  be go to give an error as  log0 be undefined  and  limx gt  0  logx   infty 
__label__predictive-modeling __label__time-series __label__algorithms __label__forecast be PRON possible to create a model that can basically take any type of uni  variate time series and forecast just the  t1th term base on the historytime series    PRON have go through ar  arma and arima  and all of these need a human intuition to decide upon the lag  from the correlogram   PRON find exponential smoothing as an appropriate method that can be use  from what PRON have read  but would like to know the different problem that may occur with this approach and other solution that may be more appropriate for the problem in context   thank in advance   
__label__python __label__neural-network __label__training can not find any example in python for train xor function use shogun library   by otherwise  be there any other easy example for feedforward neuralnetwork training with shogun for python   
__label__algorithms __label__graph-theory PRON have a tree datum structure  root  unbalanced  with unbounded branching factor   where each individual node have an associated  weight   for every node  n in the tree  PRON would like to compute the cumulative weight of all the node in the subtree root at node  n  for example  in the tree below PRON would want to compute the number in bracket   a  4  27      b  1  12          d  5  9              i  3  3              j  1  1          e  2  2      c  2  11      f  4  4      g  2  2      h  3  3   start at the root node  how can PRON compute these whilst visit each node a minimum number of time   no need for any advanced book  the easy to implement answer be   use a dfs  httpenwikipediaorgwikidepthfirstsearch   and store the cumulative sum of each subtree in the stack   for example  a possible dfs traversal in PRON example be  abdijecfgh  after compute the cumulative sum of the child of a node  add this value to that of the parent  when do compute all the sum of the child node  have the parent return PRON value to PRON own parent   eg after compute all of b s child  return the value of b to a   continue until do with tree   pseudocode   int sumint n     int cumulative0   for all child i of n  cumulativesumi   cumulativevaluen   return cumulative    PRON apologize for the similarity of the pseudocode to c    the complexity be θn   and PRON can not get any faster  since PRON must access each node at least once to find the sum  get input  if PRON exist   
__label__machine-learning __label__neural-network __label__deep-learning __label__unsupervised-learning __label__rbm in rbm  in the positive phase for update the hide layerwhich should also be binary    acually consider a node of h1 ∈ hhidden layer vector   to make h1 a binary number PRON compute the probability of turn on a hidden unit by operate activation function over total input  after the activation function operation  PRON would be get value in the range between 0 and 1  since activation function PRON be use  sigmoid    PRON doubt be that how do PRON make PRON binary by leverage the probability compute  PRON do not think if p05  make PRON 1 else 0 be a proper method to work on   by few literature review   PRON find this document  by hinton   in section 31  PRON have state  the hidden unit turn on if this probability be great than a random number uniformly distribute between 0 and 1   what do this actually mean  and also in this link  PRON say  then the jth unit be on if upon choosing s uniformly distribute random number between 0 and 1 PRON find that PRON value be less than sigj   otherwise PRON be off   PRON actually do not get this  whether the random number generate be same for all h ∈ h  another query be  what about the random in next sample iteration   PRON see this video  just watch the video from that point as per the link  how do PRON get that sample number  whether PRON have to just run rand   in matlab and obtain PRON  should PRON would be different for each hi   oh nooo  PRON do not think the machine will learn properly   whether the random number should be different for each iteration or the same random number can be use for all iteration to compare   as PRON correctly say  PRON calculate the probability of a hidden unit  hj  be one and then make PRON binary  that probability be give by    phj1   sigmaleftbj  sumi1v wijvi right  where  sigma be the sigmoid function   bj be the bias of hidden unit  hj   v be the number of visible unit   vi be the  binary   state of visible unit  i  and  wij be the weight   so  PRON matlab code for obtain the probability hiddenprobs be something like this  PRON write the sum implicitly by make a matrix multiplication    hiddenprob  sigmoidhiddenbia  datum  weight   now  PRON have the probability  phj1 for each hide unit  j in  1h now  this be only a probability  and PRON need a binary number  either 0 or 1  so the only thing PRON can do be pick a random sample from the probability distribution of  hj  which be a bernoulli distribution   as all hide unit be independent  PRON need to get one sample for each hide unit independently  and also  in each training step  PRON need to draw new sample   to draw these sample from the bernoulli distribution  PRON can use the build  in function of eg matlab  binornd  or python  numpyrandombinomial   note that these function be to sample from a binomial distribution  but the bernoulli distribution be just a special case of the binomial distribution with n1  in matlab  that would be something like  hiddenstate  binornd1  hiddenprobs   which would create vector hiddenstate which contain either 0 or 1  draw randomly for each probability in hiddenprobs   as PRON probably have notice  nobody do that  eg describe PRON in PRON practical guide to training rbm  as  the hidden unit turn on if this probability be great than a random number uniformly distribute between 0 and 1   that be exactly what hinton do in PRON rbm code  PRON get a random number for each hidden unit use rand  ie randomly sample from the uniform distribution between  01   PRON then do the comparison   hiddenstate  hiddenprobs  gt  rand1  h   this be equivalent to use binornd  but be probably faster  for example to generate a random number that be 1 with p09  PRON get a random number from  01   now  in 90  of the case  this random number be small than 09  and in 10  of the case PRON be large than 09  so to get a random number that be 1 with p09  PRON can call 09  gt  rand1   which be exactly what PRON do   tldr  pick a new random number from the range  01  for each hidden unit in each iteration  compare PRON to PRON probability with hiddenprobs  gt  rand1h  to make PRON binary  
__label__software __label__hpc back towards the dawn of os x  there seem to be a great deal of hubbub  at least in the mac world  PRON be nowhere near scientific computing at the time  about the mac os as a platform for scientific computing and hpc application   xgrid come out of the box  virginia tech have PRON fancy mac  base computing cluster  stanford be do cool thing  etc  etc   more recently however  thing have be quiet  the macresearchorg site be essentially a ghost town fill with spammer zombie  the xserve be dead  and an awful lot of the marketing literature and the like seem to even be from the pre  intel processor area  but xgrid be still there  the whole  nix os underpinning be there  and the platform seem to have decent support among python  r  and some of the new language   so  from people who know more about this than PRON do  how fare os x  be PRON viable client  side computer for scientific computing  be use PRON as a server  cluster  etc  through xgrid or something like PRON simply a novelty application   PRON can not comment on the server side of thing   on the client side  at the one computational science meeting PRON go to every year  the proportion of mac user seem to have increase  PRON switch to a mac because PRON get tired of deal with PRON school  supply dell laptop fail at the drop of a hat  PRON switch to macs for the hardware  primarily  since consumer report rat PRON highly in term of durability  PRON do not think that mac be good for scientific computing unless PRON run linux on PRON  the linux support for the hardware tend to lag  usually  PRON be the wireless card that be not support  whenever PRON change PRON in a new model   if PRON be willing to accept the resource penalty that come with run a virtual machine  PRON be an attractive option  and one that PRON personally use    macs require PRON to install a lot of library and software package before PRON can do serious scientific computing  anything that have a mac installer be easy to manage  so if PRON do most of PRON development work with matlab  mathematica  maple  python  etc   PRON be easy to install and run that software on os x natively  PRON be hard to track down hard core numerical software that have a mac installer  think thing like petsc or clawpack   package manager like macports and fink can help the situation if PRON want to use os x only  PRON will also have to compile a lot of package from source  if PRON want to run PRON code anywhere else  PRON will have to watch out for compatibility issue  since linux enjoy widespread usage in scientific computing  PRON be easy from a portability standpoint to develop code in linux   PRON have hear anecdotally from highly opinionat friend that set up a development environment on a mac be a huge pain in the ass  other have say PRON be not so bad  PRON mileage may vary   geoff give a good answer  but PRON think PRON be worth provide an alternative perspective   PRON do everything on macs  in os x  not a linux vm  include lot of scientific code development   PRON mostly work in fortran and python   for PRON  the convenience of  be able to do all PRON work in one os and  almost never deal with hardware failure or driver issue  be worth the cost of mac  specific headache   the three main headache be   lack of an os  standard package manager   once upon a time PRON use fink  but eventually PRON lead to more headache and PRON be now obsolete   PRON have hear good thing about macports and homebrew  but PRON experience with fink convince PRON to just  roll PRON own    some of the build  in software be very outdated   particularly  python and gcc   this mean that PRON need to install PRON own updated version  which can be a hassle   apple do not include a fortran compiler   PRON seem to PRON that apple be pay less and less attention to PRON unix  base power user   meanwhile  linux keep improve   eventually PRON will probably be push back into linux   but PRON will keep PRON macbook until somebody else learn how to make decent battery   PRON would argue that the mac be a good environment for computational scientist than PRON be for computational science  PRON would not want to use macs in a commodity computing environment  the hardware be  relatively speak  way too expensive for that  PRON can be a pain to get the software environment up to match the condition need for a particular package  but usually once PRON have figure PRON out the first time  PRON be a lot easy to maintain than a comparable windows installation   and  depend on the package manager  PRON can be as easy as linux  macport do do fairly well as a manager for os x    however  as a work platform for a busy computational scientist   PRON believe that the mac offer the additional bonus of be able to work and maintain in a single computing environment  rather than have to maintain a linux box for  serious  work  and still need to have another machine when colleague work in other discipline send PRON  for instance  office file that need markup  or PRON have to fill out a university form that be only available as a rtf template that will not format right use libreoffice and will be a pain in the neck to recreate in latex  at previous job  linux owner pretty much have to have a non  linux machine in addition  because to access the corporate environment more or less demand PRON  mac owner do not really need a second machine    in addition  there be a number of mac  only program  like textmate  scrivener  papers  things  omnifocus  or bibdesk  that make the mac a much more pleasant environment in which to do work than on the pc or linux  PRON find PRON spend more time focus on get thing do  than on what PRON need to do to get the software environment to do what PRON want  or need   PRON to do   use of os x in hpc and scientific computing be low and PRON have to do a lot with the pro and con of os x wrt  the alternative  linux   os x pros   polished ui  still  nix  desktop  design apps  such as ms office  adobe program well support  multimedia very well support  some people like the apple ecosystem  iphone  itunes etc    os x cons   run on expensive hardware and not everyone like macbook  specially people use to thinkpads  keyboardtrackpoint   can not upgrade hardware  eg if PRON want to try the late nvidia card with PRON cuda app  on desktop  cluster  bloated gui that can not be customize  in linux PRON can use a minimalist window manager   package mgmt in macport  fink be sub par compare to linux  debian  distribution  most package be not even actively maintain or be orphan  some useful tool  program traditionally do not run or still do not run on os x for example   sun studio still do not work  valgrind only start work recently and not all feature be support  intel compiler have also be available in recent year  apple do not even package a fortran compiler and PRON have to rely on 3rd party  mostly individual  to build binary that work only on certain os x version  which the individual have   support be rare or non  existant in such case  commericial scientific app  abaqus  ansys  fluent and many more in industry such as oil  finance  engg etc   do not run  natively  on os x  linux  debian  pros   first class package management ie  installation of compiler  most numerical  scientific library etc  be a command away  unlike fink  macport the package be much better support  in debian PRON have the option of use bleed edge  testing  stable version   most cluster  run some version of debian  red hat so less hassle in port codes   linux  debian  con   no standard ui  linux on desktop  laptops  may require some tweaking to get everything  suspend  resume  3d video acceleration  sound etc   work but this have improve a lot in recent year  bar some exception most user and cluster  system administrator find os x easy for desktop productivity and not for scientific computing  compile  use  develop stuff    otoh most linux user find the latter easy than former and this be reflect int the entire hpc  scientific computing ecosystem   PRON have be use nothing but macs on the desktop  and laptop  for many year  do scientific computing and scientific software development among other thing  as other have point out  the quality of the hardware  the high quality of much mac  specific software  and the ability to handle word and excel when necessary  make the mac a very nice platform for daily use   PRON have also be run a mac  base compute cluster as an experiment for a while  PRON be an experiment PRON be not tempt to do again  compare to a linux cluster  PRON do not see any significant advantage  other than ease of software installation if PRON have macs on the desktop anyway  just install the same stuff   the disadvantage stand out clearly  most of all the lack of proper multius gui support  on a mac  one machine equal one screen and at most one log  in user  that make gui  base tool a pain to use  compare to that  even plain x  window under linux be a joy to use  and then there be vnc and nomachine nx to do even better   yes  PRON know the mac support x window  but most gui program for the mac use the native interface  
__label__text-mining PRON have download comment from a website which ask people whether PRON support or oppose the implementation of a certain political policy relate to immigration   PRON would like to get any resource or idea on how to extract aggregate support  opposition to this policy   in particular  PRON need a method that correctly identify that all of the following comment be  anti  immigration  in some sens such as   PRON need to give americans first priority in the job market   PRON should not let americans suffer more unemployment on account of immigrant   immigrants should not be allow to take american job   similarly the method should be able to identify  pro  immigration  comment  such as   provide this service to immigrant will be good for the economy   the american economy will suffer if immigrant be not allow to continue to work here   PRON do not think that pass this law will be detrimental to american job   PRON need first to classify these comment into pro and anti immigrant   then  the next step should be analyze PRON document with text mining tool   PRON agree with thebiro  PRON could start by classify a sample of the comment as be either oppose to or in support of the policy   if PRON only need basic for  against classification  PRON can do binary classification  eg against  0  for  1    if PRON need specify the degree to which a statement be for or against the policy  then PRON could define a scale that indicate the degree to which a comment support the policy  eg strongly against  3  strongly for  3    once PRON have choose PRON scale and manually classify PRON sample comment  PRON have a dataset that PRON can use to train a model   once PRON have PRON training datum  PRON need to come up with a numerical representation of each comment   there be too many possible approach to enumerate here  but some basic concept be bag of word and word vectorsaka word embedding    this kaggle tutorial may help explain those concept   finally  PRON need to train a model that take the numerical representation of each comment as an input and output a sentiment score  0 or 1 for binary or a number on PRON scale    PRON have a lot of choice for the type of model PRON use   if PRON be work in python  PRON can use one of the supervised learning method in scikit  learn   scikit  learn also have a tutorial on work with text   PRON train PRON model on the sample PRON manually classified  something like modelfittrainingdatainput  in scikit  learn  and then predict the output for the rest of PRON dataset  modelpredicttestdatainputs  in scikit  learn    some people be also use neural network for sentiment analysis   keras be a great python library for build neural network and have sentiment analysis example available on github  
__label__eigenvalues __label__iterative-method PRON be no expert on the different type of algorithm to compute eigenvalue and vector for a real  symmetric matrix  come from linear mass and stiffness matrix for a frame fea model   PRON be look for a good blend of computational efficiency and ease of coding for a system of 300 dofs where PRON only need the first n  n between 1 and 10  eigenvalue and vector  PRON will be program PRON in vba  so the less bundled algorithm  eg decomposition and thing  that PRON need to program be a factor  too   PRON be figure on inverse power iteration with shifting  but be concerned about stability or skip an eigenvalue if PRON shift too much or too little  be this a realistic concern   a couple of option   a matrix of size 300 be very small  and compute this many eigenvalue and vector should not take more than a couple second on any modern computer  usually the qr algorithm be use when compute all eigenvalue  PRON be advisable to use lapack routine for this  PRON have great documentation and be widely use  implement this should be very easy and quick   if  1   do not cut PRON  PRON can indeed find just the small eigenvalue and vector PRON need  which should be quick if PRON can provide the inverse operator   there be many option  power iteration and the arnoldi method be often use  the arnoldi method can be implement use arpack and a whole suite of other method be include in slepc  which give PRON some flexibility   
__label__monte-carlo __label__probability PRON be give a smooth probability density function via PRON value on a reasonable fine grid  PRON assume that cubic spline interpolation  or cubic spline interpolation of the logarithm of the density  will be sufficient to evaluate  PRON at arbitrary point with high accuracy  PRON wonder how to generate random number that reproduce this distribution   PRON first shot be to approximate the cumulative distribution function of this distribution by a piecewise linear function  f  on the original grid   draw a number  r from   01 uniformly at random  and take the  x with  fxr however  PRON notice that the accuracy of PRON final result be not great  and PRON suspect that PRON lose accuracy because the piecewise constant probability density of PRON numerical random variable do not approximate the real smooth probability density function well enough  what option do PRON have   here be some of PRON idea   go to the library and look for a book about monte carlo simulation  or try to ask an expert   integrate the cubic spline analytically  which give a piecewise quartic function  f there would still be an analytic formula for the  x with  fxr  but PRON will probably be complicate to implement and slow to evaluate   approximate the smooth probability density function by a piecewise linear function  which give a piecewise quadratic function  f the analytic formula for the  x with  fxr should be simple to implement and reasonably fast to evaluate   approximate the logarithm of the smooth probability density function by a piecewise linear function  which give a piecewise  simple  analytic function  f the analytic formula for the  x with  fxr should be simple to implement and reasonably fast to evaluate   approximate the smooth probability density function  g by a piecewise constant function  f such that  g leq 11 f now use rejection sample by first sample  x via  fxr1   and then reject  x if  gx   lt  11 fx  r2  approximate  f1r by a suitable piecewise analytic function  but what do suitable mean here   PRON would go with PRON option 3   that say  PRON would have help if PRON elaborate on PRON statement  however  PRON notice that the accuracy of PRON final result be not great   PRON say so because if the mesh on which PRON pdf be define be fine enough  then PRON see no reason why PRON approach should not work  what PRON would do be try to debug thing by start with a pdf PRON know analytically  say a gaussian  and evaluate the step PRON do one by one  for example  start with a very fine mesh and piecewise constant approximation  do the result set of sample look ok  if not  do PRON get good by use a piecewise linear approximation  if not  then the error must be somewhere else  etc   if PRON pdf be bound  PRON could try approximate PRON inverse with a high  degree polynomial interpolant  this be usually consider a bad thing  but that be just a myth   some thing to keep in mind   instead of use an equispaced grid  interpolate at the chebyshev node of the first kind  ie  xi  cosleftpifrac2i12nright  for  fx define in   inftyinfty  or second kind  ie  xi  cosleftpifraci1n1right  for  fx define on a finite interval   if  fx be on an infinite interval  do not interpolate  f1r  as PRON will have singularity at the endpoint  but interpolate  f1rr2r  as this will cancel  out the singularity at  r0  and  r1 use the chebyshev node of the first kind will avoid evaluate  f1r at these singular point   PRON can evaluate PRON interpolant use barycentric interpolation  note that if PRON evaluate  f1r on chebyshev node of the first or second kind  the barycentric weight  wj have close  form expression   for a fast evaluation that vectoriz well  use a vandermonde  like matrix  v with  vijtjxi to compute the chebyshev coefficient of PRON interpolant once  if PRON use the chebyshev node   v should be well condition  and use clenshaw s algorithm to evaluate PRON for more than one  r at a time   the method describe here be more or less what the chebfun system do  disclaimer  PRON use to be part of the chebfun developer team   most of the basic chebyshev technology be describe in nick trefethen s book  approximation theory and approximation practice   of which the first six chapter be available online   PRON have solve PRON problem now  the reason why PRON lose accuracy do not even occur in the question  let PRON first address the propose solution idea from the question   try to learn more be a good idea in this case   analytical formula be attractive when PRON be reasonably simple  this be not the case here   straightforward linear interpolation sound like a good idea  at least for verification  perhaps PRON will implement PRON one day  PRON can not be too difficult  see also the answer by wolfgang bangerth   note sure  PRON have do something relate instead  PRON approximate the smooth probability density function  fx by a piecewise analytical function of the form  a cdot xb PRON antiderivative  fracab1xb1 have the same simple form  and the moment  int fx  xn dx also lead to integral of the same form   rejection method should not be outright reject  but PRON have not try this one   approximate  f1r be one of the  correct  proceeding  what do suitable mean here  the inverse cumulative distribution function be monotonically increase  and a near zero probability density over an extended range translate into a very steep slope of  f1r nonuniform rational interpolation be one option able to cope with these feature  the nonuniform grid lead to a  olog n effort for a single function evaluation  but this be normally still fast enough in practice  see also the answer by pedro   however  the  on effort for a single function evaluation with chebyshev interpolation instead of a  o1 or  olog n effort have make PRON uneasy in the past    but what about the implicit  real  question   however  PRON notice that the accuracy of PRON final result be not great  and PRON suspect that PRON lose accuracy because   PRON actually be not just give a single probability density function  but a one parameter family of probability density function  tabulate on a sufficiently fine grid relative to the one parameter  PRON handle this by linear interpolation between the inverse cumulative distribution function PRON have precomput  however  even in case the probability density function be well approximate by a linear interpolation between the tabulate probability density function  PRON above proceeding can lead to unacceptable error   the  correct  solution be much simple  PRON use the composition method  first draw a number uniform at random to select between the available precomput distribution  and then sample the value from the select distribution  this actually generate the  exactly correct  distribution  in case the probability density function be really give as a weighted sum of available precomput distribution  
__label__strong-ai __label__history __label__turing-test do alan turing expect the ai to be aware PRON be be turing test while the game be be play   PRON think PRON be slightly hard to look like a duck while pretend to be a duck than when PRON believe PRON be a duck  PRON be a step further   aside from all other criticism of original article  do alan turing have the time and motivation to address the point in any later writing   
__label__neural-networks __label__deep-learning __label__pattern-recognition __label__voice-recognition PRON wonder on the follow concept  a give neural network get two audio input  preferably music  and give a real number between 0 and 1 which describe  similarity  between the second and the first track   as far as PRON understanding of neural network go  the problem fit the concept of nn  as pattern recognition in music can help determine similarity and discrepancy in audio  see voice recognition   however  due to the nature of long and complex input  and the vague nature of learning dataset  how similar  for instance  diana ross  PRON be PRON move  and the vaporwave legend floral shoppe exactly be  09  06  other    such a network would be extremely slow and convoluted   be PRON possible today to build and train such a model  if yes  how would PRON look like   yes  PRON be possible  even if the good approach could be different from  neural network  anyway  PRON should extract some significant feature from the audio  energy  onset  root frequency  and other   usually  more feature than those really need be extract and afterwards the most sigificant be select through some algorithm  eg pca   in this way PRON will obtain an array of feature  say between 10 and 100 feature  with which PRON can train PRON nn   note that nn do not tell PRON why two audio be similar but only if PRON be or not  this be a big disadvantage  instead  algorithm base on grey  box modeling such as rule or case base algorithm  maybe use fuzzy logic  could be more useful  provide that PRON have a deep knowledge of the problem   reference and deepening source  smc lab from university of padua education material 
__label__career PRON be a 35 year old it professional who be purely technical  PRON be good at programming  learn new technology  understand PRON and implement  PRON do not like mathematic at school  so PRON do not score well in mathematic  PRON be very much interested in pursue a career in big data analytic  PRON be more interested in analytics rather than big data technology  hadoop etc    though PRON do not dislike PRON   however  when PRON look around in the internet  PRON see that  people who be good in analytic  data scientists  be mainly mathematics graduate  who have do PRON phd and sound like intelligent creature  who be far far ahead of  PRON  PRON get scared sometimes to think whether PRON decision be correct  because learn advance statistic on PRON own be very tough and require a of hard work and time investment   PRON would like to know whether PRON decision be correct  or should PRON leave this piece of work to only intellectual who have spend PRON life in study in prestigious college and earn PRON degree and phd   due to high demand  PRON be possible to start a career in datum science without a formal degree  PRON experience be that have a degree be often a  requirement  in job description  but if the employer be desperate enough  then that will not matter  in general  PRON be hard to get into large corporation with formalized job application process than small company without PRON   know people  can get PRON a long way  in either case   regardless of PRON education  no matter how high demand be  PRON must have the skill to do the job   PRON be correct in note that advanced statistic and other mathematic be very hard to learn independently  PRON be a matter of how badly PRON want to make the career change  while some people do have  natural talent  in mathematic  everybody do have to do the work to learn  some may learn more quickly  but everybody have to take the time to learn   what PRON come down to be PRON ability to show potential employer that PRON have a genuine interest in the field  and that PRON will be able to learn quickly on the job  the more knowledge PRON have  the more project PRON can share in a portfolio  and the more work experience under PRON belt  the high level job that will be available to PRON  PRON may have to start  in an entry level position first   PRON could suggest way to study mathematic independently  but that be not part of PRON question  for now  just know that PRON be hard  but possible if PRON be determine to make a career change  strike while the iron be hot  while demand be high    PRON should look more into the infrastructure side of thing if PRON do not like math  the lower PRON go in the software stack  the further away PRON get from math  of the data science sort   in other word  PRON could build the foundation that other will use to create the tool that will serve analyst  think of company like cloudera  mapr  databrick  etc  skill that will come in handy be distribute system and database design  PRON be not go to be become a data scientist without math  that be a ridiculous notion   keep in mind that  big datum  be an increasingly trendy thing for a company to say PRON be involve in   high up may read an article about PRON in hbr  and say to PRON   PRON have get to get PRON some of that   not that PRON be necessarily wrong    what this mean for PRON be that the advanced analytic be not as necessary for that company as just get something up and running may be   luckily for PRON  most of the component say company may need be free   moreover  PRON believe both hortonworks and cloudera have free  sandbox  virtual machine  which PRON can run on PRON pc  to play around with and get PRON bearing   advanced analytic on big datum platform be valuable  to be sure  but many company need to learn to crawl before PRON can run   this be a really strange question in PRON opinion  why PRON be go to move in a new direction if PRON be not sure that PRON love this new direction or at least find PRON very interesting  if PRON do love big data  why do PRON care about the phd intelligent creature that be already in the field  the same amount of phd creature be in every area of it  please have a quick read at this very nice article httpwwwforbescomsiteslouisefron20130913whyyoucantfindajobyoulove and then ask PRON if PRON love big data enough and PRON be ready to add PRON grain of sand to the mountain of knowledge  in PRON experience to have a phd do not mean necessarily be good in the enviroment of data science company  PRON work as datum scientist and PRON be just an engineer but PRON have know some universitary teacher who work in collaboration with PRON company and sometimes PRON have say PRON that PRON point of view be not right because despite of PRON idea and reasoning be right PRON be not applicable to the company activity  so PRON have to modify some data model to make PRON usefull for the company and the result lose PRON value so PRON have to seek new model  what PRON mean be that data science be a multidisciplinar area so many different people work together be need so PRON think that PRON skill could be very useful in a data scientist team  PRON only have to find where PRON fit   may be PRON will be a little offtopic  but PRON would like to highly recommend PRON to go through this mooc httpswwwcourseraorgcoursestatistic  this be a very good and clear introduction to statistic  PRON give PRON a base principle about core field in datum science  PRON hope PRON will be a good start point for begin friendship between PRON and statistic   PRON have not see this mention  but PRON be important to keep in mind that PRON may see a decrease in salary  PRON say this without know how much PRON make  but move from  PRON assume  an experienced it professional to an entry level datum scientist level may not earn PRON as much   here be a link to the a portion of the 2015 burtch works study on data science salary   httpwwwburtchworkscomfiles201505ds2015changesinbasesalariespdf  as PRON can see  the median salary for level 1 individual contributor be 90k  across the nation   the full report have the breakdown base on region but again  assume PRON be an experienced it professional  PRON be probably make more than that   anecdotal story with n1  one of PRON classmate in PRON ds master program be an experienced java developer with a house  family  etc  although PRON be very interested in datum analytic  pay for the program out of pocket  PRON potential salary do datum analytic would not be able to support the lifestyle PRON currently have as a java developer  as a result PRON essentially  waste  PRON degree and go back to development  PRON would really hate to see that happen to more people  
__label__pandas PRON want to find out what people also buy when PRON buy a  bike helmet   there be only 77 different product in PRON dataframe   PRON may solve this use a for loop  and create a counter  so whenever productname    bike helmet  look at the orderid and increase the counter by 1 for all those product name  then find the next instance of a bike helmet purchase  this approach  however  be slow   orderid  productname  priceperorder  profitperorder  quantity    239  10337  bike helmet  805200000  178080000  24  238  10337  mehmet  skor  5200000  0626667  28  237  10337  mehmet  tröja  7990000  1540000  20  236  10337  oyaki kimono  10025000  2558333  25  240  10337  dsw sandals  3600000  0628000  40  40  10262  runner shoes  14362500  1946250  15  42  10262  o  man underwear  0535680  0030720  12  41  10262  slip  on shoes  4606667  0050667  2  336  10374  onesy dress  38677500  10800000  15  337  10374  rossi bermuda shorts  13665000  2370000  30  two approach  both base on discrete mathematic  the first one be a set  theoretic approach to frequent item  set mining and association rule mining  for instance a  priori algorithm work here   the next approach be create a graph base on order and do graph mining on PRON  a  priori may be a good approach as in this case PRON probably need to deal with hypergraph which make life a bit complicated   PRON suggest PRON first extract set of different order from PRON dataframe  as PRON problem be not general and PRON already know that there be only association to one product be important  PRON can prune PRON datum much  then simply rank other product buy together with bike helmet by count number of PRON occurence and divide by number of all  PRON give PRON the probability estimate of each product be buy together with bike helmet   
__label__social PRON be look for good example of succesful ai project and theory that have a relatively good impact on society  economic and military field   so many year have pass after the first ai research  hence PRON be wonder if PRON have really increase the quality of PRON life   PRON believe which most successful ai theory be machine learning  in entire web have machine learn algorithm run  learn by what PRON do  watch  search  even by the photo PRON take   succesful ai project   tensorflow   scikit  learn   there be lot of great project in ai   self  drive car  this type of car use ai to learn the pattern of the road  speed of car  motion of car  brake power and lot of different feature and after sufficient learning  PRON be capable of drive the car autonomously  the good example of this type of car be tesla s self drive car   games  game also use ai to learn the game with the aim of win the game when play against a human or an ai player  PRON must have play lot of game on mobile and pc like chess  tic  tac  toe  etc  PRON play against the computer and accord to the difficulty value set  the computer play PRON move  this difficulty value be nothing but the ability of the ai engine to predict the next move by the opponent   chatbots  there have be lot of development and improvement in chatbots  so that human can communicate with PRON as if PRON be talk to other human  there be many chatbot design which answer any question ask by PRON  of course PRON be dependent on how much intelligence the bot hold   some example be alice bot  ibm watson  which have be the most advanced bot till now    expert systems  expert system be those system which focus on one specific domain and can solve any query relate to that domain which be give to PRON  for example  an expert system can be design to solve any mathematical equation query to PRON  an expert system  in such case  will give the solution of the equation along with the step  provide step be important because PRON be an important component in expert system which be call inference engine    prediction system  there be lot of prediction system which use ai and machine learning to predict something base on some past datum  example be weather forecast system  stock market prediction system  recommendation system  usually available in e  commerce website like amazon   etc   PRON would say the most successful be the one so commonly use that PRON do not even notice PRON   the mail system that automatically decipher handwritten address on PRON package  PRON use machine vision and have probably be do PRON since mid90   algorithmic trading bot on stock market  PRON handle something like 85  of all trade   many modern cpu use ai technique  include neural network  to guess what PRON program be go to do next and optimize branch prediction and memory fetch   most good modern fraud and spam detector use some combination of ai technique  clustering  decision tree  svm  even some machine vision to check out attached picture   and  in the oppose blackhat camp  the late automatic captcha breaker use all the late advancement in deep learning too    and of course there be google  facebook and us dod who try to put ai into anything PRON can think of  
__label__c++ __label__least-squares __label__svd __label__eigen __label__dense-matrix if  ain rmtime n   bin rm  cin rn   if PRON need to solve the least square problem via svd of  a and  at  ie  PRON need to solve the least square solution to follow linear system via svd    ax  biquad i123cdots   aty  cjj123cdots  PRON be also possible to do these via householderqrsolve   in eigen  but PRON be instable  so PRON want to do this via svd decomposition of  a and  at  obviously  the svd of  at can be obtain by the transpose of that for  a   PRON question be  how can PRON implement PRON in eigen so that only svd on  a  be ok to solve both the two type least square problem   int m100n50   vectorxd b  vectorxdrandomemc  vectorxdrandomen    matrixxd a  matrixxdrandomm  n    eigenjacobisvdlteigenmatrixxdgt   svda  eigencomputethinu  eigencomputethinv    matrixxd  x   svdsolveb    atransposeinplace      how can PRON save this sentence below which implement the svd of  at  eigenjacobisvdlteigenmatrixxdgt   svda  eigencomputethinu  eigencomputethinv    matrixxd  y   svdsolvec    PRON do not have too much experience use eigen  but when PRON be solve the system with the svd decomposition  actually PRON be do the followin    a x  usvt x  b   and PRON use the svd decomposition of a to isolate x   x  vs1ut b  the same for the other system    at y  vsut y  c   and PRON use the svd decomposition of a to isolate x   y  us1vt c  so with eigen PRON should be able to recover a pointer to the matrix  u   s and  v as something similar to that  matrixutype  u    svdmatrixu    matrixvtype  v    svdmatrixv    singularvaluestype  s   svdsingularvalue    and then use some function from the library to apply the operation to  c  matrix multiplication  traspose multimplication  and diagonal inversion  to obtain  y  PRON be sorry PRON can not give PRON more implementation detail  but PRON think that PRON can help  
__label__gpu __label__r what be the the simple way to get start use gpu computing   PRON interest be primarily in neural network and PRON would love to start use gpu  but PRON time for learn gpu computing be very limited   be there simple library for r or something that make PRON easy to multiply matrix on gpus   there be a tutorial on gpu computing in r at rtutorcom  PRON have various example PRON can look at and primarily use the rpud package which be open source and also make use of the non  free rpudplu   additionally this website have a discussion of a few different package that aid in gpu computing in r the package mention be  gputool  hiplarm  rpud  magma  gcbd  opencl  widelm  cudabayesreg  permgpu  but only the first three be discuss in any detail   if time be of grave concern  PRON would highly suggest look at intels xeon phi coprocessor  not only be PRON nearly or as fast  only require openmp to code for  but intels customer service on the intel developer forum be fantastic  PRON do not know if PRON can use r  but standard language such as c  c  and fortran can be use  PRON could also use intels mkl library directly on the coprocessor for matrix multiplication  PRON know time may be limit for PRON  but if PRON have 1  3 week to really learn cuda  PRON highly recommend cuda by example  an introduction to general  purpose gpu programming  amazoncom   PRON do a fantastic job of explain the general concept of gpu programming  and do a great job of get one up to speed with nvidia s gpu language  cuda   cuda be arguably one of the most widely  use gpu language — certainly within the high  performance computing community  eg nasa   PRON give great control over lot of detail  but for many task  PRON also require low  level knowledge of the graphic card  PRON be not something PRON will learn in a day  probably not even in a week  but PRON can be worth PRON   a few more pointer on how to start with gpu   1  the new free online course that cover library and openacc directive  httpsdevelopernvidiacomintrotoopenacccourse2016  2  the list of library already accelerate with gpu  httpsdevelopernvidiacomgpuacceleratedlibrarie  3  openaccorg  also have a lot of resource 
__label__machine-learning __label__markov-process PRON have just learn markov chains which PRON be use to model a real world problem  the model comprise 3 state  a b c   for now PRON be collection datum and calculate transitional probabilities  tab    transition from a to b   total transition to a  however PRON be stick at determine the correct transition matrix  as PRON be get more datum  the matrix be change drastically  so when do PRON finalize transition matrix  do that mean that PRON data be too random and can not be model or PRON be do some mistake here   PRON expect PRON have  or can make  a matrix of transition count  consider the datum in each row to be draw from a multinomial distribution  then PRON should be able to use sample size calculation for the multinomial to get off the ground   PRON be also possible that PRON data be not well describe by a simple markov chain  there be some available technique for this  eg multistate modelling  but which may or may not fit PRON particular problem  
__label__computational-physics __label__software PRON be face a problem solve a mechanical system which be show in the scheme below   PRON assume that coupling of the two shaft be do with a clutch  equation     j1fracdomega1dt   tdomega12   tsphi12   t1    j2fracdomega2dt   tdomega12   tsphi12   t2  where   w1  fracdphi1dt    w2  frac  dphi2dt       phi12  phi1  phi2    omega12  omega1  omega2    tdomega12   c12  omega12    tsphi12   ks  phi12   c12  and  ks be some coefficient   so the equation have the form of     fracdphi12dt   omega12    fracdomega12dt   fract1j1   fract2j2   fractdomega12jeq   fractsphi12jeq  and also i have the dependency     ccr  2jeqomegan    omegan  sqrtfrac  ksjeq    jeq  fracj1j2j1j2  PRON task be to plot how nonlinear clutch affect the rotational inertia   give     t1t   t0  1t   t0  1  t2t   0    j1  j2  5  c12  0  omegan  100    tsphi12   k1phi12  k2phi123    k20  10e14  k2   10  6  10  4  PRON need to plot the   fractsmaxt0   flogfrac  k2k20   so far PRON progress be   but i do not think that the system actually behave that way  PRON mean  i expect a lilte more stable curve without such a chaotic behaviour   whatsmore i try to solve PRON in simscape but get really confused   any help be more than appreciate   PRON get similar result use the value PRON give   when  phill1   the  phi3  term may be negligible  depend on the value of  k2  with  phigt1   the  phi3  term would become significant and probably be the reason PRON get the result that PRON show in PRON graph   PRON suspect that  phigt1  could not happen in a real system  because the material would rupture first   PRON do not remember PRON failure theory well enough to be too specific  but PRON would choose a shaft diameter that prevent failure due to maximum shear stress  and due to fatigue   base on PRON experience with steel shaft for ship propulsion   phimax should be on the order of one milliradian or less  in PRON experience  the shafting be design so that the alternate component of torque be no more than 4  of the average torque  to prevent fatigue failure   of course  PRON system could be much different than what PRON be think of   by the way  PRON would be easy to answer PRON question if PRON know more about the actual system  the value of  k1  and the unit for PRON value   which component have the nonlinear behavior  
__label__finite-element __label__quadrature PRON want to integrate a polynomial expression over a 4node element in 3d several book on fea cover the case where integrating be perform over an arbitrary flat 4noned element   the usual procedure in this case be to find jacobi matrix and use PRON be determinant to change the integration basis to the normalized one in which PRON have the simple integration limit  11  and the gauss  legendre quadrature technique be use easily   in other word  displaystyleints fx  y mathrmdxmathrmdy  be reduce to the form of  displaystyleint11int11  tildefe  n leftdetjrightmathrmdemathrmdn  but in 2d case PRON change the flat arbitrary element to the flat one but well  shape square 2 by 2   3d 4nod element be not flat in general but PRON suppose  PRON still can be map with 2d coordinate system which be somehow  relate to cartesian coordinate system  PRON can not figure out how  to express  x  y  z  in term of  e  n  and what would be the size  of the jacobi matrix in this case  PRON be suppose to be square    PRON be integrate a function on a 2dimensional manifold embed in  mathbbr3  book in analysis on manifold  like munkres  accessible book  or lee s book on manifold  be helpful in discuss the theory define this type of integral   let PRON suppose that  f be a real  value function define on the manifold  mathcalm  which be PRON 4node 3d element   PRON want to calculate   beginalign   intmathcalm   f mathrmds  endalign   suppose that  varphi be a function that map   112 to  mathcalm then  beginalign   intmathcalm   f mathrmds  int112   fvarphix  ybigdetbigmathrmdvarphimathrmtx  ymathrmdvarphix  ybigbig12  mathrmdx mathrmdy  endalign    PRON use this set of note to refresh PRON memory   above   mathrmdvarphi be the jacobian matrix of  varphi  and  mathrmdvarphimathrmt be PRON transpose   once PRON can write the integral over   112  then PRON can use numerical method to evaluate PRON   some comment   PRON be pretty sure PRON 4node 3d element be a manifold  if PRON be  the function  varphi exist  by definition   be piecewise continuous  for topological manifold   and be invertible  PRON be up to PRON to find a function with those property   the argument above assume that  mathcalm be a smooth manifold  which imply that there exist a  varphi that be continuously differentiable  in PRON case  the element PRON describe may not be continuously differentiable  if that be true  PRON could probably still partition PRON manifold into two smooth manifold  and then the argument above still hold  again  PRON must find  varphi satisfy the property of invertibility and continuous differentiability  
__label__convex-optimization __label__semidefinite-programming PRON have the follow sdp problem   max   trcx  subject to   x geq 0  PRON  x geq 0  PRON want to convert PRON into the standard form specify by csdp  PRON be use the callable c interface   which be   max   trcx  subject to   foralli  traix   ai  x geq 0  the sdp be part of an algorithm which iteratively increase the dimension of  x until the value of  max trcx converge   be there any efficient way   x be already know to be sparse  if that help   suppose that  x in mathbbrn time n  and  x be positive semidefinite  for convenience  define  beginalign   langle a  brangle  sumk  l  aklbkl   endalign   for square matrix  a  b of equal size  this operation correspond to the trace of the matrix product   so PRON formulation currently look like  beginalign   max langlec  xrangle   mathrmst  x succeq 0i  x succeq 0   endalign   where  x succeq 0  mean that  x be positive semidefinite   essentially  PRON would create a positive semidefinite matrix of slack variable  s in mathbbrn time n such that  s  x  i  and  s succeq 0 let  eij be a matrix of consist of all zero except for the   i  jth element  which be set to 1  also  define the matrix  beginalign   x   leftbeginarrayccx  amp  0  0  amp  sendarrayright     c   leftbeginarrayccc  amp  0  0  amp  0endarrayright     aij   leftbeginarraycc  eij   amp  0  0  amp  eijendarrayright    endalign   then   PRON think   the follow reformulation work   beginalign   max langlec   xrangle   mathrmst  x  succeq 0langle aij   x  rangle  deltaiji  j1ldot  n  endalign   where  deltaij be the kronecker delta symbol  
__label__neural-network __label__backpropagation PRON be work on an implementation of a neural network so PRON can really grasp how these magic box work  however the neural network PRON have write code for do not work and PRON think PRON be due to PRON implementation of backpropagation   pseudo code    last layer only  for each node  i  in last layer  errori   outputerrori   end for   all hidden  inner layer  for each layer  i  in layer  not include layer 0 or last   for each neuron  j  in layer i  for each neuron  k  in layer i1  errorij    errori1k   weightij  k   end for  errorsij   errorsij   activationij    1activationij    for each neuron in layer i1  deltaweightij  k   errorikactivationiklearningrate  numexamples  deltabiasik   errorik   biasiklearningrate  numexample  end for  end for  end for  then PRON update all the weight with this   weightij  k   weightij  k   deltaweightij  k   now the problem PRON have be that the output do not seem to get any good  PRON certainly change but PRON do not seem to minimise the cost function at all  do anyone know why   have a look at simon haykin s  neural networks and learning machines   there be a good treatment of back  propagation there for both sigmoid and radial  basis function network   some reason the network may not be get good   learning rate be too high  jump back and forth forever   learning rate be too low  when divide by train set size  go to zero   PRON be replace the error signal with the weight gradient signal   the reuse of error  base on the indentation  seem to be right on the same line as the inner loop   PRON be not sure if that be correct   first  backprop PRON error  then calculate the gradient base on the activation of the hide neuron and delta sigmoiderror    here be a very nice blog post of how to do PRON in r from scratch  parallelr nn  follow some link suggest in this post  PRON be enlighten   furthermore  additional post tackle how PRON can improve on computation time by use parallel computation on cpu and gpu  
__label__preconditioning __label__conjugate-gradient __label__krylov-method __label__symmetry in this previous thread the follow multiplicative way to combine symmetric preconditioner  p1  and  p2  for the symmetric system  ax  b be suggest   beginalign   ptextcombo1     amp  p11   p21i  a p11    ampp11   p21   p21  a p11  endalign   this combine preconditioner be not symmetric  however  PRON have try use PRON in conjugate gradient anyway in several different context  and the method always seem to converge just fine  why be this   example 1  random matrix    testing multiplicative combination of preconditioner  n  500    u  s   svdrandnn  n    a  usu     w1s1   svdrandnn  n    noise1  w1s1w1     w2s2   svdrandnn  n    noise2  w2s2w2    p1  a  05  noise1   p2  a  05  noise2   solvep  x  p1x  p2x  p2ap1x     b  randnn1    xtrue  ab   pcga  b1e6n    pcga  b1e6n  p1    x  pcga  b1e6n  solvep    normxtrue  xnormxtrue   here be the output PRON get   pcg converge at iteration 127 to a solution with relative residual 99e07   pcg converge at iteration 62 to a solution with relative residual 68e07   pcg converge at iteration 51 to a solution with relative residual 81e07   relative error 423e07  example 2  combine multigrid with incomplete cholesky for solve the poisson equation   n  50  n  n2   a  gallerypoissonn    laplacian on n  by  n grid  zero dirichlet bc  l  ichola    solvep1  x  llx     combinatorial multigrid  httpwwwcscmuedujkoutiscmghtml  solvep2  cmgsdda    solvep  x  solvep1x   solvep2x   solvep1asolvep2x     b  randnn1    xtrue  ab   pcga  b1e6n    pcga  b1e6n  solvep1    pcga  b1e6n  solvep2    x  pcga  b1e6n  solvep    disprelative error   num2strnormxtrue  xnormxtrue3     for this PRON get the result   pcg converge at iteration 131 to a solution with relative residual 84e07   pcg converge at iteration 44 to a solution with relative residual 6e07   pcg converge at iteration 19 to a solution with relative residual 7e07   pcg converge at iteration 12 to a solution with relative residual 47e07   relative error 52e07  note   PRON have also see the same qualitative behavior on matrix arise in more complicated  realistic situation   give an incorrect solution to  ax  b  the error  e and the residual  r be relate by the error equation  ae  r one can view this combo preconditioner as approximately solve the original equation use  p1  instead of  a  then approximately solve the error equation with  p2  instead of  a  then add the approximate error back to correct the original approximate solution   in short  orthogonalization of the krylov vector occur with respect to the operator  but not with respect to the preconditioner   alright  so say PRON want to solve  ax  b with preconditioner  b  the precondition  cg iteration be basically   beginalign    hatv1tildev1   amp  bb  v1   amp  tildev1  c1    hatvi   amp  bavi1  tildeviamphatvisumlimitsj1i1  langle avkhatvirangle vk  vi   amp  tildevi  ci  ci   amp  sqrtlangle tildevitildevirangle   endalign    in other word   hatvi  new krylov vector   tildevi  orthgonalized krylov vector   vi  normalized krylov vector  now  what be interesting from this be that PRON get a number of property   the two that really matter for PRON be   textrmspanvii1m  textrmspanbai1bbi1m   vtav  i  the first property follow from how  hatvi be generate   PRON be subtract thing from PRON and normalizing  but PRON be basically just get a bunch of  bavi1 where  v1  be a scaling of  bb  the second property follow from  tildevi  here  PRON do not use  b in the orthogonalization   orthogonalization depend purely on  a be symmetric positive definite   now  say PRON want to solve the system  ax  b  if PRON choose  x in PRON krylov space  PRON solve for  beta in     avbeta  b     however  this be probably overdetermin   hence  PRON solve the normal equation  so that     vtavbeta  vtb     from above  PRON know that  vtav  i  so PRON really have that     beta  vtb     or that     x  vvtb      note  this hold regardless of the preconditioner  b  in fact  PRON could take random vector and  a orthogonalize PRON against each other and the same would hold   really  the only thing that  b affect be the krylov space   as long as  b help cluster the eigenvalue of  a  the algorithm perform well   though  if  b be nonsymmetric  PRON may have to look at something like the pseudospectra to figure that out   PRON be not entirely sure   mostly  cluster the spectra   do PRON lose anything with a nonsymmetric  b   probably   normally  in cg  PRON have property like  langle bri  rjrangle  0  for all  ineq j  PRON be pretty sure that no longer hold   also  PRON normally have  langle bavi  avjrangle0  for  i  jgeq 2  that may or may not hold   really  any result that have  b PRON would be suspicious of   anyway  PRON be pretty sure PRON be possible to code cg in a way that d break everything with a nonsymmetric  b  however  if PRON code PRON use the equation above  there be nothing that really prohibit PRON from work with a nonsymmetric  b  PRON do PRON all the time and PRON seem to work fine   read httpwwwsciencedirectcomsciencearticlepiis1877050915010492  nonsymmetric preconditioning for conjugate gradient and steepest descent methods  
__label__floating-point __label__computational-physics __label__preconditioning __label__precision PRON be work with legacy physical code and PRON develop new one base on the output of PRON   PRON all use PRON own internal normalization of variable  for example all distance be divide by the physical size of the simulation box  which lead to a mess  the goal  PRON think  be to have all variable near 1  on the contrary  PRON code use the international system of unit to clarify this mess  since PRON use float point double precision  PRON do not have any problem of denormalization  but PRON colleague continue to repeat to PRON that one should normalize PRON variable  without be able to explain PRON the reason   be PRON a legacy practice  or there be still excellent reason to do this   this question be very closely related to  and possibly a duplicate of  be variable scale essential when solve some pde problem numerically    there be still good practical reason to nondimensionalize equation  if possible   PRON reduce the number of independent parameter for parametric study  which be one of the original reason for nondimensionalization in the first place   which be important for uncertainty quantification  the few parameter PRON have  the less expensive uncertainty quantification will be   PRON be equivalent to precondition PRON equation  for affine transformation of variable  only one variable per transformation  so there be no coupling   PRON amount to diagonal preconditioning  which help with convergence of iterative method  and PRON be usually cheap to do PRON PRON analytically while manipulate the equation on pen and paper  or use PRON favorite computer algebra system    PRON give PRON insight into the equation  if PRON know which term be dominant  and which term be not  PRON can sometimes develop reasonable right preconditioner by drop non  dominant coupling term  for an example that could have be deduce due to appropriate scaling  instead  PRON appeal directly to physical argument   see jacobian  free newton  krylov method  a survey of approach and application  section 341   if PRON know all of PRON dependent variable be of order 1  PRON give PRON a natural point of reference for error tolerance of numerical method  otherwise  if PRON know characteristic scale for PRON variable  PRON would need to adjust PRON error tolerance accordingly   for independent variable  PRON can be helpful when generate mesh  for instance  PRON have be model microscale device  and some mesh generator  eg  the build  in mesh generator in fenics  dolfin 13  will return error if PRON use standard metric dimension  like  106 for the edge of a square domain  if PRON nondimensionalize  PRON be mesh a unit square  and those numerical issue go away   PRON be sometimes necessary to avoid singularity in simulation   see preconditioning and the limit to the incompressible flow equation   
__label__neural-network __label__lstm PRON have a dataset with sale number for around 100 related product  every day  the number of sale of each product be record along with other relevant information  what day of the week be PRON  be PRON a public holiday  what be the weather like etc  etc     so essentially this be a time series with daily entry  and PRON be think of push this through an lstm   PRON question be  how do PRON deal with the fact that PRON have multiple observation at every point in time   day  product  wheather  numbersold  1 jan  meat  sunny  15  1 jan  apples  sunny  211  1 jan  fries  sunny  5    1 jan  carrots  cloudy  75  2 jan  meat  cloudy  10  2 jan  apples  cloudy  220      do PRON have to divide PRON dataset up by product so as to have only one entry per day to feed into the lstm  or be there a way to deal with all of the 1 jan observation as a sort of batch that be remember   if PRON be get PRON correctly PRON be face problem in feed multivariable to lstm as time series  this problem be call as the multivariate time  series problem  there be several method to solve PRON  firstly PRON will have to convert datum general supervise dataset without change sequence  this can be do use method call as  slide window method  or lag method   once data be in correct format PRON can feed PRON to lstm  here be this tut to learn PRON quickly 
__label__machine-learning __label__accuracy i be try to train a deep neural network to figure out that if there be a 1 and 0 present in the first two column of x that the output be 1 otherwise PRON 0  but PRON be only get a 75  accuracy on the model   import numpy as np  import tflearn  x    0  0  1     0  1  1     1  0  1     1  1  1    y    0  1     1  1     1  0     0  1    xtest  nparray1  1  1     0  1  1     1  0  1     0  1  1      build neural network  net  tflearninputdatashapenone  3    net  tflearnfullyconnectednet  32  activationsigmoid    net  tflearnfullyconnectednet  32  activationsigmoid    net  tflearnfullyconnectednet  2  activationsoftmax    net  tflearnregressionnet  optimizeradam     define model  model  tflearn  dnnnet    start training  apply gradient descent algorithm   modelfitx  y  nepoch5000  batchsize16  showmetric  true   pred  modelpredictxt   for i in range4    printpredi0    the output should be   0  1  1  1   training step  4999   total loss  050493  time  0004s   adam  epoch  4999  loss  050493  acc  07813  iter  44    001631585881114006  04872587323188782  09684665203094482  0019177177920937538  PRON look like PRON be train this as a multiclass classifier  to represent a binary choice  in which case  PRON y value be wrong   y    0  1     1  1     1  0     0  1    here PRON second label be not self  consistent  and thus PRON be impossible to predict use a softmax output layer  where the sum of all output must equal 1   the good PRON can do be  05  05  to match that label and PRON can see actually PRON get close to that in PRON test   PRON want this instead   y    0  1     1  0     1  0     0  1    a few aside     PRON example input all have the same third column   1   this be redundant datum  and PRON could drop PRON   PRON network be more complex than PRON need to be for this task  a single hidden layer with only a few neuron in PRON should be sufficient   for this specific task PRON could have choose a single output neuron use a sigmoid activation  and need only one column in y   
__label__fluid-dynamics PRON have a cfd solver at hand which can deal with 2 phase and so far be able to exchange momentum and energy across the interface  where both phase meet   how this be do be well document in the follow paper  a conservative interface method for compressible flow  by hu  khoo and adams  which publicly available   in the paper  equation  10  be give the general idea of how to update the conservative variable in so call  cut  cell   cell s which be cut by the interface  track by a level  set method    in this update routine  hatx   hold the average energy and momentum exchange  now PRON want to extend the code such that PRON can deal with reactive front  like combustion or cavitation    the arise riemann problem be now solve by a reactive riemann solver  which provide next to pressure and velocity across the contact discontinuity  also the mass flow through the reactive front   so far all good  but now PRON be have trouble to adjust  hatx such that PRON transport the mass  density  across the front   PRON would be very grateful for any thought or hint   
__label__convnet __label__image-classification PRON have just be read  zeiler  md and fergus  r  2014  september  visualize and understand convolutional network  in european conference on computer vision  pp  818  833   springer international publishing   link  PRON summary   and  partially   mahendran  a and vedaldi  a  2016  visualize deep convolutional neural network use natural pre  image  international journal of computer vision  pp1  23   link  PRON summary   PRON be both about visualize feature be learn in cnn   although both paper have an introduction where PRON can read thing like   PRON understanding of  cnn feature  remain limited  while the performance of representation have be improve significantly in the past few year  PRON design remain eminently empirical  in this paper  with the aim of obtain a good understanding of representation  PRON develop a family of method to investigate cnn and other image feature by mean of visualization  there be no clear understanding of why PRON perform so well  or how PRON may be improve  there be still little insight into the internal operation and behavior of these complex model  or how PRON achieve such good performance  although PRON like the image  PRON do not see how these method be good than simply push all image through the network and show the topn image which activate the neuron of interest most  be this evaluate  do the author have any insight into the feature which other author do not have before  without those technique    the zeilerampfergus paper at least add the occlusion sensitivity analysis which do help  however  a big part of the paper be this filter visualization by deconv  net  and PRON do not see how this help to address any of the point mention above   
__label__social-network-analysis __label__career PRON be interested in join some data science groups on link in   the relevant list be long   PRON do not think PRON be to PRON advantage to join all  but PRON be not sure how to evaluate the individual group s potential to elevate PRON career   be there some clear criterion for which PRON can evaluate potential group   PRON find this article at forbescom   PRON do answer PRON question directly  in summary PRON say   define what PRON be that PRON be look for from the group  connection  information  exposure   to do this PRON need to know who be PRON authentic self   put another way  what be PRON brand   be discriminant about the group PRON join   only join those that further the goal PRON have define   research the potential group to find out if PRON be in line with PRON goal   join and try the group find with PRON research effort   kdnugget use number of member  growth rate  number of conversation and member engagement in PRON analysis of datum science group   this analysis can be use to answer if the specify group be inline with PRON goal   the article go on to give advice about etiquette once join  but that be beyond the scope of PRON question   PRON be limit to  50 group   kdnugget use number of member  growth rate  number of conversation and member engagment in PRON  analysis of data science group  
__label__finite-element __label__finite-difference what do PRON actually type into the  orientation entry in an input file   be PRON a rotation matrix wrt  the global ax   the meaning of the  orientation keyword in abaqus can be check in the  abaqus keyword reference manual   which unfortunately be not make publicly available by simulia   as most abaqus keyword   orientation have a lot of optional parameter  which change PRON meaning and syntax  in most case however the line follow this keyword be the  x  y  z coordinate of three point  a  b  c   c  optional if coincident with the global ax origin  ie    000  be the origin   a lie on the  x axis   b be a point on the  xy plane  
__label__stability __label__parabolic-pde PRON have a variable coefficient pde of the form   ut  ct  xuxx   tin  0t   xin  01 with initial datum  u0u0x and  cx  tin c0ttimes01 PRON use three point discretization for the diffusion part  PRON would like to have en error estimate by prove stability and consistency  do taylor expansion  PRON can show consistency  however  PRON be confused how to choose the norm  PRON have see in the book the author choose discrete  l2  norm and by use discrete energy method the stability follow   first question for the discretized equation  if PRON pick another discrete norm  which one should PRON pick  PRON mean one of the  lp norm   which one be easy or hard  so far  PRON have obtain the estimate that  vece2oh2  where  vecevecuhtvecut but the other similar estimate would follow in an analogous way once PRON prove the stability in those norm  please provide some hint how to choose another norm and what be the  tool  to prove stability for such an equation in other discrete  lp norm   and second question  be the choice affect by the stability estimate of the original pde  that is  once PRON establish continuous dependence on initial datum   PRON follow from maximum principle that PRON have stability in  linfty  and assume PRON have manage to show PRON be well  pose in  l20ttime  01by the same energy method in continuous setting   do imply PRON be well pose in  l1  or  linfty  how be that connect to the fact  linftyin l2  in l1    for PRON case  the  l2  norm be natural  if PRON have an equation of the kind     rhox  ut   ax  uxx  f     then the natural norm would be  ul2textweight    sqrtrho  ul2 as the analog of the  l2  norm  and  uh1textweighted    sqrta  uxl2 as the analog of the energy   h1   norm  PRON get these naturally if PRON multiply the equation by  u or  ux  respectively  and integrate over the domain  if the two coefficient be bound pointwise from below and above  these norm be of course equivalent to the  l2  and  h1  norm  respectively  thus  if PRON have a result in the weighted norm  then PRON immediately have the same result in the original norm   use this technique  show convergence in  l2textweighted  then PRON immediately have convergence in  l2  as well  and consequently in  l1  PRON do not seem to see much sense in show convergence in any other of the  lp norm for  p not either  12infty nobody seem to care about other value of  p and PRON can not see a reason to care either   in general  well  posedness  with combine existence  uniqueness  and stability  hold in the space for which PRON show PRON only  there can be no solution in a small space  and multiple solution in a large space  the continuous dependence of the solution on the datum also depend on the norm  and thus on the norm space  in which PRON take datum and solution   as wolfgang bangerth already point out  there be a natural norm  the energy norm  in which stability hold for the continuous problem and thus for any consistent approximation  if    PRON data be sufficiently smooth  PRON can of course improve this to get stability in high norm   if PRON focus on a specific discretization  PRON be correct that equivalence of norm in finite dimension give PRON stability in any norm as long as PRON have stability in one norm  however  and this be the crucial point   the equivalence constant depend on PRON discretization  and usually blow up as the discretization parameter  h tend to 0  needless to say  this make the equivalence argument useless for prove error estimate as  h tend to 0   PRON should also point out that  linfty in time error estimate be pointwise estimate for  uht at any  tin0t  not just for the time step  un  utn this make these estimate much hard to obtain than  say   l2  estimate   finally  PRON think PRON have the question the wrong way around  PRON be not   let PRON show convergence  what norm should PRON pick   but  PRON need convergence in that norm  how do PRON show PRON    if any norm will do  use the energy norm   
__label__beginner __label__education PRON be currently pursue a bachelor s degree in physics from a university in the uk  most datum science job here have a strong preference for people with phds in numerate degree like physics and math  PRON do not understand the point of spend 5 year of PRON life do research in something PRON do not want to pursue a career in   PRON be confused about what master s degree PRON should choose if PRON want to apply for a job straight after graduate  a physics degree kind of leave PRON hang in the middle in term of skill  PRON have do some analysis for experimental datum use python  but only very basic stuff  a bit of predictive modelling and c  lot of math  but barely any statistic  so PRON could either get a degree in cs to get good at programming or get one in statistic  which one would give PRON the skill most relevant to data science   also  would PRON recommend that PRON get a diploma  certification in machine learning   PRON think the good thing for a beginner be some practice to let the theory sink in   if PRON need book suggestion  do a search   if PRON be weak on computer science  contribute to an open source project like scikit  learn  if PRON be weak on datum analysis  compete in a kaggle competition  write a paper andor a few blog post to prove PRON skill  do this now while PRON be in school  an undergraduate physics degree should be enough to get PRON foot in the door if PRON do the above  a degree will rarely get PRON a job by PRON  PRON still have to pass the job interview  and that be why PRON recommend get practice  
__label__pde __label__time-integration __label__runge-kutta __label__symbolic-computation PRON feel like publish some previous work from PRON phd thesis  PRON be use mathematica to build a system of 2n partial differential equation for 2n function by symbolic spatial taylor expansion  then numerically integrate PRON with ndsolve with respect to time   mathematica have a strange behaviour for some n order  stop to some apparent numerical singularity PRON be not able to manage at that time  furthermore  PRON have an access to a supercomputer  which do not offer mathematica  PRON would therefore like to rebuild PRON model in python   for the symbolic part  PRON guess sympy will handle easily the taylor expansion part  notice that after order 2 or 3  PRON take far more than a full page to mathematica to write the pde system  should PRON ask PRON to   but how do PRON numerically  runge  kutta or whatev  solve the object come out of sympy    but how do PRON numerically  runge  kutta or whatev  solve the object come out of sympy   that depend a lot on PRON specific problem and what level of optimization PRON need   most ode solver  such as those include in scipy  require that PRON provide PRON a python function represent the right  hand side of the ode   as PRON do not want thing to be horribly slow  PRON have to take care of vectorisation somehow   depend on how PRON pde be structure  this may work use sympy ’s lambdify with a numpy backend   an alternative be sympy ’s ufuncify function   finally  PRON write a module that take sympy input  compile PRON  and feed PRON into a scipy integrator  this do not require vectorisable structure  but PRON can also not exploit PRON very well  as PRON be make with non  vectorisable problem in mind    either way  PRON mean that PRON have to take care of spacial discretisation on the sympy level   PRON also will not be able to make use of pde  specific optimisation technique  in particular those involve gpu  PRON may get multi  kernel support though  if that ’ how PRON want to make use of the supercomputer   PRON convert sympy ’s result into whatev input be need by pde  specific python tool   this probably involve meta  programming  code that write code  of some sort  but could be rather harmless   note that sympy feature several code printer and similar that may assist PRON with this   PRON want to point out that pyodesys be a python library that do exactly what PRON be look for  send sympy expression to ode solver  PRON be nice because PRON link to more solver  but wrzlprmft s jitcode be probably good because PRON compile the result expression and the runtime of the user s function be very important for solve an ode efficiently  
__label__algorithms let say PRON have a 5 dimensional grid where each dimension have 10 point  there be 100000 combination  let PRON say PRON want a subset of 10000  be there a deterministic algorithm that will choose a set of point that  cover  the region the good and converge to the limit case of choose every point   if PRON have a weighting function for each point  would there be a deterministic algorithm that would tend to choose point that be equidistant in a weighting sense as well   PRON can think of some possible way in low dimension  just wonder if there be a well define solution  area of math  for these type of problem that extend to high dimension  PRON have try search sample method  multi  dimensional sampling and the like  but no luck  let PRON talk about the uniform sampling case  suppose PRON define some spatially regular ordering for the point on the grid  for the 5 dimensional point  say the first coordinate increase the slow  then the second increase the next slow  and so on  and the last coordinate increase the fast  denote the order set  pi for  i0ldot  n1  where  n100000  be the total number of point  let  n be the number of point PRON want to sample  if PRON take the set of point     pj  j   aim  mod n     for arbitrary  a and  i0ldot  n where  m and  n be coprime  then the  pj should not repeat until  n  n if PRON choose  a and  m reasonably   m be quite a bit large than  n   then these point will also be relatively uniformly distribute  but perhaps with too much structure and no guarantee that PRON have the blue noise property   for the weighted case  PRON be essentially ask for a combinatorial optimization  so PRON doubt there would be efficient way of solve PRON  on the other hand  PRON problem size of  10  5  be not that big   PRON enumerate PRON grid point along a space  fill curve  if PRON then want  m out of  n point  choose the point with index  kfracnm  where  k1dot m  depend on the curve PRON use there may be some pathological case  but in general this should give PRON a good sampling of PRON space  assume the grid point density be more or less homogeneous to begin with  
__label__machine-learning __label__python __label__scikit-learn __label__pandas __label__feature-scaling PRON be new to machine learning and PRON have a conceptual question   PRON have a scale dataset  scikit  learn and panda    after training  test PRON algo  PRON will make new prediction use new actual datum which will not be scale or normalize   will this discrepancy be a problem  if so  how should PRON resolve PRON   best   PRON should save the scaler param use to fit the training set and use the same one to transform all other datum use with the model from then on  whether cv  test or new unseen datum   after training  test PRON algo  PRON will make new prediction use new actual datum which will not be scale or normalize   no that will not work  once PRON add scale  normalisation to the training pipeline  the exact same scaling  as in same scaling param  not re  calculate  should be apply to all input feature   the scikit  learn scaler like eg standardscaler have two key method   fit should be apply to PRON training datum  transform should be apply after fit  and should be use on every datum set to normalise model input   fittransform can be use on the training datum only to do both in a single step   if PRON need to do the training and prediction in different process  maybe live prediction be on different device for instance   then PRON need to save and restore the scaling param  one basic  simple way to do this be use pickle eg pickledump  minmaxscaler  open   scalerp    wb    to save to a file and minmaxscaler  pickleload  open   scalerp    rb    to load PRON back  
__label__convolutional-neural-networks __label__lstm __label__getting-started PRON be new to deep learning   PRON have run the caffe2 tutorial to train the lenet for mnist with PRON dataset and also the lstm tutorial   initially PRON be think of combine both system to get a cnn lstm system  like donahue s lrcn    with the release of detectron  PRON be wonder whether anyone have some idea on how to integrate the detectron with an lstm for image caption task   or  where PRON can find a similar implementation of some type of r  cnn combine with an lstm   
__label__neural-networks PRON be new to neural  network and PRON be try to understand mathematically what make neural network so good at classification problem   by take the example of a small neural network  for example  one with 2 input  2 node in a hidden layer and 2 node for the output   all PRON have be a complex function at the output which be mostly sigmoid over linear combination of sigmoid   so  how do that make PRON good at prediction  do the final function lead to some sort of curve fitting   neural network be good at classify  in some situation that come down to prediction  but not necessarily   the mathematical reason for the neural network prowess at classifying be the universal approximation theorem  which state that a neural network can approximate any continuous real  value function on a compact subset  the quality of the approximation depend on the number of neuron  PRON have also be show that add the neuron in additional layer instead of add PRON to exist layer improve the quality of the approximation faster   add to that the not well  understand effectiveness of the backpropagation algorithm and PRON have a setup then can actually learn the function that the uat promise or something close   in neural networks PRON consider everything in high dimension and try to find a hyperplane that classify PRON by small change   probably PRON be hard to prove that PRON work but intuition say if PRON can be classify PRON can do PRON by add a relaxed plane and let PRON move amongst datum to find a local optimum   with neural networks PRON simply classify data  if PRON classify correctly  so PRON can do future classification   how PRON work   simple neural network like perceptron can draw one decision boundary in order to classify data   for example suppose PRON want to solve simple and problem with simple neural network  PRON have 4 sample datum contain x1 and x2 and weight vector contain w1 and w2  suppose initial weight vector be  0 0   if PRON make calculation which depend on nn algoritm  at the end  PRON should have a weight vector  1 1  or something like this   please focus on the graphic   PRON say  PRON can classify input value into two class  0 and 1   ok  then how can PRON do this  PRON be too simple  first sum input value  x1 and x2    0  00  0  11  1  01  1  12  PRON say   if sumlt15 then PRON class be 0  if sum15 then PRON class be 1 
__label__parallel-computing __label__software __label__linear-solver cholmod be very fast  but PRON be just wonder what kind of size a such that PRON can solve ax  b  PRON have a a of 200000  200000  but PRON output error  problem too large   PRON be very appreciate if anyone can tell PRON which solver  software for numerical computing can solve such large size problem  thank   the storage require by a sparse matrix depend  roughly linearly  on the number of nonzero element in the matrix   when PRON then attempt to compute a cholesky factorization  or more generally an lu factorization  of PRON sparse matrix  the factor typically have substantially more nonzero element than the original matrix had this be refer to as  fill  in    the amount of fill  in that occur will depend on the number and placement of the nonzero in the original matrix   in many case PRON can reduce fill  in by carefully order the row  column of PRON matrix before factor PRON   there be many algorithm for reduce the fill  in  but compute an optimal ordering be an np  hard problem  so in practice these algorithm use heuristic approach to get a good fill  in reduce ordering   PRON appear that cholmod could not factor PRON matrix because there be not sufficient storage available to handle the fill  in that occur during the factorization process   do PRON reorder the row  column of PRON matrix before factor PRON to reduce fill  in   if not  then use a good ordering may be enough to make this problem solvable within the memory that PRON have available   can PRON try to solve PRON problem on a computer with more memory  
__label__neural-networks __label__deep-learning __label__classification __label__computer-vision __label__ai-basics PRON be fairly a newbie to neural networks   PRON want to ask if PRON be possible to train a nn to identify only one type object  for instance  a table from a large set of image  where the nn should be able to identify if new image be table   if yes can PRON please guide PRON in the direction to get start   edit   PRON want to ask if PRON be possible to train a nn to identify only one type object    so PRON understanding as of now be that if i train an nn on one class and one class alone with a fairly large amount of datum then i can get PRON train in a small amount of time rather than train PRON on a huge amount of datum and consume a large amount of datum   objective be to train a large no of such small nn to create an ensemble of nn   yes  PRON can train a nn to detect only one type of object like a table  however  PRON probably will not want to train such a nn from scratch by show some example of table and non  table  PRON will need to use transfer learn on a model already train on several image class and teach PRON to also recognize PRON new class  this transfer learning require a small set of desire image  PRON may need to give PRON some negative example also  PRON should explore transfer learn with mobilenet  inception  and other pre  train tensorflow model if PRON be willing to use python and tensorflow 
__label__intelligence-testing the original lovelace test  publish in 2001  be use generally as a thought experiment to prove that ai can not be creative  or  more specifically  that PRON can not originate a creative artifact   from the paper   artificial agent a  design by h  pass lt if and only if  a output o   a output o be not the result of a fluke hardware error  but rather the result of process a can repeat  h  or someone who know what h know  and have h s resource  can not explain how a produce o  the author of the original lovelace test then argue that PRON be impossible to imagine a human develop a machine to create an artifact  while also not know how that machine work  for example  an ai that use machine learn to make a creative artifact o be obviously be  train  on a dataset and be use some sort of algorithm to be able to make prediction on this dataset  therefore  the human can explain how the ai produce o  and therefore the ai be not creative   the lovelace test seem like an effective thought experiment  even though PRON appear to be utterly useless as an actual test  which be why the the lovelace test 20 be invent   however  since PRON do seem like an effective thought experiment  there must be some argument against PRON  PRON be curious to see any flaw in the lovelace test that could undermine PRON premise   PRON be a future neurologist with a very complete understanding of linguistic processing in the brain   PRON be also an overprotective parent  so PRON monitor every phrase utter to PRON child  and also completely determine all the book PRON read in the course of PRON education   when PRON child write a poem  then  PRON know the dataset on which PRON brain be train  as well as the process by which PRON language input become language output  in broad outline PRON know these process be non  linear and be base on how different input along with the current collection of trillion of distinct synaptic weight update the synaptic weight   PRON do not know what PRON poem will be  of course  because there be random factor and the whole history of PRON synaptic weight be unobservable  but PRON adhere to the lovelace test and can therefore conclude that compose the poem be not a creative act   the lovelace test  like the chinese room argument  implicitly assume that what computer  ai can do in process symbol and information and what brain can do be distinct   if PRON accept that assumption  then the argument cease to be interesting PRON have merely redefine creativity as one of the distinct thing that brain can do   if PRON reject the assumption  the argument that computer be incapable of creativity cease to be valid   the thought experiment PRON do nothing to assist PRON in evaluate the truth of the assumption  
__label__convergence __label__comsol PRON have couple a comsol model for fluid dynamic with a very simple pde that model the transport of humidity in air   when PRON solve PRON for the stationary case  the solution converge easily  but when PRON try to solve the time dependent problem  the solution can not reach convergence   this seem very strange to PRON  the fluid be laminar and incompressible  the mesh be well design  the initial value be coherent  all the condition be smooth   be PRON even possible that comsol be not able to find a solution for the time dependent problem  while PRON find very easily the time independent one   thank PRON  
__label__neural-network __label__gan have anybody see any application that would use gan that would take input image and would output image of the same size  that could be use as a layer for the first image  the layer would contain in eg  point of interest in the input image  would that even be a good practice of the gan usage   PRON be look for article  example of application that use something similar   in term of generate an image  layer   that be just the same as generate an output image that can be overlay on the input use standard graphic software  if PRON want pixel  level accuracy in the output then the output will need to be the same size as the input  otherwise PRON could be small  provide PRON be the same aspect ratio  and in which case PRON would need to be scale up in order to be use as an overlay  in any case  as the output of a gan can be an image  and often be   then this part be easy   the  g  in gan stand for generative  the purpose of a generative network be to create sample from a population  where there be typically many possibility  those sample can be condition on some additional datum  and that additional datum could be an image  although many example will simple conditioning  such as a category that the training output be representative of   one possibility for use a gan  be where PRON population contain a range of trait  and PRON can calculate vector that control that trait  so PRON can take an input image  reconstruct PRON in the gan then modify PRON by add  subtract the trait  relate vector  an interesting example of this be face age with conditional generative adversarial networks  and similar example be around of add  remove glass etc  for this to work for PRON  PRON would literally need image that have PRON point  of interest in PRON and one without PRON  and then PRON would be able to control addition  removal of point  of  interest  the network would not detect these point in the input  instead PRON would add PRON into the output  from read PRON question this do not seem to be what PRON want   a similar paper use a gan to remove rain from photo  base on train many image with and without rain then learn the  rain vector   encode new image with rain in PRON into the gan s internal representation and subtract this  rain vector    gan condition on input image  as oppose to category or internal embedding  be also possible  this example of image completion may be close to PRON goal  if PRON point of interest be variable with many option feasible  then PRON could work for PRON   however  if PRON point of interest be suppose to always be the same pixel in each image  then PRON goal may be better define by strict ground truth and become more like semantic segmentation  which can be attempt with variation on cnn  such as describe in this paper by microsoft  these be much easy to set up and train than gan  so if PRON can reasonably frame PRON problem as pixel classification from the original image  this be probably the way to go  
__label__python __label__neural-network __label__time-series PRON have a problem with use neurolab python library  PRON be try to predict some time  series with help of elman recurrent neural network   import neurolab as nl  import numpy as np   create train sample   x  nplinspace7  7  20   x   0  01  02  03  04  05  06  07  08  09  1  11  12  13  14  15  16  17  18  19  2  21  22  23  24  25  26  27  28  29  3  31  32  33  34  35  36  37  38  39  4  41  42  43  44  45  46  47  48  49  5  51  52  53  54  55  56  57  58  59  6  61  62  63  64  65  66  67  68  69  7  71  72  73  74  75  76  77  78  79  8  81  82  83  84  85  86  87  88  89  9  91  92  93  94  95  96  97  98  99  10  101  102  103  104  105  106  107  108  109  11  111  112  113  114  115  116  117  118  119  12  121  122  123  124  125  126  127  128  129  13  131  132  133  134  135  136  137  138  139  14  141  142  143  144  145  146  147  148  149  15  151  152  153  154  155  156  157  158  159  16  161  162  163  164  165  166  167  168  169  17  171  172  173  174  175  176  177  178  179  18  181  182  183  184  185  186  187  188  189  19  191  192  193  194  195  196  197  198  199   x  npasarrayx   y   0000  0296  0407  0488  0552  0607  0655  0697  0734  0769  0800  0829  0855  0880  0903  0925  0945  0964  0982  0998  1014  1029  1043  1057  1069  1081  1092  1103  1113  1123  1132  1141  1149  1157  1164  1171  1177  1184  1189  1195  1200  1205  1209  1214  1218  1221  1225  1228  1231  1234  1236  1238  1240  1242  1244  1245  1246  1247  1248  1249  1249  1250  1250  1250  1250  1250  1249  1248  1248  1247  1246  1245  1243  1242  1240  1239  1237  1235  1233  1231  1228  1226  1224  1221  1218  1215  1213  1210  1206  1203  1200  1197  1193  1190  1186  1182  1178  1174  1170  1166  1162  1158  1154  1149  1145  1140  1136  1131  1126  1122  1117  1112  1107  1102  1096  1091  1086  1081  1075  1070  1064  1059  1053  1047  1041  1036  1030  1024  1018  1012  1006  0999  0993  0987  0981  0974  0968  0961  0955  0948  0942  0935  0928  0922  0915  0908  0901  0894  0887  0880  0873  0866  0859  0852  0844  0837  0830  0822  0815  0807  0800  0792  0785  0777  0770  0762  0754  0747  0739  0731  0723  0715  0707  0699  0691  0683  0675  0667  0659  0651  0643  0634  0626  0618  0610  0601  0593  0584  0576  0567  0559  0550  0542  0533  0525  0516  0507  0498  0490  0481   y  npasarrayy   sample   20  201  202  203  204  205  206  207  208  209  21  211  212  213  214   sample  npasarraysample   size  lenx   inp  xreshapesize1   tar  yreshapesize1   smp  samplereshapelensample1    printinp   printtar    create network with 2 layer and random initialized   net  nlnetnewelmminx   maxy5  1    neurolabnetnewffminmax  size  transf  none   net  nlnetnewelmminx   maxy      16  1    nltranstansig    nltranspurelin       set initialized function and init  netlayers0initf  nlinitinitrand01  01    wb    netlayers1initf  nlinitinitrand01  01    wb    netinit     train network  error  nettraininp  tar  epochs1900  show100  goal00001    simulate network  out  netsimsmp   printout   PRON work fine with only one input time series  input vector   but PRON need more than one  in fact  PRON do need five input vector   example   PRON be go to predict 6 row of  tobepredicted  column  the datum  pastebincom7z1deikj so column  usd    euro    gdpbln    inflation    cpi  be the input and  tobepredicted  be a target in PRON case   do anybody know how to solve this issue  thank for PRON help   
__label__classification __label__scikit-learn __label__feature-selection PRON have set of feature of different naturefor example  300 feature from fft  transform  1000 categorical feature and so on   however there be only 900 sample and im try to select important feature use lasso   so the question be  should PRON perform feature selection differentially on subset of feature or better firstly  concatenate feature and only then perform feature selection   let lasso pick the good one  if the feature be highly correlate and PRON want PRON pick as a group  add some l2 regularization too  this be call elastic net regularization  and PRON be a generalization of l1 and l2 regularization  other than that  do not feel oblige to artificially group feature  
__label__classification __label__dataset __label__unsupervised-learning __label__supervised-learning __label__unbalanced-classes PRON have create a synthetic dataset  with 20 sample in one class and 100 in the other  thus create an imbalanced dataset  now the accuracy of classification of the datum before balance be 80  while after balancing  ie  100 sample in both the class   PRON be 60   what be the possible reason for this   imagine that PRON data be not easily separable  PRON classifier be not able to do a very good job at distinguish between positive and negative example  so PRON usually predict the majority class for any example  in the unbalanced case  PRON will get 100 example correct and 20 wrong  result in a 100120  83  accuracy  but after balance the class  the good possible result be about 50    the problem here be that accuracy be not a good measure of performance on unbalanced class  PRON may be that PRON data be too difficult  or the capacity of PRON classifier be not strong enough  PRON be usually good to look at the confusion matrix to better understand how the classifier be work  or look at metric other than accuracy such as the precision and recall   f1  score  which be just the harmonic mean of precision and recall   or auc  these be typically all easy to use in common machine learning library like scikit  learn   accuracy be probably not a good metric for PRON problem   for the original dataset  if the model just make a dummy prediction that all sample belong to the big class  the accuracy will be 83   100120   but that be usually not what PRON want to predict in an imbalanced dataset   let PRON take a fraud detection problem  the probability that a transaction be a fraud be very small  let PRON say 001   but the loss of an undetected fraud transaction be enormous  ex  1 million dollar   on the other hand  the cost of manually verifying if a transaction be relatively small  in that case  PRON would like to detect all possible fraud  even if PRON have to make a lot of false positive prediction   to tackle an imbalanced dataset  first PRON have to choose which question PRON want to answer  then  what be the good metric for this question  answer these 2 question first before decide which technique PRON should use   come back to the original question  why do accuracy reduce when PRON oversample the small class   that be because this technique put more weight to the small class  make the model bias to PRON  the model will now predict the small class with high accuracy but the overall accuracy will decrease  
__label__pde __label__constrained-optimization __label__fourier-analysis __label__spectral-method PRON have a pde optimization problem  and a scalar field  which PRON be optimize over  be suppose to be nonnegative everywhere in the domain  since PRON be work in fourier space for solve this problem numerically  PRON need to convert this non  negative constraint into fourier domain  ie constraint on fourier coefficient    assume the solution be smooth  what be good way to go about PRON   PRON really can not think of anything more complicated than just choose a  large   grid of point in space  say  mathbfxl   l12  m  and add m linear constraint of the type     fmathbfxlsumj1nhat fmathbfjei2pi mathbfjmathbfxlgeq 0  PRON believe the smoothness of  f will ensure that this suffice   do anyone have any other idea  thought on this   this be really a comment  but PRON be one point short of be able to comment   can PRON show PRON the rest of PRON optimization problem  to include clearly identify all optimization  aka decision  variable  as well as other constraint   be PRON fx1  actually linear in the optimization variable  can PRON evaluate the gradient of the objective function by pde solve as oppose to require finite difference   how much  magnitude and location  of nonnegativity violation can PRON tolerate   can PRON objective function be evaluate if nonnegativity be violate   why be PRON assume the solution be smooth  and even if PRON be  why do that mean there will not be  smooth  violation between grid point   perhaps PRON want an adaptive or multi  grid approach in which the grid be make fine as the overall algorithm progress to a solution  or for instance  after initial convergence of the optimization algorithm  put in a fine grid and re  optimize  use the final solution from the coarser grid optimization as PRON starting value for a new optimization use a fine grid   this be more of a comment  but PRON believe the more common name for this be  positive trigonometric polynomial   so this book may be helpful   one approach  httpwwwmiteduparrilocdc03workshopvandenberghepdf  be to use the result that the polynomial    xt   r0  2r1cos t  cdot  2rn cos nt    be nonnegative if and only if there be an   n1timesn1 positive semidefinite matrix  ysucceq 0  such that    rk  mathrmtrek y   sum0leq jleq n  k  yjk  j   qquad e  beginpmatrix0ampin0amp0endpmatrix  PRON be not really too familiar with this  unfortunately  and there exist other characterization as well  
__label__matlab __label__matrices how can PRON determine a matrix  r in matlab such that  give a know matrix of coefficient  a give PRON back PRON row reduced echelon form  obviously PRON need an algorithm  function that work also with matrix that be not fully rank      ratextrrefa     PRON do not know how to handle this  please can anybody help PRON  thank a lot    PRON can solve the equation for  r  then use the result term     r  rrefaa1  try this  example here have rank 2 for a 3x3 matrix   a   1 1 1  2 2 2  5 7 6    rrefa  rrefa    disprrefa   rref of a  ca eyesizea      rrefc  rrefc    r  rrefc46    dispra   should get rref of a  dispr   PRON still can not understand what this  r be good for  but in matlab PRON can simply  abuse the slash operator    gtgt   rrefa  jb   rrefa     gtgt  r  rrefajbajb    here an example  show that this method work also for non square matrix    gtgt  a  randn103randn34     gtgt  ranka   an   3   gtgt  sizea   an   10  4   gtgt   rrefa  jb   rrefa     gtgt  r  rrefajbajb     gtgt  rankr   an   3   gtgt  normra  rrefanorma   an   17351e15  please note that this answer be just code golf  reduce row echelon form be not the most stable rank reveal factorisation  as note above by christian clason  
__label__dataset __label__data PRON be the product manager for an online app   PRON be currently research a new feature where PRON user will be able to access  all PRON raw application datum    this data be likely to be use by datum scientist  PRON be also likely to be load into bi tool   the dataset will contain up to a few million row   how should PRON actually expose the datum in a practical sense   for example   online datasource like amazon redshift  some other rdbms available online  eg a dedicated postgre installation   csv file available on s3  csv file available for download in a web interface  dumps into google sheet  do not matter as decent datum scientist can easily handle and automate anything  usually  people be indeed quite indifferent to the format  as long as there be an easy way in which PRON can transfer PRON to PRON favorite format   csv be a very common format  since most tool can load PRON   note that there be some dataset whose csv representation be inconvenient  eg  the text have many comma  datum set in which the type be importnat yet hard to deduce    as for the storage of the format most people be even more indifferent to the source too  web interface be the common option   if PRON use a relational database PRON gain few advantage   the dataset be structure so PRON be protect from wrong structuring problem   the dataset can be naturally update  eg  PRON can keep append record to PRON on a daily basis    PRON can allow the user load only part of the dataset  which be convenient with large dataset  eg  give PRON just america user from the last month    in case that PRON be willing to enable that  PRON can let the user work on PRON database directly  eg  as google big query host github datum   to summarize  the location of the information be not important   unless PRON need one of the advantage provide by a database  use a csv   need a database   online datasource like amazon redshift  some other rdbms available online  eg a dedicated postgre installation   big query  the raw data be enough  csv file available on s3  csv file available for download in a web interface  dumps into google sheet  general true  do not matter as decent datum scientist can easily handle and automate anything  anything reasonable but yet  PRON be nice that PRON be look for the most convenient format   
__label__neural-networks __label__research __label__recurrent-neural-networks __label__lstm as PRON be learn about lstm  but also about neural network in general  PRON be try to find some exist research on how to select the number of hidden layer and the size of these   be there an article where this problem be be investigate  ie  how many memory cell should one use  PRON assume PRON totaly depend on the application and in which context the model be be use  but what do the research say   welcome to stackexchange   PRON question be quite broad  but here be some tip   for feed forward network  see this question   doug s answer have  work for PRON  there be one additional rule of thumb that help for  supervised learning problem  the upper bind on the number of hide  neuron that will not result in over  fitting be     nh  fracns    alpha   ni  no   ni  number of input neuron    no  number of output neuron    ns  number of sample in training datum set    alpha  an  arbitrary scaling factor usually 2  10   other recommend  set  alpha to a value between 5 and 10  but PRON find a value of 2  will often work without overfitt  as explain by this excellent  nn design text  PRON  want to limit the number of free parameter in PRON model  PRON  degree or number of  nonzero weight  to a small portion of the degree of freedom in PRON  datum  the degree of freedom in PRON data be the number sample   degree of freedom  dimension  in each sample or  ns   ni  no   assume PRON be all independent   so  alpha be a way to indicate  how general PRON want PRON model to be  or how much PRON want to prevent  overfitt   for an automate procedure PRON would start with an alpha of 2  twice as  many degree of freedom in PRON training datum as PRON model  and work  PRON way up to 10 if the error for training datum be significantly  small than for the cross  validation datum set   and specificely on lstm s  PRON may want to check out this   but the main point  there be no rule of thumb for the amount of hidden node PRON should use  PRON be something PRON have to figure out case  specifically by trial and error  select the number of hidden layer and number of memory cell in lstm be always depend on  application domain and context where PRON want to apply this lstm   for hidden layers   the introduction of hide layers  make PRON possible for the network to exhibit non  linear behaviour   the optimal number of hide unit could easily be small than the number of input  there be no rule like multiply the number of input with n  if PRON have a lot of training example  PRON can use multiple hidden unit  but sometimes just 2 hide unit work best with little datum  usually people use one hidden layer for simple task  but nowadays research in deep neural network architecture show that many hidden layer can be fruitful for difficult object  handwritten character  and face recognition problem   PRON assume PRON totally depend on the application and in which context the model be be use  
__label__fourier-transform __label__signal-processing there be example for fast numerical inversion of the laplace transform  for example here   httpwwwmathworkscommatlabcentralfileexchange32824numericalinversionoflaplacetransformsinmatlab  PRON question be  be there implementation of fast laplace transform   for example if one take and array of value   a1 ldots an   and transform then to    b1 ldot bn     
__label__neural-network __label__deep-learning __label__cnn PRON have search online  but be still not satisfied with answer like this and this   PRON intuition be that fully connect layer be completely linear  that mean no matter how many fc layer be use  the expressiveness be always limit to linear combination of previous layer  but mathematically  one fc layer should already be able to learn the weight to produce the exactly the same behavior  then why do PRON need more  do PRON miss something here   there be a nonlinear activation function inbetween these fully connected layer  thus the result function be not simply a linear combination of the node in the previous layer  
__label__matlab __label__fourier-analysis __label__discretization as state in the title  PRON need to make a discrete 3d convolution of two matrix  value function  or  actually  PRON need to convolve a matrix  value function with a vector  value function  that is  PRON need to compute the discrete version of   f ast xx for some finite set of  x s  where  fmathbbr3tomathbbr3 time 3 and  xmathbbr3tomathbbr3 oh  and PRON be use matlab   PRON question be  how do PRON do this in the fast way  PRON be currently use four nest loop  which  needless to say  be quite slow  be there any other way   PRON have think about do PRON as a multiplication in fourier  space instead  but be there a risk of loose some information or precision this way  give that  f be actually pretty ugly  an odd  slowly decay function  with a hole cut in the center  and  x be random  as in  be actually a stochastic process    PRON would be pretty surprised if use fourier technique do not help  moreover  someone have already write a matlab code for 3d fast convolution use fft  that author test PRON and find that   good usage recommendation   in 1d  this function be fast than conv for na  nb  gt  1000   in 2d  this function be fast than conv2 for na  nb  gt  20   in 3d  this function be fast than convn for na  nb  gt  5  where na  nb be the length of the input vector a  b at the very least  PRON can test that output against PRON program to compare the two  without worry of waste time write a fourier  base method PRON and have PRON work poorly   that say  PRON may be right to be cautious about use the fourier transform of PRON function  how be these function give to PRON  if PRON be define at a discrete set of point to begin with  PRON need not worry  the discrete fourier transform be invertible after all   however  if PRON be give some analytic expression that define PRON in all of 3space  for example   x be a brownian sheet  then PRON may be in trouble  in that case  sample PRON at a finite number of point and use the discrete fourier transform may either discard high  frequency component of the field or alia PRON to low frequency  lack of smoothness be what hinder fourier analysis   finally  if these concern do prejudice PRON against use the fft  PRON could try use matlab s parallel programming toolbox to speed up PRON nest loop  PRON be also worth consider the order in which PRON perform those loop  matlab store array in column  major order  so PRON be much fast to access ai  than PRON be to access ai     nest PRON loop one way as oppose to the other could dramatically slow down a program because PRON have to take big skip around in memory rather than read entry consecutively  
__label__algorithm __label__game-ai __label__prediction __label__game-theory __label__path-planning so  PRON be try to create an ai to handle the construction layout of a real time strategy game like age of empires ii  the process have too many step to be handle effectively by brute force  but also have enough structural requirement that PRON also can not just be do randomly   assume a limited area to work with  which can be represent by tile  the ai must be able to place several structure within the area  a layout once fully create can be give a score base on the pathable distance between certain structure as well as a few other factor  if path between certain structure become completely block  the ai have fail   the start area that be define by tile be also random in nature  contain a few predefined element that the ai can not control  which include randomly place terrain and resource node   PRON know this problem case have be quite vague  but the actual question relate to the type of ai that would best fit solve this issue  PRON currently have an algorithm that run through what PRON believe be the good few result in a series of step try to create the most optimal layout  but PRON fail with many starting layout  which require manual adjustment to the ai before PRON can process PRON properly  even then  PRON be just an approximation at good  since PRON start out assume that the step PRON give PRON to check contain the most optimal layout   here be an example of what a finished layout may look like  PRON do not state which color be which object  but PRON may help in determine what type of ai PRON should be look at use   because the op be a bit longer  PRON make a small mindmap to illustrate the topic and a possible answer  at first  the problem be call  walling  and be describe in the literature in the context of the  starcraft ai  challenge  the question be  where to place building to win the game  a possible answer to the problem have to do with formalization of knowledge  usually  the human player be aware what a good layout be  and PRON also know where to place PRON unit  the question be only how to express this knowledge in sourcecode   in PRON opinion the answer be call  computer base training   cbt be a technology  not with the aim to program an ai  but to formalize knowledge in a game for train human player  the idea be  that a human player play the walling game  and while PRON be do so PRON learn to take the right decision  from the standpoint of ai PRON be important  that every cbt game have an evaluation algorithm which be often call a tutorial mode  this be do in case  for example  in case 1 the player learn how to use a give terrain  in case 2 how to protect the structure and so forth  so PRON recommendation be  at first create a computer  base training game as a tutorial for human  player  and use the formalized knowledge in step 2 for build the ai for  starcraft ai wall   
__label__machine-learning __label__python __label__deep-learning __label__tensorflow __label__decision-trees PRON be try to implement decision tree classifier to classify PRON data set  PRON be use python  now PRON be easy to implement in scikit learn  but how can PRON implement this in tensorflow   basically PRON guess tensorflow do not support decision tree  PRON quote from here  this be a big oversimplification  but there be essentially two type of machine learning library available today  deep learning  cnn  rnn  fully connect net  linear model  and everything else  svm  gbm  random forests  naive bayes  k  nn  etc   the reason for this be that deep learning be much more computationally intensive than other more traditional training method  and therefore require intense specialization of the library  eg  use a gpu and distribute capability   if PRON be use python and be look for a package with the great breadth of algorithm  try scikit  learn  in reality  if PRON want to use deep learning and more traditional method PRON will need to use more than one library  there be no  complete  package   PRON can see from here that there be other learn algorithm implement in tensorflow which be not deep model   PRON can take a look at here for track algorithm implement in tensorflow  
__label__discretization __label__finite-volume __label__poisson PRON have be try to debug this error the last few day PRON wonder if anybody have advice on how to proceed   PRON be solve the poisson equation for a step charge distribution  a common problem in electrostatic  semiconductor physics  on a non  uniform finite volume mesh where the unknown be define on cell centre and the flux on the cell face      0   phixx  rhox      the charge profile  the source term  be give by      rhox  begincas   1amp  textif  1 leq x leq 0  1amp  textif  0 leq x leq 1  0    amp  textotherwise   endcases      and the boundary condition be      phixl0   fracpartialphipartial xbiggxr0     and the domain be   1010  PRON be use code develop to solve the advection  diffusion  reaction equation  PRON have write PRON see PRON note here  httpdanieljfarrellgithubiofvm   the advection  diffusion  reaction equation be a more general case of the poisson equation  indeed the poisson equation can be recover by set the advection velocity to zero and remove the transient term   the code have be test against a number of situation for uniform  nonuniform and random grid and always produce a reasonable solution  httpdanieljfarrellgithubiofvmexampleshtml  for the advection  diffusion  reaction equation   to show where the code break down PRON have make the follow example  PRON setup a uniform mesh of 20 cell and then make PRON nonuniform by remove a single cell  in the left figure PRON have remove cell  omega8  and in the right  omega9  have be remove  the 9th cell cover the region where the source term  ie the charge  change sign  the bug appear when the grid be nonuniform in a region where the reaction term change sign  as PRON can see below   any idea what could possibility be cause this issue  let PRON know if more information regard the discretisation would be helpful  PRON do not want to pack too much detail into this question    just as an aside  PRON github documentation be fantastic   this be just a guess from dg method  which can have similar issue if numerical flux be not choose carefully  PRON figure fv method be a subset of dg method   if PRON be use interpolation from cell center to define PRON flux  then this should be equivalent to use the average as a numerical flux in dg and use piecewise constant basis  for standard dg method for poisson  this lead to numerically non  unique solution  PRON can get a non  trivial null space for the discrete operator  which PRON think be what be cause PRON issue in the 2nd example   see this dg paper for PRON theory on PRON from the dg side   PRON will try to mock up an example for fv which show how this come into play   edit  so here be a small example of what be go on  consider cell 1  9 and 11  20 in which  rhox   0  from the right side  11  20   PRON have  fx20    0  due to the neumann condition  which tell PRON from conservation for that cell that  fx19    ldot  fx11    0 since the flux be the average of cell value  this tell PRON that  phix be constant over all these cell   from the left side  1  9   PRON have  fxi1fxi   0 if  f10   0  and PRON use ghost cell  then  f10   phi95   phirm ghost   phi95 conservation over the next few cell give that  fxi   f10   phi95  ie constant slope   however  note that this can be any slope  just constant   the issue come about in the middle cell   like jan mention  PRON undersample the forcing in the second mesh  this throw off the balance equation at that point  give PRON an error in  f10  which then propagate backwards and mess up both the slope in the left half of the domain as well as the value of  phi95  this sensitivity to error in force be what be problematic  unlike fem or fd method which explicitly enforce the dirchlet condition at  x10   fv enforce PRON weakly use ghost node  intuitively  ghost node weak imposition be like set a neumann condition at PRON left boundary as well  if PRON have two neumann condition for a diffusion problem  PRON problem be ill pose and have a nonunique solution  PRON can add any constant to that problem and still have a solution    PRON do not quite get that at the discrete level here  but PRON do get very sensitive and mesh  dependent behavior as PRON see in PRON experiment   the first thing to notice be PRON boundary condition  since PRON can change the slope and the value  PRON have neither dirichlet  nor neumann condition   then  every straight line be a solution where the right hand side be zero  PRON get that part   PRON flux be probably dependent on  h do PRON use the correct  h where PRON eliminate a cell  
__label__density-functional-theory theoretically  how do the time to do a density functional theory  dft  calculation scale with the number of electron  PRON be interested in  typical  dft implementation such as vasp  abinit  etc   not on  code   the simple correct answer be that dft be in  one3  this come from the idea that PRON be ultimately diagonalize a hamiltonian with dimension proportional to the number of election and diagonalization be technically  on3  in reality  dft be a bunch of step and different step be rate  limiting in different context   if PRON restrict PRON to plane  wave  pw  dft  vasp  abinit  qe  and other   PRON can make some strong statement   an important idea to understand for pw dft code be that the hamiltonian be never store as a big matrix  instead  the action of the hamiltonian operator be compute and use in what be generally  in house  iterative diagonalizer  conjugate gradient  davidson  etc    these diagonalizer be formally  one mv  where  mv be the cost of compute the action of the hamiltonian  but give PRON role in an large self  consistent algorithm  PRON tend to perform much faster   the process of compute the action of the hamiltonian occur in a couple step   local potential in real space require an fft   onv ln nv  projection can occur in real or g  space   ona np or  ona np nv  respectively  the non  local potential be either diagonal or block diagonal   ona np or  ona np2  respectively  all of this have to happen once per electron  really  wave  function   so add a factor of  ne to all of PRON   through some mean  gram  schmidt  for example  the wave  function  eigenfunction of the hamiltonian  must be keep orthogonal to one another   one2 nv  finally  the wave  function need to be compose into an electron  density   in pw code  this be accomplish with one last fft per wave  function  and a sum    one nv ln nv  note that PRON have put in a few different  n s   nv be relate to the volume  really  PRON be the basis size    np be the number of projector per atom   na be the number of atom  and  ne the number of electron   formally  nv   na  and  ne be all linearly relate to one another   np be a small integer   but PRON could imagine increase the volume with fix number of electron  add vacuum in slab  wire geometry  or increase the number of projector with fix number of atom and electron  use a more accurate pseudo  potential    PRON be common that problem be fft  limit  in which case PRON be effectively  on2 ln n  which be a somewhat common answer in the literature  if not technically correct  
__label__finite-volume __label__numerical-analysis __label__advection-diffusion __label__advice PRON wish to solve an equation of the form      fracpartialpartial t  left  fracpartial phipartial x  right   fracpartialpartial xmathcalf       for the variable  phi  eg mass    on the right hand side be the flux  mathcalf of quantity  phi  this equation  look like  an advection  diffusion equation  but with the rate of change of the spatial derivative of  phi appear on the left hand side   apply the finite volume approach PRON integrate the equation over the cell  omega      fracpartialpartial tintomegaleft  fracpartial phipartial x  rightdx   intomegafracpartialmathcalfpartial x  dx        fracpartialpartial tleft  fracphij12hj   fracphij12hjright   left  fracmathcalfj12hj   fracmathcalfj12hjright      where  hj width of the cell   be this basic approach correct  PRON have never need to solve an equation which be the time  derivative of a spatial derivative before  this be the approach PRON have take  do anyone have any advice or direction  PRON have not yet try to implement this numerically   what PRON describe below be a version of the method of characteristic  PRON application be limit  but if PRON work in PRON case  PRON would be a fairly simple  fast  and accurate process  if PRON want to discretize on a mesh  PRON may not be that helpful   assume PRON solution  phi be smooth  PRON can reduce this to a family of ode problem  exchange the order of derivative on the left first  obtain  begingather   fracpartialpartial xfracpartialpartial t  phi  fracpartialpartial xfracpartialpartial x  phi   endgather   now PRON define  psi  partialx phi and solve  begingather   fracpartialpartial t  psi  fracpartialpartial x  psi   qquadtextorqquad  partialtpartialxpsi0   endgather   which be a first order hyperbolic conservation law and linear  thus   begingather   psit  x   psi0x  t    endgather   that is   psi be already determine by PRON initial value   finally  observe that by construction  psi  partialx phi  PRON compute  boundary condition permit    begingather   phi  int psit  x  dx   endgather   the equation be   partialt  psi  partialx  fphi  where  psi  partialx phi  the time  integration can be do  eg  by explicit time  stepping    psij1i  psiji  fractau2 dx   fji1fji1   which require that  f be know at each grid node  i at the  old  time slice  j  this can be achieve by add a numerical  inversion  operator   phixiintx0xi  psix use each time before evaluate the function  f  if  psi be know at each grid point  i at the old time  slice  j then the integral can be evaluate  eg  by gaussian quadrature  so  phi can be find at each grid point  i  therefore  f can be find at each grid point as well  then the time  step can be carry out and  psij1 be find  this method would work for implicit time  stepping as well  
__label__scikit-learn __label__classifier PRON have a classifier with a heavily imbalanced dataset  1000 of each negative label for each positive    PRON be run a gradientboostingclassifier with moderate success  auc 75  but the curve have this strange look   any good idea on what would because the curve to have this behaviour   PRON think there be some predictor q for one or a few of PRON positive example that also apply to a lot of negative example  because PRON have so few positive example  there be not much to separate the good from the mediocre predictor for PRON  when PRON get to the validation set  q must have apply to a great proportion of negative example than PRON do in the training set   to mitigate this  try n  fold cross  validation   davis and goadrich have explain the relationship between roc and pr curves in PRON paper  PRON be always recommend to use pr curve over the roc curve in the presence of highly imbalanced datum   back to the behavior of PRON roc curve  PRON seem that PRON do not have more threshold point  PRON would also agree with dan and do k  fold cv   davis  j and goadrich  m  2006  june  the relationship between  precision  recall and roc curve  in proceeding of the 23rd  international conference on machine learning  pp  233  240   acm  
__label__machine-learning __label__neural-network __label__statistics PRON hope this be the right forum bcoz PRON could not find one which seem exactly relevant  question be  be any of the follow topic from statistics really useful from point of view of be applicable while work with machine learning  ai and neural networks   adescriptive statistic  analysis of quantitative data    measure of central tendancy   measure of dispersion   moments   skewness and kurtosis  correlation of bivariate datum    fitting of curve   correlation coefficient   rank correlation   intra  class corelation  regression and multiple correlations   linear regression   plane of regression   multiple correlation   partial correlation  theory of attributes   classification of attributes   independence of attributes   association of attributes  bprobability theory  basic concepts in probability   introduction to probability   different approaches to probability theory   laws of probability   bayes  theorem  random variables and expectation   random variables   bivariate discrete random variables   bivariate continuous random variables   mathematical expectation  discrete probability distributions   binomial distributions   poisson distributions   discrete uniform and hypergeometric distributions   geometric and negative binomial distributions  continuous probability distributions   normal distributions   area property of normal distributions   continuous uniform and exponential distributions   gamma and beta distributions  cstatistical inference  sampling distributions   introduction to sampling distributions   sampling distributions of statistics   standard sampling distributions  estimation   introduction to estimation   point estimation   interval estimation for one population   interval estimation for two populations  testing of hypothesis   concepts of testing of hypothesis   large sample tests   small sample tests   chi  square and f  tests  non  parametric tests   one  sample tests   two  sample tests   k  sample tests   analysis of frequencies  dstatistical techniques  sampling designs   introduction to sample surveys   simple random sampling   stratified random sampling   other sampling schemes  analysis of variance   introduction   one  way analysis of variance   two  way analysis of variance   two  way analysis of variance with m observation per call  design of experiments   completely randomized design   randomized block design   latin square design   factorial experiments  random numbers generation and simulation techniques   random numbers generation for discrete variables   random numbers generation for continuous variables   simulation techniques   applications of simulation  eindustrial statistics  i  process control   introduction to statistical quality control   control charts for variables   control charts for attributes   control charts for defects  product control   acceptance sampling plans   rectifying sampling plans   single sampling plans   double sampling plans  decision and game theory   introduction to decision theory   decision make process   two  person zero  sum games with saddle point   two  person zero  sum games without saddle point  reliability theory   introduction to reliability   reliability evaluation of simple system   reliability evaluation of k  out  of  n and standby system   reliability evaluation of complex system  findustrial statistics  ii  optimisation techniques PRON   introduction to operations research   linear programming problems   simplex method   transportation problems  optimisation techniques ii   assignment problems   queue theory   sequencing problems   inventory models  regression modelling   simple linear regression   statistical inference in simple linear regression   multiple linear regression   selection of variables and testing model assumptions  time series modelling   trend component analysis   seasonal component analysis   stationary processes   time series models  edit  in reply to smallchess  honestly  both these area be new to PRON but PRON find PRON really interesting  ultimately PRON wish to work in physics and be able to apply PRON there  example  follow link   link1  link2  link3  perhaps this be still quite broad  but PRON have not really put PRON finger on anything specific as yet  good way PRON can narrow PRON down be probably through above link and follow excerpt from PRON    machine learning be use in physics just as PRON be use in other field of science or in industry  ie when deal with complex problem andor lot of datum  have computer share the hard thinking part    nearly all of what PRON would call physics consist of fairly simple model  and the urge to construct and understand simple model drive much of what PRON do  major exception to this  observational astronomy and high energy experiment like the large hadron collider at cern where machine learning can be effectively apply  another promising application be in turbulence modeling   PRON hope this help to narrow PRON down a bit  thank   PRON agree emre  to PRON  PRON be all important to machine learning  of course  PRON depend what exactly PRON want to do in machine learning  for instance  if PRON be just do image recognition PRON do not need to understand time series   another example   decision and game theory  be absolutely critical for machine learn in board game like chess  eg reinforcement self  learn    everything PRON list here apply to machine learning  machine learning be really just statistical algorithm for learn an unknown function   why do not PRON tell PRON what PRON want to do  and PRON tell PRON what PRON should learn   PRON would give PRON the top five PRON think most important in PRON list  subjective    normal distribution  linear regression  random variables  mathematical expectation  binomial distributions  PRON agree with smallchess  but PRON have something to addsince PRON can not comment yet   two concept would help PRON to understand most of the learning algorithms   gradient descent  monte carlo tree search  these two concept help PRON understand most other concept  hope PRON help  
__label__finite-difference __label__maple  displaystyle fracpartial upartial talphax  tcdot fracpartial2 upartial x2bx  t   ux0fx initial condition   ux0t0  2nd type boundary condition   ux1t0  2nd type boundary condition  there be PRON code in maple for the 1st type bc and PRON really do not know  what PRON should change in this  code to have a solution for PRON problem with 2nd type bc  what should PRON change in the loop  boundary cond   where do PRON have to use central finite difference for boundary condition   what will be change in the explicit scheme  for 2nd type bc    PRON code for explicit method for 1d heat equation with 1st type bc be below   code httpi069radikalru140502b40b08e45d3bjpg  PRON will be very grateful  if someone help PRON   PRON want to point out that PRON equation probably do not have a unique solution in 2nd type bc because if  u be a solution then  uc be also a solution for PRON equation where  c be an arbitrary constant   numerically  the 2nd type bc usually be approximate by euler s method  for instance  in PRON problem  PRON can set  unm  un1m where  m be time step and  n1  be partition point at boundary of  x PRON can modify PRON code PRON this method  
__label__machine-learning __label__algorithms PRON have a data set of 36 k process each with 7 feature  each set represent a task and PRON know how long PRON take to complete each of the task  PRON want to build a model which will be able to predict the duration of the future task   PRON have only learn about decision tree  dt  and try to apply PRON to PRON problem  the result accuracy score be 003  PRON believe dt be not suitable because the time be continuous and dt be mean for categorization   which algorithm be suitable for duration prediction   PRON environment  python with sklearn  if that matter   PRON may want to look into scikit s decisiontreeregressor   class  a decision tree regressor will predict a real number  the decision tree classifier  by contrast  will predict a discrete class for an observation  a more advanced version would be the randomforestregressor   class  which build a random forest of regression tree  a third option to consider may be a gradientboostedregressor     PRON would also recommend this flow chart from scikit that can help PRON choose an estimator   PRON be face a regression problem  PRON aim be to predict the value of a continuous variable give the value of a set of input variable  these input variable can be of any type  number  category  etc   a decision tree be usually apply to classification problem  in which PRON be aim at predict a discrete value   there be several regression method in sklearn that PRON could use  the simple one  and maybe the one PRON should start from  be linear model  httpscikitlearnorgstablemoduleslinearmodelhtml   notice that PRON may need to transform PRON input datum  for instance  if one of the feature be categorical  PRON may need to transform PRON into a set of binary variable  other process  like datum normalisation  may also be advisable in order to get good result   edit  as state in the comment  decision trees can also be apply to regression problem  however  and in PRON own experience  the output curve that PRON obtain by mean of this algorithm usually have a step  wise shape that may affect the final bias  see  for instance the example in the scikit  learn doc    PRON would suggest not to constraint PRON and try different type of algorithm   what other have say be accurate  that PRON need to build a regression model of some sort  depend on the scale of the duration of PRON task  PRON will have to model PRON slightly differently   in fact  there be an entire class of model that try to predict duration  these be call survival model  here be a python library on survival analysis   but these model be fairly academic  a typical hack be to use gamma  poison  or log  normal regression which work out nicely because these model predict non  negative value  
__label__numerical-analysis to get a numerical evaluation of the first  k  and second  e  complete elliptic integral     kkint0  1fracdt1t2121k2t212         ekint0  1frac1k2t2121t212dt  in a left neighbourhood of the point  k1 what numerical method do PRON recommend to get a  good  approximation of k and e in a left neighbourhood of the point  k1    a truncate power series about  k1  be one way  PRON can find several form at functionswolframcom for  kk and  ek the number of term use should allow PRON to estimate PRON error for a give neighborhood size   matlab s ellipke use a simple arithmetic – geometric mean method  see abramowitz  amp  stegun  to find value for any  k  the scipy package for python have ellipkm1  the complete elliptic integral of the first kind around m1   
__label__linear-algebra __label__sparse __label__eigenvalues __label__eigensystem __label__mpi in advance PRON be sorry for PRON noobish question  PRON be a physics phd student  and basically PRON use python for PRON math  physics problem   but now PRON have a problem which require more computing capacity and  PRON intend to use fortran or c and supercomputer   those thing be very new to PRON    so PRON problem be   PRON have a system of 17576 atom and real symmetric weighted hessian  m  atrix which describe interatomic interaction and have 52728x527282780241984 element   this matrix be very sparse  156  of element be nonzero  the minimum and maximum of eigenvalue and density of state be know  PRON need to diagonalise this matrix and find all eigenvalue and eigenvector  with high precision  PRON hear that there be feast and slepc  however slepc be not  suitable for whole eigenspectrum calculation  be PRON ok to use feast  for this kind of problem or PRON should use something different   in feast PRON can define search interval and in documentation PRON be write  that high accuracy can be obtain only for up to 1000 eigenpair   so PRON idea be to calculate whole eigenspectrum by splitting this problem  into multiple interval with less than 1000 eigenvalue   or PRON intuition be wrong about how should PRON approach this problem   so in summary PRON question be   be PRON ok to use feast for this kind of problem or PRON should use something different   any other recommendation be very acceptable  if feast be suitable for this problem  how PRON scale and how should PRON distribute resource   also suggestion about what should PRON read or learn be very appreciate   because PRON do not know from where to start and PRON do not have anyone to consult  thank for PRON attention  lukas  
__label__neural-network __label__deep-learning __label__hyperparameter __label__regularization __label__dropout take the follow case of a hyperparameter and prediction error   imagine that the hyperparameter be a l2 penalty or a dropout rate  something that PRON think that should have a single sweet spot  too high and PRON be underfit and too low and PRON be overfit   PRON keep get nonconvex plot like the one above when do cross  validation   PRON guess this just point to a lot of noise during training  PRON have get a lot of variable for a modest sample size  and PRON need to regularize heavily to get a good model   but still PRON be a little bit unsure whether this sort of thing may point to a bug in some aspect of PRON implementation   have anyone come across this sort of thing before   and do PRON just shrug off the nonconvexity and go with the model that minimize the prediction error   if so  that beg the question  why not just compute a prediction error at each update during training  save any set of weight that minimize prediction error  even if the model be nowhere near converge   basically let the noise work in PRON favor   this seem appeal  because sometimes PRON get really low prediction error early on  only to have PRON evaporate as the loss function decline   this seem horribly unprincipled  but PRON ask PRON  why should PRON care if PRON be    and  be PRON unprincipled anyway    the underlying true performance be likely convex or at least likely only have one minimum  but PRON do not know the true underlying performance  PRON only get a stochastic sample with a tiny sample size because PRON apply cross validation  the performance be a random variable where PRON be interested in the expect value  but PRON only get k sample where k be the number of fold  PRON think if PRON would increas k PRON curve would look more smooth  that say  even if PRON hypothesis count in general  for this specific set there may be some weird data point that make random setting perform bad than expect still   the problem with PRON suggest approach be that PRON be treat this noise as signal  if PRON have a normally distribute random variable  PRON sample 1000 time from PRON and take the max value  this will be much high than the true expect value  an alternative explanation could be that PRON be not noise  but that PRON be actually overfitt  then PRON approach make more sense  and if the sample surround PRON during training be also relatively low then this be actually a good strategy  PRON be call early stopping  a different form of regularization and commonly use in neural networks   edit  PRON think PRON be right that PRON do not explain that analogy very well  PRON be try to optimize a function gx  where x be PRON hyperparameter and gx  be a random variable function  let PRON say PRON have a true underlying value fx  and additive noise  epsilon if  epsilon be gaussian distribute with a high variance compare to the underlying value  if PRON take the max without look at anything else PRON could take one that have lucky noise  PRON think what PRON be see in PRON graph be relatively high variance due to low sample size of PRON datum  which mean small validation set in PRON fold  whih will lead to non  convex behaviour  that say  PRON explanation of early on see low validation loss do imply overfitt which early stopping can help with  
__label__cvx PRON have the follow matrix inequality  a1a  bk    p 0  0 p a1pa1  q krk0  i need to solve this and find value for p and k  where p be a symetric matrix pn  n  and k1n  with n3  the value be give  a02523 04856 0646705290 02616 0312804415 02713 06967   b056560546009389   q002 0 00 002 00 0 002   r001  please help be solve this  
__label__machine-learning __label__classification __label__scikit-learn __label__decision-trees PRON have 2 digits number and 9 feature   PRON must pick 2 feature  so decide to plot the feature against each other to see whether PRON can get any insight on the good feature to train PRON algorithm   the plot colour indicate the two digit   algorithms PRON consider to use be  k  nearest neighbour and decision tree   PRON be very new to machine learning  PRON choose these two algorithm simply because PRON have come across PRON   feature matrix of f1 to f9 against f1 to f9  decision tree decision boundary  PRON have a few question   will choose the feature x against feature y with the least amount of overlap will help achieve the optimal decision boundary   when PRON look at feature should PRON initially consider linear datum separation   then work PRON way up to use an algorithm that can deal with non  linear separate feature point   what important visual property should PRON look out for when choose optimal feature for training   how can PRON visualise the tree in sklearn python   thank   visual property to look out for when choose optimal feature for training  choose the two feature that show the separate group the good  in the basic a good visual split be a good starting point  and yes  PRON be smart to keep in mind how the algorithm divide the space   a good strategy  PRON personally like to apply be to start of with simple learner to learn how PRON data be structer  hoe well do nn work  be there hint of local behavior  how well do naive bayes work  be the concept complex or do individual feature hold information  etc   as for select the feature  PRON could try rank PRON feature on method that compare PRON use  such as information gain   or simply write a scheme that try all combination of two on PRON two method  PRON be only 9  8 run   if the space be a little big PRON would suggest a combination of the two  PRON may also want to try combine feature  fi  pca     like other say good visual split be a good starting point   to PRON PRON seem the f8f1 be a good starting point   however PRON could get good result by transform the feature set via pca and use top eigen factorsnew combination feature  to train   PRON have not clarify whether PRON be a supervisedyou know the no  of class and actual class for some datum set   assume PRON do not and PRON be unsupervised  PRON would also try an algorithm like dbscan  PRON be generally fast   if PRON want to try the neural network approach try either sompy or neural gas 
__label__orange PRON be learn orange and PRON be look to performe some statistical test on PRON datum   the test PRON usually do be z  test  t  test  chi  square test and anova   can PRON use a widget to do these or should PRON do a python custom script   thank in advance   the box plot give a t  test for two  group comparison and anova for  2 group comparison   the sieve diagram give a chi2 test  
__label__algorithms __label__roots PRON need to find all the root of a scalar function in a give interval  the function may have discontinuity  the algorithm can have a precision of ε  eg PRON be ok if the algorithm do not find two distinct root that be close than ε    do such algorithm exist  could PRON point PRON paper about that   actually  PRON have a function to find a zero in a give interval use brent s algorithm  and a function to find a minimum in a give interval  use those two function  PRON build PRON own algorithm  but PRON be wonder if a good algorithm exist  PRON algorithm be like that   PRON start with an interval  a  b  and a function f if signfaε   ≠ signfbε    PRON know there be at least one zero between a and b  and PRON find z  zeroa  b    PRON test if z really be a zero  PRON could be a discontinuity   by look a the value of zεand zε if PRON be  PRON add PRON to the list of find zero  if faε  and fbε  both be positive  PRON search m  mina  b    if fm  still be positive  PRON search m  maxa  b   because there could be a discontinuity between a and b PRON do the opposite if faε  and fbε  be negative   now  from the point PRON find  z or m  PRON build a stack contain the zero  discontinuity  and inflection point of PRON function  after the first iteration  the stack now look like  a  z  b   PRON start again the algorithm from interval  a  z  and  z  b when  between two point a and b  the extrema have the same sign than both interval end  and there be no discontinuity at both extrema  PRON remove the interval from the stack  the algorithm end when there be no more interval   if PRON be use matlab  PRON may want to try the chebfun system  disclaimer  PRON use to be an active developer of this project   PRON can find all the root of a one  dimensional function in a closed or open interval to machine precision   the main idea behind the chebfun root  finder be to use a combination of recursive bisection and the colleague matrix  an analogue of the companion matrix  on the coefficient of an interpolant of the target function   PRON have a simplified version of the code here  the function chebroot take an anonymous function as PRON first input  the finite interval as a second and third argument  and a degree n as PRON be fourth and final argument  for reasonable result  PRON can set n to 100   in general  this be a hopeless quest without some information about the continuity andor differentiability of the function  anything could happen   consider for example the matlab function define on the interval from 0 to 1   function y  fx   y10   if  x001   y00   end  if  x0013   y00   end  if  x0753124   y00   end  treat this function as a block box  there be no way to see that PRON have zero at these three point and no other point in the interval from 0 to 1 without check every float point number between 0 and 1  
__label__matrices __label__matlab __label__linear-solver consider a vector  mathbfg  in mathbbrm and a matrix  mathbfa  equiv mathbfag   in mathcalmptime q   mathbbr  a function of  mathbfg  furthermore  let  mathbfs  in mathcalmp time r   mathbbr  independent of  mathbfg  be PRON a way to calculate  displaystyle fracpartial lmathbfa   mathbfspartial gi  where  lcdot be the linsolve matlab function   one can suppose that  displaystyle fracpartial mathbfapartial gi be know   note   x   linsolvea  b solve the linear system  ax  b use lu factorization with partial pivoting when  a be square and qr factorization with column pivot otherwise   let PRON simplify slightly  suppose the overdetermined  mtime n system  a admit the factorization  q r  a with  r full rank  let    adagger  r1  qt  be the linsolve solution operator and observe that    PRON  r1  qt q r   adagger a  differentiate the above  PRON have    0  fracpartial  adagger apartial g   fracpartial adaggerpartial g  a  adagger fracpartial apartial g      PRON can not solve for  fracpartial adaggerpartial g algebraically because PRON be not uniquely determine in this form  but PRON can compute    fracpartial adaggerpartial g  q qt   adagger fracpartial apartial g  r1  qt   r1  qt fracpartial apartial g  r1  qt   adagger fracpartial apartial g  adagger    where  qqt be the identity on the range of  a   edit  if PRON need the rest   fracpartial adaggerpartial g   i  q qt  PRON can formally differentiate the qr factorization     fracpartial adaggerpartial g   fracpartial r1partial g  qt  r1  fracpartial qtpartial g  and decompose into the part in the range of  a  which yield expression equivalent to that derive above  and in the left null space of  a     beginalign  fracpartial adaggerpartial g   i  q qt   amp left  fracpartial r1partial g  qt  r1  fracpartial qtpartial g  right   i  q qt     amp r1  fracpartial qtpartial g   i  q qt    endalign     note that only perturbation  dg that change  q can contribute to this second term  PRON can rewrite in term of  fracpartial apartial g by    beginalign   fracpartial adaggerpartial g   i  q qt   amp r1  rt  left  rt fracpartial qtpartial g   fracpartial rtpartial g  qt right   i  q qt     amp r1  rt  fracpartial atpartial g   i  q qt     amp  at a1  fracpartial atpartial g   i  a adagger   endalign  where the last line be the second term in stefano s derivation  though the qr form    fracpartial adaggerpartial g   r1  left  qt fracpartial apartial g  r1  qt  rt  fracpartial atpartial g   i  q qt  right  be preferable to compute with for numerical stability and because PRON reuse more computation and operate in a small space   PRON can apply similar argument to rank  deficient and under  determine case   x  linsolvea  b  be a convenience function of matlab that try to find a sensible numerical solution to the linear system   ax  b in the general case   ainmathbbcmtime n with  m gtreqqless n  for  numerical  full rank a  linsolve operate in such a way that the result be linear in the second argument  so PRON can assume that the solution to the above problem be define as   x  adagger b where  adagger be a function of  a alone  the matlab approximation to  adagger can be compute as  ad  linsolvea  eyesizea1      if the question at hand be to compute  fracpartial xpartial gi  the the proble reduce to compute   fracpartial adaggerpartial gi  PRON will discuss only the case of real  a  with  mgtn and  mathrmranka   n  ie an overdetermined full rank system  in this case matlab compute the least square solution  and PRON have   adagger   ata1at now  beginmultline   fracpartial adaggerpartial gi    fracpartial  ata1partial gi  at    ata1  fracpartial atpartial gi         ata1  left   fracpartial atpartial gia   at fracpartial apartial gi  right   ata1  at    ata1  fracpartial atpartial gi       adagger fracpartial apartial gi  adagger   ata1  fracpartial atpartial gi   i  aadagger    endmultline   analogous calculation can be perform in the rema case  with reference to the actual problem matlab be solve   the question ask for the derivative of the solution of  x of  aax  s0  with respect to a parameter  t  gi on which  a  but not  s  depend   differentiation of the define equation with respect to  t  denote by a dot  give  dot aax  s   adot a xadot x0   hence  dot x be the solution of the system of equation  aadot x  bdot as  axadot a x  the right  hand side  b can be compute after  x have be find  typically from  an orthogonal factorization  a  qr  but a cholesky factor  r of the normal equation matrix  ar would also do   then  dot x can be find without refactorization by solve  rrdot x  b  in case a  qr factorization be use  a slightly more numerically stable version first solve  rydot as  ax and then  rdot x  y  qdot a x  derivative with respect to all  gi can be obtain in both case more efficiently by stack the right hand side  use blas 3 routine  
__label__optimization __label__algorithms __label__linear-solver PRON have 10000 variable  each of PRON be binary   vector of positive coefficient and a matrix  a   10000times10000    if  aij1   then  ith and  jth variable can take 1 simultaneously  if PRON be 0  then PRON be not possible  the goal be to maximize PRON weighted sum  what algorithm and software could PRON use to solve this problem   beginarrayl   max fleft  x1  xm  rightsumlimitsi1m  bixi    endarray   beginarrayl   xiin left  01 right    i1  m   xixjle aij   i  j1  m   aijin left  01 right    i  j1  m   endarray   the good idea be to use an algorithm tailor to PRON particular optimization problem structure   without use any specialized algorithm tailor to this problem  PRON have an minlp  and there be no really good way of solve general large minlp   large  at the time of this writing be probably somewhere in the hundred or thousand   there be some work by glover on reformulate these problem to milp  the basic idea be to note that for  xi   xj  in 0  1  some valid inequality hold  like  beginalign   xixj   ampgeq xi   xj   1   xi    ampgeq xixj    xj    ampgeq xixj   endalign   PRON add those inequality and replace  xixj with the variable  zij  in 0  1 however  such a reformulation greatly increase the number of binary variable in PRON problem  so PRON doubt PRON be a viable option  other version of this idea vary in the size of the reformulation  but the order of magnitude of the size of each reformulation be more or less the same   the following answer from before the edit interpret the word  not possible  as  not possible to take 1    if PRON read PRON as  not possible to both take 1   then this answer do not apply   if PRON be read the question correctly  PRON seem to PRON this problem be far easy than maximum weight clique problem   PRON appear to be  om2  the give coefficient  bi be all positive  so PRON be apparent that the maximum weighted sum for a give set of coefficient would be where  xi be all 1   so PRON want PRON to be 1 unless PRON can not be   therefore  initialize all  xi to 1  iterate over all  m2  element of  aij  and if those matrix element be 0  set the correspond  xi and  xj to 0   then PRON have all the 1 s in the vector  x that PRON be allow to have by the matrix  and the weighted sum be maximize  
__label__deep-learning PRON have a function  fx1  x2   y and the goal would be a user give PRON a variable list of y s where each y be generate from the same constant  x2  parameter but  x1  varie   PRON want PRON program to give PRON good guess of  x2 PRON would like to try and train a deep neural network to do this   a few problem be that the order of y that the user can give PRON will be random and the number of y s will vary as well  PRON would hope the more y s PRON give PRON the more accurate PRON prediction of what  x2  would be be   PRON be not quite sure how to setup a model where PRON be unordered with variable length input   example   fx1  x2   x1 cdot x2  noise where  x1  and  x2  be  in mathbbr3    x2  be unknown and PRON be give lot of example where  x2  be fix but  x1  vary  PRON do not know what  x1  be but PRON observe the output  fx1  x2  PRON can train the neural network with synthesize datum  PRON would imagine if the user give more observation PRON would get a more accurate response  
__label__parallel-computing __label__c++ __label__hpc __label__libraries __label__data-visualization PRON be basically look for a good open source c ide with scientific and visualization library build in  can any one suggest few project   try ceemple httpwwwceemplecom  PRON a ide with some scientific computing package  
__label__finite-difference __label__matrix __label__advection-diffusion __label__implicit-methods objective  PRON be try to simulate the follow advection  diffusion  reaction equation in 2d space  x  y  and time     beginalign   textadr equation   fracpartial cpartial t   nablaleftv  c  dnablac  right  alpha  c  endalign  PRON discretiz the above adr equation in 2d use finite  difference implicit scheme and as a result PRON get the follow discretized equation     beginalign   p1cn1i  j1p2cn1i1jp3cn1i  jp4cn1i1jp5cn1i  j1   cni  j   endalign  where   p1  p2  p3  p4  p5  be constant in time   PRON want to solve this as a system of equation use  an1cn1cn  with no  flow ie  c0  outside the boundary domain  here   an1 would be a penta  diagonal  symmetric  not sure about this  and a diagonally dominant matrix  PRON have derive matrix  a for  2times2    3times3  and  4times4  system  for example  below PRON can see matrix  a for  3times3  and  4times4  system  respectively   issue   PRON be not sure if the form of matrix  a PRON have derive be correct because as per PRON understanding PRON should be symmetric  however  PRON be not as per PRON derivation   owe to an unsymmetric form of the matrix  a PRON need help to efficiently form  a for  ntimes n system   would appreciate if someone could use PRON awesome numerical skill to answer these issue   this be mostly just a matter of assign an index to each vertex  if PRON have  ntime n unknown  ci  j   0leq i  jleq n1   then PRON would typically map the variable correspond to the vertex   i  j to something like an index  ii  j   i  n j so each variable  ci  j that be index by a pair of number   i  j be equivalent to the variable  cii  j index by a single number   with this mapping  PRON rewrite the equation as    p1 cii  j    p23  ciipm1j    p45  cii  jpm1    cdot     this mean that the  ii  jth row  the   ijnth row  of the matrix  an1 have the entry  p12345 in column  ii  j   iipm1j   ii  jpm1  with this indexing method the nonzero entry will all fall into the diagonal  pm n  coefficient  p4    p5     pm1    p2    p3    and  0    p1    the main thing to bear in mind  when construct such a matrix know PRON diagonal element  be that the diagonal element for non  existent boundary variable such as  ii  n   ii1   i1j   in  j should be zero out   so for a  3time 3  grid  PRON would get the matrix  PRON drop the minus sign here      beginpmatrix   p1  amp  p2  amp  0  amp  p4  amp  0  amp  0  amp  0  amp  0  amp  0   p3  amp  p1  amp  p2  amp  0  amp  p4  amp  0  amp  0  amp  0  amp  0   0  amp  p3  amp  p1  amp  0  amp  0  amp  p4  amp  0  amp  0  amp  0   p5  amp  0  amp  0  amp  p1  amp  p2  amp  0  amp  p4  amp  0  amp  0   0  amp  p5  amp  0  amp  p3  amp  p1  amp  p2  amp  0  amp  p4  amp  0   0  amp  0  amp  p5  amp  0  amp  p3  amp  p1  amp  0  amp  0  amp  p4   0  amp  0  amp  0  amp  p5  amp  0  amp  0  amp  p1  amp  p2  amp  0   0  amp  0  amp  0  amp  0  amp  p5  amp  0  amp  p3  amp  p1  amp  p2   0  amp  0  amp  0  amp  0  amp  0  amp  p5  amp  0  amp  p3  amp  p1  endpmatrix    
__label__neural-network the idea of apply filter to do something like identify edge  be a pretty cool idea   for example  PRON can take an image of a 7  with some filter  PRON can end up with transform image that emphasize different characteristic of the original image  the original 7   can be experience by the network as   notice how each image have extract a different edge of the original 7   this be all great  but then  say the next layer in PRON network be a max pooling layer   PRON question be  generally  do not this seem a little bit like overkill  PRON just be very careful and deliberate with identify edge use filter  now  PRON no longer care about any of that  since PRON have blast the hell out of the pixel value  please correct PRON if PRON be wrong  but PRON go from 25 x 25 to 2 x 2  why not just go straight to max pooling then  will not PRON end up with basically the same thing   as an extension the PRON question  PRON can not help but wonder what would happen if  coincidentally  each of the 4 square all just happen to have a pixel with the same max value  surely this be not a rare case  right  suddenly all PRON training image look the exact same   PRON can not go directly from input layer to max pool because of the convolution layer in between  the reason for convolution be to extract feature  max pool down  sample the feature that have be extract  if PRON think there be feature which be miss because of the direct jump from a large matrix to a max pool layer  PRON can add more layer of convolution in between till PRON seem satisfied with a size and then do max pool onto PRON so that PRON be not an overkill   max pooling  which be a form of down  sampling be use to identify the most important feature  but average pooling and various other technique can also be use   PRON normally work with text and not image  for PRON  the value be not normally all same  but if PRON be too  PRON would not make much difference because PRON just pick the large value   a very good understanding from wiki  the intuition be that once a feature have be find  PRON exact location be not as important as PRON rough location relative to other feature  the function of the pool layer be to progressively reduce the spatial size of the representation to reduce the amount of parameter and computation in the network  and hence to also control overfitt  PRON be common to periodically insert a pool layer in  between successive conv layer in a cnn architecture  the pool operation provide a form of translation invariance   max pooling do not down  sample the image  PRON down  sample the feature  such as edge  that PRON have just extract  which mean PRON get more approximately where those edge or other feature be  often this be just what the network need for generalisation  in order to classify PRON do not need to know there be a vertical edge run from 105 to 1020  but that there be an approximately vertical edge about 13 from left edge about 23 height of the image   these rougher category of feature inherently cover more variation in the input image for very little cost  and the reduction in size of the feature map be a nice side effect too  make the network faster   for this to work well  PRON still need to extract feature to start with  which max pooling do not do  so the convolutional layer be necessary  PRON should find PRON can down  sample the original image  to 14x14  instead of use the first max  pool layer  and PRON will still get pretty reasonable accuracy  how much pool to do  and where to add those layer be yet another hyper  parameter problem when build a deep neural network  
__label__pde __label__finite-difference __label__nonlinear-equations __label__crank-nicolson PRON be try to solve numerically the follow 1d ebm    cfracpartial tx  t    partial t   fracpartial   partial xleft  d1x2fracpartial tx  t    partial x  right   it   sx  t1at  where  c and  d be constant   PRON want to use the crank  nicolson method to solve PRON but PRON be unsure how to implement PRON with all of the non  linear term   what would the crank  nicolson discretization of this equation be   
__label__computational-geometry give two set of  two  dimensional  point  say   aa1a2ldot  ana and  PRON guess PRON   bb1b2ldot  bnb  and  d2i  jmid ai  bjmid 2 the  matrix   not necessarily square  of distance between PRON  PRON want to map  m  ato b  ie  with each  a point  ain a associate a correspond  bpoint  main b  such that the follow   the maximal point  to  point  di  mi distance should be minimal  and once PRON have accomplish that first  m  ito mi correspondence  each set get one point small   ato abackslashai and  bto bbackslashbmi and now  the second point  to  point correspondence make by  m should similarly minimize the maximal distance on these one  point  small set  and then iteratively  so  PRON question  what be an efficient algorithm for this    na  nbsim10  4   way too large for brute force   and PRON be not necessary that  na  nb if  nalt nb  discard any of the excess  nb  na  bpoint PRON like such that the set of all those maximal distance be minimize  that is  just do the iteration  and when PRON be do  just discard the  unused   bpoint  on the other hand  if  nblt na just discard the  a point result from the first  na  nb iteration step  ie  again minimize the set of maximal distance   at least  PRON think that be the right  discard  scheme  but please correct PRON if PRON be wrong   the  point   sorry   of all this be  as per the subject  to  morph  one bitmapp image into another  whereby each  a point denote a source image pixel  and each  bpoint denote a target image pixel  and if  nbgt na  PRON will just inconspicuously  randomly  pop  a few extra  bpixel into place during each frame of the morph   since this kind of morphing be all over the place  PRON would have think PRON could easily google algorithm  code  etc  but PRON strangely could not get google to cough PRON up  so if PRON be familiar with this kind of stuff and can just point PRON in the right direction  that would be great  too  thank   PRON may consider numerical optimal transport   PRON do not exactly fit PRON specification  but for PRON image morphing application PRON may be well suit   in the discrete setting  give PRON two set of point  a and  b  optimal transport compute an assignment matrix  m between  a and  b that have unit row sum and unit column sum  a so  call bi  stochastic matrix  and that minimize  sumi sumj mij  cij   for some cost  cij  for instance   cij  the euclidean distance between point  i and point  j  optimal transport be a general theory  that apply in differente setting  comprise continuous function and discrete object like PRON point set   there be good book on the general theory  1  and how to use PRON in practice  2    in PRON discrete case  the standard algorithm to be use be the so  call  auction algorithm   3   but PRON be very slow  thus  some accelerated counterpart be invent  4   base on the observation that add a regularization term  entropy  make PRON possible to use a much fast algorithm  sinkhorn iteration   PRON be also possible to approximate transport use sum of gaussians  5    an interesting alternative be to compute a continuous function that approximate one of the pointset  while keep the other one discrete   this  assymetric  setting be thus call  semi  discrete   and can be solve numerically use efficient algorithm  6   and PRON own one in  7    PRON be implement in PRON geogram software library  8     1  optimal transport old and new  cedric villani   2  optimal transport for applied mathematicians  fillipo santambrogio   3  httpsenwikipediaorgwikiauctionalgorithm   4  httpsarxivorgabs13060895   5  displacement interpolation use lagrangian mass transport  bonneel etal  acm transactions on graphics  siggraph asia   2011   6  a multiscale approach to optimal transport  quentin merigot  eurographics   7  a numerical algorithm for semi  discrete optimal transport in 3d  bruno lévy  m2an   8  geogram  httpaliceloriafrsoftwaregeogramdochtmlindexhtml 
__label__machine-learning __label__azure-ml PRON have create an experiment and train a model from azure document db dataset  datum in PRON documentdb keep come regularly  now PRON need an endpoint which can retrain PRON publish web service with new datum in documentdb   be there a programmatic way to do this   
__label__fluid-dynamics __label__python PRON have solve 1d shock tube problem   euler s equation   use follow step   1  define riemann problem over the domain  2  carry out local linearisation  3  base on linearisation  write eigen value and eigenvector  4  use upwinding  ie find out numerical flux and use update equation  when PRON implement above strategy in matlab  PRON work fine  but  same  problem be when cod in python  PRON show problem for some value of the left and right state   when the left and right state be alter to some new parameter  PRON work fine and get the expect result   why should this happen   be this a problem with python  or the  thinking  strategy of python and matlab be different   PRON use numpy for define array and matrix  PRON use numpylinalgsolve   function for find the vector of characteristic variable and hence the vector of flux difference  PRON use pylabplotxy  for plot    PRON cross check the program several time  PRON guess there be no problem with PRON  and PRON run well  PRON just that the graph give be not correct for some value of the left and right state   PRON guess the problem be with the pressure condition PRON give   1  condition 1  domain divide equally in 101 number of point  two half be define as  pressureleft10   pressureright01   densityleft1  densityright01  velocity0 everywhere   result  2  condition 2   pressureleft1   pressureright01   density and velocity same as above  whereas on matlab  PRON check only on 2nd case  but with the same code   which on python be also work fine   so i guess PRON question be not mature  and PRON should have do this work before ask PRON  here be high resolution picture take 501 equally spaced point on domain   PRON think the nature of graph in first case be because the method can not handle this big jump in pressure    surprisingly when pressure and density both be set to  101   PRON work well     any input on this be welcome  
__label__machine-learning __label__deep-learning __label__keras PRON be try to implement a 1 channel cnn by slightly change this article  this article   the problem be that PRON be new to kera and deep learning and PRON do not know this far why PRON be get this error   valueerror  negative dimension size cause by subtract 100 from 1 for  conv2d1convolution   op   conv2d   with input shape     170100    10010010064   obviously  PRON be a mismatch in the dimension   PRON be use this code code   from keraslayer import embedding  from keraslayer import conv2d  from kerasmodel import sequential  from keraslayer import maxpooling2d  from keraslayer import reshape  import pdb  vocabsize11123  maxsequencelength70  embeddim100  model  sequential    embed1embeddingvocabsize1embeddim  inputlength  maxsequencelength  inputshapemaxsequencelength  embeddim1    nblabels6  model  sequential    modeladdembed1   modeladdreshape1maxsequencelength  embeddim     modeladdconv2d64  strides5  kernelsize  embeddim  activationrelu   paddingvalid     modeladdmaxpooling2dmaxsequencelength511     modeladdflatten     modeladddense256  activationrelu     modeladddropout03    modeladddenselennblabel   activationsoftmax     modelcompilelosscategoricalcrossentropy    optimizerrmsprop    metricsacc     edit1   PRON update as mentionn by medium padding to same   now PRON have another problem for the next layer   valueerror  negative dimension size cause by subtract 66 from 1 for  maxpooling2d1maxpool   op   maxpool   with input shape     11464    PRON guess PRON should change the following line to solve the problem   modeladdconv2d64  strides5  kernelsize  embeddim  activationrelu   paddingvalid     instead use this code   modeladdconv2d64  strides5  kernelsize  embeddim  activationrelu   paddingsame     this will keep the dimension of PRON input  if PRON do not work  let PRON know to help PRON more  
__label__machine-learning __label__python __label__neural-network __label__scikit-learn PRON have a problem to train PRON classifier   PRON have 10 different kind of music genre  each genre with 100 song  after make an mfccs PRON have a numpy array of  1293  20   if all together with npvstack PRON have an array of  1293000  20  and another for the label   when PRON run modelfit  feature  label   PRON take a lot of time   PRON have also try with   from sklearnmanifold import tsne  xembedd  tsne  ncomponent  2fittransformfeatures   xembeddedshape  PRON have try to reduce the song from 1000 to 100 but PRON be still take a long time   any idea how PRON can classify song with array with so much datum   PRON put some code   scaler  sklearnpreprocessingstandardscaler    y  sr  librosaloadexample1    mfcc  librosafeaturemfccy  sr  sr  nmfcc20t  mfccscal  scalerfittransformmfcc   mfccscaledshape   1293  20   y  sr  librosaloadusersjosetorronterasanacondaprojectsneuralnetworksgenrespoppop00044au    mfcc2  librosafeaturemfccy  sr  sr  nmfcc20t  mfccscaled2  scalerfittransformmfcc2   mfccscaled2shape   1293  20   tmparr     tmparrappendmfccscal   tmparrappendmfccscaled2   mafcclist  npvstacktmparr   mafcclistshape   2586  20   a0  npzeroslenmfccscal    a1  nponeslenmfccscaled2    label  npconcatenatea0  a1    labelsshape   2586    thank  
__label__apache-hadoop __label__apache-pig PRON be try to calculate maximum value for different group in a relation in pig  the relation have three column patientid  featureid and featurevalue  all int    PRON group the relation base on featureid and want to calculate the max feature value of each group  her the code   grpd  group feature by featureid   dump grpd   temp  foreach grpd generate  0 as featureid  max1featurevalue  as val   PRON  give PRON  invalid scalar projection  grpd exception  PRON read on different forum that max take in a  bag  format for such function  but when PRON take the dump of grpd  PRON show PRON a bag format  here be a small part of the output from the dump    56622257956621      56632833156631262456631      56642759156641      566530217566513152656651      566627783566613098356661324245666128064566612893256661      566731257566712728156671      56693104156691     what s the issue   the issue be with column address  her the correct work code   grpd  group feature by featureid   temp  foreach grpd generate group as featureid  maxfeaturesfeaturevalue  as val  
__label__pde __label__software PRON be search for software which can solve 3d ibvp like pdetool in matlab  user  friendly gui  easy to learn how to use   what could PRON advise to PRON   ansys  comsol  openfoam be too difficult for PRON   PRON may want to consider pde2d  despite the name  PRON also apply to 3d problem   the advantage of this software be that PRON do provide a gui base interface   PRON can download and use a free trial version for an unlimited amount of time  but there be limitation to the size of the problem that PRON can solve in the free version  
__label__adaptive-mesh-refinement __label__mesh in light of this question and some stuff PRON read online PRON be wonder if large fe libs  eg  dealii  libmesh etc   that do amr keep the entire mesh or possibly a coarse version of the entire mesh on each core  also  do this limit the size of the problem that can be solve on machine with low ram per core   finally how be load balancing perform  be the entire mesh repartition after each refinement  coarsen step   deal  ii keep the entire coarse mesh on every processor  but of course PRON can not do that with the actual mesh after many refinement step  PRON be true that this somewhat limit the size of problem PRON can solve to maybe a few hundred thousand coarse mesh cell on typically  sized cluster node  however  this be plenty for most realistic case for a reasonable coarse approximation of the geometry  adaptive mesh refinement can then resolve the detail  and for the refined mesh PRON can go about as far as PRON want  PRON have do mesh with several billion cell   as far as load balancing be concern  yes  PRON do that after every refinement  coarsen step  PRON turn out that this be not an operation that require a noticeable fraction of the overall run time of program  even on large number of processor   PRON admit that PRON do not know whether the observation above be true for other library  the author of these library need to speak for PRON own product  
__label__singularity PRON be really all in the title   for those less familiar  the fermi paradox broadly speak ask the question  where be everybody   there be an equation with a lot of difficult to estimate parameter  which broadly speak come down to this  simplification of the drake equation     lot star in the universe    non  zero probability of habitable planet around each star    lot of time span   PRON seem there really should be somebody out there   there be  of course  plenty of hypothesis as to why PRON have not see  observe  detect any sign of intelligent life so far  range from  well PRON be unique deal with PRON  to  such life be so advanced and destroy everything PRON come across  so PRON be a good thing PRON do not happen    the technological singularity  also call asi  artificial super intelligence  be basically the point where an ai be able to self  improve  some think that if such ai see the light of day  PRON may self  improve and not be bind by biological constraint of the brain  therefore achieve a level of intelligence PRON can not even grasp  let alone achieve PRON    PRON certainly have PRON thought on the matter  but interested to see if there be already an hypothesis revolve around the link between the 2 out there  PRON never come across but could be   or perhaps an hypothesis as to why this can not be   for reference to those not familiar with the fermi paradox  if the technological singularity always lead to the extinction of all intelligent life  then yes   if the technological singularity always lead to intelligent life migrate into high plane of existence that be not accessible to PRON right now  then also yes   otherwise PRON be exactly the assumption of unbridled technological progress that make the fermi paradox perplexing  a post  singularity culture should have the ability to spread through the galaxy  if there be a lot of post  singularity culture some of PRON should have spread through the galaxy  and if there be enough culture that be spread through the galaxy  PRON should notice PRON  
__label__neural-network __label__gradient-descent PRON be research to implement rmsprop in a neural network project PRON be write   PRON have not find any published paper to refer for a canonical version  PRON first stumble across the idea from a coursera class present by geoffrey hinton  lecture 6 PRON think   PRON do not think the approach have ever be formally publish  despite many gradient  descent optimisation library have an option call  rmsprop   in addition  PRON search be show up a few variation of the original idea  and PRON be not clear why PRON differ  or whether there be a clear reason to use one version over another   the general idea behind rmsprop be to scale learning rate by a move average of current gradient magnitude  on each update step  the exist squared gradient be average into a running average  which be  decay  by a factor  and when the network weight param be update  the update be divide by the square root of these average squared gradient  this seem to work by stochastically  feel out  the second order derivative of the cost function   naively  PRON would implement this as follow   param    gamma geometric rate for average in  01    iota numerical stability  smooth term to prevent divide  by  zero  usually small eg 1e6   epsilon learn rate  term    w network weight   delta gradient of weight ie  fracpartial epartial w for a specific mini  batch   r rmsprop matrix of run average squared weight  initialise    r leftarrow 1   ie all matrix cell set to 1   for each mini  batch    r leftarrow  1gammar  gamma delta2   element  wise square  not matrix multiply    w  w  epsilon fracdeltasqrtr  iota  all element  wise   PRON have implement and use a version similar to this before  but that time around PRON do something different  instead of update  r with a single  delta2  from the mini  batch  ie gradient sum across the mini  batch  then square   PRON sum up each individual example gradient square from the mini  batch  read up on this again  PRON be guess that be wrong  but PRON work reasonably well  good than simple momentum  probably not a good idea though  because of all those extra element  wise square and sum need  PRON will be less efficient if not require   so now PRON be discover further variation that seem to work  PRON  call PRON rmsprop  and none seem to come with much rationale beyond  this work   for example  the python climin library seem to implement what PRON suggest above  but then suggest a further combination with momentum with the teaser  in some case  add a momentum term β be beneficial   with a partial explanation about adaptable step rate  PRON guess PRON would need to get more involved in that library before fully understand what PRON be  in another example the downhill library s rmsprop implementation combine two move average  one be the same as above  but then another  the average of gradient without square be also track  PRON be square and take away from the average of squared weight    PRON would really like to understand more about these alternative rmsprop version  where have PRON come from  where be the theory or intuition that suggest the alternative formulation  and why do these library use PRON  be there any evidence of good performance   rmsprop be indeed an unpublished method  and in the lecture geoffrey hinton give just the general idea behind rmsprop  to divide the gradient by a move average of the gradient magnitude  PRON can watch the lecture here   httpsyoutubeo3sxac4hxzu  when this principle be apply to stochastic gradient descent  the update rule PRON show be obtain  since hinton do not propose an exact algorithm  this principle have be apply to different optimization method  PRON agree PRON be confuse all these method call PRON rmsprop   climin implement rmsprop with nesterov momentum  momentum method try to avoid the oscillation that often happen with sgd by slowly change the current direction of update  the algorithm give in climin documentation introduce the  beta parameter that control how much of the previous update direction be retain  nesterov momentum be implement by first take a step towards the previous update direction  vt  calculate gradient at that position  use the gradient to obtain the new update direction  vt1  and finally update the parameter  the climin implementation also include the smoothing term  iota inside the square root for stability  1e8   even though PRON be not mention in the documentation   the implementation in the downhill library be base on the algorithm describe in the paper by a graves  PRON be not mention in the article why the square of the average gradient be subtract from the average square gradient  PRON have see this do sometimes when rmsprop be implement with a momentum method  but PRON be not sure why  
__label__neural-networks __label__machine-learning __label__ai-design __label__algorithm __label__world-knowledge PRON be try to develop an editor that can be base on notepad  the only purpose for this development be PRON want to use this for PRON coding suggestion and possibly the next input parameter that PRON be go to write  PRON have see notepad  editplus etc  and what PRON think be that this can be definitely achieve  what be the good tool or api s PRON can use  any suggestion   thank in advance   
__label__fluid-dynamics __label__boundary-conditions __label__finite-volume __label__navier-stokes __label__numerical-modelling PRON know that in outflow boundary PRON assume a zero normal gradient condition and use upwind scheme for approximation  however  PRON see this sentence in a book which PRON do not understand   convective flux be usually assume to be independent of the coordinate normal to an outflow boundary   what do PRON exactly mean and why be PRON allow to make that assumption   source  link  thank   PRON understand PRON as a neumann boundary condition   fracpartial fpartial n   0  where  f be the convective flux and  n be the unit normal vector at the boundary  this be the most convenient boundary condition to solve numerically the problem    independent of the coordinate normal to  the boundary   mean that  in the local coordinate system of the boundary define by the vector   n  t1  t2  the field  f may locally vary only along the tangential coordinate   t1  t2 hence  fracpartial fpartial n   0 
__label__python __label__data-cleaning __label__encoding PRON be try to read and process a text file that be roughly of size 6 gb  PRON be a text dump after crawl the datum   import codec  b  codecsopenfilename   r   encodingutf8    corpus  bread    below be the error that PRON be face   unicodedecodeerror   utf8  codec can not decode byte 0xac in position 1037239810  invalid start byte  
__label__finite-element PRON want to solve  beginalign   nabla4psialphanabla2psibetapsi  fx  yquad  nabla psicdot hatnnabla3psicdot hatn0quad texton boundary   endalign   with a 2d fem scheme  with the biharmonic operator  linear test function be rule out   PRON can integrate the first term by part twice  so that the weak version of the pde include a term  beginalign   intanabla2phii nabla2phijda  endalign   quadratic test function will allow PRON to evaluate the integral   PRON understand that quadratic test function have node at the vertex and at the mid  point of each side of each triangle   PRON question relate to how to deal with this at assembly   if the coordinate of the vertex of each triangle be in p  and the connectivity matrix be t  then what be the procedure to extend these for the additional node locate on the side of each triangle   this wonderful reference httpwwwcoloradoeduengineeringcascoursesdafemdafemch23dafemch23pdf  2327  six node quadratic interpolation  describe the quadratic element perfectly  but PRON can not figure out how to integrate the midway point into PRON node list   follow tylerolson s suggestion here be an octave  matlab script that generate a mesh with six point in each triangle  the three vertex and the three mid  point   if anyone know a way to simplify this inelegant code  PRON would appreciate PRON   clear    persson s routine to make simple triangle in a rectangular grid  nx4ny3    x  yndgridlinspace11nxlinspace11ny     form x and y list  pxy       t1nx2nx112nx2    t  kront  onesnx11krononessizet0nx2      t  kront  onesny11krononessizet0ny2nx    np  lengthp1nt  lengtht1       make a list of all side from t   there will be redundant entry  s  zeros3nt6    for kt1nt  for ks13  if ks1  s3kt1ks1mintkt12     s3kt1ks2maxtkt12     s3kt1ks3kt   elseif ks2  s3kt1ks1mintkt23     s3kt1ks2maxtkt23     s3kt1ks3kt   else  s3kt1ks1mintkt13      s3kt1ks2maxtkt13      s3kt1ks3kt   endif  endfor  endfor    once this have be do for all triangle  work on s to eliminate dupe  for kk1maxs1    order1finds1kksorder1      junk  order2sortsorder12     dupe  findjunk1end1junk2end     n  sorder1order2       for kl1lengthdupe   ndupeend1kl4ndupeend1kl13 add on triangle no of dupe  endfor   junk  order3mojunkuniquen2first     n  norder3      if kk1  snodupe  n   else  snodupesnodupen    endif  endfor    add to position table the position of new node  for kn1lengthsnodupe   pnpkn105psnodupekn11psnodupekn21     pnpkn205psnodupekn12psnodupekn22     endfor    build a new connectivity table  t6  t6zerosnt6    for kt1nt   tkt     for ks13  if ks1  ss  tkt1    se  tkt2    elseif ks2  ss  tkt2    se  tkt3    else  ss  tkt3    se  tkt1    endif    where be this side in snodupe table     to lookup in snodupe need end to be in order  s1minss ses2maxss se     ind1findsnodupe1s1    ind2findsnodupeind12s2    sideno  ind111ind2   t6kt2ks1ss   t6kt2ksnpsideno   endfor  endfor  here be the version that PRON have in mind  PRON be a bit terse  but there be a comment version up on github  use that one  PRON actually define the input  output and explain the logic for the various intermediate calculation   base on some initial benchmarking  PRON beat out PRON loop  branch  heavy code by about a factor of 300  matlab 2017a on a large  ish mesh   since PRON spend all of PRON time in vectorized call   here be a link to the real code   httpsgithubcomtjolsenmeshutilitiesblobmasterlineartoquadtrislineartoquadraticm  function  quadtris  points   lineartoquadratict  p   nt  sizet1    tidx   1nt     alledge   t1  t2  tidx tidx 3onesnt1    t2  t3  tidx tidx 1onesnt1    t3  t1  tidx tidx 2onesnt1     forwardedge  alledges2   gt  alledges1    alledgesforwardedges4   0   alledgesforwardedges3   0   alledgesforwardedges12    alledgesforwardedges21      uniqueedgesic   uniquealledges12     row     edges   uniqueedge   accumarrayic  alledges3sum    accumarrayic  alledges4sum     t2f  accumarrayalledges3alledges4alledges5     signalledges3   alledges4ic     sum    newpoint    pedges11pedges212    pedges12pedges222    points   p  newpoint    np  sizep1    quadtris   t  npabst2f      end  PRON suspect that PRON will be disappoint to find out that  once PRON be able to implement the quadratic shape function  the solution PRON will get be not correct  that be because for the bilaplacian operator   nabla4   PRON be not enough to have shape function that be quadratic polynomial on each cell  but PRON also need to have special kind of  hermite  shape function that have no  kink  across cell interface but be continuously differentiable at face  these be substantially more difficult to implement than the quadratic shape function PRON be look for  but necessary to produce the correct solution of the equation PRON be try to solve  
__label__acceleration __label__computational-physics __label__software PRON have acceleration datum from a sensor  x y  amp  z  PRON move the senor in the y axis  mostly in a straight line  so PRON ignore x  amp  z  from the sensor documentation  521 acceleration output ：  axaxhltlt8axl32768  16gg be gravity acceleration，98m  s2   ayayhltlt8ayl32768  16gg be gravity acceleration，98m  s2   azazhltlt8azl32768  16gg be gravity acceleration，98m  s2   the data be in  m  s2   PRON need a simple calculation that java or c  can take easily  PRON want to write something in code that calculate the acceleration over time to maximum velocity and average velocity  PRON need a  speed  value that PRON can display   for ex  max speed 12mph and average speed 8mph   in this datum the device be move from the zero point to about 6 inch away less than 1 second   times  acc x  acc y  acc z  48547  04756  00864  12207  48563  02051  02651  13350  48563  00044  06621  13140  48578  02876 10117  14292  48578  00732 15586  14653  48594  00659 18984  13447  48594  02344 24453  14043  48641  02690 32148  13677  48656  04072 30083  14995  48656  02573 32700  13545  the equation at the start look like PRON be convert sensor datum  pair of 8 bit integer value  into float point number   PRON be irrelevant to PRON problem   what PRON need to do be     vk  vk1aktk  tk1  for each time  tk and with  t0  and  v0  the initial time and velocity   to find the average velocity over the whole time PRON can do     ufrac  sum vktk  tk1sumtk  tk1  the sum on the bottom simplifie to  tn  t0 
__label__data-mining PRON be try to pull datum from PRON csv file  PRON be use this command   mydatamydatamodeldata30cmodeldatadate     as PRON can see from the result below  PRON be count all instance  however  PRON wish to set this to a 64 line ignore upon first match instance  how would PRON suggest PRON go about add this  ignore  n  line function    snippet of datum here   modeldata  date  1  8788927 19670103  2  5625603 19670104  3  4853577 19670105  4  4558040 19670106  5  4322114 19670109  6  3011257 19670110  7  6234991 19670111  8  3970446 19670112  9  3144710 19670113  11  3121524 19670117  15  3659759 19670123  314  5034324 19680401  316  4395672 19680403  320  4042018 19680410  485  3647299 19690113  750  3632671 19700203  785  4167759 19700325  809  4520325 19700429  829  4116661 19700527  1138  7950606 19710816  1139  3260332 19710817  1493  3929633 19730111  1502  3094216 19730124  1515  3728929 19730213  1570  3369889 19730503  1934  3254718 19741010  2008  3845721 19750127  2021  3039563 19750213  2714  4134147 19771110  2820  6223156 19780414  2821  7745218 19780417  2827  4743293 19780425  2828  3033731 19780426  2896  4192446 19780802  2897  4422611 19780803  2958  4189009 19781030  2960  4180385 19781101  3183  3427686 19790920  3196  4392758 19791009  3197  6126659 19791010  3259  3585480 19800109  3264  3165421 19800116  3275  3521842 19800131  3314  3699859 19800327  3468  5436180 19801105  3510  4302425 19810107  3917  3917657 19820817  3918  4391777 19820818  3920  3173933 19820820  3921  3354431 19820823  3924  3382543 19820826  3930  3257510 19820903  3953  3201376 19821007  3955  3558906 19821011  3957  3060809 19821013  3972  3596346 19821103  4414  4728832 19840802  4415  6362526 19840803  4416  3995445 19840806  4419  3081986 19840809  4420  3271267 19840810  4468  3220568 19841018  4510  3585172 19841218  4759  3046736 19851213  4775  5164241 19860108  4818  3460404 19860311  4899  3051578 19860707  4946  5638514 19860911  4947  3806834 19860912  5039  6066431 19870123  5095  3536616 19870414  5224  5730824 19871016  5225  11180750 19871019  5226  6897399 19871020  5227  4537756 19871021  5228  3229374 19871022  5582  3155970 19890317  5728  6600688 19891013  5729  6704284 19891016  5931  4177266 19900803  6046  4380562 19910117  6257  3193782 19911115  6480  3983774 19921005  6533  3139862 19921218  6572  4023227 19930216  6574  3056474 19930218  6605  3035592 19930402  6637  3505759 19930519  6819  3674848 19940204  6857  3853518 19940331  6858  3048781 19940404  7150  3038139 19950531  7184  5645942 19950719  7242  3910097 19951010  7301  3255512 19960104  7305  3574967 19960110  7346  5189287 19960308  7435  5112708 19960716  7568  3238173 19970123  7577  3007577 19970205  7760  5930298 19971027  7761  7836272 19971028  7953  3613173 19980804  7970  3899849 19980827  7972  5255132 19980831  7973  5310272 19980901  7999  3806758 19981008  8130  3407432 19990419  8265  3204466 19991028  8311  3483312 20000104  8314  3101274 20000107  8324  3273686 20000124  8361  3905785 20000316  8374  5991309 20000404  8382  4642381 20000414  8563  4287756 20010103  8635  3294676 20010418  8736  3260459 20010917  8738  3009326 20010919  8947  3005049 20020719  8950  4684101 20020724  9830  3024372 20060120  9926  3253976 20060608  10106  6153445 20070227  10108  3018874 20070301  10208  3016551 20070724  10210  5810003 20070726  10213  3048170 20070731  10214  3093512 20070801  10216  3203759 20070803  10217  3358139 20070806  10220  3897666 20070809  10225  4290941 20070816  10333  3479401 20080122  10334  4699377 20080123  10497  3575420 20080915  10498  3417303 20080916  10499  3578305 20080917  10500  4763519 20080918  10501  3362684 20080919  10507  4354753 20080929  10512  3611450 20081006  10515  3786056 20081009  10516  4791725 20081010  10517  3406449 20081013  10910  7382690 20100506  10911  3423236 20100507  10912  3022318 20100510  11225  4906076 20110804  11226  4635050 20110805  11227  6636959 20110808  11228  5464225 20110809  11229  4077466 20110810  11230  3956520 20110811  11235  3315989 20110818  11852  3357452 20140203  12029  4794625 20141015  12243  3784646 20150821  12244  6559546 20150824  12245  4181079 20150825  12246  3869772 20150826  12344  3089881 20160115  12346  3519323 20160120  12455  3676909 20160624  PRON almost have everything   take example from iris  n  lt 10  irisirissepal  lengthgt55  csepal  length    species1n   
__label__parallel-computing __label__fortran __label__mpi __label__lapack not really much more to say   of several computer available for use  these be two of the large one  one have 2x e5  2670  and the other have 4x e5  4640  the problem PRON be look at essentially boil down to large matrix diagonalisation problem  ie i have be deal with 125 k rank matrix  and in the past have look at large one  both sparse and non sparse  use parpack    all intuition would tell PRON that the quad cpu 4640 should be much faster  but PRON be not  and PRON can not work out why at all  both have hyperthread enable  both have 250 gb ram   could PRON be that the gain from the parallelisation of parpack beyond 16 core just start to diminish  and be not worth add the additional processor due to the increase in inter cpu communcation require   the code be write in fortran if that make much difference   
__label__neural-network __label__deep-learning be there any resource out there  book  blog  PRON own answer post etc   that give advise on model strategy of deep neural net   PRON know how to fit a neural net  PRON know how to change setting like number of hidden layer  activation function  dropout etc  PRON know how to use cross  validation to validate model   but what PRON need be advise on the actual modelling process  ie give a dataset  which have be clean and explore   where do PRON start  what type of neural net do PRON train first  how do PRON then tune PRON   PRON can not recommend highly enough this online book on neural network   the tricky part about neural network be the stuff PRON mention  what value to use for the learning rate  what topology to use for the network  etc etc  PRON call these thing hyperparameter to distinguish from parameter which be estimate by the optimization process   this big section of the aforementioned book cover exactly that  PRON do not know if PRON will find what PRON want though  the reason be because there be no silver bullet   there should be a distinction between the term of neural network and deep neural network  PRON find this link  httpswwwquoracomhowdoesdeeplearningworkandhowisitdifferentfromnormalneuralnetworksappliedwithsvm   unfortunately  there be no general rule and probably will never be one that say that PRON should use a certain type of network  a priori   PRON depend on the task PRON be try to solve  but PRON already know that   PRON suppose PRON be wonder what be the meaning of the additional hidden layer  how to figure out how many of PRON should be there and how large PRON should be  also  how the additional parameter affect this network and how to think through PRON model when something do not work right   again  PRON depend  there be lot of type of network that be good for additional problem  for example  lstms be good when work with series of datum  if PRON want to tune PRON lstm to work well  PRON should go into PRON and figure out how exactly do PRON work   PRON highly recommend this playground site for neural network   httpplaygroundtensorfloworg  PRON also suggest httpswwwtensorfloworgversionsr09howtosgraphvizindexhtml to inspect PRON code s architecture  additionally  try to explore the tensorflow documentation   in the end  PRON advise PRON to read  read more   httpkarpathygithubio  
__label__graphs PRON would like to test a new algorithm for link prediction on graph  PRON be interested in  both weight  unweighted and direct  undirected graph   what be some common benchmark dataset that researcher often use to test PRON algorithm  PRON know that within computer vision people often use mnist or cifar  but PRON have not find similar dataset for link prediction   a cursory google search give PRON   httpnoesisikororgdatasetslinkprediction  httpialeecsucfedutravianphp  httpkonectunikoblenzdenetworks  another way would be to create PRON own dataset by download say github user s graph  define a network by add edge  between user a and user b if a follow b and so on  then PRON can take snapshot of this graph at different time instance and see if PRON can use last t snapshot to predict the edge in the t1th snapshot  of course that would be a lot more extra effort and the link PRON post may suffice  please let PRON know if PRON help  
__label__sparse __label__c PRON be write finite  difference method program use c the stiffness matrix be symmetrical and band  for PRON storage PRON would like to use sparse diagonal storage format   could someone tell please  what solver can use diagonal storage format  intel mkl s paradiso use only crs  format  sparselib do not have  spooles seem also   thank PRON   there be surprisingly few reference to  sparse diagonal storage  format on google  here be a pdf of some slide PRON find that provide a brief overview of compressed row  jagged diagonal  and sparse diagonal format  most of the other link be pointer to a single paper  use of sparse diagonal storage in finite element analysis by  kk yalamanchili and SouthCarolina anand   so this be not good news  really  PRON suggest that sparse diagonal storage have not be adopt very widely  PRON be go to hazard a guess  the format be probably more useful for iterative method than for direct  factorization  method  implement sparse matrix  vector multiplication be relatively straightforward for any of these format  so for iterative method PRON have the freedom to store as PRON wish  at least until PRON need an incomplete factorization for a preconditioner    if PRON matrix have a relatively dense diagonal band  then PRON may want to consider one of lapack s banded solver  these store the diagonal along the row of a matrix with b row  where b be the bandwidth of the matrix   edit  or  b12 row if the matrix be symmetric  hermitian   but if PRON truly require a sparse direct solver  then PRON look like PRON will either have to implement PRON own or convert to a more common storage format   if PRON decide to implement PRON own  perhaps PRON will quickly learn why sds be not suitable p   edit  the abstract for the yalamanchili  anand paper suggest that PRON be focus on vector  matrix multiplication  ie  iterative method   found   but have not try   in intel mkl  sparse blas level 2 and level 3 routine   mkldiatrsv  triangular solver with simplified interface for a sparse matrix in the diagonal format with one  base indexing   httpsoftwareintelcomsitesproductsdocumentationhpcmklmklmanguid9fbf0f88473145dabde9af4d7a0a18a4htmguid9fbf0f88473145dabde9af4d7a0a18a4  description of the use diagonal format be in the middle of the page  httpsoftwareintelcomsitesproductsdocumentationhpcmklmklmanguid9fceb1c4670d473881d2f378013412b0htm 
__label__computational-biology __label__pdb __label__c PRON background be in genomic  but PRON have recently be work with problem relate to protein structure  PRON write a few relevant program in c  build PRON own pdb file parser from scratch in the process  PRON do not worry about make a really robust parser  PRON just know that build one PRON would be the good way to force PRON to really understand the pdb format   now that PRON have go through this process  PRON be look for something a bit more robust and mature  be there be any open  source protein structure library implement in c  PRON be able to find a few on google  but PRON have never hear of any of PRON before and PRON do not seem to be very mature or stable  a slightly relate question  be everyone really do all of these type of computation use python  or homebrew code   ps  PRON be essentially look for a library that include a pdb file parser  function for calculate bond angle  bond length  torsion angle  surface accessible surface area  etc   there be a c library which be quite mature  this be probably as close as PRON will get to c PRON PRON have not find any usable c library yet  PRON could use the c library and still write most of PRON code in c use extern c   in the c code   openbabel be c  but PRON be commonly use for general structural thing  PRON main focus be conversion between and the ability to read a wide range of format  as far as PRON know PRON do not have the ability to calculate sas area  though   if PRON be willing to go with c  then PRON would recommend esbtl   instead of deal with file in pdb format  PRON may consider download PRON from the protein data bank in pdbml format  which be actually xml  PRON can then parse pdbml file use PRON favorite xml library for c  for example  with libxml2    PRON know this be about c  but there be a great way to do this use the glgraphics library  which implement opengl in processing  a java base framework with c like syntax   opengl be basically the same regardless of what language PRON use  so java should not make too big of a performance difference  anyway  the glgraphics library come with a pdb viewer that work out of the box  PRON super easy to tweak and the processing forum have a really responsive following  PRON could be a good starting point   glgraphics library  httpsourceforgenetprojectsglgraphicsfilesglgraphics10  see glgraphics  example  pdbview  processing  httpprocessingorg  lastly  PRON may want to check out this  less 3d but still cool example  httpwwwmydisksetgn380webpagereceptor  to start with PRON  related question   PRON do pdb parse in python  even when the subsequent processing need to be do in some compile language for speed  c be simply not a good language for parse  in particular not for messy format like pdb  and that be probably why there be not any stable and mature pdb parser in c  something else PRON can consider  assume PRON goal be to work with structure from the pdb  be to use the mmcif format rather than the pdb format  mmcif be much more regular and thus easy to parse  PRON also get more parseable information from the mmcif version of the pdb entry than from the pdb version  PRON can find a c parser for mmcif on the pdb web site  PRON be call cifparse   although PRON will note that PRON development have be drop in favor of a c rewrite call cifparse  obj   PRON may also consider another  language  like cns  which be specifically design for work with molecular structure and have the function PRON mention   while this be prehap not the intend use of the software  PRON have find the parser and general protein  structure processing method find within the vmd source code to be robust and comprehensive   vmd be an open  source molecular visualisation tool  PRON be relatively staright  forward to extend when that be appropriate  otherwise code can be cannibalise from PRON   pdb file can be read and manipulate easily in c use the open  source  bsd licensed  chemkit library   as an example here be some code that will read in a pdb file for hemoglobin  pdb id  2dhb  and print out the total number of atom  number of chain  and solvent accessible surface area    include  ltiostreamgt    include  ltchemkit  polymerhgt    include  ltchemkit  polymerfilehgt    include  ltchemkit  molecularsurfacehgt   use namespace chemkit   int main      polymerfile file2dhbpdb     bool ok  fileread     ifok    stdcerr  ltlt   fail to read file    ltlt  fileerrorstring    ltlt  stdendl   return 1     polymer  protein  filepolymer     ifprotein    stdcerr  ltlt   file be empty    ltlt  stdendl   return 1     stdcout  ltlt   number of atom    ltlt  proteingtatomcount    ltlt  stdendl   stdcout  ltlt   number of chains    ltlt  proteingtchaincount    ltlt  stdendl   molecularsurface surfaceprotein    surfacesetsurfacetypemolecularsurfacesolventaccessible    stdcout  ltlt   surface area    ltlt  surfacesurfacearea    ltlt   a2   ltlt  stdendl   return 0     output   number of atom  2201  number of chains  2  surface area  14791 a2  for more information see the documentation for the class present above   polymer  polymerfile  molecularsurface 
__label__finite-difference __label__c PRON have be write some code in c for particle  in  cell simulation   one of the step of the pic algorithm require to solve  numerically  poisson s equation     delta varphi   4 pi rho      initially PRON want to limit the program to 2d case  instead of 3d  and use the finite difference method  instead of finite element    much to PRON surprise  PRON be not able to find any free open source c library for this task  ie to solve 2d poisson s equation use the finite difference method    the only thing PRON have find be a fishpack  collection of fortran77 routine  part of a famous slatec library   PRON be fine with fortran  but a call to a fortran routine from c require some unnecessary array transformation and  native  c solution seem more convenient   another reason PRON hesitate to use fishpack be that PRON use single precision float with implicit type declaration   use the gcc  fdefault  real8  flag PRON be able to convert PRON to double precision  but PRON be not sure whether or not everything still work correctly after this   so  PRON question be  be there any free  as in freedom  c  c librari to solve 2d poisson equation use the finite difference method   and since PRON do intend to switch to 3d and finite element  advice on c library for this purpose be also welcome   2d poisson by finite difference look to be a few example in petsc  ksp  ex29  ex32  ex50   PRON would probably use a nonlinear solver  snes  or time  stepper  ts  in petsc anyway for the add flexibility  2d poisson via finite difference be probably an example in other package  for instance  probably trilinos   PRON know PRON be an example give in use mpi   there be many free c  c library to solve poisson via finite element  PRON could try petsc again  deal  ii  dune  fenics  any other library in this list on wikipedia  or  probably  trilinos  use a library like deal  ii or fenic  PRON look to be relatively easy to change the dimensionality of a code  ie  go from 2d to 3d   so PRON may consider write a 2d finite element code to start  and then convert PRON to 3d 
__label__classification __label__unbalanced-classes PRON just start train a mlp model on a data set which have the follow statistic  notice that both train and validation set be unbalanced  884  negative    note  the unbalance be not cause by sample and be because that be the natural ratio of the class  eg when PRON look back in a 3 month period  PRON see PRON have eat 88 apple and 12 orange  and look forward  PRON may still favor apple more than orange by the same ratio  PRON may be wrong here but PRON think PRON should not give class weight nor resample to balance the two class  because that be PRON natural ratio   train datum   shape  feature    891473  122   shape  label    891473     of class    0  788118    1  103355     of class    0  088406266931247501    1  011593733068752503    validation datum   shape  feature    251141  122   shape  label    251141     of class    0  222009    1  29132     of class    0  08840014175303913    1  01159985824696087    PRON be use three 8sized dense layer each follow a relu layer  finally a sigmoid since PRON be binary classification   however  the training process converge after only 2 epoch and basically only learn the bias term and and end up predict everything to be negative  the confusion matrix stay unchanged like this since the 2nd epoch till the end  20 epoch    train confusion matrix   p  0  p  1  total  l  0  0884  0000  0884  l  1  0116  0000  0116    total  1000  0000  0884  validation confusion matrix   p  0  p  1  total  l  0  0884  0000  0884  l  1  0116  0000  0116    total  1000  0000  0884  PRON have try to increase the size of the dense layer  eg from 32  128  512  1024   PRON all yield the same result   PRON would expect that with 3 x 1024 neuron the model would be complex enough to learn something beyond a single bias term  so PRON be quite confused as what PRON be do wrong to make PRON not learn anything other than the bias   question   what could be the possible reason for the model to learn not much   what should PRON try andor what kind of info should PRON be look for at this point to move this forward   thank PRON for PRON time in advance   the following be a snippet of PRON code to show what PRON be try to do   learningrate  1e4  batchsize  10000  epoch  5  densesize  1024  train   trainingfeaturesvalue  traininglabelsvalue   validation   holdoutfeaturesvalue  holdoutlabelsvalue   test   evalfeaturesvalue  evallabelsvalue   evalset      train   train      validation   validation      test   test      model  sequential    modeladddensedensesize  activationrelu   inputshape  train0shape1      modeladddensedensesize  activationrelu     modeladddensedensesize  activationrelu     modeladddense1  activationsigmoid      initiate optimizer  opt  rmsproplr  learningrate    let PRON train the model  modelcompilelossbinarycrossentropy   optimizer  opt  metricsaccuracy     modelfit   train0    train1    batchsize  batchsize   epoch  epoch   validationdata  validation   shuffle  true   verbose2   callback  callback     edit   the three answer as of write all suggest that the inefficiency in learning be associate w the imbalanced class and PRON should try to balance the class by either use class weight or resampl   while PRON be happy to see an apparent agreement of what the problem be what  could anyone provide more insight about why imbalanced class because difficulty even for a complex model  in theory a complex enough model can at least memorize all the rarely positive sample and achieve a high  than  bias accuracy on the training set   though PRON know that dataset be naturally balance but PRON model will still consider PRON as unbalanced dataset  if PRON take a example financial fraud detection  number of fraudulent transaction be always less than the ligit translation  to handle such such case datum scientist will have to balance the class for good result   PRON will have to balance PRON dataset to get the good result from model   generally there be a number of way to deal with class imbalance  more detail here fore example    specifically in PRON case  PRON would either try training with the training sample evenly distribute in class or PRON would play around change the loss function  to take into account the different class  support  for the latter  PRON can check this discussion also   PRON be not a matter of model complexity  in order to address class imbalance  PRON should specify class weight as argument to the fit function in order for the logit of the cross entropy to be scale accordingly   PRON can find further info in stackoverflow and example on how to provide the weight to fit here  
__label__data-mining __label__r __label__predictive-modeling PRON want to achieve during PRON internship a credit risk scoring system base on  credit german dataset    PRON develop a model with r use logistic regression and i validate PRON  now i must use this model to calculate a new applicant score and deduce whether good or bad   PRON do not know how to proceed   first of all  PRON need to decide how PRON evaluate  good  vs  bad   which mean pick a metric  from the top of PRON head PRON have three choice   accuracy  simple  but potentially misleading  if PRON have few default and predict all 0s  PRON still get high accuracy   logloss  penalize assign a high def probability to good quality credit and vice versa  roc  sort the credit from good to bad   once PRON have make PRON pick  make sure PRON model can generate prediction in the right format  ie output probability and not just 01  and the go through the training  validation  test paradigm  this should get PRON start  
__label__machine-learning __label__python __label__classification __label__scikit-learn __label__svm PRON have about 500k row of datum  PRON be new to data science and PRON be try to train a model utilize support vector machine as part of PRON analysis  on PRON little macbook pro  PRON seem endless  right now PRON be use an rbf kernel and be not see an end to this computation   PRON do however have access to 100  compute node with 16 core a piece  PRON problem be  PRON do not know how to utilize this to PRON extent as PRON be lack knowledge on how PRON should approach this svm  right now PRON be use scipy   scipy code  def makemodelandpredictiontraindata  trainlabel  testdata    model  svm  svc   c10   cachesize200   classweight  none   coef000   decisionfunctionshape  none   degree3   gammaauto    kernelrbf    maxiter1   probability  false   randomstate  none   shrink  true   tol0001   verbose  false     modelfittraindata  trainlabel   prediction  modelpredicttestdata   return prediction  PRON have already preprocess the datum and do a 7030 train  test split on the datum  can someone point a beginner in the right direction   if PRON want to stick to svcsklearn   PRON can use ensembling  PRON have many way of ensembl  good guide be here  httpmlwavecomkaggleensemblingguide  for PRON problem PRON can train multiple svc model on different subset of PRON datum  and as quick solution PRON can combine PRON whit baggingclassifier like this   clf  baggingclassifiersvcc10   cachesize200   classweight  none   coef000   decisionfunctionshape  none   degree3   gammaauto    kernelrbf    maxiter1   probability  false   randomstate  none   shrink  true   tol0001   verbose  false      if PRON want to use each record be use only once for training in the baggingclassifier  set the bootstrap parameter to false   now PRON can parallel each of this model in on core of PRON cluster  
__label__convex-optimization PRON be go through this lecture relate to convex optimization  PRON be prove that logdet function be concave  however  PRON do not get the derivation at a part  PRON do not get how the step mark in red in the give picture be derive  which property be use  there be three property be use here   the determinant of a product of square matrix of equal size be equal to the product of the determinant of PRON square  equally sized factor  the logarithm of a product be equal to the sum of the logarithm of PRON factor  the determinant of a matrix be equal to the product of PRON eigenvalue   let PRON start from  beginalign   gt   log det  z12i  tz12vz12z12     endalign   the determinant product property yield  beginalign   gt   logbigdetz12deti  tz12vz12detz12big    endalign   and by commutativity of multiplication  PRON can rearrange PRON so that  beginalign   gt   amp logbigdetz12detz12deti  tz12vz12big      amp logbigdeti  tz12vz12detzbig    endalign   use the product  sum property of logarithm  this expression become  beginalign   gt   amp logdeti  tz12vz12    logdetz    endalign   and PRON be most of the way there with the  detz term already separate out   if the eigenvalue of  z12vz12 be  lambdai  then   the eigenvalue of  tz12vz12 be  tlambdai  because the eigenvector of each matrix be the same  the eigenvalue of  i  tz12vz12 be   1  tlambdai because the eigenvector of  z12vz12 will also be eigenvector for  i  this aside about eigenvalue be important  because to complete the derivation  PRON need to use the property that the determinant of a matrix be the product of PRON eigenvalue   beginalign   gt   amp logleftprodi1n1tlambdairight   logdetz    endalign   to complete the derivation  PRON use the product  sum property of logarithm  again   and get  beginalign   gt   sumi1n  log1  tlambdai    logdetz    endalign  
__label__finite-element __label__stability __label__advection-diffusion the transport equation be actually an advection  diffussion  reaction equation  which have the form as    fracpartial cpartial t   v1 fracpartial cpartial x   v2 fracpartial cpartial x   d leftfracpartial2cpartial x2fracpartial2cpartial y2rightrx  ycdot cs  where  c be the unknown substrate concentration   v1  and  v2  be the fluid velocity in the  x and  y direction  respectively   d be the diffusion coefficient   r be the reaction term  and  s be the source term  PRON use the traditional fem scheme with backward euler for the time advance  but PRON seem that there be some negative value appear in the numerical solution PRON solve  be there any method  scheme to avoid negativeness   typically PRON would use a slope limiter  or artificial diffusion and just cross PRON finger  which detect where the solution have go negative and modify the solution to restore positivity  often by modify the gradient of the solution in order to maintain conservation  at least in conservative scheme like discontinuous galerkin and finite volume    there be also more general option  ridzal  bochev and shashkov have a nice trick where PRON solve a bound  constrain optimization problem to minimize difference between the compute solution and a new positive solution   this turn out to decouple into a cheap iterative method which be compute independently for each solution coefficient  the iteration appear to converge very rapidly  
__label__visualization __label__octave __label__vectorization do anyone know why PRON be see these error  PRON be not give PRON a matrix    gtgtgt  t  0012pi    gtgtgt  quiver34t3cost3sint     error    quiver    operator   nonconformant argument  op1 be 3969x1  op2 be 6  3x1   error  call from   error   usr  share  octave362m  plot  privatequiverm at line 179  colum  n 10  error   usr  share  octave362m  plot  quiver3m at line 75  column 11  matlab be a bit more explicit than octave   quiver34t3cost3sint       error use    quiver3 at 53  not enough input argument   there be no way of use quiver3 with only 3 argument  what do PRON want to do  
__label__fortran __label__matrix __label__eigenvalues __label__lapack PRON be work on an eigenvalue problem in fortran  PRON have use lapack to solve the problem and get the eigenvalue and eigenvector  this be do for  201times101  wavenumber  only half the wavespace due to symmetry  and for each grid  point in a large domainin the ocean   PRON be search for the maximum eigenvalue for each gridpoint  and PRON would like to not just pick the absolute maximum in the  201times101  matrix of eigenvalue  but perform an azimuthal average in wavespace  and then pick the maximum average  PRON be struggel with see how to do this   at first PRON cod PRON like this    wavenumber domain  dx4000   pi  4atan1    do m1ktot  kxm   2pidx     m12pi100dx   end do  do l1ltot  lyl   l12pi100dx   end do   radial distance  do m1ktot  do l1ltot  raddistm  lsqrtkxm2lyl2   end do  end do   azimuthal average  omegai be the eigenvalue PRON have find  a ktotltot large matrix   do i1ltot1  indraddistigelyiandraddistiltlyi1    length  countind   where  ind  averageomegai  sumomegailength  end do  but PRON seem PRON be sum a horizontal chunk for all wavenumber in k  direction  between lyi  and lyi1   PRON rather need to make a semicircle in wavespace to sum all omegai  value in between PRON  can anyone help with this   thank so much in advance     btw  PRON also try  do m1ktot1  do l1ltot1  indraddistm  lgelylandraddistm  lltlyl1    length  countind   where  ind  averageomegai  sumomegailength  end do  end do  but PRON right now PRON be blind to what PRON be do wrong  PRON problem state differently would be  be there another way in fortran to find index in a matrix which fulfill some criterion  the find  function in matlab would do the work  but PRON have not find an equivalent intrinsic function in fortran    PRON have not spend much time look at PRON code  but PRON would approach this in a slightly different way  rather than ask which wave number correspond to a give radius range  PRON would divide the space into the  radial  bin PRON want and then loop over the wave vector ask which bin contain each wave vector  eg use integer division   by sum the value in each bin and count how many be in each bin PRON can easily compute the average  this should be more efficient too   to find index in fortran array  create PRON sellecting mask  and use maxloc or minloc 
__label__fluid-dynamics PRON be new to ansys fluent  the ansys cfd product   be PRON a window  only software   PRON institute own copy that work on window only   online tutorial on how to install fluent on ubuntu be not clear   PRON can be use on linux system but PRON be not free  so if PRON university only provide windows version  PRON will have to convince PRON to pay for linux version as well or buy PRON PRON  
__label__recommender-system __label__evaluation __label__score PRON be build a small recommender system which aim at recommend 10 product to customer  instead of use a multi  label classification model  PRON have opt to build a separate scoring model for each product allow PRON to take the other target product as feature  eg  if a customer have product x  that information could help predict detention of product y  conversely  know that a customer have product y could help predict detention of product x    so PRON end up with 10 model  say model x  model y  etc   with almost the same dataset  only the target change  and the feature correspond to that target be remove   however  as each model be different  PRON be not sure how to compare the score PRON obtain for all PRON product and how to make the good recommendation   for example  suppose that for a give customer   score for product x be 07 and score for product y be 08 but the precision of model y be not as good as model x should PRON really recommend product y   PRON be suggest to standardize the score for each product across customer to end up on the same scale  however  if a product end up with very high score than other  this step would in effect penalize this product   any idea   ps  the detention of each product be pretty low  between 05  to 2    PRON think a first step would be to re  balance the dataset for each model  say with under of over sample  in order to have the same class imbalance for each model as PRON observe that re  balance bias the score upwards  unfortunately without really improve the performance of PRON model    PRON can continue to use machine learn for this model  combination step too  PRON be describe a form of ensembling  how about   set aside some purchase as PRON validation set v  and keep the remain as training set train  for train  use k  fold cross  validation to create PRON separate model  use train again with k  fold cross  validation to ensemble the model together  report performance of the ensembl model on v  ensemble technique will take into account the predictive quality of each individual model   PRON would be great if PRON can find any trend which be similar  as PRON know a generalize model give PRON good result when compare to the model which be build for a specific purpose   1st question be how many datum point do PRON have   assume that PRON have good amount of datum to train for each model in each product then PRON be fineyou can go ahead with PRON current process  but in a scenario where PRON have less data point then  PRON can find if there be a trend similar be follow in 2 different productswill be outcome of PRON exploratory analysis   then PRON can combine such datum together  by this PRON be generalize the model   as dan jarratt have say  PRON would also agree with PRON  ensemble model would  help PRON in get good result and with good accuracy  for good understanding PRON can go through this link on ensemble modelling in r  do let PRON know if PRON have any question  
__label__recommender-system __label__linear-regression __label__regularization PRON have be read the paper    the bellkor solution to the netflix grand prize   httpswwwnetflixprizecomassetsgrandprize2009bpcbellkorpdf  and PRON notice for PRON linear base model PRON use the follow learning and regularization parameter on page 4   PRON be wonder if anyone have any information in regard to the rationale behind the different choice   PRON guess be that for the feature that have the high potential opportunity for overfitt there be a high level of regularization  for example  model time base model rating   but then PRON be confuse as to why but  which be time  relate  have low regularization to bu which be just an average across time for the user   PRON be not sure about the different learning rate either   can anyone explain this to PRON please   
__label__clustering __label__sequence PRON have a set of user session  session consist of an order list of type of action that user make  for example  buy a gun  play a mission  etc    PRON want to create  calculate session that have most possible similarity to all provide session  most common type of action user make in order PRON make PRON   unfortunately  PRON know nothing about datum science but PRON try to google a way to do that   PRON have find this document  httpscranrprojectorgwebpackagestraminervignettestraminerstatesequencepdf  and PRON look like 91 and 92 describe thing similar to what PRON want  but PRON do not know this for certain and even if PRON be true PRON stil do not know how to use PRON for PRON scenario   one way would be not to approach this as a calculation per session  most data science solution like to end up with a number  probability or classification  PRON suggest PRON structure PRON datum differently so that PRON try to answer the question  what next action be likely give the last action   in order to do this PRON would have to restructure PRON session datum and use information from across all PRON session  for example  if PRON compare in how many session a player  buy a gun   and if so record over all those session what PRON next action be  eg in 60  PRON  play a mission  next  PRON will then have a probability of PRON next action base on the number of choice player make in all those session   once PRON have those probability  PRON will be able to answer the question   what come next    this will in turn enable PRON to build that most average session that PRON be after by step through a session and build PRON by the most probable next step  
__label__convex-optimization PRON have this little confusion relate to interior point method  in this method PRON use the log barrier function to approximate the real barrier which be not differential  now when PRON find the optimal solution for this objective with log barrier function let say  xt and PRON dual  lambdat and  gammat where  lambda be lagrangian multipli for the inequality constraint and  gamma for the equality constraint  now PRON be prove that the duality gap for  xt and  gammatlambdat be m  t   PRON believe  x and  gammatlambdat be for the approximate function with log barrier  however  the real function be still there  PRON say that the   fxt    p   le m  t where  p be the actual optimum of the original function  PRON do not get how this come to be true  PRON mean m  t bind be for the primal and dual optimal of the approximate function  how come PRON apply for the optimal of the approximate function and optimal of the original function  suggestion   give that PRON already accept that the duality gap along the central path be  m  t  then the inequality PRON be struggle with be really rather simple  remember  the dual problem provide low bound for the optimal value of the primal  so  p be necessarily in between the objective value of any feasible primal point and any feasible dual point   that be  PRON have    fxt    ggammatlambdat    m  t  where  g be the dual objective  but PRON also have    p  leq fx  quad forall xtext  feasible  and    p  geq ggammalambda  quad forall gammalambdatext  feasible  thus for any feasible primal and dual point  PRON have    fxp  leq fx   ggammalambda  quad forall xgammalambdatext  feasible  and along the central path     fxtp  leq fxt    ggammatlambdatm  t  edit  actually  PRON look like PRON may not accept the premise that the  m  t difference apply to the original problem  so let PRON look at this for an lp  the primal and dual problem be    beginarrayllcll   textminimize   amp  ct x  amp  quad  amp  textmaximize   amp  bt lambda   textsubject to   amp  a x  b  amp    amp  textsubject to   amp  at lambda  gamma  c    amp  x succeq 0  amp   amp   amp  gamma succeq 0  endarray  PRON prefer  y and  z above to  lambda and  gamma  but for consistency PRON be keep PRON variable name   lambda be the lagrange multipli for the equality constraint   gamma be the lagrange multipli for the inequality  the barrier problem for the primal be    beginarrayll   textminimize   amp  t ct x  textstyle sumi1m log xi   textsubject to   amp  a x  b  endarray  note that the domain of this problem be limit to  xsucc 0  by the barrier term   the optimality condition for a fix  tgt0  satisfy    t c  mathoptextrmdiagx1  mathbf1   aty  0 quad ax  b quad x succ 0  where  y be a lagrange multipli for the equality constraint  that second term be a bit clumsy  define  zitriangleq xi1 for  i12dot  m so    t c  z  aty  0 quad ax  b quad x  z succ 0 quad zitriangleq xi1i12dot  m  by inspection  PRON see that if   x  y  z satisfie these equality constraint  then   gammalambdat1z  t1y be a feasible dual point for the original problem  the duality gap for this primal  dual pair   xgammalambda be    ctx  btlambda  ctx  t1bty  ctx  t1xtatyc  t1atytx  t1ztx  m  t  so even though PRON be look at the modify barrier model  the set of optimal point  xt be all feasible for the original  and PRON all lead to feasible dual point   gammatlambdat for the dual problem as well  the duality gap for the original model be  m  t 
__label__machine-learning __label__nlp __label__text-mining __label__feature-extraction PRON be a newbie in machine learning but PRON have a coursework to create program that can extract some concrete feature from the give text   for example   if PRON want to extract number of red apple and green apple  PRON will extract 3  5 from  on that tree 3 red apple and 5 green apple grow  and 10  1000 from  in the box there be 1000 green apple and 10 of red    actually the real example can be more complicated like a extract a car specification from the ad   the concrete feature be know before the data set processing   could PRON suggest what kind of algorithm PRON should use  previously  PRON use only linear regression   PRON guess if PRON know what PRON want to extract PRON can just find PRON use regular expression for integer or car specification  
__label__linear-algebra __label__blas originally post on statsstackexchange  PRON will pair the post down to something a bit more general   suppose PRON have vector  mathbfdelta   mathbfx1  ldot  mathbfxj  where  delta in mathbbrj and  mathbfxi in mathbbri furthermore  let  mathbfa  in mathbbritime j be such that     mathbfa    beginpmatrix   delta1 mathbfx1   delta2 mathbfx2   vdot   deltaj mathbfxj  endpmatrix      be there any sequence of blas  optimize operation that can make  mathbfa from PRON set of vector   to be concrete  PRON understand that if PRON have the matrix     mathbfb    beginpmatrix   mathbfx1   mathbfx2   vdot   mathbfxj  endpmatrix      then     textdiagdelta     mathbfb     beginpmatrix   delta1  amp  0  amp  cdot  amp  0   0  amp  delta2  amp  cdot  amp  0   vdot  amp  vdot  amp  ddot  amp  vdot   0  amp  0  amp  cdot  amp  deltaj  endpmatrix   mathbfb    mathbfa      but PRON be not clear to PRON if make  textdiagdelta be an efficient operation  or even if the multiplication between  textdiagdelta and  mathbfb be efficient   edit  PRON notice that a similar  fortran  specific question be ask and answer  however that question address efficient way to multiply diagonal matrix  in fortran  no less   whereas  PRON question reference the use of diagonal matrix as one way in which the solution can be approach  PRON thought be that this question be a bit more general than the aforementioned one    suppose PRON have vector  mathbfdelta   mathbfx1  ldot  mathbfxj  where  delta in mathbbrj and  mathbfxi in mathbbri furthermore  let  mathbfa  in mathbbritime j be such that     mathbfa    beginpmatrix   delta1 mathbfx1   delta2 mathbfx2   vdot   deltaj mathbfxj  endpmatrix      there be some confusion about notation here  as write   mathbfa  in mathbbrij  that is  be have one column and  ij row  instead of  i row and  j column   be there any sequence of blas  optimize operation that can make  mathbfa from PRON set of vector   what PRON think PRON be propose essentially reduce to  beginalign   beginpmatrix   mathbfx1   amp  ldot  amp  mathbfxj   endpmatrix   beginpmatrix   delta1  amp  0  amp  cdot  amp  0   0  amp  delta2  amp  cdot  amp  0   vdot  amp  vdot  amp  ddot  amp  vdot   0  amp  0  amp  cdot  amp  deltaj  endpmatrix    endalign   follow by reshape the result  PRON would not do that   instead  dscal each of the  mathbfxi by PRON respective  deltai in  place  as douglipinski point out  this be basically just a for loop  or do loop in fortran   a tune blas implementation may do some loop unrolling   to be concrete  PRON understand that if PRON have the matrix     mathbfb    beginpmatrix   mathbfx1   mathbfx2   vdot   mathbfxj  endpmatrix      then     textdiagdelta     mathbfb     beginpmatrix   delta1  amp  0  amp  cdot  amp  0   0  amp  delta2  amp  cdot  amp  0   vdot  amp  vdot  amp  ddot  amp  vdot   0  amp  0  amp  cdot  amp  deltaj  endpmatrix   mathbfb    mathbfa      but PRON be not clear to PRON if make  textdiagdelta be an efficient operation  or even if the multiplication between  textdiagdelta and  mathbfb be efficient   the memory movement be not worth PRON  also  the multiplication PRON have propose be not conformal  so PRON would not work  likely  PRON intend to do what PRON show above  
__label__computational-geometry __label__data-analysis PRON be about to get start try to use javaplex for the first time for some application in topological datum analysis  tda  and PRON want to use a custom define metric  from look over the document  PRON can do this if PRON build the distance matrix for all of PRON point  PRON question be twofold   can PRON do PRON by just define the metric between two point  PRON be a simple function to define  just the euclidean cross  product metric between a euclidean space and a circle   and if so   would PRON make sense computationally  or should PRON just build the distance matrix for each of PRON dataset  for the context  PRON have 800 dataset  each with about 40  50 point on a torus in   x  ythetacoordinat  so PRON need to define PRON torus metric   
__label__recommender-system the question be   what algorithm  and library  should i use if i want to build a recommender system with the follow datum in mind representation   userid  zip  movie1  movie2  movie3  1  2483  5  0  3  2  2483  4  1  5  3  2345  3  1  5  basically i want to factor in user datum into a recommendation of a movie    zip can be transform to long  lat but that s another question and out of scope now   i be search the internet for hour with no success  so i will be grateful if someone can point PRON in the right direction   collaborative filtering   match user to people with similar taste –recommend what PRON like  commonly use in e  retail  avoids the issue of user only be recommend more of what PRON already like  allow serendipity   example   PRON scenario   method  euclidean distance  cosine distance  pearson correlation coefficient  most common    content  base recommendation   match user directly to product and content  recommend base on what PRON have buy or view in the past  commonly use for document recommendation  webpage  news article   blog etc   example   which movies  may PRON user like   option1  the user select preferenc for the various feature use pull  down menu etc  PRON match against the movie use vector  space method  describe next   method  vector  space method  k nearest neighbourknn   option2  the user rat a sample of the movie  explicitly or implicitly  as like  dislike  PRON then build a user profile model for that user use machine learning   method  decision trees 
__label__statistics __label__social-network-analysis this be in continuation to a previos question  PRON would like to know where in practice will the follow m  ary tree  structured social network propagation model arise  PRON be look for some concrete example in social medium  twitter and youtube etc    eventually  PRON want to do some statistical analysis on such social network   imagine each node in PRON graph as a user  and each edge as an action such as  share   PRON may also be a bidirectional relationship  share  and  view    some social scientist and engineer estimate the probability that a message be  share  and  view  give that a particular user decide to share PRON  this process be call  information diffusion    information propagation    cascading   etc   if PRON be interested in the detail of how these calculation be perform  check out these paper   httpcsstanfordedupeoplejurepubsnetratenetsci14pdf  httpcsstanfordedupeoplejurepubscascadeswww14pdf  httpcsstanfordedupeoplejurepubsinfopathwsdm13pdf  similar topic   httpsnapstanfordedupapershtml 
__label__linear-solver __label__hpc PRON have a sparse  2times10  5  by  2times10  5  matrix with  32times10  9  non  zero element   PRON want a sparse solver with out  of  core functionality  PRON have attempt to use intel s mkl pardiso  with a vary degree of success  for small problem  PRON be stellar  but for the problem size above  PRON be take a huge amount of ram  even with the out  of  core enable   presumably during fill  in reduce ordering stage   the intel staff have be incredibly helpful with debug several problem  but PRON feel that PRON may be prudent to try a different solver   be there any good reference compare the performance of solver for big problem   
__label__fluid-dynamics __label__benchmarking PRON be try to benchmark a new numerical method for the incompressible navier  stokes equation   PRON would like to use the most widely  accept test case possible   in particular  PRON be look for a reference setup for the 3dimensional rayleigh  taylor instability in a periodic domain   PRON would be especially helpful if   the initial condition be describe precisely  the diagnostic  measure of correctness  be describe precisely  code and datum be available  if necessary  PRON will settle for a reference for which code be not available or the data be only available in figure   PRON would prefer single  mode rti  but a benchmark setup for multi  mode rti that be widely accept would be okay   edit  PRON have forget about this  but a while ago a friend of mine do PRON whole thesis work on the rayleigh  taylor instability in the context of statistical analysis of turbulence  PRON can find PRON enclose here   httpwwwthesesfr2011dens0035  the thesis be in french  but the work be extremely thorough concern the rt instability  PRON may find the quantitative information  growth rate  turbulent fluctuation  etc   that PRON be interested in as well as the problem setup  if PRON need help with some part  PRON can help with the translation   edit  on another note  if PRON wish to study the order of convergence of PRON method  if PRON wish to study stability  ignore what PRON be about to say  the good approach to test PRON convergence be to design test case use the method of manufacture solution     reference book  httpswwwamazoncaverificationvalidationscientificcomputingoberkampfdp0521113601  example of a recent article use this  this one be by PRON  but there be many other   httpwwwsciencedirectcomsciencearticlepiis0045793015000675    concretly  the method of manufacture solution allow PRON to create analytical solution to the incompressible navier  stokes equation that be infinitely continuous  that stimulate every term in the navier  stokes equation equally and that may even possibly be unsteady  this allow PRON to fully assess the order of convergence  or the run time  etc   on fully complex test case that be representative of the real application of PRON cfd code  furthermore  PRON can be combine with any type of boundary condition  dirichlet  periodic  neumann  robin    that be a good way to benchmark a numerical method and may be even use to evaluate rans model  with or without wall model   for the latter  see the work of pr  pelletier s group   if this be not what PRON be look for  then the rayleigh  taylor flow can be good example of an unsteady test case  PRON will try to look  up a reference   PRON be not quite the problem for which PRON be look for solution  but if PRON be willing to consider something slightly out of the box  there be numerous benchmark for the r  t instability in the case of the stokes equation without time derivative  ie  the boussinesq approximation  rather than list a bunch of paper  PRON be simply go to reference the aspect manual that reproduce many of PRON  with appropriate reference   httpaspectdealiiorg   go to info  manual   
__label__gaming __label__reinforcement-learning print actionspace for pong  v0 give  discrete6   as output  ie012345 be action define in environment as per documentation  but game need only two control  why this discrepency  further be that necessary to identify which number from 0 to 5 correspond to which action in gym environment   PRON can try to figure out what exactly do an action do use such script   action  0   modify this   o  envreset    for i in xrange5    repeat one action for five time  o  envstepaction0   ipythondisplaydisplay   imagefromarray   o140142    extract PRON bat   resize300  300     big image  easy for visualization    action 0 and 1 seem useless  as nothing happen to the racket   action 2  amp  4 make the racket go up  and action 3  amp  5 make the racket go down   the interesting part be  when PRON run the script above for the same actionfrom 2 to 5  two time  PRON have different result  sometimes the racket reach the topbottom  border  and sometimes PRON do not  PRON think there may be some randomness on the speed of the racket  so PRON may be hard to measure which type of up2 or 4  be fast   there seem to be no difference between 2  amp  4 and 3  amp  5  the inconsistency mention by icyblade be due to the mechanic of the pong environment    each action be repeatedly perform for a duration of k frame  where k be uniformly sample from  234    so the action be just repeat a different number of time due to randomness  PRON can try the action PRON  but if PRON want another reference  check out the documentation for ale at github   in particular  0 mean no action  1 mean fire  which be why PRON do not have an effect on the racket   here be a good way   envunwrappedgetactionmeaning   
__label__neural-network __label__deep-learning __label__convnet __label__image-recognition __label__dropout PRON be play a bit with convnet  specifically PRON be use the kaggle cat  vs  dog dataset which consist on 25000 image label as either cat or dog  12500 each    PRON have manage to achieve around 85  classification accuracy on PRON test set  however PRON have set PRON the goal of 90  accuracy   PRON main problem be overfitt  somehow PRON always end up happen  normally after epoch 8  10   the architecture of PRON network be loosely inspire by vgg16  more specifically PRON image be resize to 128x128x3  and then PRON run  convolution 1 128x128x32  kernel size be 3  stride be 1   convolution 2 128x128x32  kernel size be 3  stride be 1   max pool  1 64x64x32   kernel size be 2  stride be 2   convolution 3 64x64x64   kernel size be 3  stride be 1   convolution 4 64x64x64   kernel size be 3  stride be 1   max pool  2 32x32x64   kernel size be 2  stride be 2   convolution 5 16x16x128   kernel size be 3  stride be 1   convolution 6 16x16x128   kernel size be 3  stride be 1   max pool  3 8x8x128   kernel size be 2  stride be 2   convolution 7 8x8x256   kernel size be 3  stride be 1   max pool  4 4x4x256   kernel size be 2  stride be 2   convolution 8 4x4x512   kernel size be 3  stride be 1   fully connected layer 1024  dropout 05   fully connected layer 1024  dropout 05   all the layer except the last one have relu as activation function   note that PRON have try different combination of convolutions  PRON start with simple convolution    also  PRON have augment the dataset by mirror the image  so in total PRON have 50000 image   also PRON be normalize the image use min max normalization  where x be the image  x  x  0  255  0  the code be write in tensorflow   the batch size be 128   the mini  batch of training datum end up overfitt and have an accuracy of 100  while the validation datum seem to stop learn at around 84  85    PRON have also try to increase  decrease the dropout rate   the optimizer be use be adamoptimizer with a learning rate of 00001  at the moment PRON have be play with this problem for the last 3 week and 85  seem to have set a barrier in front of PRON   for the record  PRON know PRON could use transfer learn to achieve much high result  but PRON be interesting on build this network as a self  learn experience   update  PRON be run the same network with a different batch size  in this case PRON be use a much small batch size  16 instead of 128  so far PRON be achieve 875  accuracy  instead of 85    that say  the network end up overfitt anyway  still PRON do not understand how a dropout of 50  of the unit be not help  obviously PRON be do something wrong here  any idea   update 2  seem like the problem have to do with the batch size  as with a small size  16 instead of 128  PRON be achieve now 928  accuracy on PRON test set  with the small batch size the network still overfit  the mini batch end up with an accuracy of 100   however  the loss  error  keep decrease and PRON be in general more stable  the con be a much slow run time  but PRON be totally worth the wait   there be several possible solution for PRON problem   use dropout in the early layer  convolutional layer  too   PRON network seem somehow quite big for such an  easy  task  try to reduce PRON  the big architecture be also train on much big dataset   if PRON want to keep PRON  big  architecture try   image augmentation in order to virtually increase PRON training datum  try adversarial training  PRON sometimes help   PRON suggest PRON analyze the learn plot of PRON validation accuracy as neil slater suggest  then  if the validation accuracy drop try to reduce the size of PRON network  seem too deep   add dropout to the conv layer and batchnormalization after each layer  PRON can help get rid of overfitt and increase the test accuracy   ok  so after a lot of experimentation PRON have manage to get some result  insight   in the first place  everything be equal  small batch in the training set help a lot in order to increase the general performance of the network  as a negative side  the training process be muuuuuch slow   second point  data be important  nothing new here but as PRON learn while fight this problem  more datum always seem to help a bit   third point  dropout be useful in large network with lot of datum and lot of iteration  in PRON network PRON apply dropout on the final fully connected layer only  convolution layer do not get dropout apply   fourth point  and this be something PRON be learn over and over   neural netword take a lot to train  even on good gpu  PRON train this network on  floydhub  which use quite expensive nvidia card   so patience be key   final conclusion  batch size be more important that one may think  apparently PRON be easy to hit a local minimum when batch be large   the code PRON write be available as a python notebook  PRON think PRON be decently document  httpsgithubcommorianolocolearningblobmastercatsvsdogscatsvsdogsipynb  one thing that have not be mention yet and that PRON can consider for the future  PRON can still increase PRON dropout at the fully connected layer   PRON read a paper once that use 90  dropout rate  although PRON have many many node  2048 if i recall correctly   PRON have try this PRON on layer with few node and PRON be very helpful in some case   PRON just look up which paper PRON be  PRON can not recall which paper PRON just remember but PRON find these that also have some success with 90  dropout rate   karpathy  a  toderici  g  shetty  s  leung  t  sukthankar  r   amp   fei  fei  l  2014   large  scale video classification with  convolutional neural network  in proceeding of the ieee conference  on computer vision and pattern recognition  pp  1725  1732    simonyan  k   amp  zisserman  a  2014   two  stream convolutional  network for action recognition in video  in advance in neural  information processing system  pp  568  576    varol  g  laptev  i   amp  schmid  c  2017   long  term temporal  convolution for action recognition  ieee transaction on pattern  analysis and machine intelligence  
__label__matlab __label__data-storage __label__maple PRON have a a function  fk  calculate in maple  which be huge and store in a variable call  sum  on PRON drive  with the help of  save  command in maple   since the function be huge  maple be unable to plot PRON and be take endless time  thus  PRON want to read the variable  sum  into matlab and plot PRON  PRON be unable to also just copy paste as the function be really big  PRON have search around but be unable to find a solution  can somebody help PRON out   PRON can use the codegeneration package to do this  PRON allow to translate to different language  be matlab one of those   a simple example here   withcodegeneration    suma   sumsinnxfactorialn   n  0  10    then  matlabsuma   with answer  cg3  sinx   sin02e1  x   02e1  sin03e1  x   06e1  sin04e1  x   024e2  sin05e1  x   0120e3  sin06e1  x   0720e3  sin07e1  x   05040e4  sin08e1  x   040320e5  sin09e1  x   0362880e6  sin010e2  x   03628800e7   off course  PRON can store this in a string and then write PRON to a text file  or translate a maple procedure to a matlab function  
__label__research suppose PRON goal be to collaborate and create an advanced ai  for instance one that resemble a human being and the project would be on the frontier of ai research  what kind of skill would PRON need   PRON be talk about specific thing like what university program should PRON complete to enter and be competent in the field  here be some of the thing that PRON think about  just to exemplify what PRON mean   computer science  obviously the ai be build on computer  PRON would not hurt to know how computer work  but some low level stuff and machine specific thing do not seem essential  PRON may be wrong of course   psychology  if ai resemble human being  knowledge of human cognition would probably be useful  although PRON do not imagine neurology on a cellular level or complicated psychological quirk typical to human being like the oedipus complex would be relevant  but again  PRON may be wrong   one of the comment suggest a phd in machine learning  as a full  time ai researcher PRON  PRON would say that would certainly be one useful option   however  in order make much  need progress  ai need avoid fall into the trap of think that currently fashionable method be any kind of  silver bullet   there be some danger that a phd that head straight into  say  some sub  sub  sub area of dl would end up impose too much bias on the student s subsequent perspective   ai research be an essentially multi  disciplinary activity  other possible background therefore include   mathematic or physics  to first degree or phd level   a strong background in either of these never do anyone any harm  people who be competent in these field tend to be able to turn PRON ability to new domain relatively easily   software engineering  one of the thing that ai need be integrative architecture for knowledge engineering  here be why  PRON believe that one of the reason that PRON have not yet manage to do ocr at the level of a 5 year old be that PRON have yet to accept that PRON have to  build a sledgehammer to crack a nut   software architect be use to manage large  scale complexity  so PRON may be able to help   cognitive science  psychology  cognitive linguistics  the reason here be obvious   above all  PRON personally think that a good ai researcher should be creative  inquisitive and prepared to question receive wisdom  all of which be more important in practice than specific of PRON background   research on ai seem to be get wide these day  2016   first   obvious  few department  no order    computer science  eg computation theory  algorithm   ai researcher there assume that intelligence be a kind of computation  under various form  eg a neural network  a logic system    software engineering  assume PRON find a good model for ai  how do PRON make PRON  this be what the engineer will want to figure out  and PRON can be hard to map mathematical model to an engineered piece   statistics and probability  more specific than just mathematics  which be also close to computer science   this be about data science  notably as a foundation to machine learning  the most active branch in ai  which  just  cover the learning part   physics  this be particularly relevant now for hardware  see below    neuro science  understand how the brain work  as an inspiration to create an artificial one  be the home for connectionists  recently  hassabis and PRON team at google deepmind make several breakthrough relate to reinforcement learning  memory  attention  etc   recently electric engineering be get a lot of light  together with the relate branch of physics  several public and private laboratory focus on  brain chip   to name a few  ibm  who be work on that for some time already   nvidia  and facebook  circa 2010  PRON become clear that technique like deep learning require horsepower  thus an increase focus on create more powerful  small  more energy efficient chip  and on top of that  there be all the work in quantum computing   but the thing be  there seem to be many more field that be get involve in ai research  PRON should mention chemistry and biology  as both inspiration and tool to make new model or hardware  eg chip that do not use silicon  so PRON can get small    as for 2016  the above field be the most active  and promise to remain very active for quite some time  pick PRON own depend on PRON interest  skill  or mere intuition   to finish  PRON may be surprised in a few year when PRON look back at where ai have come from  PRON believe that if PRON manage to build an agi  PRON will leverage all these field anyway  PRON guess the thrill be to be part of the story  
__label__machine-learning __label__python __label__classification __label__scikit-learn __label__cross-validation PRON have a question about cross  validation use sklearn in python  27    PRON have update this to include the code PRON use prior to cross  validation   PRON import a csv into a dataframe   some of these column contain category   so PRON use getdummie to convert PRON to binary column  ie  one  hot encoding   this retain the non  categorical  remove the categorical datum  and substitute the categorical datum with binary column   featuredf  pdgetdummiesdf  column  featurecat   x  featuredfdroptargetcat  axis1   print x   visually confirm PRON be format correctly  y  dftargetcat    targetcat be a string contain the name of the column contain PRON target datum   PRON have print this out to confirm PRON be correctly extract this from the df   x  scalex    scaling be important for prevent misleading result  x  pd  dataframex    PRON also print this out to confirm PRON contain and be format as PRON be suppose to be     note  sklearn leaveoneout   cross  validation need x to be a data frame  although right now PRON be only use k  fold   then PRON create PRON pipeline   model  decisiontreeclassifier    pipe  pipelinepca   pca       clf   model     pipefitxy   after this    make prediction on training set   prediction  pipepredictx    print initial accuracy  accuracy  metricsaccuracyscorey  prediction   print  accuracy   s     03formataccuracy   kfold  kfoldnsplits100   accscore  npmeancrossvalscorepipe  x  y  scoringaccuracy   cv  kfold    print accscore  when PRON measure PRON classifier accuracy use accuracyscore PRON get   accuracy   88416   and PRON normalized confusion matrix be     097  0   002  0   001  001    008  089  001  0   001  001    009  001  088  0   001  001    006  002  004  084  001  002    006  002  004  002  083  002    006  001  004  002  004  083    when PRON run cross validation use   kfold  kfoldnsplits10   crossvalscorepipe  x  y  scoringaccuracy   cv  kfold    pipe be pca and decisiontreeclassification pipeline    PRON give PRON a mean accuracy of   0231595981914  with the accuracy for each of the 10 fold    026901521  020176141  022177742  029967949  022435897  024919872  027163462  021153846  021794872  020192308   while PRON would expect variation in the accuracy of the classifier  PRON would not expect PRON all to be this completely different from the original accuracy  and PRON certainly would not expect PRON to be consistently this different   to further test the feasibility of the these result  PRON shuffle the row of PRON original data frame contain both the feature and target  PRON run PRON again  and now average cross  validation accuracy be   0553633419556  with individual fold accuracy of    055644516  056044836  053242594  055849359  053926282  054326923  055128205  055528846  057532051  054887821   PRON try this a few more time by change the seed for random andor shuffle the shuffle dataset  PRON cross  validation accuracy remain pretty much the same   for example   0553075665661  fold accuracy    05236189  054923939  054043235  055769231  055048077  055208333  054567308  055769231  056570513  058092949   so then PRON increase the fold to 100  and the mean accuracy be still 055  and the 100 result accuracy from each test run be all between 49  min  and 64max    if PRON run PRON with 100 fold on PRON original  non  shuffle dataset  PRON get a mean cross  validation accuracy score of 026  with individual accuracy range between 09 and 52  with most be 2 or 3   be PRON PRON or be this weird   should not these result be more consistent   be there something wrong with metricsaccuracyscore  which PRON use to compute PRON original classification accuracy  prior to cross  validation   presumably this be also what crossvalscore use when PRON set scorer to accuracy   any advice andor feedback would be greatly appreciate     for what PRON be worth  PRON be use pycharm on a mac  sierra    thank PRON   
__label__linear-algebra __label__linear-system __label__matrix-equations let  sigma be positive definite and the  di positive diagonal   let the  xi be unknown square matrix   consider the system of equation      isigma dixisigmahspace5mmtextforhspace5mmi1  n  be PRON possible to numerically solve all of these equation without have to completely recompute the lu factorization of   isigma di for each  i   PRON have be mess around with the cholesky and eigen  decomposition of  sigma  but no luck so far  PRON think PRON be probably not possible  but PRON think PRON would ask   if PRON help  note that the solution  xi will all be positive definite  since    xi1   sigma1   di  edit   PRON look like this answer here by the all know brian borchers mean that this be infact impossible   PRON can always try a conjugate gradient  httpsenwikipediaorgwikiconjugategradientmethod    once PRON have find the solution via lu  if the new change affect only to a small number of equation  or even a large one   the convergence should be very fast   here PRON be clearly explain how to do PRON  ftpftpnumericalrlacukpubtalksisdstanford50pdf 
__label__machine-learning __label__data-mining __label__algorithms __label__visualization PRON do not understand how k  bucket simple split and merge work  PRON get that the split take place when the threshold be pass  but PRON do not get the next step  which merge two consecutive bucket  PRON question be which one to merge and how be the merge do vertically or horizontally   let PRON suppose that PRON have a set s    3    1    3    5    2    3    4    1    5    3   and  the histogram be have at most 5 bucket  the threshold 10  how will the k  bucket work in this case  
__label__data-visualization __label__visualization do paraview or visit have any filter to draw a smooth surface for a set of point   in paraview PRON can try the follow   delaunay2d  for create a surface  see httpwwwvtkorgwikivtkexamplespythonfilteringdelaunay2d   smooth  loopsubdivision  will also create new cell  new point   PRON know that PRON ask for filter in paraview or visit  but  PRON could use another open source tool to reconstruct the surface first  such as meshlab 
__label__nonlinear-equations PRON be try to solve a problem of find the displacement of an elastic material subject to external force   those external force be PRON a nonlinear function of the material displacement   the first step be to define a functional for the energy in term of the displacement   the solution be then the displacement which minimize that functional over sufficiently smooth function   from this functional  euler  lagrange equation can be derive  which must be satisfy by the solution   those equation be a system of elliptic pde   to solve this problem in practice  PRON be necessary to discretize   this can be do directly for the functional  so that the integral of the lagrangian over the spatial domain become a sum over a spatial grid  with the appropriate translation of spatial derivative to finite difference   the problem be to minimize the sum  which be generally non  convex   the alternative be to discretize the euler  lagrange equation  which result in a large nonlinear system of equation   the functional contain term which be fourth order polynomial of the displacement  while the euler  lagrange equation have term which be third order polynomial of the displacement   there be no discontinuity in the problem   PRON question be about the relative advantage and disadvantage of the two approach   PRON have be try quite hard to solve the second problem  ie solve the large nonlinear system  and PRON just can not seem to make progress   PRON have an analytic expression for the jacobian  and PRON can ensure that PRON have a good initial guess in the sense that PRON be  close  to the solution as measure by the norm of the difference  but PRON have no guarantee that  for example  newton s method will converge from this initial guess   PRON have attempt some simple globalization strategy  such as a parametriz continuation  pseudo  transient continuation  and some ad hoc regularization   the problem with newton s method without a continuation be that PRON seem to always find a point at which the jacobian become singular   the parametriz continuation seem to avoid this  but the newton iteration stagnate   quasi  newton method like l  bfgs work very well up to a point  and then fail catastrophically  with massive increase in the residual norm in a single iteration   PRON have not attempt recast the problem as a minimization for two reason  first  PRON do not understand what advantage that would have over solve the nonlinear pde  and second  PRON would have to do a lot of calculation arise from discretiz at an early step   PRON realize this question may be a bit vague but PRON would really appreciate any guidance  thank   edit  PRON will try to include some more information   one advantage PRON can see of discretiz the energy functional and approach the problem as a minimization be that the hessian would be symmetric   this contrast with the discretization of the euler  lagrange equation  for which the jacobian be not symmetric   in both approach the hessian  jacobian be not positive definite   PRON should be possible to calculate a value which can be add to the diagonal to make a positive definite matrix  as discuss in nocedal and wright  but PRON be unsure how to actually implement that   PRON suppose the idea be to put in a positive definite approximation to the jacobian without alter the function PRON   edit 2  another think about why PRON seem to be have so much trouble be that because PRON external datum come on a cartesian grid  PRON be use finite difference instead of finite element   PRON realize that the vast majority of work in elasticity use finite element  but PRON have yet to see an explicit discussion of the advantage in term of convergence   PRON use center difference operator on a 27 point stencil   the boundary condition be homogeneous dirichlet   
__label__clustering be there any way to check if more datum can help the quality of kmean cluster   the close research PRON could find be for em algorithm   any thing similar for evaluate kmean   the estimation quality of the mean improve with sqrtn    so with more and more datum  PRON mean will get more precise  but the improvement get slow and slow   PRON will not help with all the other issue of k  mean  such as problem with different cluster diameter and outlier  and more datum probably mean more outlier   PRON can try a similar approach to that paper with k  mean with some slight modification  try plot the sum of squared deviation between the holdout set  and the near centroid each  as long as this improve  the cluster fit PRON holdout set better  
__label__nlp __label__similarity __label__search __label__information-retrieval PRON have a database of about 200 document who be ngram PRON have extract  PRON want to find the document in PRON database that be most similar to a query document  in otherword  PRON want to find the document in the database that share the most number of ngram with the query document  right now  PRON can go through each one and compare PRON one by one  but this will take on  time and be expensive if n be very large  PRON be wonder if there be any efficient datum structure or method in do efficient similarity search  thank  the datum structure typically use be invert index  eg  in database    please note that match all ngram be a good heuristic but PRON may want to improve PRON   take into account the probability of each term and stem be direction PRON may benefit from   table  ngram  docid  pk  primary key  ngram  docid  depend the database may change a bit but this be for tsql  x be the document PRON be match  select top1  with tie   from    select tmdocid  count    as count  from table td  join table tm  on tmdocid  ltgt  tddocid  and tmngram  tdngram  and tddocid  x  group by tmdocid   tt  order by count desc  the join be on an index  pk  so this be very fast  PRON do this on a million document in just a few second  with more advanced condition    this be go to favor large document but that be what PRON ask for   question seem to be change  declare table query  varchar ngram    insert into query value   ng1      ng2      ng3     select top10  with tie   from    select tmdocid  count    as count  from table td  join query  on tmngram  queryngram  group by tmdocid   tt  order by count desc  PRON could use a hashing vectorizer on PRON document  the result will be a list of vector  then vectorize PRON ngram in the same way and calculate the projection of this new vector on the old one  this be equivalent to the database join on an index  but may have less overhead   from PRON clarification   by database  let just say that there be a huge list of the ngram  model that represent the document  PRON would do well to do something a bit more structured and put the datum into a relational database  this would allow PRON to do much more detailed analysis more easily and quickly   PRON guess when PRON say  ngram  PRON mean  1gram   PRON could extend the analysis to include 2grams  3grams etc  if PRON want   PRON would have a table structure that look something like this   1grams  id  value  docs  id  doctitle  docauthor  etc   docs1grams  1gramid  docid  1gramcount  so  in the record in the docs1gram table when 1gramid point to the 1gram  the  and the docid point to the document  war and peace  then 1gramcount will hold the number of time the 1gram  the  appear in war and peace   if the docid for  war and peace  be 1 and the docid for  lord of the rings  be 2 then to calculate the 1gram similarity score for these two document PRON would this query   select count    from docs1gram d1  docs1gram d2  where d1docid  1 and  d2docid  2 and  d11gramid  d21gramid and  d11gramcount  gt  0 and  d21gramcount  gt  0  by generalize and expand the query this could be easily change to automatically pick the high such score  count compare PRON choose document with all the other   by modify  expand the d11gramcount  gt  0 and d21gramcount  gt  0 part of the query PRON could easily make the comparison more sophisticated by  for instance  add 2grams  3grams  etc  or modify the simple match to score accord to the percentage match per ngram   so if PRON subject document have 00009  of the 1grams be  the   document 1 have 0001  and document 2 have 00015  then document 1 would score higher on  the  because the modulus of the difference  or whatev other measure PRON choose to use  be small  
__label__neural-network __label__deep-learning __label__text-mining PRON have a large label dataset of address that i need to parse to extract the string of interest out of   label available be   country    examplecountry    city    examplecity   streetexamplestreet  example   example string     rod deputado antonio heil 3400 sala 4 itaipava  itaja  sc  88316003  brazil     japan  〒 105  0023 tōkyō  to  minato  ku  shibaura  1 chome−15 ゆで太郎芝浦１丁目店    example label      country    brasil    city    itajaí  sc    street    rod  antônio heil  3400  4  itaipava       country    japan    city    tokyo    street    〒 105  0023 minato  ku  shibaura  1 chome−１５ １ 丁目 １５      PRON want to make a deepneural model that can learn from this dataset to parse the address  PRON be look for suggestion on what would be the good way to proceed into and the framework that would be good to go along with  also if PRON have some helpful link that PRON can provide regard the same  that would be great   
__label__neural-network __label__beginner apology if this question be not a suitable format  PRON be a novice in datum science   PRON have a database of specie observation datum consist of 16 million record   each record consist of   latitude  longitude  date  time  specie observe  that be specie singular  not plural   this data have be manually vet by expert  so there be an additional field for each specie in a record that classify the observation as either valid or invalid  or more accurately speak  likely correct  likely incorrect   PRON be explore the idea of train a neural network on this datum to automatically classify new record as be valid or invalid   invalid  datum will be flag for manual expert review    the vast majority of record be classify as  valid   so PRON worry be that there be not much information to train the model on what constitute  invalid    however  a good predictor of whether a record be valid be  informally speak   be there other record of this specie close by  spatially andor temporally    PRON be not sure where to start with formulate a neural network for this problem  eg  inputs  latitude  longitude  date  time  speci  output  validity  or  inputs  latitude  longitude  date  time  output  one output for each know specie indicate validity  PRON like the idea of this second model as PRON can input a time and location and get out a list of likely specie   so PRON concrete question be   do this sound like an application suitable for a neural network   if so  where may PRON start with formulate a model for PRON problem  or can someone point PRON in a good direction to learn more about this topic   PRON may or may not apply neural network to PRON problem  neural network will give prediction for sure but PRON can not be say how much efficient will PRON be for this problem  PRON have to code and check PRON for PRON  also  if PRON have all the above datum in tabular form and PRON know the labelswhich PRON guess be valid  inavlid   PRON will say try PRON with xgboost  neural network be exceptional for unsupervised learning but in the case of supervised learning  there may be a model which may outperform a neural network   before decide on the model  PRON would recommend to re  formulate the dataset to good suit PRON problem  PRON could approach this problem as follow   since the output PRON be try to predict be validity of the observation  keep  validity   true  false  or  10 as the target variable   one of the parameter be a categorical variable  specie   and PRON be expect this to have a high cardinality  since there be approximately 87 million specie on earth  if PRON use this variable in a model PRON could possibly expand into 87 million individual column  in on  hot encoding form   even a conservative estimate of 100000 specie make PRON nonviable to be use as PRON be  so PRON need a way to convert this specie information to a few feature   one approach PRON could try out be to create geographical cluster for each specie  use only valid marked record   then identify the near center and max  avgquartile measure of distance from PRON cluster center for each specie  do this for each quarter of the year separately to account for seasonal change  next  add this information back to the main dataset to indicate for each record  all the geographical center of that specie cluster  in the next step  for each record find the near cluster center and calculate this particular observation s distance from PRON cluster center  then calculate the ratio of PRON distance from cluster center vs max distance and vs avg distance from that cluster s center  use this metric instead of the geospatial coordinate and speci identifier   another approach could be to add additional feature such as the climate of each location and average historic temperature at that location during the time of the year when the observation be take  this be because some animal may migrate north  south base on the season and so if a species  location be find valid in the summer  PRON may be impossible to find PRON in the same location in winter due to PRON be unable to survive the cold weather  if PRON combine this with  3 above  PRON would enrich the observation significantly   after do this extensive hard work  PRON should do some exploratory analysis and plot subset of this datum to better understand PRON  by visualize the datum  sometimes PRON be able to figure out good course of action more quickly than without visualize the datum   next  PRON may explore different machine  learn algorithm to fit a model to this refined datum  PRON would recommend try out other algorithm such as logistic regression  svm  ridge  regression  random forest and gradient boost machine in addition to neural network and then select the best perform one  most machine learning suite  framework implement these  so PRON should not be difficult to find out how to apply these to PRON dataset   neural network be fine to try out  but as with all algorithm PRON need to be careful about the usual pitfall such as   avoid over  fit the model to training datum  to avoid this use regularization and keep cross check accuracy with an independent hold  out validation set   use cross  validation  10fold  and repeat several time to get good estimate of the model s performance metric on new datum   since the data be highly class imbalanc  many valid record but few invalid record by proportion   use a performance metric other than simple true positive accuracy  try use f1 score  precision  of identify invalid record   kappa metric  etc   due to high class imbalance  PRON would help if PRON either over  sample the minority class  invalid  or under  sample the majority class  valid   or do both together  this will improve the model s ability to classify mode precisely   adjust the hyper  parameter such as learn rate and hide layer  no  of unit for good model performance   the basic of machine learning be this   be there a pattern to the datum   if so then there be a formula which describe the pattern   if the formula be know  use the formula   if there be a pattern but the formula describe the pattern be unknown  PRON can use machine learn to determine the formula which best approximate PRON  
__label__machine-learning be PRON possible to utilize  create a large datum set with like datum in order to be able to improve the ml model optimization andor ml model feature option that PRON can utilize on PRON subset of datum  if so what should one take into account when build this datum  ie when source additional datum be there mistake to avoid andor how many multiple of PRON subset datum should PRON ideally add   this be a common approach to address class  imbalance  PRON will generally see PRON refer to as  upsampling  or  over  sampling   in addition to simply reuse existing datum  PRON can  augment  PRON datum by transform PRON or add noise  or generate synthetic datum  a good demonstration of datum augmentation be the win solution to the kaggle galaxy zoo contest  a popular technique for generate new datum be smote  keep in mind  if PRON change the class proportion in PRON dataset  PRON effectively change the prior for those class  depend on PRON choice of algorithm and what PRON be try to accomplish  PRON may want  need to apply a probability adjustment to the model s output   another technique PRON can use be to just pick a different decision threshold  for example  if PRON be perform a logistic regression and do not care about perform inference on parameter  duplicate the datum for one class will not actually change the slope of the decision boundary  PRON will just shift PRON location  PRON could equivalently use the decision boundary learn on the unmodified datum and then pick a cutoff other than  px1  ge 05 a good heuristic be to use the threshold which maximize youden s j  which can be trivially calculate from a roc curve  if PRON have a specific model evaluation metric PRON be concerned with  choose PRON threshold relative to that  
__label__convolution in this nice tutorial about cnn  the author build a single  layer cnn  the initial convolution weight be set randomly  accord to a uniform distribution   by the end of this scetion  the author note that the randomly initialise kernel behaf very similar to an edge detector and give the follow input and output as example   why do the randomly initialise kernel behave like an edge detector   take this with a grain of salt  but PRON think this be simply not true  PRON can evaluate PRON with the code PRON just write   only 2  probably 3 of the 25 look like edge filter to PRON  the result of an edge filter look like this  
__label__tensorflow __label__backpropagation PRON be currently try to understand the bptt for long short term memory  lstm  in tensorflow  PRON get that the parameter numstep be use for the range that the rnn be roll out and the error backpropagat  PRON get a general question of how this work   for reference a repitition of the formula  PRON be refer to   formulas lstm  httpsarxivorgabs150600019   question   what path be backpropagat that many step  the constant error carousel be create by the formula 5  and the derivate for backpropagation   strightarrow st1  be  1  for all timestep  this be why lstms capture long range dependency  PRON get confused with the dependency of  gt   it   ft and  ot of  ht1 in word  the current gate do not just depend on the input  but also on the last hidden state   do not this dependency lead to the exploding  vanish gradient problem again   if PRON backpropagate along these connection PRON get gradient that be not one  peephole connection essentially lead to the same problem   thank for PRON help    do not this dependency lead to the exploding  vanish gradient problem again   absolutely  and PRON better have vanish gradient otherwise PRON have a training problem   vanish gradient in this case be not a bad thing  PRON be a good thing  unlike in feedforward   let  ct be the cost function evaluate at time  t and  wt be some weight of the network at time  t what vanish gradient in this case mean be that  dctdwt  u become small and small as u become big and big  that be good because   dctdw   sumu0numstep  dctdwt  u    so if the gradient do not vanish in time  then the gradient would explode  so  w get a proper non  vanish gradient even if the gradient in time vanish because the gradient for  w be the sum at all time of the gradient for  w  in lstms the gradient be sure to vanish in time because the activation function be sigmoid and tanh be so PRON derivative be less than or equal to one  so as PRON get multiplied PRON slowly become small   this compare to what be normally call the vanish gradient problem which occur when gradient vanish while pass from top layer to bottom layer  because that mean that  dc  dw  for  w  of the low layer be vanish and so the low layer do not get train  only the upper layer get train   also  as mention in the comment  the above apply to any rnn  not only lstms  what set lstms appart from vanilla rnn in with regard to this question be the gating function which allow the lstm to control what PRON remember and what PRON forget and how much of the new input PRON take in  while the above be true in practice for lstms  and be true on average also in theory   in theory  one could have a time step  t where the output have ignore the last 10 input and only depend on the input 11 timestep back   t11    in which case the gradient for the weight 11 timestep ago will not have decay  of course that mean that at the next time step   t1   the gradient for 11 step ago   t1 11  t10   will be zero because the input be totally disregard at  t10 so on average PRON average out and PRON still have the same situation for lstms   PRON find a very good explanation for this in the paper  lstm  a search space odyssey   httpsarxivorgabs150304069   read chapter 3 if PRON want to understand the history of the trainig algorithm   there be a version of truncated bptt for lstm which be use first  where the cell state be propagate back many step  but the gradient along other part of the lstm be truncate  in later paper also full  gradient  bptt ist use  where the gradient along gate and so on be also backpropagat in time   hope this help PRON guy   cheers  torben 
__label__data-mining __label__missing-data what all option be available for fill miss datum   one obvious choice be the mean  but if the percentage of miss datum be large  PRON will decrease the accuracy   so how do PRON deal with miss value if PRON be in great number   there be a difference between datum with miss value and sparse datum  miss value be generally there because of invalid input  loss or error during datum collection or be create when clean or processing datum   if these value be very less in number  the correspond instance can be ignore or if be around 5  10  of the datum  can be fill use various method  carry forward last observation  fill with mean  median  interpolate the datum etc   if PRON be work in python  go through pandas documentation for work with miss value  to learn in detail about these option  even if PRON be not work in python  this be a good read    but if PRON data set have a large number of miss value  say   30    then the data be sparse  such datum set create various bias in PRON modelling  and there be special way to deal with PRON  though PRON do not about PRON much   there be of course other choice to fill in for miss datum  the median be already mention  and PRON may work better in certain case   there may even be much good alternative  which may be very specific to PRON problem  to find out whether this be the case  PRON must find out more about the nature of PRON miss datum  when PRON understand in detail why data be miss  the probability of come up with a good solution will be much high   PRON may want to start PRON investigation of miss datum by find out whether PRON have informative or non  informative missing  the first category be produce by random datum loss  in this case  the observation with miss value be no different from the one with complete datum  as for informative missing datum  this one tell PRON something about PRON observation  a simple example be a customer record with a miss contract cancellation date mean that this customer s contract have not be cancel so far  PRON usually do not want to fill in informative missing with a mean or a median  but PRON may want to generate a separate feature from PRON   PRON may also find out that there be several kind of miss datum  be produce by different mechanism  in this case  PRON may want to produce default value in different way   when PRON come to miss datum  there be many different method of fill these value  however  the imputation method PRON choose  depend largely on the amount of miss datum and the type of variable  for example  PRON will not impute the mean value for miss categorical datum  PRON would choose the mode instead  no matter which method PRON choose  there will be some bias associate with PRON  one method which do a good job at reduce the bias associate with impute miss value  be multiple imputation  PRON can be quite a long  wind approach but PRON be the most sound approach PRON have see so far to impute large amount of miss value  PRON believe there may be an r library for multiple imputation   of course  another alternative may be that if variable x have 50  miss datum for example  there may be a good explanation as to why this be  rather than try to impute PRON or lose the information associate with the variable  PRON can sometimes be useful to create a new variable  call variablexflagmiss  this would be a binary indicator variable where an observation be cod as 1 if variable x contain a miss value and cod as a 0 if PRON do not   if the value be miss at random and PRON be sure that PRON data matrix be of low rank  PRON can use nuclear norm basis pursuit method  also know as matrix completion   the method  among other  be implement in tfocs   in many real  world application  the datum matrix have rarely full rank  so the assumption of low  rank matrix can be acceptable  on the other hand  the value may not be miss truly at random   another approach would be to use singular spectrum analysis  ssa   also know as the caterpillar algorithm  PRON can be use for time  series datum with miss value  this algorithm be not very well  know but in literature PRON be sometimes call  pca for time  series datum   
__label__time-series __label__matlab __label__model-selection PRON be try to build arima model  PRON have 144 term in PRON standardized time series  which represent residual form original time series  this residual  on which PRON would like to build arima model  be obtain when PRON subtract linear trend and periodical component from original time series  so residual be stochastic component   because of that subtraction PRON model residual like stationary series  d0   so model be arimap  d  qarima0      acf and pacf function of PRON residual be not very clear as case in literature for identification arima model  and when PRON choose parameter p and q accord to criterion that PRON be last value outside of confidence interval  PRON get value p109  q97  matlab give PRON error for this case   error use arima  estimate  line 386   input response series have an insufficient number of observation   on the other side  when PRON be look only to n4 length of time series for identify p and q parameter  PRON get p36  q34  matlab give PRON error for this case  warning  nonlinear inequality constraint be active  standard error may be inaccurate   in arimaestimate at 1113  error use arima  validatemodel  line 1306   the non  seasonal autoregressive polynomial be unstable   error in arima  setlagop  line 391   mdl  validatemodelmdl    error in arima  estimate  line 1181   mdl  setlagopmdl   ar   lagop1 coefficientsiar      lag    0 lagsar      how do PRON need to correct identify p and q parameter and what be wrong here  and wwhat do PRON mean in this partial autocorrelation diagram  why be last value so big   
__label__matlab __label__ode __label__boundary-conditions PRON have a system of couple ode that PRON want to solve  the function be ax   bx   cx   PRON be a boundary value problem  PRON be use matlab bvp4c   so far PRON be not satisfied with PRON solution  for the boundary that be of interest for PRON  the solver fail  matlab return  a singular jacobian encounter    for some other boundary  the result depend of the initial guess  so PRON think that PRON system be ill define  PRON be think of add one constraint but PRON do not find how to implement PRON  how to enforce  intabd xaxbxcxn   PRON could augment PRON system of ode to include one more equation  if PRON let  beginalign   ix   intaxas   bs   csmathrmds   endalign   then  ia   0    ib   n   dotix   ax   bx   cx  and PRON have another boundary value problem that PRON can solve in matlab use bvp4c  
__label__python __label__scikit-learn PRON be try to print a accuracy score and get this message    ltfunction accuracyscore at 0x0000018a76046840gt   so PRON be wonder if PRON be miss a package or be something instal incorrect   code   from sklearn import dataset  from sklearnmetric import accuracyscore  from sklearn import svm  iris  datasetsloadiris    x  irisdata  y iristarget   print  iriskeys   print                 print  irisfeature name     print                 print  iris    print  irisdatashapelegth    leniristarget      clf  svm  svcgamma0001  c100   clffitxy   yhat  clfpredictx   accuracyscorey  yhat   score  accuracyscore  print  score   the reason be that PRON be print the object of function  instead call the function  suppose the name of PRON function be func  PRON be do something like  printfunc   instead try to call PRON by add parenthesis   printfunc     this be how PRON must be   score  accuracyscorey  yhat   print  score   PRON assign empty function to score   PRON entire code should now be   from sklearn import dataset  from sklearnmetric import accuracyscore  from sklearn import svm  iris  datasetsloadiris    x  irisdata  y iristarget   print  iriskeys   print                 print  irisfeature name     print                 print  iris    print  irisdatashapelegth    leniristarget      clf  svm  svcgamma0001  c100   clffitxy   yhat  clfpredictx   score  accuracyscorey  yhat   print  score  
__label__finite-volume __label__navier-stokes __label__projection lately  PRON be read some seminal paper on fractional step algorithms and PRON find this one   kim  d  choi  h a second  order time  accurate finite volume method for unsteady  incompressible flow on hybrid unstructured grids  journal of computational physics  volume 162  issue 2  pp  411  428  2000   and PRON be confuse on as to how to use  define the boundary condition  for the auxiliary variable  in the method propose  specifically  in this paper the  use of a second auxiliary velocity be propose   hatui  and a certain  boundary condition be propose for PRON in equation  19   anyway  the  propose integration method describe by equation  1215  seem not to require   hatui  on the boundary  the rotational part of the momentum  equation  equation  12   be solve for  delta hatu which  PRON think  PRON  can be extrapolate from the cell  center to the face  in specific  the boundary face   and equation  13  only require the surface integration of the left  hand side which only involve the pressure  PRON question be  be the latter assertion  the extrapolation of  delta hatu  correct  if not  how can PRON couple the  boundary condition  hatui   uin1odelta t2 with equation  12  and equation   14   how can be implement   
__label__optimization __label__algorithms __label__data-analysis __label__data-sets PRON be work on a project involve some pattern recognition  for this PRON need to find the maximum value in a huge multidimensional dataset  for example PRON have a discrete 5dimensional space contain 10  10 datum point  of course PRON can just do an exhaustive search but time be of the essence so PRON be search for an time  and memory  efficient algorithm that can help PRON with this  probably something like gradient descent  fyi  the project be do be java   without know more about PRON datum  PRON be not possible to do good than the exhaustive search  this would be memory efficient  but linear in time   if PRON data set do not contain local minima  PRON could simply follow the steep descent  this still have a bad case linear time however   a possibly good solution would be the downhill  simplex algorithm 
__label__classification __label__deep-learning __label__unbalanced-classes PRON be try to do binary classification of news articles  sports  non  sports  use recurrent neural net in tensorflow  the training data be highly skewed  sport  non  sports19    PRON be use cross  entropy as PRON cost function  which treat both class equally   what be the way by which user can penalise one class  or be there any other cost function suitable for this purpose   this have already be answer both in stackoverflow and crossvalidat   the suggestion in both case be to add class weight to the loss function  by multiply logit   lossx  class   weightsclass    xclass   logsumj expxj      for example  in tensorflow PRON could do   ratio  310   5000  310   classweight  tfconstantratio  10  ratio    logit    shape  batchsize  2   weightedlogit  tfmullogit  classweight   shape  batchsize  2   xent  tfnnsoftmaxcrossentropywithlogit   weightedlogit  label  namexentraw   
__label__numerical-analysis __label__quadrature __label__error-estimation PRON be do a nest integral via quadrature  to give a definite example  let say      int0  2dx leftx  int0x dy   2yright      so effectively PRON be integrate  x  x2  from 0 to 2  although obviously PRON actual problem can not be analytically solve like this   PRON implement in code something like this   innerx   quadygt2y  0  x  tolerance  tinner   quadx  innerx   0  2  tolerance  touter   edit  also note  in this example  x appear only as a limit to the inner integral  but PRON actual problem PRON appear in the integrand as well  generally PRON just mean that the inner quad call depend on  x in some way   the point of this example be that for small  x  the first term be big than the second  whereas for large  x  the second term  the inner integration  dominate   this would dictate that PRON could speed up PRON code by set the inner quadrature error tolerance  tinner  large when  x be small  since error on the inner integral at small  x contribute less to PRON overall error  then PRON can increase tinner when PRON get to large  x when this inner quadrature really matter   PRON question be  be there some general prescription for figure out the optimal way to do this kind of thing  be there some way PRON can just choose PRON  global  tolerance  touter  and have tinner pick automatically as a function of  x   PRON have not fully think this through  but PRON seem like tinner should be change by function of the derivative of the inner integral with respect to x for example  let say PRON have a function tinner  ab  d2ydx   PRON can pick value for a and b by define boundary condition at the integral limit  or by set touter  tinner at the value of x where x  innerx   intuitively PRON seem like PRON do not need both a and b  either a1 or b0  but PRON be not sure which be correct   in any case  as PRON be iterate through x PRON can then calculate a new tinner for each x by look at the numerical change in innerx  over the previous iteration  as long PRON x spacing be fairly smooth with respect to innerx  PRON seem like this should work   if the two term in PRON integral have substantially different behavior  the good approach be likely to split the integral into two separate integral  this way  PRON can choose parameter for each integrand separately  
__label__machine-learning __label__image-classification __label__training can anyone recommend a tool to quickly label several hundred image as an input for classification   PRON have 500 microscopy image of cell  PRON want to assign category such as  healthy    dead    sick  manually for a training set and save those to a csv file   basically the same as describe in this question  except PRON do not have proprietary image  so maybe that open up additional possibility   PRON just hack together a very basic helper in python  PRON require that all image be store in a pyton list allimages   import matplotlibpyplot as plt  category    pltion    for i  image in enumerateallimage    pltimshowimage   pltpause005   categoryappendrawinputcategory      checkout labelbox  httpswwwlabelboxio   labelbox be a tool to label any kind of datum  PRON can simply upload datum in a csv file for very basic image classification or segmentation  and can start to label datum with a team  PRON be probably the fast tool to get PRON start with datum labeling   labelbox support basically any datum as long as PRON can be load into a browser  PRON have open source labeling frontend  ie  PRON could create PRON own frontend with basic html and javascript that suit PRON data labeling need  see PRON github repo to learn more  httpsgithubcomlabelboxlabelbox  this be how easy PRON be to setup a project and get start with labeling  
__label__machine-learning __label__prediction __label__evaluation __label__grid-search PRON have a question relate to evaluate out  of  sample prediction   for PRON research PRON want to tune two parameter relate to support vector machines  and use these optimize parameter to predict the hold  out sample as good as possible  to evaluate PRON model PRON obviously have to split PRON datum in a training sample  80   and a hold  out sample  20    when tune the parameter PRON also use 10fold cv  but this only involve the training sample   now PRON think that PRON approach be not super valid  as the 20  80 split of hold  out versus training sample be only do once  and thus may be too subject to randomness  however  PRON feel that this approach be use quite often  for example in httpwwwsciencedirectcomsciencearticlepiis095741740400096x  in PRON early research PRON have do leave  one  out prediction  where PRON use n1 observation and predict the remain observation  so that every single observation be predict once  use the model that be train base on only the other n1 observation   however  in PRON new project this be not possible  because PRON then also need to tune the parameter n time  which would require too much time and would mean PRON lose the interpretation  plot relate to these parameter value   in short  be PRON a bad thing that PRON only split PRON training and hold  out sample once  do anyone have any comment about PRON current approach   in general  PRON be a good idea to split up PRON datum into three set   training set  60  80  of PRON datum   cross  validation set  10  20  of PRON datum   test set  10  20  of PRON datum   when PRON select a model use only a train and test set  PRON be select the model which perform the good on the test set after  this seem reasonable at first  but this be usually an overly optimistic estimate of PRON model s generalization error  this be because PRON be essentially fit PRON model to another parameter  d  where  d be PRON test set   in order to address this problem  PRON can train a model on PRON training set and then choose the model that perform good on PRON cross  validation set  now  this model be no longer fit to the test set and PRON can safely estimate the generalization error on the test set   PRON sound like PRON be already follow this approach in select the model by choose which one perform good on a 10fold cross  validation on PRON training set  in this case  the error on PRON test set should be a good estimate of generalization error   PRON also mention that randomness may affect PRON training due to outlier in PRON training datum    as the 20  80 split of hold  out versus training sample be only do once  and thus may be too subject to randomness   in machine learning  PRON be often assume that PRON datum be iid  independent and identically distribute  from some unknown distribution  on average  PRON should get data point that be representative of this distribution when PRON split up PRON datum  a common practice to better ensure this be to randomly shuffle the entire dataset before splitting up PRON datum  
__label__fortran __label__navier-stokes __label__precision __label__fourier-transform __label__fftw PRON have develop a pseudospectral solver of the navier  stokes equation use fftw  PRON test PRON formulation of right hand side  rhs  of the ns equation against standard trigonometric function  sine  cosine and PRON combination   for example  PRON set  density  sin 5x  xvelocity  5cos 5y  6sin7z  yvelocity  4sin4y  cos x  zvelocity  1  pressure  cos z  supply these value to the solver  PRON compute the rhs of the ns equation  PRON do the same by hand and compare the result with that obtain by the solver  result be to good agreement  the maximum error between the exact answer and that compute by the solver be of the order of e13 for a 128  128  128 grid   next PRON use a different function of the follow form   density  constant1constant2tanhx  constant3tanhx  constant4    xvelocity  0  yvelocity  0  zvelocity  0  temperature  1  pressure gt  from ideal gas equation connect density  temperature and pressure  the density be adjust suitably base on the constant  to have a period of 2pi  on calculate the rhs of the x  momentum navier stokes base on these value give and compare PRON with PRON answer  calculate by hand   PRON obtain a maximum error of the order of e03   further  use these value as initial value of the variable and move forward in time by a runge  kutta 4 scheme  PRON get value of the density that seem to diverge very quickly  after about 30 time step  PRON get nans   be there a specific reason why PRON notice a decrease in precision when non trigonometric periodic function be use   be 1  relate to why PRON code seem to produce unstable result when march forward in time   PRON would not mind paste the code here but PRON be pretty large   PRON think PRON would plot the initial density and PRON variation  but turn out PRON can not as PRON do not have enough reputation to do so   the initial plot  t  00s  be a density plot that look like a rectangular wave with the tanh function use to smoothen the wave at the various corner   at around t  010s  the time step be 001s so  after 10 iteration   PRON develop spike and become non  differentiable  still continuous    there be three issue that be likely to because such problem in pseudospectral method   gibbs oscillation  alias  time step too large  in any case PRON likely develop oscillation in the solution until some point end up with a negative density  result in a nan when compute the pressure or sound speed or some other term  the solution to 3 be obvious  decrease the time step until the time integration be stable  the other two be more nuanc   gibbs oscillation  gibbs oscillation arise when compute the fourier series of discontinuous function  gibbs oscillation arise in the derivative if the function be non  smooth  if PRON have large jump in PRON initial condition then the fourier series will match the value at grid point exactly  but the derivative will have large oscillation  lead to loss of precision in the derivative  right hand side  computation  see the image below for a demonstration of this  the value match but the derivative do not  as a rule of thumb  jump must be smooth out over about 10 grid to prevent this behavior   even if PRON initial condition be smooth on the scale of the grid  the state variable may quickly steepen  in compressible navier  stokes  the viscous term act to prevent shock from form  but if PRON simulation be not sufficiently resolve PRON will still develop jump in PRON simulation  sufficiently resolve mean have grid space small enough to capture the viscous dissipation  which can be estimate by look at the kolmogorov scale  see this pdf  this quickly lead to large  non  physical oscillation and a divergent solution   alias  alias occur in pseudospectral method due to the presence of nonlinear term  eg  u ux  in the evolution equation  computation of the derivative in spectral space assume that PRON be resolve a certain number of wavelength  however  nonlinear term continuously generate high and high wavenumber  in a discrete problem  these high wavenumber be  alias  back to affect the low wavenumber that can actually be represent at the choose resolution  this corrupt the low wavenumber value and can quickly lead to oscillation  non  physical result  and the simulation blow up   a simple demonstration of how nonlinear term generate high wavenumber  and how those wavenumber be alias to low wavenumber be show in the following scenario  see image below    take a grid of 7 point on the interval  01   let  a  cos6pi x   and  b  sin6pi x these term both have  angular  frequency of  6pi the term    ab  cos6pi x  sin6pi x   frac12sin12pi x  have frequency  12pi this frequency can not be resolve on the choose grid  but the value of  ab on the grid be equal to the value of  sin2pi x2  on the grid  so instead of a  12pi frequency component  which be not capture in the discretiz space   the result of  ab appear as a  2pi frequency component   there be multiple option available to prevent aliasing from corrupt PRON result  commonly term dealias  the most common method be zero padding  32rule  effectively increase grid resolution before nonlinear multiplication and then discard the high frequency   23rule truncation  zero out the high wavenumber before nonlinear multiplication  original paper   or filter procedure  eg this method    additionally  there be evidence that for well resolve simulation dealias be not crucial since the high wavenumber component  which may produce alias  be small due to the viscous dissipation   this presentation  pdf  also provide a good overview of dealias  include some of the history  
__label__scikit-learn __label__clustering __label__tfidf PRON be cluster text base on tf  idf feature and dbscan  density base   and try to rank point base on PRON  belong  to the cluster  since PRON clustering be density base and PRON point can spread very randomly  PRON find kernel density estimators relevant   however  the score of kde be very sensitive to the choice of bandwitdh hyperparameter  which PRON could not pre  estimate  most bandwidth value end up with either infinite score for point outside the cluster  of zero for the point in the cluster  PRON need a way to  automatically  choose the bandwidth  so that PRON result will yield score that make sense for the point in the cluster  large value  and the point outside of PRON  small value    PRON try   both silverman and scott factor method to evaluate the bandwidth depend on  point and  feature  both be far from relevant in PRON case  gridsearchcv return the minimal bandwidth in the grid  different kernel type  all the relevant one be similarly sensitive   reduce dimension  but as expect this severely hurt the kde result without make the bandwidth that less sensitive  from sklearnneighbor import kerneldensity   index of point in cluster3  doc  npwhereypr   30   kde  kerneldensitykernelgaussianbandwidth0399fitxtfidfdocstodense      evaluate score on all point  score  npexpkdescoresamplesxtfidftodense      note that there be 2200 feature in the tf  idf  a few dozen point  40  120  in each cluster the kde be fit to  and about 4000 point in total   any idea on anything  even beyond kde  be welcome  thank PRON   gridsearchcv return the minimal bandwidth in the grid  then alter PRON grid to have a lower lower bind  
__label__machine-learning __label__data-mining __label__decision-trees in which case be PRON good to use a decision tree and other case a knn   why use one of PRON in certain case  and the other in different case   by look at PRON functionality  not at the algorithm   anyone have some explanation or reference about this   PRON serve different purpose   knn be unsupervised  decision tree  dt  supervise    knn be supervise learn while k  means be unsupervised  PRON think this answer cause some confusion    knn be use for cluster  dt for classificationboth be use for classification    knn determine neighborhood  so there must be a distance metric  this imply that all feature must be numeric  distance metric may be effect by vary scale between attribute and also high  dimensional space   dt on the other hand predict a class for a give input vector  the attribute may be numeric or nominal   so  if PRON want to find similar example PRON could use knn  if PRON want to classify example PRON could use dt   from sebastian raschka s python machine learning   the main advantage of such a memory  base approach  the knn  be that  the classifier immediately adapt as PRON collect new training datum   however  the downside be that the computational complexity for  classify new sample grow linearly with the number of sample in  the training dataset in the bad  case scenario — unless the dataset have  very few dimension  feature  and the algorithm have be implement  use efficient datum structure such as kd  trees  j h friedman  j  l bentley  and r a finkel  an algorithm for find good match in  logarithmic expect time  acm transaction on mathematical software   toms   33209–226  1977  furthermore  PRON can not discard training  sample since no training step be involve  thus  storage space can  become a challenge if PRON be work with large dataset   the decision tree  however  can rapidly classify new example  PRON be just run a series of boolean comparison   classifier like decision tree  bayesian  back  propagation  support vector machine come under the category of  eager learners   because PRON first build a classification model on the training dataset before be able to actually classify an  unseen  observation from test dataset  the learn model be now  eager   read hungry  to classify previously unseen observation  hence the name   the knn  base classifier  however  do not build any classification model  PRON directly learn from the training instance  observation   PRON start process datum only after PRON be give a test observation to classify  thus  knn come under the category of  lazy learner  approach   base on the above foundational difference  PRON can conclude the following  since knn perform on  the  spot learning  PRON require frequent database lookup  hence  can be computationally expensive  decision tree classifier do not require such lookup as PRON have in  memory classification model ready   since knn perform instance  base learning  a well  tune k can model complex decision space have arbitrarily complicated decision boundary  which be not easily model by other  eager  learner like decision trees    eager  learner work in batch  model one group of training observation at a time  so PRON be not fit for incremental learning  but knn naturally support incremental learning  data stream  since PRON be an instance  base learner   further  knn classifier give test error rate closer to that of bayesian classier  the gold standard   as quote in islr   the bayes error rate be analogous to the irreducible error  PRON would add that decision tree can be use for both classification and regression task  dt on the other hand predict a class in the accept answer would be more specific by describe classification tree which be technically a subtype of the generic dt concept   one referenceignor the bottom layer that discuss specific implementation    from here  httpwwwsimaforecomblogbid624822maindifferencesbetweenclassificationandregressiontree 
__label__eigensystem __label__lapack __label__eigenvalues lapack contain a driver routine to solve dense generalize hermitian positive definite eigenvalue problem of the form  axlambda bx  where  a and  b be both hermitian  and  b be positive definite  PRON be wonder if there be a method  either code or a modification of zhegv  when  b be indefinite  specifically   b will almost certainly be non  singular   PRON know that the driver routine first perform a cholesky decomposition of positive definite  b  and transform PRON into a standard eigenvalue problem  but do this still work when  b be indefinite  PRON would rather use a specialized method instead of the fully general solver because PRON want the realness of the eigenvalue and orthogonality relation for the eigenvector to be guarantee   when  b be indefinite  the eigenvalue may be complex  and there be little advantage exploit the symmetry  instead  one generally use the qz algorithm   edit  if PRON know a priori that all PRON eigenvalue be real  PRON be probably because PRON can establish a priori that some linear combination  c  satb be positive definite  in this case  PRON should consider PRON problem with either  a or  b replace by  c  PRON have the same eigenvector  and the eigenvalue be relate by a moebius transform  
__label__optimization __label__scipy __label__integration PRON have a function   delt1  trial  def fz    return   1  2znpexpdelt  z1z2deltz2delt     PRON also have a variable   import scipyintegrate as integrate  var  integratequadf0050    equal 0040353419593637516  now PRON be try to find the value p such that  integratequadf05p var  and manually PRON can check that PRON be around 0605  PRON define the follow function to be use in optimization   def integralp    return integratequadf05  p0var  however  i get the following result   import scipyoptimize as op  in26   oprootintegral061   out26    fjac  array1      fun  0040353420516861596  message   the iteration be not make good progress  as measure by the n  improvement from the last ten iteration    nfev  18  qtf  array   004035342    r  array   000072888    status  5  success  false  x  array   050002065    in27   opfsolveintegral061   out27   array   050002065    any idea why both root and fsolve may be fail   as stelios mention  PRON seem like the integral from 05 to 0605 be close to 0  and then PRON turn negative  the antiderivative of PRON function be give by    int fz  t1  dz  fracefrac1z   1lefte   1  2zz  e1zz12operatornameeileftfracz1zrightright2z12      keep in mind that PRON do not work for  z0   indeed  PRON original function be not define at  z0   and the limit do not even exist  if PRON make the plot of the antiderivative PRON can see these feature 
__label__finite-element __label__time-integration __label__fenics __label__quantum-mechanics PRON need to solve the schrödinger equation with a time dependent hamiltonian    ihbar fracpartialpartial t  psi   leftfrachbar22mnabla2  frac12  ktx2y2   vrrightpsi    can anybody recommend PRON an efficient numerical method or software package for solve the problem   be the fenic efficient for solve the equation   fenics user have solve this problem before  but keep in mind that fenic do not natively support complex number right now in PRON code  therefore PRON have to make a workaround   see  httpsfenicsprojectorgqa9209howtousecomplexnumbersiterativesolver  httpsfenicsprojectorgqa10671mixedfunctionspacesforcomplexvaluedproblemsin20160  PRON would suggest to try PRON on PRON own  do an expansion of PRON wavefunction in term of spherical harmonic      psimathbf r     sumell  rellr  t    yell 0   thetaphi       note that PRON have set the index  m in  yell m to zero  in order to account for the symmetry of PRON hamiltonian with respect to rotation around the  x  y plane  this make PRON problem essentially two  dimensional  PRON can also write the spherical harmonic in term of legendre polynomial      yell0thetavarphi   sqrtfrac2ell14pi   pellcostheta       insertion into PRON schrödinger equation and project on the spherical hamonic  and PRON will end up with a couple equation for the radial function  rlr  t here   couple  mean couple in  ell  ie the function  rellr  t depend on all the other quantum number  0leq ell leq lmax solve that with standard finite difference  PRON be not that hard   in the insertion  and  projection step above  the only problem appear in evaluate the matrix element with  x2y2 if PRON write PRON as  r2 y10  PRON boil down to an integral over three spherical harmonic  which be relate to the clebsch  gordan or wigner3j coefficient  but PRON be an easy one  for which analytical formulae exist  just google for the buzzword in the previous sentence    if PRON arrive at the work formula and need further assistance  let PRON know   edit  summarize PRON lengthy discussion in the comment section  here be the final equation which be about to be solve numerically      ifracpartialpartial t  uellr  t   leftfrac12  fracpartial2partial r2   fracellell12r2   vrright  uellr  t   qquad qquadqquadqquadqquadquad  frac23 kt  r2   sumellprimemaxell20minell2lmax   left  deltaellellprime   sqrtfrac4pi5  alphaellellprime  rightuellprimer  t      here the coefficient  alphaellellprime which PRON introduce be give by     alphaellellprime     int yastell 0omega  y20omega  yellprime 0omega domega      PRON can also express that more in standard term such as wigner3j symbol  see eg here    note the restriction in the summation index which come from the fact that  0leq ellprime leq lmax  where  lmax be the maximum angular quantum number choose in the numerical representation   
__label__pde __label__finite-volume __label__crank-nicolson PRON be have difficulty with numerically solve the inviscid burger equation  godunov s scheme be use in most of what PRON have find in literature  now PRON question be if use a crank nicolson shceme be wrong or not    fracpartial upartial t   ufracpartial upartial x0   with this bc   at  tgt  0  x0      u1   use a finite volume method    fracpartial   partial tint udv  int ufracpartial upartial xdv0     fracpartial upartial tdelta v   uua    euua    w0    fracpartial upartial t   feuefwuw0     ffracudelta x  and for discretization in time PRON use crank  nicolson   upn1upnfracdelta t2feuefwuwn1fracdelta t2feuefwuwn  the final form of equation be as  upwind scheme      1fracdelta t2feupn1fracdelta t2fw   uwn1   upnfracdelta t2fwuwfeuen     and for the first block     1fracdelta t2feupn1 upnfracdelta t2feuendelta t fwuw  and after that PRON try to solve a set of linear algebraic equation  the result in every time step seem to converge but with time pass the magnitude of velocity be also increase which be an indication of a mistake   if PRON understand correctly  PRON be use a center finite difference in space and the implicit trapezoidal method in time   that scheme be unconditionally absolutely stable  but will generate spurious oscillation   so PRON should expect to see some increase in the maximum value of  u  but PRON should not blow up   if PRON blow up  PRON have an implementation error  bug    PRON will add that this be a lousy method for a purely hyperbolic pde  since PRON generate oscillation and be not very accurate for large time step anyway   thank PRON for PRON clarification   the thomas algorithm solve  ax  b when  eg  a be a tridiagonal matrix  there be other special case PRON believe  but this be not one of PRON   PRON  final form of equation  do not appear to be in this form  nor do PRON look like this be possible   as david ketcheson mention  implicit time marching be not very attractive for problem like this  usually the diffusion term be treat implicitly in time  and for good reason   explicit time marching have numerical stability restriction due to the advection and diffusion term   delta t  lt  c1 fracdelta xmaxu   delta t  lt  c2 fracdelta x2nu  respectively  the latter be often more restrictive  for diffusion dominant problem or when node point be highly cluster   often  the advection term be treat explicitly and the cfl condition be use to determine the large allowable time step when the diffusion term be not a problem   as david ketcheson mention in PRON comment  explicit euler be not a good choice for central differencing since PRON be unconditionally unstable  however  upwinding scheme with explicit time marching be frequently use in high reynolds number cfd  so PRON imagine that PRON be applicable here  however  PRON can not use central differencing since PRON have no diffusion   as for spatial discretization  there be a lot of literature on upwinding scheme  
__label__fortran __label__quadrature __label__mesh-generation __label__computational-physics __label__symmetry PRON have some function  fkx  ky  kz that PRON wish to numerically integrate over a polygon domain  physically  PRON be integrate over the first brillouin zone  bz  of the fcc lattice  a truncate octahedron    PRON goal be to tell fortran what region of  kspace and which point to integrate over  give this shape   PRON can write plane equation and inequality for this region and perhaps stick PRON into a logical structure  then integrate over the whole bz  but that seem inefficient to PRON  PRON seem that PRON should be able to use any available symmetry to pick out a unique bit of the bz  then reflect  rotate  etc PRON answer use those symmetry   PRON be tell that there already exist routine which can do something like this  but PRON be unsure if what PRON google be appropriate  vasp  etc    would anyone be kind enough to suggest an appropriate packages  if one exist  if not  be there perhaps multidimensional integration method PRON should investigate  if so  be there an efficient way to restrict PRON integration domain   another avenue be to simply sum  fk over a uniform grid within the zone  would PRON then be more efficient to pre  calculate a rank3 array with 1 s and 0 be in PRON desire shape  call  calculate and sum  fk only for nonzero element  just try to get a feel for which direction would be most efficient and accurate   PRON would seem to PRON that PRON can subdivide PRON domain into a relatively small set of tetrahedra  then  PRON be trivial to do the integration because there be many good  and pre  tabulate  quadrature rule on tetrahedra that will yield reasonably high accuracy  this be what one do in the finite element method all the time  so there be a lot of information out there  the integral over PRON brillouin zone be then simply the sum of the integral over the tetrahedra PRON have decompose PRON zone into   if the function PRON want to integrate   fkx  ky  kz  happen to be highly oscillatory or otherwise not very smooth  then the solution be to split each of PRON tetrahedra into four small tetrahedra  and if necessary repeat the process  and integrate over each of the now small cell   many plane  wave code use a very simple k  point weighting scheme   generate a uniform mesh in k  space   k  assign each k  point in the irredicuble wedge a weight  wk   q in k  textk and q be symmetric  write integral as  intbz  fk  dk  sumk in k  wk  fk  this be quick and dirty  but PRON usually be not the leading error  for pwdft  at least    PRON can find an example of such an integral be use to sum the electron number give a chemical potential  to find the fermi energy  in quantum espresso s sumkg   PRON believe the code that find the irreducible wedge and generate the k  point weight   wk  be here   quantum espresso can also use tetrahedra  the equivalent integral of sumkg be call sumkt   routines to generate the tetrahedra and uniform mesh in k  space be here  
__label__career hello PRON work as datum scientist for a private company   PRON be interested in work for a nonprofit company  such as a research institute  public or private  or a company that take care of  issue such as  environment  public health   social improvement  even an internet company like wikipedia can be interesting   do anybody know if nonprofit company hire data scientist   PRON see at least five way to approach this problem of find a data scientist position  work specifically at non  profit  non  governmental or similar organization  as PRON describe below  PRON hope that this be helpful   first  and the most obvious  way be to search major job portal  such as indeedcom  dicecom  monstercom  careerbuilder  glassdoor and other  for datum scientist or similar position  such as data analyst  data engineer  quantitative analyst  statistical modeler  or even market researcher   the second and also obvious way be to tap PRON professional social network and research or inquire PRON contact for any potential data science work opportunity in the area of PRON interest   the third way be to search web site  focus on the non  profit and related topic  PRON usually have a job listing or similar section  as well as specialized non  profit job portal  for example  httpencoreorg  httpidealistorg  httpbridgespanorg  httpcgcareersorg  httpopportunityknocksorg  httpfoundationcenterorg  httpthenonprofittimescom  httpphilanthropycom  httpynpnorg  httpphilanthropyjournalorg  httpnonprofitjobsorg  cof jobs  section be not easy to find   httpcareerscouncilofnonprofitsorg  httpnonprofittalentmatchcom   the fourth way be to perform internet search and research on non  profit or similar organization that PRON may be interested in work with  criterion may vary from organization s size to industry focus or geographical location   base on the information present  on PRON website  make note and then approach those organization  directly inquire or apply for position of interest   the fifth way be to consider various non  profit  for  profit and otherwise social good  theme data science  focus organization  initiative and kaggle  like competition  such as datakind  drivendata  datalook and data science for social good   PRON may be a good idea to start with voluntary work and see if that lead to pay position  datakind  that have be mention above  be where PRON would start  especially if PRON live in england  since PRON can register PRON interest online and even do work with PRON as a data scientist for a weekend in a meetup event   there be also various meetup in many city where PRON be likely to find people work on nonprofit company and expand PRON network  
__label__r __label__xgboost this be a really simple example where PRON training datum have a single feature vector  123  and an equivalent target vector  123    PRON can get xgboost to build a regression tree that perfectly fit the datum  but when PRON use PRON to make prediction  the prediction against the training datum  the prediction do not match the target   what give   train  lt matrixc123    target  lt matrixc123    bst  lt xgboostdata  train  nrounds1  label  target  evalmetricrmse   maxdepth3  minchildweight1  gamma0  lambda0  alpha0   predictbst  train    return 065 095 125  xgbplottreef1   model  bst   xgbmodeldttreef1   model  bst   id feature split yes  no missing quality cover tree yes  feature yes  cover yes  quality no  feature no  cover no  quality  1  0  0  f1  25 0  1 0  2  0  1  150  3  0  f1  2  050  leaf  1  075  2  0  1  f1  15 0  3 0  4  0  3  050  2  0  leaf  1  015  leaf  1  045  3  0  2  leaf  na  na  na  na  075  1  0  na  na  na  na  na  na  4  0  3  leaf  na  na  na  na  015  1  0  na  na  na  na  na  na  5  0  4  leaf  na  na  na  na  045  1  0  na  na  na  na  na  na  
__label__r __label__classification __label__logistic-regression PRON be perform a logistic regression on PRON training datum  PRON use the glm function to get the model m now use the below code from this link  PRON calculate auc   testscorelt  predictm  type   responsetest   pred  lt predictiontestscore  testgoodbad   perf  lt performancepredtprfpr  where score be the dependent variable  0 or 1    to score the tpr  true positive rate  and fpr  false positive rate   PRON have to classify the predict probability into 1 or 0   what be the cutoff use for that  how can PRON change PRON   could not find anything useful in this main documentation as well   PRON can not access an r console at the moment to check  but PRON be quite certain the cutoff be 05  if PRON glm model do prediction  PRON first produce real value and then apply the link function on top  to the good of PRON knowledge  PRON can not change PRON inside the glm function  so PRON good bet be probably to check roc  find what the optimal threshold be and use that as cutoff   if PRON be not sure which label rocr take as  ve then check strpredobjlabel  and the great one show be consider  ve  if PRON want to change that then use labelorder argument supply a vector contain ve and  ve label while create prediction object 
__label__tensorflow __label__loss-function PRON have a classification problem with highly imbalanced datum  PRON have read that over and undersampl as well as change the cost for underrepresented categorical output will lead to better fitting  before this be do tensorflow would categorize each input as the majority group  and gain over 90  accuracy  as meaningless as that be    PRON have notice that the log of the inverse percentage of each group have make the good multipli that PRON have try  be there a more standard manipulation for the cost function  be this implement correctly   from collection import counter  count  countercategorytrain   weightsarray     for i in rangenclass    weightsarrayappendmathlogcategorytrainshape0maxcountsi11   classweight  tfconstantweightsarray   weightedlogit  tfmulpr  classweight   cost  tfreducemeantfnnsoftmaxcrossentropywithlogitsweightedlogit  y    optimizer  tftrainadamoptimizerlearningratelearningrateminimizecost   this seem like a good solution for the loss function  PRON have have success with a similar approach recently  but PRON think PRON would want to reorder where PRON multiply in the classweight   think about PRON logically  the classweight will be a constant wrt  the output  so PRON will be carry along and apply to the gradient in the same way PRON be be apply to the cost function  there be one problem though   the way PRON have PRON  the classweight would affect the prediction value  but PRON want PRON to affect the scale of the gradient  if PRON be not wrong PRON think PRON would want to reverse the order of operation    take the cost like normal  error  tfnnsoftmaxcrossentropywithlogitspr  y    scale the cost by the class weight  scalederror  tfmulerror  classweight    reduce  cost  tfreducemeanscalederror   PRON would be very interested to know how this perform in comparison to simply oversampl the underrepresented class  which be more typical  so if PRON gain some insight there post about PRON    interestingly PRON successfully use a very similar technique in a different problem domain just recently  which bring PRON to this post    multi  task learning  find a loss function that  quotignoresquot  certain sample  checkout tfnnweightedcrossentropywithlogit     computes a weighted cross entropy   this be like sigmoidcrossentropywithlogit   except that posweight  allow one to trade off recall and precision by up or down  weight the cost of a positive error relative to a negative error   this should let PRON do what PRON want  
__label__python PRON run the neat python xor example and a neural network PRON find have the follow property   good individual in generation 22 meet fitness threshold  complexity    2  8 number of evaluation  3450  best genome  node   nodegeneid0  type  input  bias00  response4924273  activation  sigmoid   nodegeneid1  type  input  bias00  response4924273  activation  sigmoid   nodegeneid2  type  output  bias00  response4921960812206318  activation  sigmoid   nodegeneid3  type  hide  bias2340619398413843  response4930091408143276  activation  sigmoid   nodegeneid4  type  hidden  bias04674809078722945  response4918937894580367  activation  sigmoid  connection   connectiongenein0  out2  weight10534262789427578  enable  true  innov0   connectiongenein1  out2  weight16479324848766828  enable  true  innov1   connectiongenein0  out3  weight35479647367966765  enable  true  innov2   connectiongenein3  out2  weight5970619032226201  enable  true  innov3   connectiongenein1  out3  weight4634338591600734  enable  true  innov4   connectiongenein0  out4  weight019203601809123594  enable  true  innov5   connectiongenein4  out2  weight06588709650159562  enable  true  innov6   connectiongenein4  out3  weight30010419817357072  enable  true  innov8  node order   4  3   output  expect 000000 get 011990  expect 100000 get 100000  expect 100000 get 092939  expect 000000 get 000395  2   2   0   1  3  4   0  1  2  3  4   however  when try to implement the neural network in python  PRON do not get the same result   import math  def sigmoidx    f  1   1  mathexpx    return f  node0  1  node1  1  node2  0  node3bia  2340619398413843  node4bia  04674809078722945  connection02  10534262789427578  connection12  16479324848766828  connection03  35479647367966765  connection32  5970619032226201  connection13  4634338591600734  connection04  019203601809123594  connection42  06588709650159562  connection43  30010419817357072  def main     activatednode4  sigmoidnode4bias  1   node0  connection04   activatednode3  sigmoidnode3bia  1   activatednode4  connection43   node0  connection03   node1  connection13   activatednode2  sigmoidactivatednode4  connection42   activatednode3  connection32   node1  connection12   node0  connection02   printthe value be    activatednode2   if   name        main      main    for node 1 and 1 PRON get   the value be   030957048732254944  which do not match the expect output above   can anyone please help PRON with some pointer as to what PRON be get wrong   thank PRON   in case anyone have the same issue  please follow the github thread at   httpsgithubcomcodereclaimersneatpythonissues79 
__label__machine-learning __label__nlp __label__deep-learning __label__data-cleaning __label__data-augmentation data augmentation technique for image datum and audio datum  eg speech recognition  have prove successful and be now common   be there library or technique for augment text datum   for example   in   how be PRON    out    how be PRON     how are you     hwo be y ou     how be PRON     how r u      PRON can code certain simple rule like the one PRON have mention in the question  additionally  PRON can use knowledge base like freebase and wordnet to enrich PRON language model  note that this will not necessarily  noisify  PRON datum but would have effect similar to the effect on datum augmentation on say image for downstream task   PRON PRON want some kind of data  set like google spell check datum PRON suggest PRON look into the the wiked error corpus dataset  the corpus consist of more than 12 million sentence with a total of 14 million edit of various type  this edit include  spell error correction  grammatical error correction  stylistic change  all these from the wikipedia correction history  the owner  author  of the data  set describe the datum mining process in this paper  also check this question in quora PRON contain link to various datum  set with spelling error  finally this page can also be useful  
__label__stability __label__advection PRON find PRON kind of counter intuitive  that the result of an advection get more smear out at the border when decrease the timestep  which should make PRON more accurate    let there be a equally space grid  x1  dot  xn with a constant 1d advection  v1 let the initial value be    u00  0  1  1  0  0  with the correspond divergence field    nablacdot  uv    001010  with a timestep of  deltat  1  PRON get for one timestep with the explicit euler forward time scheme and forward difference for the derivative     u10  0  0  1  1  0  but with a small timestep  deltat05  PRON get      u05left00frac12   1  frac12   0right  u1left00frac14   frac12   frac12   frac14right      so while the cfl number require a small timestep for stability  in this case a small number lead to unsharp boundary and decrease the  deltax help   be there some other stablity condition than cfl and peclet number for this problem   PRON solve the 1d  advection equation with c a constant velocity      fracpartial upartial t   cfracpartial upartial x01      when PRON discretize this equation  with an explicit scheme in time and an upwind scheme in space for instance   PRON get      fracuin1   uindelta tfracuin  ui1ndelta x   0 2      but here PRON approximate PRON solution and by do so PRON commit an error  call the truncation error  let PRON find PRON   the key idea be to use the taylor expansion for each of PRON discretiz quantity      uin1   uin  delta t fracpartial upartial tfracdelta t22fracpartial2 u   partial t2odelta t3         ui1n   uin  delta x fracpartial upartial xfracdelta x22fracpartial2 u   partial x2odelta x3      then PRON plug those expression into  2   PRON should find      fracpartial upartial t   cfracpartial upartial x   frac12delta t fracpartial2 u   partial t2   c fracdelta x2  fracpartial2 u   partial x2   0     PRON recognize the first two term  PRON be PRON equation  1  and by put the rest in the right hand size  PRON find that this equation be now equal to      fracpartial upartial t   cfracpartial upartial x err     with   err   frac12delta t fracpartial2 u   partial t   c fracdelta x2  fracpartial2 u   partial x2  3  which be not equation  1   let PRON simplify a little this expression  PRON use the fact that from  1   PRON derive in time      fracpartial2 upartial t   cfracpartial2 upartial tpartial x0     omit mathematical argument  PRON can switch partial derivative and by use  1  again  PRON get      fracpartial2 upartial t   c2fracpartial2 upartial x2  4      plug  4  into  3  and by introduce the cfl number    cflfraccdelta tdelta x PRON should end up with      err  fraccdelta x2fracpartial2 upartial x21cfl      this be the error PRON commit by approximate derivative  the truncation error depend on the second derivative in space which be call a diffusion term  hence a smear effect  think about the heat equation   now PRON understand that when PRON cfl tend to 1  that be to say a large dt for c and dx fix   the error vanish and so PRON solve exactly  1   that be PRON first result  on the other hand  when PRON cfl decrease  small dt   the error get large and PRON solution be smear  as PRON have experience in PRON second example  indeed  decrease dx help because by do so PRON increase PRON cfl even if PRON time step be fix by PRON stability condition  PRON recover the fact the result get good with a refined mesh  fortunately  
__label__evolutionary-algorithms __label__heuristics __label__minimax PRON be currently write an alpha  beta  prune algorithm for a board game  now PRON need to come up with a good evaluation function  the game be a bit like snake and ladder  PRON have to finish the race first   so for a possible feature list PRON come up with follow   field index should be high  in the low field PRON fuel should be high  when come to the end PRON should be low  maximum of  10  require to enter the goal   all  power  up  must be spend to enter the goal  so prioritize PRON  if PRON be possible to enter the goal  a legit move   do PRON   there could be some more for some special case   PRON have read somewhere that PRON be the good  and easy  to combine PRON in a linear function  for example   i  field index  p  power  up  f  fuel  gt  075  i  5  p  025  f  maxfieldindex  i  since PRON can not ask an expert and PRON be not an expert by PRON PRON have nobody to ask if those parameter be good  if PRON have forget something or if PRON have combine the factor correctly   the parameter be not that big of a deal because PRON could use a genetic algorithm or something else to optimize PRON   PRON problem and question be  what do PRON have to do to find out how to put together PRON feature optimally  how can PRON optimize the function  parameter arrangement PRON    base on PRON description  PRON would maximize the follow term   i  maxf  10   maxfieldindex  i   0   assume consumption of one fuel per field  this become negative when PRON have too much fuel  a similar function of p  as spend PRON get more important when approach the goal  as have fuel be probably a good thing in the beginning  PRON could use a term like f similarly for the  power pack   or be PRON rather  weakness pack      PRON would combine the term use a linear function like PRON do and let PRON optimize  PRON may need more such term  maybe PRON be simple to get rid of the power pack when PRON have enough fuel  then something like maxp  f  0  may help   PRON may generate some ad  hoc expression or add some product of PRON term as new term  PRON may want to do this after the coefficient of the simple term have already be optimize  so PRON help the more complex optimization with a good staring point   
__label__neural-networks __label__history in PRON famous book entitle  perceptron  an introduction to computational geometry   minsky and papert show that a perceptron can not solve the xor problem  this contribute to the first ai winter  result in funding cut for neural network  however  now PRON know that a multilayer perceptron can solve the xor problem easily   backprop be not know at the time  but do PRON know about manually build multilayer perceptron  do minsky  amp  papert know that multilayer perceptron could solve xor at the time PRON write the book  albeit not know how to train PRON   there do not appear to be an historicial consensus on this   the wikipedia page on the perceptrons book  which do not come down on either side  give an argument that the ability of mlp to compute any boolean function be widely know at the time  at the very least to mcculloch and pitts    however  this page give an account by someone present at the mit ai lab in 1974  claim that this be not common knowledge there  allude to documentation in  artificial intelligence progress report  research at the laboratory in vision  language  and other problem of intelligence   p31  32  which be claim to support this  
__label__machine-learning __label__neural-network __label__bayesian-networks __label__probability PRON be read some article about bayesian network  PRON come across many occurrence of belief network  PRON want to know  be these both term mean the same  or be there some difference between bayesian network and belief network   both be literally the same  a belief network be the one  where PRON establish a belief that certain event a will occur  give b the network assume the structure of a direct graph  the term bayesian be coin after the name of thomas bayes  
__label__python __label__nlp __label__nltk PRON be try to figure out how to use nltk s cascade chunker as per chapter 7 of the nltk book  unfortunately  PRON be run into a few issue when perform non  trivial chunk measure   let PRON start with this phrase    adventure movie between 2000 and 2015 feature performance by daniel craig   PRON be able to find all the relevant np when PRON use the follow grammar   grammar   np    ltdtgtltjjgtltnngt    however  PRON be not sure how to build nested structure with nltk  the book give the following format  but there be clearly a few thing miss  eg how do one actually specify multiple rule     grammar  r     np    ltdtjjnngt    chunk sequence of dt  jj  nn  pp    ltingtltnpgt     chunk preposition follow by np  vp    ltvbgtltnpppclausegt   chunk verb and PRON argument  clause    ltnpgtltvpgt     chunk np  vp      in PRON case  PRON would like to do something like the following   grammar   medium    ltdtgtltjjgtltnngt   relation    ltvgtltdtgtltjjgtltnngt   entity    ltnngt     PRON occur to PRON that a cfg may be a good fit for this  but PRON only become aware of nltk s support for this function about 5 minute ago  from this question   and PRON do not appear that much documentation for the feature exist   so  assume that PRON would like to use a cascade chunker for PRON task  what syntax would PRON need to use  additionally  be PRON possible for PRON to specify specific word  eg  direct  or  act   when use a chunker   PRON grammar be correct   grammar     medium    ltdtgtltjjgtltnngt   relation    ltvgt      ltdtgtltjjgtltnngt   entity    ltnngt       by specify  relation    ltvgt      ltdtgtltjjgtltnngt   PRON be indicate that there be two way to generate the relation chunk ie   ltvgt   or   ltdtgtltjjgtltnngt   so  grammar     medium    ltdtgtltjjgtltnngt   relation    ltvgt      ltdtgtltjjgtltnngt   entity    ltnngt       chunkpars  nltk  regexpparsergrammar   tag  nltkpostagnltkwordtokenizeadventure movie between 2000 and 2015 feature performance by daniel craig     tree  chunkparserparsetagg   for subtree in treesubtree     if subtreelabel      relation    printrelation    strsubtreeleaf      give  relation     feature    vbg    
__label__python __label__statistics __label__correlation PRON have a dataset with election result and crime rate per city  for each variable PRON have an absolute value  ie total vote  total crime  and a relative value  ie percentage share of vote    PRON want to calculate the correlation coefficient for some variable  but in the process PRON have a question about what value PRON need to use  if relative value or absolute value   first PRON calculate z score for absolute value and then PRON calculate the correlation use excel  PRON also use panda  dataframecorr   and pearsonr from scipystatsstat in python  in order to corroborate result   for example  if PRON use absolute value PRON will get a positive correlation between candidate 1 and candidate 2   x  dfabs cand 1tolist    y  dfabs cand 2tolist    print  pearsonrx  y     095209664861187004  00   however  if PRON use relative one PRON will get a negative correlation   x  dfrel cand 1tolist    y  dfrel cand 2tolist    print  pearsonrx  y     099704737036262991  00   PRON be confused when PRON see both result  and now PRON need some orientation to understand those difference   thank in advance   in general  the correlation coefficient be  invariant to separate change in location and scale in the two variable   in particular  PRON can mix relative with absolute value   however  that only work if PRON scale the variable globally  PRON can not scale every individual data point  here on a city level   if this be a county wide election  PRON could scale the city value by the county population   but PRON sound like PRON crime rate be on a per city level  in this case PRON should scale the vote on a city level as well to make PRON comparable  this will change the correlation coefficient and give a different result than with absolute value  PRON think use percentage be more intuitive in PRON case  
__label__statistics __label__graphs __label__graphical-model PRON be try to find an interesting way to interpret and display a set of datum for the research PRON be work on  column 2  4 show the net change from time 1 to time 2 in antibiotic coverage for different type of bacteria  type a  d   1 mean coverage for that type of bacteria be add  and 1 mean coverage for that bacteria be stop  and 0 mean there be no change in coverage  ie PRON be not cover at time 1 or at time 2  or PRON be cover the same at both time   column 1 show whether bacterial culture be positive or negative  PRON be interested in show graphically the difference in coverage for a  d when culture be positive vs negative  each row represent a different case   PRON will notice that there be no change  0  in a  d in the majority of case  the fact that most case have no change may be the most interesting point  but PRON be not sure how to make that point stand out on a graph either   the graph s PRON have try making have only include the case in which there be at least 1 change  so far PRON have try mostly mostly bar  column graph show the proportion of positive  negative culture with each change  so the column may be b  add with a culture  positive and culture  negative bar show the relative proportion of each to see if the culture make a difference in add b  and the same for b  stop  c  add  c  stop  etc  for each category   for the most part PRON have see that the culture do not really impact whether coverage for each category be add or stop  but the chart PRON be make do not tell that in a convincing way   any thought   culture  a  b  c  d  negative  0  0  0  0  positive  1  0  0  0  positive  0  0  0  0  negative  1  1  1  0  negative  0  1  1  0  negative  0  0  1  0  negative  0  0  0  0  negative  0  0  0  0  positive  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  1  0  0  negative  0  0  0  0  positive  1  0  0  1  positive  0  0  0  0  positive  0  1  1  0  negative  1  1  1  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  1  0  0  negative  0  1  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  positive  0  0  0  0  positive  0  1  1  0  positive  0  0  1  0  negative  0  0  0  0  negative  1  1  1  0  positive  0  0  0  0  positive  1  1  1  0  negative  1  1  1  0  negative  0  0  0  0  negative  0  0  0  0  negative  1  1  1  0  positive  0  0  0  0  negative  0  0  0  0  positive  1  0  0  0  negative  0  1  0  0  negative  0  0  0  0  positive  0  0  0  0  negative  1  1  0  0  negative  0  0  0  0  negative  0  0  1  0  negative  0  0  1  0  positive  0  0  0  0  negative  0  0  0  0  positive  0  0  1  0  negative  0  0  0  0  negative  0  0  0  0  positive  1  0  0  0  negative  0  0  0  0  negative  0  0  1  0  negative  0  0  0  1  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  1  0  negative  0  1  1  0  positive  1  0  0  0  negative  0  0  0  0  negative  0  0  0  0  positive  0  0  0  0  negative  0  0  0  0  negative  1  1  1  0  positive  0  0  0  0  negative  0  0  0  0  negative  1  0  0  0  positive  0  1  0  0  negative  0  0  0  0  positive  1  1  0  0  positive  1  0  0  0  positive  0  0  0  1  positive  0  0  0  0  negative  0  0  0  0  positive  0  1  0  0  positive  1  1  1  0  negative  0  0  1  0  negative  0  0  0  0  negative  0  1  1  0  negative  0  1  0  0  negative  0  0  0  0  positive  0  0  0  0  negative  1  1  1  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  1  1  1  0  positive  0  0  0  0  positive  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  positive  0  0  0  0  negative  0  1  0  0  negative  1  1  0  0  negative  0  0  0  0  positive  1  1  1  0  negative  0  0  0  1  negative  0  0  0  0  negative  0  0  0  0  negative  1  0  0  0  negative  1  0  0  0  negative  1  0  0  0  negative  0  0  0  0  positive  1  0  0  0  positive  0  0  0  0  negative  0  1  0  1  negative  1  1  0  0  negative  1  0  0  0  negative  0  0  0  0  negative  1  0  0  0  negative  1  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  1  1  1  0  negative  0  0  0  0  negative  1  0  0  0  positive  0  0  0  0  negative  0  0  0  0  negative  0  0  0  1  negative  0  0  0  0  negative  0  0  1  0  positive  0  0  0  0  negative  0  0  0  0  negative  0  0  1  0  negative  0  0  0  0  negative  1  0  0  0  negative  0  0  0  0  negative  0  0  0  0  positive  0  0  0  1  positive  1  0  0  0  positive  1  1  1  0  positive  1  0  1  0  negative  1  0  0  0  positive  0  1  0  0  negative  0  0  0  0  positive  0  0  0  0  negative  0  0  0  0  negative  1  1  1  1  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  1  1  0  0  negative  0  0  0  0  negative  1  1  1  0  negative  0  1  0  0  negative  1  0  0  0  positive  0  0  0  0  negative  1  1  0  0  negative  0  0  0  0  positive  0  0  0  0  negative  1  0  0  0  negative  1  0  0  0  negative  1  1  1  0  negative  0  1  0  0  positive  0  0  1  0  negative  0  0  0  0  positive  1  0  0  0  negative  0  0  0  0  positive  0  1  1  0  positive  0  0  0  0  negative  1  1  1  0  negative  1  1  1  0  negative  1  1  1  0  negative  0  0  0  0  negative  0  0  1  0  positive  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  1  1  0  0  positive  0  0  0  1  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  0  0  0  negative  0  1  0  0  positive  1  1  1  0  negative  0  0  0  0  negative  0  0  0  0  positive  0  0  1  0  positive  0  1  0  0  negative  1  0  0  0  negative  1  1  1  0  negative  1  1  1  0  positive  1  0  0  0  positive  0  0  0  0  negative  1  0  0  0  positive  0  1  1  0  negative  0  0  0  0  negative  0  0  0  0  negative  1  0  0  0  positive  0  1  1  0  very first input  please confirm if PRON be really ask about graphs as there be tone of misunderstand between plot and graph as a mathematical object  in case PRON mean visualization and not graph as a math object please let PRON know to edit PRON tag and title   PRON will continue with PRON answer anyway as graph be the right tool for visualization and moreover for analysis of PRON datum even if PRON touch PRON accidentally   PRON can see PRON problem from graph theory point of view in different way  PRON assume case as node and change as edge in simple way  PRON may continue develop the idea to more sophisticated structure   let PRON assume each case be a node in PRON graph  then PRON can model each bacteria with one graph in which there be no edge between two node if the change be zero and an edge if there be  where the weight of edge be either 1 or 1 for different case  then PRON come up with 4 graph on each of which PRON may do statistical analysis accord to network topology   as a more detailed modeling  PRON may consider two node for each case correspond to each timestamp and connect PRON with weight 1 or 1 or not connect PRON in case the change be zero  to avoid weight PRON can simply use direct graph in which an edge go from node of case  x in time  t0  go to node of case  x in time  t1  if the change be one and if PRON be 1  an edge go from node of case  x in time  t1  to node of case  x in time  t0 the topology of connection also tell a lot in this case   in both case the visualization of graph may be intuitive may not  if not PRON can visualize different statistical measure on node  edge  degree  graph PRON  etc   hope PRON help   ok  here be PRON attempt with r  amp  ggplot2  1 simple stacked histogram  2 dodged stack histogram  bacteria  3 dodged histogram  culture  4 dodged histogram  change  5 grouped by number of unique  pattern  change  bacteria  6 grouped by number of unique  pattern  change  bacteria  jitter  7 grouped by number of unique  pattern  change  bacteria  jitter  const width   8 grouped by number of unique  pattern  change  bacteria  jitter  const width   9 table with all pattern  amp  PRON count  remark  the chart show the number of  pattern  do not take into account the fact that the number of  negative  observation be significantly high than the number of  positive  observation  146 vs 57  see the table   so in some sense the comparison be not fair  but this can be fix easily  
__label__pde __label__finite-difference __label__parabolic-pde be the crank  nicolson method appropriate for solve a system of nonlinear parabolic pde like  partial upartial t  adelta u  u4  0    PRON try to apply this method for solve such system but the solution be oscillating  maybe because of a small value of the coefficient of the time derivative  and the implicit euler method calculate a correct solution   this phenomenon be often call  ring  and plague method that be not  lstable   this can be see in this motivate example from hairer  amp  wanner  1999   stiff differential equation solve by radau method    consider the equation    dot y  50  y  cos t     and apply explicit euler with time step near the stability limit  implicit midpoint  or equivalently for this problem  trapezoid rule  aka  crank  nicolson   and implicit euler   the result  show below  illustrate that the  a stable  but not  lstable  implicit midpoint method produce a poor  quality solution   to avoid this problem when solve stiff system  PRON should use an  lstable method such as bdf2  a suitable dirk  or a radau method   see hairer and wanner s second volume for extensive discussion of this topic   theoretically  if the implicit euler method work for this equation   crank  nicolson scheme should also work  let  tau be the step in time and if PRON only consider the temporal discretization  the linearize crank  nicolson scheme be give by     fracun  un1tau   fraca2   delta un delta un1     frac3 un1un224   the above scheme be okay under the condition that the regularity of the equation be good  moreover  spatial approximation do not affect the stability of the scheme and PRON can use both finite difference and finite element method to solve PRON  
__label__python __label__pandas __label__scraping PRON be try to read in a csv file contain some datum  PRON only need to read in specific chunk of row from the file  such as line 15 line 20  line 45line 50  and so on  however  the file contain text and copy write information like  such as © 1990  2016 aar  all right reserve in several place  such line seem to be produce the error valueerror  no column to parse from file  because when PRON just copy line without such information use pdreadcsv    PRON work fine  PRON goal be to automate the process of download these file from the web and read PRON into panda to grab chunk of row and then do some processing with PRON  so PRON can not just manually specify the window of text lack such character   here be what PRON try  pdreadcsvfilenamecsvencod  utf8  skiprow  14  and pdreadcsvfilenamecsvencodingutf16  skiprow  15   after look at similar answer in stack exchange  but this do not work  can anyone give PRON some guidance on this   there be dfdrop command that can be use as follow to remove certain row  in this case  15  amp  16    dfdropdfindex1516     if the row PRON do not need be regular  eg PRON never need row 15  then this be a quick and dirty solution   if PRON only want to drop arbitrary row contain some value  this should do the trick   df  dfdropdfcolumnname   © 1990  2016 aar   
__label__machine-learning __label__r __label__data-mining __label__regression __label__logistic-regression dataset features  insurance underwriting dataset for 8 year   age  location  amount insure  some other feature  medical evidence   not all feature will be available to all applicant   target variable  decision on whether the applicant can be insure  question  what technique can be use and which one would work best   outline a high  level overview  PRON do not think PRON have to go into too much detail as PRON do not have any datum   thing PRON have consider  PRON be think to first slice the datum and analyse PRON in part and see can PRON find  a pattern   regression analysis could be carry out  possible logistic regression   PRON could take a sample and perform hypothesis test   PRON know that this be an ideal machine learn situation  PRON do not have any experience in this field and PRON think PRON be good to stick with method PRON have some knowledge of   PRON know this be very ambiguous  but a little nod in the right direction and PRON would be very appreciative   PRON think PRON need to do couple of test to see what all variable be important with respect to PRON target variableclient can be insure  yes  no   this kind of test be call predictor importance test   as PRON PRON have mention this be sector be new for PRON  PRON would suggest PRON to take all the variable PRON think be useful  convert categorical variable to factor use asfactor   numeric variable to numeric as asnumeric    the reason for explicit transformation be sometimes algorithm can not understand like   test1  lt c1224412   summarytest1   min  1st qu   median  mean 3rd qu   max   1000  1500  2000  2286  3000  4000  test1  lt asfactortest1   summarytest1   1 2 4  2 3 2  now once the data be ready  PRON can give PRON as an input to boruta  by use this PRON get the predictor importance graph   for good understanding PRON can go through this link  or else if PRON want to learn why  how and when PRON can go through this link for different test for different variable   boruta do all the above by PRON and the outcome be set of important feature  with respect to that PRON can feed those respective feature to PRON model for get good result  accuracy   as PRON problem be binary classification  PRON can use the following algorithms   decision trees  random forest  bayesian network  support vector machinessvm   neural network  for all the above algorithm PRON have attach a link for PRON reference in which PRON can find at  least one example for each   hope PRON answer be helpful  mark as answer if PRON get what PRON need  
__label__machine-learning __label__neural-network __label__deep-learning __label__supervised-learning before face this question  PRON always think non  learnable problem be those which the provide datum for the problem have high amount of outlier  those which do not have sufficient feature or those for which the bayes error be large because of have same feature with different label  as PRON can see  PRON seem that the data be fine because the learning should be comparable with human level inference  a human can distinguish between even or odd number by just look at PRON  PRON know that PRON as human begin  do modulus two operation in PRON mind to decide whether a number be even or odd  the feature extraction part  but PRON be do that with just the number PRON  PRON be clear that PRON can not find a decision boundary to be able to generalize because the input have alternative behavior  1 be even 2 be odd  3 be even 4 be odd and all the other number in this manner  PRON want to know this kind of problem  which do not have the mention problem which may because an algorithm not to learn  have any special name   in this discussion  PRON be describe how a neural network that distinguish odd and even number can be construct   PRON question can be rephrase more generally  be there a function that can not be learn by a machine learn algorithm  this question be discuss here and here  both discussion refer to the universal approximation theorem that basically state that any computable function on a give finite range can be approximate by a neural network  so  what be leave be uncomputable function or undecidable problem  these can not be learn by a machine  learn algorithm  
__label__machine-learning __label__classification __label__svm __label__unbalanced-classes PRON have a dataset with several individual and feature  PRON be study behavior over the year  for instance  average or iteration of money gain  job  etc     PRON ultimate goal be to implement a classifier since PRON have a specific feature for every person  which be equal to 0  1  or 2   when PRON first try to implement a svm  PRON end up with bad result because PRON do not have enough datum  feature  PRON have too many number 1 individual and not enough 0 and 2 s  so PRON classifier almost always put people into category 1  therefore  PRON try to increase PRON number of line by separate PRON datum into quarter  ie jan  fev  mar  then apr  may  jun  then jul  aug  sept  and finally oct  nov  dec   PRON be wonder two thing   would that be a good idea  do PRON have to be cautious of a particular hypothesis that could impact PRON result   in case PRON be a good idea  PRON have some datum available for some quarter of the year but sometimes PRON be miss  for instance let PRON imagine PRON do not have  age  available for PRON last quarter   do PRON have to drop the feature  or would PRON be wise to abandon the last quarter  or be PRON possible to make the classifier work despite that lack of information without actually delete anything    would that be a good idea   that be hard to tell from PRON description  PRON be not an immediately bad idea  if PRON result in a good classifier  accord to cross  validation   then PRON have probably work   the main thing that would concern PRON about splitting behaviour datum by quarter and treat as independent be   PRON data sample will very likely be correlate when PRON share a person  PRON can work around this by careful splitting between training and cross  validation  test set  do not make a fully random split  but split by person  any individual record should appear only in one of the training  cross  validation or test set  assume PRON goal be to take similar datum in production from user who be not in PRON current database  and predict PRON class    there could be seasonal variation in the record that reduce the effectiveness of the split  so a  type 1  person s record in apr  jun may look like a  type 0  person from jan  mar   how will PRON receive datum in production  when PRON want to classify new user  if PRON only want to work on single  quarter datum  then PRON new classifier be fine  if PRON have more datum  PRON have to deal with PRON classifier maybe predict different target variable for the same person depend on the quarter  PRON could combine these in some way  but if PRON do so  PRON should also do this in test to see what the impact of do this be  which may be counter  productive  PRON end up with the same number of test example as if PRON have not do the split   PRON may also be ok  perhaps PRON will add some regularisation   do PRON have to be cautious of a particular hypothesis that could impact PRON result   PRON have to be very cautious about test PRON classifier  because PRON could get datum leakage from the cross  validation and test set to the training set  which would make PRON think the classifier be generalise well when in fact PRON be not  the fix for this describe above  split by person when decide train  cv  test split   PRON have some datum available for some quarter of the year but sometimes PRON be miss  for instance let PRON imagine PRON do not have  age  available for PRON last quarter   do PRON have to drop the feature   handling miss datum be a complicated topic in PRON own right  there be lot of option  PRON can start with   if datum be miss at random  ie there be no reason to suspect PRON be relate to the target variable  or only impact certain type of record   PRON can substitute the mean value of that feature from the training set  or impute PRON base on a statistical model from the other feature   if datum be miss for reason that may impact the target variable  then PRON should give that information to the classifier  because PRON may be an important feature in PRON own right  PRON can take the mean or more complex imputed value as before for the original feature  but also PRON should add a new boolean feature  feature x be miss    whether or not PRON should use the partial datum or drop PRON be not possible to say in general  if PRON be not sure  then try both and pick the version with the good cross  validation result  
__label__machine-learning __label__predictive-modeling __label__model-selection PRON know PRON depend on the datum and question ask but imagine a scenario that for a give dataset PRON could either go for a fairly complex nonlinear model  hard to interpret though  give PRON a good prediction power perhaps because the model may see the nonlinearitie present in the datum  or have a simple model  perhaps a linear model or something  with less prediction power but easy to interpret  here be a very good post discuss idea on how to interpret machine learning model   industry  while be very cautious  be slowly become more interested in adopt more complex model  still PRON want to know the trade  off clearly  a data scientist perhaps be the one sit between datum team and decision  maker  and often need to be able to explain these stuff in layman s term   PRON be try to brainstorm here to see what analogy PRON would come up with to describe such trade  off to a non  technical person   interesting question  PRON think that PRON can illustrate this by think about different use case  the one example PRON have hear that PRON like be around lending decision for loan application  that be an algorithm but  because of regulation  PRON can not be strictly  black box   the decision have to be  effectively  interpretable because the bank have to give PRON a reason for decline on the loan  so  there be certainly good algo out there for loan that can give a binary result  but do PRON want a bank to just tell PRON yes or no   another question PRON can ask PRON be whether PRON have a business goal to understand a system in such a way that give PRON information about the input and PRON relationship and how change to those input impact PRON outcome  prediction    a recent example of a problem PRON work on that fall into this case be predict the number of market lead by month  week  day  use spend by channel  tv  radio  digital   here the goal be not just to predict how many lead would be generate give spend but also to have a framework to use to optimize lead generation around spend distribution  ie  what be the most cost effective distribution of spend across tv  radio  and digital to generate the large number of lead   because of this business requirement  a neural network or svm would not have meet PRON goal because  while PRON would have provide a prediction of lead generation  PRON would not have provide the understanding of the input  spend by channel   
__label__finite-element __label__linear-solver __label__discretization __label__svd be there a way to understand what happen when a singular operator be discretiz and invert use the pseudoinverse  say use the svd moore  penrose pseudoinverse    for example  if PRON discretize   nabla u  nabla v   with a finite element method or something  the operator should usually be singular due to the fact that constant be in the nullspace  be PRON know what the pseudoinverse do in such case  ie will PRON project the solution onto  umathbbr  or do PRON effect depend other factor such as the choice of basis  discretization   PRON depend on how PRON define PRON pseudoinverse  if PRON define PRON as the operator that produce the leastl2norm element  then PRON will get that  u  u0c that have the small norm  u0cl2 where  u0  be an arbitrarily choose solution and  c be a constant  in general  of course   u will not have zero mean value  but PRON will be unique   if PRON pseudoinverse be define through svd  then the correspond singular vector will span the nullspace in  uh  so PRON exist  but  give the generality of the question  PRON will have to answer two more question in order to get the answer to PRON   do PRON discrete space actually contain a nullspace  for instance  PRON could use a finite element space with vanish value on the boundary  this be not exclude by PRON current quesation   in that case  PRON operator would actually be definite   be the discrete nullspace a subspace of the continuous subspace  PRON can not come up with an example for the neumann problem  but if PRON look at the ample literature on divergence conform discretization  PRON see that indeed there may be a discrete nullspace which be not in the kernel of the continuous operator   if PRON answer both question to the affirmative  PRON finite element be unisolvent  and PRON discrete subspace be conform  the answer be yes  but this do not tell PRON yet  in what sense PRON be a projection  that be where the choice of PRON basis and also of the discrete inner product define the svd come into play   the moore  penrose pseudoinverse of a matrix have the property that   x  adaggerb  be a least square solution  and that among all least square solution  if there be a nontrivial nullspace of  a    x will be the least square solution that minimize   x 2  when PRON come to discretiz a pde and solve the linear system of equation  ax  b that result from that discretization  use the pseudoinverse will get PRON a least square solution that minimize the norm of  x  but depend on the discretization that PRON have use  or even a change of basis for the linear system of equation   minimize the norm of the vector  x may not be equivalent to minimize the norm of the solution   PRON be easy to construct example of base where this do not work out   if PRON change of basis be orthogonal  then PRON will actually be ok   for example  consider use the discrete fourier transform as a change of basis  
__label__neural-network PRON read somewhere on the stackexchange that a neural network can not approximate the pi number as a function of circle length and radii  maybe this be an incorrect example or wrong information  help PRON to understand   what about the sum or multiplication of any arbitrary number  be there any other specific function neural network can not approximate or not   not sure what PRON mean by nn not approximate  pi  nn can approximate any continuous function  provide PRON contain enough node  one layer suffice   see universalapproximationtheorem  a neural network can approximate any continuous function  provide PRON have at least one hidden layer and use non  linear activation there  this have be prove by the universal approximation theorem   so  there be no exception for specific function  PRON ask   PRON read somewhere on the stackexchange that a neural network can not approximate the pi number as a function of circle length and radii   a neural network to approximate  pi be very easy  possibly what PRON read be that a neural network can not generate new digit of  pi that PRON have not already be show  more on that later     what about the sum or multiplication of any arbitrary number   yes  a neural network can approximate that   be there any other specific function neural network can not approximate or not   no  there be no specific function that a neural network can not approximate   however  there be some important caveat   neural network do not encode the actual function  only numeric approximation  this mean there be practical limit on the range of input for which PRON can achieve a good approximation   a neural network be able to approximate a function in theory be not the same thing as PRON or PRON be able to construct a neural network that approximate that function  there be no known method to construct a neural network by analysis of a function alone  PRON can be do for specific simple function such as xor    the usual way to achieve approximation be to train a neural network by give example datum  the network will approximate to datum PRON have be show  there be no guarantee that this will generalise to new input that PRON have not be train on and approximate the correct output  in fact for certain type of input  output PRON can not possibly do so  for instance  PRON will not learn how to generate the 4th digit of  pi if PRON have be show digit 12356789  the good generalisation result occur for function that have smooth transition between the training example   neural network do not extrapolate well to input outside of the datum PRON have use for training  PRON  fit  to the training datum  imagine a rubber sheet drape over all the point in the training set    neural network do not learn to copy algorithm  only function  so if PRON take a complex algorithm  such as aes encryption  and attempt to train a neural network to perform this give lot of input example  have no real chance of work  now  aes encryption can be consider a function eg  output  encrypt  input  key    so the nn can approximate PRON  but PRON will only do so for the specific input and output PRON have be show  in addition aes do not respond well to approximation  a single bit wrong will because PRON to be a bad encryption  so PRON will not see nn use to encrypt or decrypt in cryptography   the capability of a neural network to approximate be limit by the number of neuron and connection PRON have  more complex function require large network  in order to train a large network on a more complex function take more time and more training datum  PRON could in theory train a neural network to learn a random number generator function  however  that would take an impossible amount of resource  memory to store the network  and time to train PRON against the whole output of the rng  
__label__orange PRON be use orange 3 and when PRON score plot use the scatter plot widget how do PRON view the numerical score associate with the order list of plot  do that feature get remove with the upgrade from 27 to 3   thank   hm  what be the meaning of this score   see  this be why PRON be remove    as PRON recall  PRON be the average probability assign to the correct class by the k  near neighbour classifier on the projection  or something similar  the number be useful for rank projection  but do not have any meaningfyl interpretation or absolute scale  like  070 be good   090 be excellent  hence there be no point in show PRON   
__label__human-like PRON recently read an article about how artificial intelligence replicate human stereotype when apply to biased dataset   what technique exist to prevent bias in artificial intelligence system   although the question be broad  the field of statistical psychology offer many methodology to remove bias from dataset  or rather gather a dataset with minimal unknown bias  this will be the responsility of the programmer  an ai that learn from dataset will not be able to find bias in those dataset   PRON be important to note that ultimately  the statistical method PRON currently use in ml research be just that  statistical method  so when PRON show some  bad behaviour  PRON be not because of problem with the statistical method  but with the datum PRON give PRON  but if the datum PRON give PRON be as  genuine and unfiltered  as PRON get  then PRON probably show something about PRON   from a cognitive science perspective  PRON probably the case that the same heuristic and bias that create stereotype be also the one that make PRON powerful agentsnote the similarity between category and stereotype   so at least at this moment PRON unclear how PRON can segregate desire from undesired behaviour   to combine the point mode above  PRON seem PRON can only either  1  remove  bad content  by curat the datum by hand or by some metric that PRON do not know of yet 2  accept that PRON method will produce ai as  bad as PRON be  because that be what PRON be  and let PRON operate under the knowledge that PRON may produce undesired behavior sometimes   unless PRON have some crazy new theory of mind that PRON can begin to analyze this in a more rigour manner  PRON seem like there be no clear cut solution  
__label__pde __label__numerical-analysis __label__hyperbolic-pde __label__testing PRON read the whole list of this question  where can one obtain good data set  test problem for test algorithm  routine  but the answer be in different area and PRON want to ask a specific area   PRON be look for rest problem for wave equation  telegraph equation or klein  gordon equation  etc  what PRON have in common be that PRON be second  order linear hyperbolic partial differential equation  by testing problem  PRON mean those can show how well PRON numerical algorithm behave  which be exactly the meaning in the similar question PRON list above   the first two can be decompose into a system of first  order advection equation of which there be a number of standard test problem often involve both smooth and discontinuous initial condition and all of which can be solve analytically   for instance this solution be to the linearize acoustic equation   the choice between one or the other be mainly base on what PRON want to do  a spectral method base code for instance would not behave well for discontinuous initial condition while code that tend to be able to capture discontinuous solution do not tend to do smooth solution as well  hence use both initial condition   for the klein  gordon equation this link will give PRON a way to construct analytical solution and test PRON   PRON be not certain about a particular solution that would test a code to PRON limit though  
__label__algorithms __label__data-analysis be there an efficient way to measure similarity  distance between two sequence of rank number  letter  the two sequence be of different length  and only have some element in common   for example  if PRON have three rank order numeric sequence like this   sequence a  123456  sequence b  2345678910  sequence c  6342587109  intuitively  PRON guess sequence a and b be more similar  since PRON have more number in common and the common number have same order in both sequence  sequence a and c be less similar since PRON have less number in common and the common number have difference order in each sequence  damerau  levenshtein distance seem to be an ok option  PRON measure the similarity between two string of letter  which consider insertion  deletion  substitution and adjacent transposition of letter  but PRON also want to consider non  adjacent transposition  ie  more that two letter between two swap letter  for example  in sequence c above   2  and  6  be swap  but PRON be not adjacent letter since there be  3  and  4  in between PRON  damerau  levenshtein distance do not seem to take this into account   
__label__machine-learning __label__python __label__regression __label__decision-trees __label__categorical-data be there a way to take a set of datum that consist of discrete value and predict a continuous value  take for instance datum that look like   sample matrix of jewel datum  color   size  shape    red    largesquare      bluesmallcircle      bluesmallsquare    sample array of price label   999  700  637   can PRON do decision tree regression on this to predict the price of a jewel with a give set of feature  what if some of the datum be continuous  also be there any way PRON can  should pre  process the categorical datum other than onehot encoding   yes  most software implementation of tree will allow PRON to predict a continuous target variable with all binary predictor  this be because the predictor be only use as split  and the prediction come from the average value at a give terminal node  the prediction will not be truly continuous across all terminal node in the same way that linear regression be continuous  but in practice  this be generally not a problem  if PRON tree be under  fitting  not continuous enough  PRON can always add more terminal node  also  one  hot encoding should be sufficient  
__label__neural-network __label__predictive-modeling ripley s nnet package  for example  allow PRON to model count datum use a multi nomial setting but be there a package which preserve the complete information relate to a count   for example  whereas an ordinal multinomial model preserve the ordering of the integer that make up the count  a fully develop model of count datum as a glm such as poisson or negative binomial regression include how large the integer count be in relation to each other   another phrasing may be   what kind of model come closest to combine the advantage of neural network  in term of  as an example  easily model non  linearity in the predictor  and count datum glm  which be good at take into account that the data be in fact a count    PRON skim over a paper recently that aim to use neural networks as poisson regression  the method PRON propose be basically a standard multi  layer perceptron where PRON use a different loss function  namely     e  sumn1ntn  ynlogtn  this be a version without regularization to prevent the overfitting  PRON use regular weight decay   PRON mention that PRON write PRON in r and matlab but PRON do not have a clue if PRON be available online somewhere  but any neural network package where PRON can pass PRON own loss function should suffice   httpwwwmathstatdalcahguneural20comput20amp20applicpdf 
__label__hpc __label__programming-paradigms the opencl programming paradigm promise to be a royalty free open standard for heterogenous computing  should PRON invest PRON time in develop software base on opencl  pro  con   whether PRON should invest PRON time in develop software base on opencl be a question only PRON can answer  if PRON look like PRON have the potential to solve the problem PRON be face right now  and no other open solution do  PRON good course of action be probably to take a risk on implement a small project with PRON   if that go well  PRON can try PRON with large project and so on until PRON either build up enough confidence to standardise on PRON  or discard PRON in favour of some other solution  which may be PRON own proprietary solution  another open solution or even another proprietary solution    the wonderful thing about the open source movement be that because PRON have the source PRON have everything PRON need to fork the project if necessary  even if the community PRON do not give PRON the facility PRON need  there be nothing stop PRON implement those facility yourself  also  if PRON want those facility  there be a distinct possibility that other user may want PRON  so would appreciate PRON if PRON contribute those change back to the core project   not only that  but if PRON make PRON good from PRON perspective  PRON may make PRON good for other  encourage PRON to submit PRON own enhancement and ultimately make the software better for everyone   finally  yes this be a rather generic answer to rather a generic question  to answer more fully  PRON need to know what PRON concern over opencl be  be PRON maturity  community support  ease of use  time need to learn  time to develop  change PRON procedure  when PRON ask about the pros and cons  which other product be PRON attempt to compare opencl to  what research have PRON already do  what feature do PRON need to support PRON heterogeneous computing environment   the question be too broad and vague to be really be answer  however  PRON do see one notable point against opencl  from the point of view of scientific computing  which be rarely emphasize  so far  there have be no effort to produce open source  infrastructure library for opencl  whereas cuda have several excellent option   cublas  cufft  cusparse  thrust  cusp  opencurrent  petsc support  PRON believe this will really hurt opencl since a major facilitator of adoption be high quality  open library   the most important factor be that cuda will remain support only by nvidia hardware   thus  if PRON want to make robust and portable software  opencl be the only option  at most PRON can build around some currently cuda  power library and hope PRON will get extend over opencl in future pull PRON code with PRON   PRON think opencl be currently suffer from a lack of a  champion   for example  if PRON visit the nvidia site right now  12162011   PRON have get several  ken burns effect  style shot on the splash page focus on the scientific  industrial side of gpu computing  and 14th of PRON navigation option point PRON toward thing that will probably end up at cuda  manufacturer sell  gpu computing  server and workstation be sell nvidia solution   compete offer from ati be mix in with the general amd site  hard to find  and not as heavily featured in third  party solution  those solution  and the ability to do opencl base programming certainly exist  but PRON be leave a perception  at least in PRON mind  but in the mind of some other folk PRON have talk to  that the opencl platform s big corporate sponsor have already  quit the field   people use os x for example  be all probably too busy speculate about whether or not a apple workstation will even exist in a year to have faith in PRON push opencl gpu computing   opencl vs what   if the question be opencl vs cuda  PRON see a lot of hand  wringing over this question  and PRON seem crazy to PRON   PRON do not matter   honest   the kernel  where all the hard thinking have to go  be practically identical between the two language  PRON could write macro for PRON favourite editor to do 99  of the work to bounce between opencl and cuda   PRON have to be that way  PRON be low  level control of ultimately pretty similar sort of hardware   once PRON have figure out how to write PRON important kernel in  opencl  cuda   PRON be trivial to port PRON to  cuda  opencl  the boilerplate host code PRON have to write be similar  too  but cuda keep simple case simple   that be why PRON teach cuda in PRON centre  PRON can leap straight into write kernel code  whereas PRON would have to spend 1  2 hour of PRON daylong course just explain the kernel launch stuff for opencl   but even there the difference be not that important  once PRON start do more complicated thing  asynchronous kernel on multiple gpus   PRON be both equally complicated and again PRON can pretty much do a line  by  line translation from one to the other   if PRON be opencl vs the directive  base approach  openacc or hmpp or something  those be probably  hopefully   go to be good way of program these kind of architecture in the future  where PRON can get 90  of the performance for 10  of the work   but which choice will  win  remain to be see and PRON would not recommend spend a lot of time work with those just yet   so PRON would say  between cuda or opencl pick a language that be convenient for PRON and use PRON  and do not worry too much about PRON   the valuable part  figure out how to decompose PRON problem into massively  parallel simd code for small core with very little memory  be go to be pretty easily portable between programming model   if PRON be use nvidia hardware  and PRON probably be  then PRON typically recommend cuda  matt knepley s point about the library be dead  on   if PRON be not  then opencl   one big pro be the number of vendor behind opencl  PRON have some anecdotal experience about this  have meet a research group that spend a large amount of time and effort to develop a fairly complicated cuda code for an nvidia powered system  a year later after the code be develop  the research group get access to a large and fast amd base system but PRON could not use PRON as PRON do not have the  human  resource to port the code   even if the core set of feature of cuda and opencl be almost identical  as jonathandursi well point out   if the original developer be not the one assign with the task of convert the code  the whole port project can be quite time consume   however  there be some official incompatibility between cuda and opencl  most remarkably cuda support c template while opencl do not yet officially support PRON  however  there be an effort make by amd to develop an extension to opencl with support for template and other c feature  more information in this post from amd dev central  PRON hope a future revision of opencl may add this work   at this point in time  early 2012   the awesome library that mattknepley link to be closed source or use template  so PRON will not become available for hardware other than nvidia  at least in the mean time   for someone that be learn gpu  computing  PRON would say that opencl c can be quite difficult  as there be a lot of detail that distract the learner from the basic idea  whereas cuda be simple and straight forward  however  there be tool that make opencl much more simple to learn and use like pyopencl  the python wrapper for opencl  which bring all the python sugar to opencl  note that there be also a pycuda   for instance  the pyopencl demo for add two array be just under 25 line and PRON include  creation of the array on host and device  the datum transfer  the creation of the context and queue  the kernel  how to build and execute the kernel  get the result from the gpu and compare PRON against numpy  see link below    pyopencl  httpmathematiciandesoftwarepyopencl  pycuda  httpmathematiciandesoftwarepycuda  for experienced gpu  programer  here PRON agree with jonathandursi  cuda and opencl be fundamentally the same and there be really no mayor difference  moreover  the hard work of develop an efficient algorithm for gpu be very much language independent  and the opencl support from vendor and the documentation be now much more mature than say 2 year ago  the only point that still make a difference  be that nvidia be really do some great work with PRON support to the cuda community   opencl have the add benefit that PRON can run on cpu and be already support by intel and amd  so PRON do not need to change PRON algorithmic framework if PRON want to take advantage of any available cpu core  PRON be not PRON opinion that opencl be the good solution for a single cpu  multicore orient application as a cpu optimize kernel may look significantly different than a gpu optimize kernel  however  in PRON experience code development do benefit from be able to run on the cpu  
__label__machine-learning __label__deep-learning which be good for beginner in machine learning  the deep learning book write by yoshua bengio or the video and note in cs231n from stanford    httpneuralnetworksanddeeplearningcom  httpwwwdeeplearningbookorg  PRON be the two very popular online free book  the first link even have work deep learning code  
__label__molecular-dynamics PRON want to simulate a molecular system at a certain temperature  PRON would be good if PRON could be implement by nvt molecular dynamic  however  if one be to use a nve simulation  like velocity verlet   how could PRON fix the temperature  or let the temperature only fluctuate a little bit around the desire value   PRON depend on what PRON mean by  nve  if PRON want to do dynamic in a constant energy ensemble  and PRON consider the total system energy to be compose of the sum of kinetic and potential energy of PRON system  one can contemplate three ensemble   nvetot   nvekin and  nvepot  where the total  kinetic and potential energy be conserve  respectively  for all time   temperature  in the md sense  correspond to the instantaneous kinetic energy of a system  however  there be no constraint place on the kinetic energy of the system in the  nvetot ensemble  other than sum with the potential energy give a constant value for all time    in contrast  the isokinetic ensemble   nvekin  require that the kinetic energy  and therefore the instantaneous temperature  remain constant for all time  which appear to be what PRON want  tuckerman discuss the various scheme one can use in PRON book quite comprehensively   in PRON quite limited knowledge  there be no straightforward way to simulate a system with  nveensemble and get a  nearly  constant temperature   PRON could approach this problem by start with an  nvtensemble  set up the system  run energy minimization  generate velocity and use the velocity rescaling method until PRON be close to the wanted temperature  once PRON be close to  twanted  switch to a more gentle method  like berendsen weak coupling or use a thermostat  such as nose  hoover   after the system have thermaliz  PRON can switch to  nve and run the production simulation   PRON do not have the privilege to post any link  but a simple google search with the keyword  nve md simulation temperature control  will provide a good amount of reference  
__label__pde __label__unstructured-mesh __label__heat-transfer PRON want to write a simple simulation for heat conduction in a unstructured triangular mesh   PRON already make PRON work for a structured rectangular grid with the adi method  but now PRON need more complex geometry   with  dxfrackappa delta tdelta x2 and  dyfrackappa delta tdelta y2 PRON can say     dx ti1jn1dx ti1jn12dx2dy1ti  jn1dy ti  j1n1dy ti  j1n1ti  jn     and PRON basically have a penta  diagonal equation system   for the triangular grid  the only thing PRON come up with till now be to calculate the heat  flux  density over every edge  sum PRON up for every cell and add PRON to the temperature of the cell  for PRON this seem to be a explicit method  which  if PRON follow the same behaviour than the ftcs approximation  should be pretty instable   that s why PRON want to come up with an implicit method  but till now  PRON be not able to   have anybody some advice or some estimation on the stability of PRON explained method   
__label__predictive-modeling __label__time-series __label__apache-spark __label__pyspark PRON be new to predictive analytic but PRON be good at programming  like spark  r    PRON have 10 variable and PRON know PRON score be for over a long period of time  PRON want to know which variable will have high score in the future   if PRON be a linear plot  plot between score and time for a variable   PRON can extrapolate the value and find the maxfuturescore  among the 10 variable   but the plot be non  linear  although continuous    any suggestion   thank for PRON help   can PRON show PRON some plot of PRON  PRON really depend on what PRON look  there be a lot of temporal series forecast   PRON will depend on   be there any seasonality   be there a trend   be there a high volatility   on how long do PRON want do to PRON forecast  
__label__scikit-learn __label__svm the documentation say   the loss function to be use  default to ‘ hinge’  which give a  linear svm  the ‘ log’ loss give logistic regression  a probabilistic  classifier   modifiedhuber’ be another smooth loss that bring  tolerance to outlier as well as probability estimate   when PRON use  modifiedhuber  loss function  which classification algorithm be use  be PRON svm  if yes  how come PRON be able to give probability estimate  which be something PRON can not do with hinge loss   
__label__machine-learning __label__text-mining for a multi label  multi class categorization on a social medium dataset  PRON have collect around 5000 sample from the dataset and have manually annotate PRON  5000 sample be label by 3 people  and 1500  of PRON be same post   how should PRON decide the category label now   for example   annotatorid  post  labels    annotator1  post1  a  b  c  annotator2  post1  a  d  e  annotator3  post1  b  d  e  will the label for post1 be a  b  d through majority voting  or  be there some good  commonly use approach   that be one valid way of approach the problem  in PRON final solution  though  PRON will be helpful to quantify the overall inter  rater agreement  for example  cohen s kappa be a commonly  use metric   begineqnarray   kappa  ampamp  fracpope1pe   ampamp  1  frac1po1pe     endeqnarray   where  po and  pe be the amount of agreement PRON observe and that due to chance  respectively  the reason this be important be that the amount of agreement PRON human annotator achieve be a theoretical upper  bind on the performance of PRON machine learn solution — PRON provide context for interpret the performance of PRON algorithmic approach  
__label__data-cleaning __label__normalization __label__etl PRON be curious if anyone can point to some successful extract  transform  load  etl  automation library  paper  or use case for somewhat inhomogenious datum   PRON would be interested to see any exist library deal with scalable etl solution   ideally these would be capable of ingest 1  5 petabyte of datum contain 50 billion record from 100 inhomogenious datum set in ten or hundred of hour run on 4196 core  256 i28xlarge aws machine   PRON really do mean ideally  as PRON would be interested to hear about a system with 10  of this functionality to help reduce PRON team s etl load   otherwise  PRON would be interested to see any book or review article on the subject or high quality research paper  PRON have do a literature review and have only find low quality conference proceeding with dubious claim   PRON have see a few commercial product advertise  but again  these make dubious claim without much evidence of PRON efficacy   the dataset be rectangular and can take the form of fix width file  csv  tsv  and psv  number of field range from 6 to 150 and contain mostly text base information about entity   cardinality be large for individual information  address   but small for specific detail like car type  van  suv  sedan    mapping from abbreviate datum to human readable format be commonly need  as be transformation of record to first  normal  form   as be likely obvious to the cognoscenti  PRON be look for technique that move beyond deterministic method  use some sort of semi  supervised or supervised learning model   PRON know this be a tall order  but PRON be curious to assess the state  of  the  art before embark on some etl automation task to help guide how far to set PRON sight   thank for PRON help   PRON do not think PRON will find anything that check all of PRON requirement  but here be some thing to look at   automated etl mapping  there be a tool call karma start by a team at usc s information sciences institute  PRON learn from PRON etl mapping and help automate future mapping  PRON be the only open source tool PRON be aware of that help automate the etl process  but PRON would be very interested if there be other out there   large scale etl  there be many many tool PRON could look at for the scalability PRON be look for  PRON can personally recommend look at storm and spark  storm be excellent for string a connection of process step together that  give enough resource  can compute in near real time on streaming datum  not too dissimilar spark have a streaming component with a similar use case  but standard spark may fit PRON need good if the datum PRON be need to etl be a fix set to be process once   data storage  PRON may also need to consider where all this datum will live during the etl lifetime  PRON may need something like kafka to deal with large stream of datum  or maybe hdfs to store a static collection of file   just about any etl tool can manage fix width  csv  tsv  or psv input  and just about any tool should be able to manage 100b record   the limit part of the question really have to do with what PRON destination format be  and what disk throughput PRON need   expect throughput on an i24xlarge be 250mb  s   if an 8xlarge be double that  time 32 machine  PRON be look at the ability to write a petabyte in 138 hour   not to mention the time and bandwidth of bring in the source datum in the first place   unless PRON math be completely off  that mean 30 petabytes can get write to disk in about 6 month   PRON seem odd that PRON be look to either normalize or turn into human readable format that much datum  PRON be only go to get big   and even odder that PRON would want to leverage machine learn as part of a transformation  load of that size   PRON solution will need to be on local hardware in order to keep cost reasonable   PRON could not recommend a system  commercial or open source  that would scale to the degree necessary to perform this kind of etl on 30 petabytes in a matter of day   at that scale  PRON would be look into lot of memory  ram back  front ssd  and custom development on fpgas for the actual transformation   of course  if PRON math on the write timing be wrong this whole answer be invalid  
__label__fluid-dynamics __label__fortran __label__diffusion __label__heat-transfer PRON be a cfd student that be try to develop a program where PRON use the finite volume method to calculate the temperature distribution  PRON already understand the philosophy and math of this method  PRON guess that PRON problem be deal with the source term   first of all the equation that PRON be work with be     fracpartial   partial xleft  gamma fracpartial phipartial x  right    fracpartial   partial yleft  gamma fracpartial phipartial y  right    s0  and the result after the discretization process be     apphip  asphis  anphin  aephie  awphiw  b  where     aefracgamma edelta xdelta xe    awfracgamma wdelta xdelta xw    anfracgamma ndelta xdelta xn    asfracgamma sdelta xdelta xs    ap  aeawanas    b  sp  note   intxwxeintysynsdydxapprox sp delta x delta y  the coefficient  ap  as  an  ae  aw will be the same during the iteration process  so the source term  that must go to zero  will only change with the update of  phi  s  to update  phi  s PRON will be use the tdma method  but something be really wrong in PRON program   program  this be the subroutine calct where the coefficient be calculate and the source term must be calculate  the modt be the subroutine that keep insert the boundary condition   subroutine calct  include  commoninc   do i2nim1  do j2njm1  ani  jki  j1snsjdynpi    asi  jki  j1snsjdypsi    aei  jki1jsewjdxepi    awi  jki1jsewjdxpwi    end do  end do  call modt  resort00  do i2nim1  do j2njm1  api  jani  jasi  jaei  jawi  j   resort  resort  ani  jti  j1asi  jti  j1aei  jti1jawi  jti1japi  jti  j   end do  end do  nswpt5  do  n1nswpt  call tdma22nim1njm1it  jt  t   end do  return  end subroutine  tdma algorithm  subroutine tdmaistart  jstart  ni  nj  it  jt  phi   implicit real8  a  h  o  z   integer  i  n   parameter  nn100   dimension phiit  jtannbnncnndnn   common  coef  apnn  nnannn  nnasnn  nnaenn  nnawnn  nnsunn  nnspnn  nn   writetdma activated    nim1ni1  njm1nj1  jstm1jstart1  ajstm100   ccommence w  e sweep  do 100 PRON  istart  nim1  cjstm1phii  jstm1    ccommence s  n traverse  do 101 j  jstart  njm1   cassemble tdma coefficients  ajani  j   bjasi  j   cjaei  jphii1jawi  jphii1jsui  j   djapi  j    ccalculate coefficient of recurrence formula  term1djbjaj1    ajajterm  101 cjcjbjcj1term   cobtain new phis  do 102 jj  jstart  njm1  j  njjstm1jj  102 phii  jajphii  j1cj   100 continue  return  end  
__label__nlp __label__text-mining PRON need a method  algorithm for identify which adjective describe which noun in a sentence   sample input    the product PRON be good however this company have a terrible service   as an output PRON would like to get something like    product  good    service  terrible   could PRON please point PRON to some resource  name  algorithm or library  no matter which programming language  that may help with solve this   
__label__machine-learning __label__visualization __label__scikit-learn __label__data __label__decision-trees PRON be try to understand how to fully understand the decision process of a decision tree classification model build with sklearn  the 2 main aspect PRON be look at be a graphviz representation of the tree and the list of feature importance  what PRON do not understand be how the feature importance be determine in the context of the tree  for example  here be PRON list of feature importance   feature ranking   1  featurea  0300237   featureb  0166800   featurec  0092472   featured  0075009   featuree  0068310   featuref  0067118   featureg  0066510   featureh  0043502   featurei  0040281   featurej  0039006   featurek  0032618   featurel  0008136   featurem  0000000   however  when PRON look at the top of the tree  PRON look like this   in fact  some of the feature that be rank  most important  do not appear until much further down the tree  and the top of the tree be featurej which be one of the low ranked feature  PRON naive assumption would be that the most important feature would be rank near the top of the tree to have the great impact  if that be incorrect  then what be PRON that make a feature  important    variable importance be measure by decrease in model accuracy when the variable be remove  the new decision tree create with the new model without the variable could look very different to the original tree  splitting decision in PRON diagram be do while consider all variable in the model   what variable to split at the root  and other node  be measure by impurity  good purity  eg  everything in the left branch have the same target value  be not a guarantee for good accuracy  PRON datum may be skewed  PRON right branch have more response than PRON left branch  therefore  PRON be no good just correctly classify the left branch  PRON also need to consider the right branch as well  therefore  the splitting variable might or may not be an important variable for overall model accuracy   variable importance be a good measure for variable selection   just because a node be low on the tree do not necessarily mean that PRON be less important  the feature importance in sci  kitlearn be calculate by how purely a node separate the class  gini index   PRON will notice in even in PRON crop tree that a be split three time compare to j s one time and the entropy score  a similar measure of purity as gini  be somewhat high in a node than j  however  if PRON could only choose one node PRON would choose j because that would result in the good prediction  but if PRON be to have the option to have many node make several different decision a would be the good choice   in scikit  learn the feature importance be the decrease in node impurity  the key be that PRON measure the importance only at a node level  then  all the node be weight by how many sample reach that node   so  if only a few sample end up in the left node after the first split  this may not mean that j be the most important feature because the gain on the left node may only affect very few sample  if PRON additionally print out the number of sample in each node PRON may get a good picture of what be go on  
__label__data-mining __label__predictive-modeling as a total beginner PRON be try to apply some  prediction  on top of a bunch of csv file which contain house transaction for the last 20 year divide per area  what PRON would like to predict be the trend of the transaction for let say the next year for a specific area   what general step would PRON follow  to analyse those datum and then predict   PRON read different article but what PRON be look for PRON be a sort of  good general practice  for this sort of problem   regression will work well if PRON data set be large  but only for predict current house price  say  for example  estimate the value of PRON house   that be what people generally mean when PRON talk about predict house price from current house sale datum   the question of how house price will behave in the next year be much  but much more complicated  and would not depend simply on the datum PRON currently have  PRON would need to involve other information and a much more complex model  which would need to involve thing like current level of household debt  inflation rate  economic outlook  etc  daunting   generally  speculative price follow some stochastic process  PRON depend on the current value  but diverge more and more the farther PRON go in the future  
__label__least-squares __label__svd sunny day today  be not PRON  please  PRON need help with PRON problem  PRON have write a program to do 2d ray tomography  accord to this paper   for the result  PRON use formula  415  from the paper  now PRON need to do svd regularization on the result  in order to get good result for low count of ray  let say PRON will have  n box and  n ray   now PRON be purely impossible with the formula  415   PRON be not sure how to understand the regularization  PRON think PRON should be only to add something to the diagonal  right  please can PRON help PRON with PRON   PRON have find the paper  regularize matrix computation  from ae yagle  but PRON be not sure how to implement PRON in PRON case  many thank   first  never calculate an explicit inverse  allthough sometimes PRON can not avoid PRON  most of the time however  the solution can be restate as multiplication of a vector with an inverse matrix  in that case several method exist  which do not exlpicitely calculate the inverse   that say  common regularization technique be truncate singular value decompositionsee truncate svd  or tikhonov regularization  both technique essentially filter small singular value   this be necessary because the error introduce by small singular value to the solution vector can become dominant to that extent  that all digit of the solution vector be off   now have two different technique at hand  the next question be  how to choose the regularization parameter  the idea be to balance the error in the solution vector with the error of the residuum   for PRON problem  the l  curve criterion work usually fine   PRON can highly recommend that paper  PRON describe all step for a calculation of the l  curve criterion start from the svd of the datum matrix   PRON also do a quick comparision between the truncate svd and tikhonov regularization  both give very similiar result if the cut off  regularization parameter be choosen accord to the l  curve criterion  PRON will probably update PRON post later with a short python program  demonstrate both technique   as promise a code snippet calculate tikhonov parameter of an ill pose problem  the tikhonovlcurve routine use the same notation as in the paper     coding  utf8      create on mar 10  2013     import numpy as np  import matplotlibpyplot as pl  import os  import sys  def funx     witch of agnesi funcion   because of the two complex pole near the x  ax at z    PRON epsilon   many chebyshev mode be necessary for approximation  ep  003  return ep   2   x   2  ep   2   def main      set up an ill  pose problem   approximate the witch of agnesi function   up to order 100 with 100 random point  nprandomseed1   n  100   number of chebyshev mode  nn  100   number of datum point which be not gauss quadrature point  x  2  nprandomrandnn   1  100 random point in the interval 11  n  nparangen   nn  xx  npmeshgridn  x   l  npcosnn  nparccosxx    b  funx    ill pose problem state  l c  b   l in this case be PRON data matrix  u  sigma  vh  nplinalgsvdl  fullmatrice  false   do the svd  print  condition of l    sigma0   sigma1   beta  npdotut  b   setup right side of the ill  pose problem  x1  y1  truncationlcurveucopy    sigmacopy    vhcopy    betacopy     x2  y2  xb  yb  tikhonovlcurveucopy    sigmacopy    vhcopy    betacopy      plot the balance between norm of solution vector and norm of residuum vector  fig  plfigure    ax  figaddsubplot111   axloglogx2  y2   labeltikhonov    axloglogx1  y1   labeltruncated svd    axloglogxb  yb   olabelb tikhonov parameter    axsetxlim1e8  1   axsetylim001  1e10   axsetxlabelax  b    axsetylabelx    pllegend    plshow    def truncationlcurveu  sigma  vh  beta    l  nparange90   10   truncate singular value from 100 down to 10 and check the effect on the lcurve  will  ss  npmeshgridl  sigma   f   0  s  1   for i  v in enumeratel    s  nponessigmashape   sv    0    set singular value after cut off to zero   truncation   singular value filter  f    i   s    bbeta  npmeshgridl  beta    xl contain the regularize solution for all l trunction  xl  npdotvht  f  bbeta  ss    the follow calculation be necessary for the l  curve criterion  see publication  nxl  npsumnpabsxl    2  axis0     1   2   norm of the solution vector  axmb  npsumnpabs1  f   bbeta    2  axis0     1   2   norm of the resdiuum vector  return axmb  nxl  def tikhonovlcurveu  sigma  vh  beta     as show in the publication  the kink be between the low and high singular value   let PRON try 1000 regularization parameter in the interval of the singular value  l  nplogspacenplog10sigma1    1  nplog10sigma0    1  1000   will  ss  npmeshgridl  sigma    regularize the singular value with ll2  note if ss  gtgt  will PRON have no effect    if ss be of order of will however  damp kick in   that be why one call both regularization technique   a filter for singular value  f  ss   2   ss   2  will   2     bbeta  npmeshgridl  beta    xl contain the regularize solution for all l parameter  xl  npdotvht  f  bbeta  ss    the follow calculation be necessary for the l  curve criterion  see publication   eg kappa be the curvate of the l  curve on the double log scale  nxl  npsumnpabsxl    2  axis0     1   2    norm of the solution vector  axmb  npsumnpabs1  f   bbeta    2  axis0     1   2   norm of the resdiuum vector  eta  nxl   2  rho  axmb   2  etap  4  l  npsum1  f   f   2  bbeta   2  ss   2  axis0   kappa  2  eta  rho  etap   l   2  etap  rho  2  l  eta  rho  l   4  eta  etap    l   2  eta   2  rho   2     3   2   bi  kappaargmin    lb  lbi   print  best tikhonov parameter   lb   xbl contain the solution where the tikhonov regularization   be near the kink of the l  curve  xbl  npdotvht  sigma   sigma   2  lb   2   beta   nxbl  nplinalgnormxbl   axmbxbl  nplinalgnorm1  sigma   2   sigma   2  lb   2    beta   return axmb  nxl  axmbxbl  nxbl  if   name        main      main   
__label__neural-networks __label__genetic-algorithms PRON be really new to neural network  PRON be try to make a neural network with genetic algorithm which will make a snake learn to look for the food and avoid hit PRON tail   the thing be that PRON think that PRON have do PRON  but as there be no wall the snake learn to go one direction only without make a 180 turn  gif here    PRON have try to incentivate mutation that make PRON turn by decrease the score of the snake that always take the same direction  but PRON do not work  PRON have only make PRON dumb  need more breed to reach another  smart  linear snake   PRON have make a network with 5 input   food position relative to PRON position and direction  2 input  x and y   nearest wall  PRON tail  if PRON turn leave  nearest wall  PRON tail  if PRON do not turn  nearest wall  PRON tail  if PRON turn right  the output be 3  be the first one turn leave  the second do not turn and the third go right  PRON make the snake go the high one of the 3 output   PRON have add 1 hide layer of 8 neuron  input  output premise    the way PRON calculate the score be   each step  1 point   each food eat  10 point   if the snake last too much time without eat food  die   if hit PRON tail  die   then PRON save each time the direction this snake have go  up  right  down  leave  and increment PRON by one  when the snake die  PRON weight the final score by the difference between the low and high value  if the difference be high  PRON receive a big penalty  down to 025 of PRON score   this way if a snake be pretty much linear get a high penalty and if a snake do a cool pattern  get a low penalty   also  PRON keep a record of each time a direction change happen compare to the last direction change  so if a snake keep go in circle do not get a high score because of  cool pattern method  for go all 4 direction   with all this  PRON do not understand why PRON good snake be always the linear one   PRON spawn 20 snake and get the good 4 of each generation when everyone die   for generation PRON use neatapticsjs and for neural network PRON use synapticsjs  PRON have a fiddle here  httpjsfiddlenetllorxgunsct5r  at line 10 PRON can see the network definition  at 211 PRON can see the snake  view   food position and wall  where PRON get the input  and at 164 PRON can see the score weight calculation depend on step take that PRON mention before   all input be normalize from 0 to 1   PRON be sure that PRON be do  not one  but a lot of thing pretty bad  as PRON be a newbie on this  but some light on this will be really cool   PRON observe the scoring system PRON use and compare PRON to the field size and starving rate  the worm either die or do not try enough hard with the prize of 10 of eat  so PRON add the amount to 1000   this do still not introduce the 180 degree turn yet  PRON have to adjust these setting on neat   mutationrate  07   mutationamount  6   which introduce  where the worm optimize PRON path as diagonal and when force to change 90 direction  actually a tiny 180 happen on screen   on same mutationrate and mutationamount  but 10 point PRON once observe a horizontal 180  when the food appear back and diagonal movement be not invent  also PRON observe with this last setup a motion of constant 180s flow where consecutive vertical or horizontal 180 form a two pixel thick line on screen   these observation do not 100  do the same as PRON animation  but do instead introduce various way that be quite near  or can be in certain condition an initiative to such phenomenom   white worm introduce two pixel line  red 90 degree turn with miniature 180   edit  PRON change the nice curve ratio like this   var ratio  075    mindir  maxdir   025  difstepsratio    and around generation 200 there live one worm to be able to move to all direction and make 180 if need   unfortunately PRON die before PRON find screen shoot button from PRON phone and  she happen to become second in the race and thus be leave as a unicorn who exist only once   still remain mystery whether that mutation could survive and become the main specie some day   further remark   PRON notice that when mutation take place in snake code the same pattern be replicate many time  if snake get idea of consecutive turn left or turn right when PRON be long  a suicide with tail be quite often take place   PRON try to taggle this with a high score on snake collision than starvation and around generation  180 the dna of suicidal 360 loop be bear and remain some generation   however  this generation do not evolve to the desire intelligent 180 worm PRON previously have see  instead  the suicide be so rewarding that PRON kill that branch quite soon away   as far as PRON understand  PRON do not give the snake reward for eat fast   in fact   each step  1 point   mean that the slow PRON find food  the good  so why would PRON do turn  consider PRON be dangerous  
__label__reference-request so  let say for the family of the explicit runge  kutta method     yn1   yn  sumi1s bi ki  where     k1  hftn  yn    k2  hftnc2h  yna21k1    vdots    ks  hftncsh  ynas1k1as2k2cdotsas  s1ks1  be there any high  order runge  kutta scheme  preferably more than 3rd order  where either all the node be find on  ci1 for  i2  s or if  cmneq1m2  s then the weight on this node should be zero   bm0   the follow be necessary  though not sufficient  condition for a method to have order  p     sumi1s bi cik1   frac1k             k12dot  p  a method with the property PRON specify would have  for any  kgt1      sumi1s bi cik1   suminotin js bi  where the sum on the right be independent of  k  here  j be the set of index for which  ci0     to attain order three  this sum would have to be equal to both  12  and  13  so such method can have order at most two   note that this be really a statement about quadrature rule  more general than the statement about runge  kutta method    could PRON give PRON a reference of where PRON can find these condition to achieve order p  PRON be not sure if PRON be valid though for all the method  for example  the classic rk4 method that have   b10b205b305b41andc116c213c313c416  then if PRON evaluate for  k4   since PRON fourth  order  the above expression that PRON mention give     sumi1s bi cik1   frac12  neq 14    so  there be no way for example to take  b10  b2  b3  b4  10  and find the proper  ci in order to have 4th order accuracy   thank  
__label__linear-algebra __label__matrices __label__c++ PRON need to implement a matrix inversion of a very big matrix that currently be exceed the memory limit of PRON machine  unfortunately PRON have little memory run on 32bit machine   PRON would like to use a few pc together to get the result  PRON be use linux and PRON have access to several machine with mpi   can PRON suggest a good c library that implement that function  if that be not available  any good article or algorithm on the topic be appreciate as well   at the moment  PRON be read these   towards a distributed gpu  accelerate  matrix inversion  mse 16940  in  place inversion of large matricie  like highperformancemark already point out  one do not need to invert a matrix explicitly to solve linear system  in the levenberg  marquardt algorithm  the matrix be positive definite and symmetric and  consequently  PRON can use the conjugate gradient method to solve the linear system  this avoid the need to compute the exact inverse of a matrix and be generally fast and less memory intensive  PRON should not be a problem at all to solve linear system with a few 100000 unknown with this even on small pc  depend on how dense  sparse PRON matrix be   
__label__optimization __label__monte-carlo __label__graph-theory __label__combinatorics consider a planar graph  where each node be associate with a weight  PRON would like to partition the graph such that the sum of the node weight in each group satisfy a minimum requirement  however  PRON would also like as much  resolution  as possible  that is  PRON want to maximize the number of group  minimize the number of node per group   internal edge should be reward  to avoid long  daisy chain  of node   do anyone have any suggestion as to how PRON can compute an  approximately  optimal solution  PRON instinct be to approach this use monte carlo  but PRON be not sure how PRON would implement PRON here   thank in advance for any insight or comment PRON may have   if all PRON be look for be an approximate solution  PRON would suggest start with one of the well  know graph partitioning package  for example metis  PRON allow PRON to attach weight to node  partition PRON into  p group and check whether PRON minimal weight sum condition be satisfied  if yes  try partition into  p1  group and check again  then repeat until one of PRON group violate the minimal weight sum condition  
__label__machine-learning __label__data-mining __label__predictive-modeling __label__data-cleaning __label__preprocessing what be the generic way to preprocess datum for machine learning and predictive model  what be the sequence of step to be take   PRON be have a dataset with both continous and categorical datum  1centre the datum  for numerical variable  center usually do by subtract mean of the column and some time by minimum value of the column  2scal  scale the datum mean convert range of datum between 0 and 1   PRON be do by different method some follow by divide with range and some follow by divide with varianceunit var   3skewness  check for the skewness of the variable in give datum  if the skewness factor be not around zero then try to perform data tranformation use exponential transformation  box cox and logarithmic etc  4one on n encode  PRON be also call as one hot encoding also PRON be use to encode categorical variable that to nominal variable  or else if PRON can use equilateral encoding etc   for ordinal variable PRON can encode PRON as increase or decrease order  5feature selection and importance  if PRON want to remove unwanted variable in PRON datum then PRON use any feature selection algorithm like recursive feature selection or feature importance by tree etc   6dimensionality reduction  to reduce the number of dimension in PRON datum go with algorithm like principal component analysis  unsupervised  or partial least squaressupervised   and select the number of dimension which can nearly describe variance of PRON datum   7removal of outlier  outlier be the portion of PRON datum PRON be not explore in some situation  to remove outlier PRON can go with technique like spatial sign and some other  technique   8miss value  miss value be the most common problem in datum science  in order to over come to that PRON can impute value use different approach like knn impute and build some model to predict miss datum use other variable   9binn datum  this be like convert continous datum into categorical or interval datum this sound interesting but often lead to loss of valuable information  these be some of the important and basic step of datum preprocess for majority of the algorithm but some algorithm do not need some of the step like  random forest accept factor value  so no need of one on n encoding   xgboost accept miss value etc  
__label__optimization __label__convex-optimization __label__constrained-optimization __label__linear-programming __label__mixed-integer-programming PRON be have an issue solve an lp of the form     min z  ctx    textst    ax geq b    xgeq p   1 leq aij  ll bi   p leq 0   and  c geq 0   the specific problem PRON be run into with the ortool glop solver be   the lp take an insanely long time to solve on a distribute system where each worker have  geq 30  core and roughly  240  gb of memory  to PRON seem a little too long  PRON have solve instance of tsp in less time on small machine that do more computation  with cplex  gurobi via ampl   furthermore  lasso regression be reducible to simplex and can run on similarly sized datum in a fraction of a fraction of the time  latency be extremely important and PRON would love to get this cooking in less than  45–60  minute   PRON have check and double check the formulation generate  and the output  after  sim 6  hour  be that the problem be infeasible by return a solution of  x  0 obviously  this can not be the case since if PRON take  x  b  PRON have a basic feasible solution  which imply the existence of an optimal solution   about the datum    a have  sim 3 cdot 10  6  row and  sim 1 cdot 10  6  column  however   a be also very sparse with no more than  2   nonzero entry per row   unfortunately  PRON be new to the google s ortool and be have difficulty find documentation on the specific object and method in the  python api   PRON guess as to where issue can be arise and possible solution be   if the solver be use two phase simplex or  big m  to find an initial bfs  PRON want to know if PRON can speed up the process by provide an initial bfs to the glop solver  eg  x  b  to speed thing up   if there be not an issue in   1  then the problem must be too hard  doubtful   but  a good formulation may help  sparsity of  a suggest PRON may be able to look at this as a network problem  anyone have any experience with this   can PRON go a step further by use something like dantzig wolfe decomposition to separate the  easy  constraint   x geq p  from the hard one  relate  but PRON be wonder if PRON can also lift the easy constraint into the objective  any experience with this    relate to 3  not sure if column generation work here  but can PRON  do anyone have any experience with PRON for problem of this nature  PRON intuition be that PRON can look at x as a set of pattern since x s be tightly couple and belong to independent variable set        most importantly  if PRON have work with google s ortool before  can PRON point PRON to traditional api doc  not example  for  python  and  if PRON can  can PRON share what PRON experience have be as well as common gotcha and helpful performance improvement tip   last  PRON would love to use an open source solver like ortool rather than pay for something more expensive for this application  if PRON know of another open source solution that exist  please share in the comment  anything that have a decent java  scala  or python api would be awesome  extra point for distribute solution  extra extra point for something that leverage apache spark s distributed environment   thank a lot   
__label__deep-learning __label__deep-network PRON be build model with medical dataset use deep learning method   medical dataset consist of both numerical datum such as age  sex  and image of xray scans1024 x 1024    labels consist of type of cancer   PRON believe that age and sex go to affect output of network   but include image will make the network bias towards image  because image will occupy most of the input layer   how can PRON design the input layer of network   additional information  PRON be not use cnn but normal deep learning network with two hidden layer  PRON would give some advice here   PRON should probably use convolutional neural network for image datum or PRON will end up with too many input feature  and too much computation  what be PRON constraint on this point   PRON would not mix image datum with numerical datum  maybe PRON can implement two neural network  one handle image datum and another manage the numerical datum  each one output PRON result  a second step would be to use PRON result to handcraft a global result or let another nn manage this  
__label__data-mining __label__ab-test PRON be work on a plan sequence of n independent a  b testsrun a maximum of n test or stop earlier if a good improvement be find  and in order to keep the significance level within an acceptable level005  PRON be consider way of control the false positive rate while keep the sample size as low as PRON canit be for a low traffic website   PRON be aware of the bonferroni  benjiamimi  hochberg and relate method design to control the proportion of false positive in the multiple comparison situation  however the problem of compute the sample size still remain  the simple approach seem the one of compute the require sample size start from alpha  005n as dictate by the bonferroni correction and then perhaps use benjiamimi  hochberg for the actual testing  be there a good way of compute the sample size when benjiamimi  hochberg be use   further as the a  b test be design to test possible improvement PRON be think that if the sequential test be base on independent sample then then probability of two consecutive false positive would be alpha2  0025  PRON understanding be that the sequential test would be independent if the respective sample do not refer to the same user  these could for example be user that join the website after the previous test   if the above thinking make some sense  PRON could use a  weak  bonferroni  ie alpha   alphan02   correction in order to run test with a reasonable power and low sample size and once a positive be foundthe null hypothesis be reject   PRON could repeat the test and if PRON be positive again then the result be positive with p  lt  0025 which mean that PRON would accept the result  this approach would allow for some control over the sample size but  be PRON sound   any opinion would be greatly appreciate  thank in advance  amar    run a maximum of n test or stop earlier if a good improvement be find  do not do that  do not stop before n test have be run if PRON like what PRON have see so far   httpwwwevanmillerorghownottorunanabtesthtml  in general  however  figure out how big an n PRON need give an expect effect size and alpha level be the domain of power analysis  read more here   httpwwwutaedufacultysawasthistatisticsstpowanhtml  like emre say  if PRON have a certain type PRON and type ii error rate that need to be meet  PRON can consider the sequential probability ratio test or other method like distilled sensing   if PRON be conduct the a  b test in an online fashion  ie decision need to be make for each location as PRON arrive  then PRON can consider some online fdr procedure  generalize alpha  invest method  like lord  lond   alphainvest or spending and gai   bonferroni type correction be for fwer control and tend to be overly conservative  
__label__machine-learning __label__r __label__logistic-regression currently  PRON be build a credit rating model base on logistic regression and face a problem with insert panel variable in PRON  be PRON possible to do so in logistic regression and if PRON be  what package and function can deal with PRON   
__label__machine-learning __label__classification __label__naive-bayes-classifier say PRON have the follow training set for a naive bayes algorithm   outlook  person  play golf         sunny  joe  yes  sunny  mary  yes  rain  joe  yes  rain  mary  no  rain  harry  yes  if try to predict whether harry will play golf on a sunny day  which PRON have no data for    would PRON be correct to exclude the person attribute and use the remain outlook attribute to calculate the probability of this happen  or could that potentially because problem with a large datum set that PRON be unaware of   among naive bayes assumption the main one be that feature be conditionally independent  for PRON problem PRON would have     pplayoutlook  person  propto pplaypoutlookplaypnameplay  to address question be harry go to play on a sunny day   PRON have to compute the following     pyessunny  harry   pyespsunnyyespharryyes    pnosunny  harry   pnopsunnynopharryno  and choose the probability with big value   that be what theory say  to address PRON question PRON will rephrase the main assumption of naive bayes  the assumption that feature be independent give the output mean basically that the information give by joint distribution can be obtain by product of marginal  in plain english  assume PRON can find if harry play on sunny day if PRON only know how much harry play in general and how much anybody play on sunny day  as PRON can see  PRON simply PRON would not use the fact that harry play on sunny day even if PRON would have have that record in PRON datum  simply because naive bayes assume there be no useful information in the interaction between the feature  and this be the precise meaning of conditional independence  which naive bayes rely upon   that say if PRON would want to use the interaction of feature than PRON would have either to use a different model  or simply add a new combine feature like a concatenation of factor of name and outlook   as a conclusion when PRON do not include name in PRON input feature PRON will have a general wisdom classifier like everybody play no matter outlook  since most of the instance have play  yes  if PRON include the name in PRON input variable PRON allow to alter that general wisdom with something specific to player  so PRON classifi wisdom would look like player prefer in general to play  no matter outlook  but marry like less to play less on rainy   there be however a potential problem with naive baye on PRON data set  this problem be relate with the potential big number of level for variable name  in order to approximate the probability there be a general thing that happen  more datum  good estimate  this probably would happen with variable outlook  since there be two level and add more datum would probably not increase number of level  so the estimate for outlook would be probably good with more datum  however for name PRON will not have the same situation  add more instance would be possible perhaps only by add more name  which mean that on average the number of instance for each name would be relatively stable  and if PRON would have a single instance  like PRON be the case for harry  PRON do not have enough datum to estimate  pharryno  as PRON happen this problem can be alleviate use smoothing  perhaps laplace smoothing  or a more general for like lindstone  be very helpful  the reason be that estimate base on maximum likelihood have big problem with case like that   PRON hope PRON answer at least partially PRON question  
__label__iterative-method __label__nonlinear-algebra __label__roots PRON appologize in advance if this question be silly   PRON need to compute the root of  beginequation   u fu   0  endequation   where  u be a real vector and  fu be a real  vector value function   PRON start with newton s method  which work   but then realize a much simple method would be an iterative solution  beginequation   ui1   fui    endequation   this be much quick and apparently as accurate  stable as newton s method   now the question   be this the correct approach or should PRON use a different method   be there anything that can be say about PRON be convergence rate  stability  acc  etc   be PRON globally convergent   thank PRON all in advance for the attention   the feigenbaum fractal be a good example of how strange fixpoint iteration can be   httpenwikipediaorgwikifeigenbaumfractal  httpenwikipediaorgwikifilelogisticbifurcationmaphighresolutionpng  the second link plot the behavior of fixpoint iteration apply to the logistic map as one of the parameter vary   for certain value PRON converge  though only linearly   for other value PRON converge to a cycle of vary length   for yet another class of value  PRON behave completely chaotically   in other word  the behavior of fixpoint iteration depend entirely on the function in question   even function that look similar may exhibit radially different behavior   note  as jed point out  newton iteration can be equally weird   this method be correct and PRON be call  successive substitution   please  look at page 189 of this reference for detail   the banach fix  point theorem describe the standard situation when a fix  point iteration be globally convergent  especially the uniqueness part of the theorem indicate that PRON can only expect local convergence if the solution be not unique   most situation of local convergence can be explain by this theorem  at least in theory  this be even true for the convergence occur in some of the fractal mention above  PRON be just that the theorem have to be apply to  fn  fcirc ldotscirc f instead of  f at some basin of attraction   if  qfxlt1   where  x be the solution  the fix point iteration PRON talk about be locally linearly convergent with convergence rate  q thus if  q be small or zero  the method be competitive with newton s method   far away from the solution  convergence be difficult to predict in the absence of global information  such as a lipschitz constant   lt1   which produce a contraction    PRON may consider useful this reference  a homotopy for solve large  sparse and structured fixed point problems  r saigal  mathematic of operations research  vol  8  no  4  November  1983   pp  557  578  
__label__matrices __label__lapack __label__factorization rather than  gesv   solve  ax  b  gemm   compute  m  bx   somehow PRON feel there be good way to compute  m with lapack  mkl   without special property of matrix PRON can not have a smart decomposition or think of use special method of resolution for the linear system  for example iterative method with preconditioning technique to mitigate the ill  conditioning  always with in mind wolfgang bangerth s comment    consider toepliz ’s matrix PRON meet sometimes   from a more link to numerical library perspective for what concern lapack certainly until early 2013  PRON be not available a special function for toepliz ’s matrix  see lapack archive  also now PRON do not find dedicate function  about mkl PRON do not know special function for toepliz ’s matrix   perhaps PRON may be useful these function of netlib  so PRON remain in an environment similar to lapack  mkl  for toepliz ’s matrix   if numerical stability and robustness be PRON only concern  PRON may try compute the svd of  a  and use that to compute the product  specifically  let the svd of  a be     a  u sigma vt  then    b a1b   bvsigma1utb  this may even work in case where  a be ill  condition  but  b a1  b be not  due to the space on which  b and  bt span  and PRON interaction with the dominant subspace of  a  and if PRON fail  PRON can look and see how exactly PRON be fail base on which mode that be be amplify  then decide what to do from there   compute the svd be  of course  very expensive  in this case avoid divide and conquer svd algorithm implementation since those  although often faster to compute  tend to be less accurate for the very small singular value  
__label__bigdata __label__data-mining PRON be work on multiclass logistic regression model with a large number of feature  numfeatures  100   use a maximum likelihood estimation base on the cost function and gradient  the fmincg algorithm solve the problem quickly  however  PRON be also experiment with a different cost function and do not have a gradient   be there a good way to speed up the calculation process  eg  be there a different algorithm or fmincg set that PRON can use   if PRON do not have a gradient available  but the problem be convex  PRON can use the nelder  mead simplex method  PRON be available in most optimization package  for example in scipyoptimize   conjugate gradient  the cg in function minimization  nonlinear  conjugate gradiant  require PRON to have a gradient function  or approximation  since that be a critical part of the algorithm PRON  PRON need to find the steep descent direction quickly   fminsearch implement nelder  mead  a nonlinear gradient  free method  PRON convergence property be not anywhere near as good   what be PRON cost function  be there approximation that be differentiable  pref  twice so PRON can use the very powerful quasi  newton method    PRON have be able to optimize very strange function with simulate annealing  and PRON do not require a gradient  instead  PRON use random number in a way very similar to markov chain monte carlo  which help PRON avoid get stick in local optima  a decent explanation that give the intuition behind PRON can be find in this lecture  simulate annealing  scipy 014 include this algorithm in PRON optimization module  scipyoptimizeanneal  
__label__petsc __label__eigenvalues PRON be work on the flow stability problem  in this work the main complication be solve generalize eigenvalue problem for a large scale non  hermitian matrix  PRON need only one eigenvalue  most leave on real axis   so PRON use shift  invert transformation  PRON use slepc library to solve this problem  especially krylov  shur method with mumps lu decomposition  PRON work fine for small problem  matrix size up to 500k   but real case give PRON matrix size about 15 m  so PRON just do not have such amount of memory to solve these case with mump  PRON try to find an iterative method for PRON problem  method with small memory requirement   but PRON do not work for PRON  generalized davidson and jacobi  davidson be not converge  also PRON try iterative linear solver instead of direct mump for krylov  shur method  but PRON also be not converge  PRON concentrate on one relatively small problem  about 70k matrix size  that solve fine use mump and can not find any iterative method to solve PRON   be PRON possible that iterative eigenvalue solver  or solver with small memory requirement  do not exist for PRON matrix  PRON will very appreciate if someone give PRON an advice what PRON can do more to solve PRON problem   
__label__data-mining in the datum  there be 355 observation include one continuous dependent variable  y  range from 15  55  and 12 independent variable  continuous  categorical  and ordinal   the x1  2 level  and x6  3 level  be consider as categorical variable  here be some question that PRON have   can PRON assume that all the coefficient  except x1 and x6 which be categorical  be linear with respect to y   can PRON consider x5 as continuous variable  however  PRON be ordinal and range from  1  7    can PRON get the x7  year  as continuous variable  however  PRON ’ ordinal and rage from 2002  2006  in fact  year of datum per se do not improve the response  PRON be the other factor occur in the same time period which result in improvement and PRON do not know those factor   do this approach seem logical    in general if PRON use different transformation on independent variable such as log  squared  square root  and inverse  do PRON need to standardize the datum also   here be the scatter plot   any feedback and insight would be highly appreciate  thank PRON  PRON do not think  can  be the right question to ask  PRON be not go to give PRON a syntax error  the right question be  what could go wrong    any modeling technique will have assumption that may be break  and know how those assumption impact the result will help PRON know what to look for  and how much to care when those assumption be break    the good test of whether or not linearity be appropriate be whether the residual be white or structure  for example  PRON look like x9 may have a nonlinear relationship with y but that may be an artifact of the interaction between x9 and other variable  especially categorical variable  fit PRON full model  then plot the residual against x9 and see what PRON look like   treat PRON as continuous will not because serious problem  but PRON may want to think about what this imply  be the relationship between 1 and 2 in the same direction and half the strength as the relationship between 2 and 4  if not  PRON may want to transform this to a scale where PRON do think the difference be linear   same as 2  except PRON be even more reasonable to see time as linear   standardization be not necessary for most linear regression technique  as PRON contain PRON own standardization  the primary exception be technique that use regularization  where the scale of the parameter be relevant   PRON be also worth point out that multivariate linear relationship  while PRON can capture general trend well  be very poor at capture logical trend  for example  look at x3 and x4  PRON could very well be that there be rule like y  x3 and y  x4 in place  which be hint at but not capture by linear regression   thanks matthew   there be a confusion to PRON  PRON mean PRON will build the model through multiple linear regression method and then the residual will be check vs the significant factor to see if PRON be structureless  PRON should not first check whether or not the variable  coefficient  be linear with respect to y  since the x9 and x5 be not statistically significant  shall PRON check the scatter plot of residual vs those two   what be PRON insight about the scatter plot of residual vs time  do PRON think PRON still need any transformation  if yes  can PRON explain the reason   here be the scatterplot of residual vs significant variable and different residuals plot vs y to check if the normality assumption be meet   please let PRON know PRON feedback  on the other hand  can PRON explain a bit more what PRON last paragraph meant  note that both x3 and x4 be statistically significant    thank PRON again   in fact  PRON use additive method to calculate the x3 value  x3 be the summation of 21 binary variable  and multiplicative method to calculate x4  x4 be the multiplication of 3 continue variable   PRON be wonder if this information will change the way of PRON calculation   here be the residual plot versus x7  x5  and x9 plus probability plot of residual for the x7  time or year   PRON can be discover that the residual of x7 in each time horizon be normal   however  PRON be still confused about this point  PRON mean PRON will build the model through multiple linear regression and then the residual will be check vs the significant or none  significant factor to see if PRON be structureless and follow the normal distribution  PRON should not first check whether or not the variable  coefficient  be linear with respect to y  thank PRON for PRON feedback  
__label__r __label__categorical-data __label__encoding PRON be try to do one hot encoding on a data set contain 4 categorical feature in r unique level per feature   400  60  6  5  respectively  PRON get the follow error during the first call to the ohe function  acmdisjonctif    error in sortlisty    x  must be atomic for  sortlist   have PRON call  sort  on a list   PRON can not do sort on a per column basis because PRON will remove the datum relationship between column   what be PRON miss in understanding   PRON search on the web but could not find anything helpful   here be the code PRON be run in rstudio   ohefeat  crsncd    rsncat    eligsyscd    prodcd    for  f in ohefeat    dfalldummy  acmdisjonctifasfactornodupsf      package  ade4  nodupsf   null  nodup  cbindnodup  dfalldummy     PRON may be miss the point  but PRON do not need to do this one variable at a time in a loop and then bind the column together  that be not the r way   here be an alternative    create some simulate datum  n  lt 10000  x  lt dataframefa  factorsample1400  n  replace  true     fb   factorsample160  n  replace  true     fc   factorsample16  n  replace  true     fd   factorsample15  n  replace  true     y  rep1  n     use r s inbuilt understanding of formula to do the  one hot encoding  for PRON  xonehot  lt modelmatrixy    datum  x  
__label__machine-learning __label__neural-network what be the default number of internal layer and internal node in train a neural network   PRON datum have 62 observation with roughly 200 predictor  PRON have a target variable with two class  PRON have implement neural network with one internal layer and one internal node without repeat and PRON have also try with two internal layer with 5 internal node in one and 2 internal node in second layer  PRON want to find the accuracy first on default value and then PRON will try to optimise the model performance   what be the criterion to choose the number of layer and internal node in neural network training model  in case of random forest PRON can choose try to be roughly equal to square root of the number of predictor   there be website that explain these pretty well   decide on the number of neuron in the hide layers   from httpswwwrbloggerscomselectingthenumberofneuronsinthehiddenlayerofaneuralnetwork   the most common rule of thumb be to choose a number of hide neuron between 1 and the number of input variable   decide on the number of layer of hidden layer  from httpsstatsstackexchangecomquestions181howtochoosethenumberofhiddenlayersandnodesinafeedforwardneuralnetw   for most problem  one could probably get decent performance  even without a second optimization step  by set the hidden layer configuration use just two rule   i  number of hide layer equal one  and  ii  the number of neuron in that layer be the mean of the neuron in the input and output layer   hope that answer PRON question   one potential approach can be iterative design of a neural network architecture such as multi  layer perceptron  mlp  as describe in the following post   httpsstatsstackexchangecomquestions238637deepneuralnetworktuninghyperparameter  PRON can restrict PRON to 4  8 layer with 8  128  power of 2  neuron per layer  in addition  PRON can assume recommend relu activation with PRON normal weight initialization and adam or sgd with nesterov momentum optimizer   in order to avoid overfitt on a small dataset  PRON be important to add l1 or l2 regularization  weight decay  and a dropout layer  eg with keep probability of 05    PRON can then use cross validation with random search or bayesian optimization to choose the good architecture as describe in the cross validated article above  
__label__classification __label__multilabel-classification __label__score PRON be search for a score to compare two different classification of the same observation   PRON be think about adjusted rand index or adjusted mutual information  but the problem be that the first classification split the observation in 4 class  the second in 5 class  if PRON understand these metric correctly  the number of class should be the same between the two classification  only permutation of the label be allow    the idea be that  if the score be between 0  no concordance  to 1  full concordance  PRON may get a result like this    permutation be ok  score   1234    2134      1   subset be ok  score   1234    1123      1   this should be less than 1  last item in the second classification   may be 12 but not 3   score   1121    1233     lt   1  be there a similar score to evaluate concordance of two classification with different number of class   
__label__stability __label__nonlinear-equations let PRON assume PRON have an a  stable numerical scheme  PRON believe that give any linear equation  y   ay  PRON mean that the numerical scheme apply to this equation be stable  and therefore convergent since PRON be consistent  if the eigenvalue of a have a negative real part   PRON question be then  do this result extend to the non  linear case  PRON be interested in particular in a system  y   fy  t where the jacobian of  f have negative real eigenvalue  forall y  edit  also PRON guess PRON could give a particular example here  but PRON question be not limit to that case   for example let PRON say the ode describe a 2nd order chemical reaction network  ie  f in rn rightarrow rn be a polynomial function of the  y of order 2  where  n be the number of specie  consider the network PRON be able to prove that the eigenvalue of the jacobian be real negative for any  y  however PRON do not know if PRON can make the further argument that any a  stable method will converge  by the way  what about l  stable method  b  stable method      PRON be go to answer a more general question than the one PRON ask  do the eigenvalue of an initial value ode determine the stability of the solution   here PRON be refer to mathematical stability  not numerical stability   of course  a  yes  to this question be a necessary condition for a  yes  to PRON question   and unfortunately  the answer be  no    in general  for nonlinear andor non  autonomous problem  study the eigenvalue of a frozen linearization  ie the jacobian with  t fix  give very useful insight   however  there be two major caveat   for non  normal matrix  eigenvalue do not tell the whole story  and one must consider pseudospectra instead   this caveat apply even to linear  constant  coefficient problem   see the book by trefethen  amp  embree   there exist pathological example of system where the solution behavior be completely unrelated to the eigenvalue   PRON will give an example that be attribute to vinograd     yt   at  yt  with    at   beginpmatrix  1  9cos26t6sin12 t   amp  12cos26 t   frac92sin12 t   12sin26 t   frac92sin12 t   amp  1  9 sin26t6sin12 t  endpmatrix  the eigenvalue of  at be  lambda1  10   independent of  t  but the solution be    yt   c1 exp2 t  beginpmatrix  cos6 t   2sin6 t   2cos6tsin6 t  endpmatrix   c2 exp13 t  beginpmatrix  sin6 t   2cos6 t   2sin6tcos6 t  endpmatrix  so in this case  the eigenvalue suggest stability  but the solution exhibit unbounded exponential growth   for a nice discussion of generalization of this example  see this excellent book   PRON be out of print  but use copy can sometimes be find  for a hefty price  on amazon   some further comment  PRON do not know of a nonlinear  autonomous example exhibit this kind of behavior  and PRON may well be that PRON do not exist   PRON also do not know of an example arise in application   people do generally use the eigenvalue of the frozen jacobian as an indicator of stability in practice  
__label__algorithms give large  x in mathbbr  PRON want to know whether or not  2x be an integer  be there any fast way to answer the question for  xgt2500   PRON have also ask a slightly different form of this question on mathse  how many significant figure be need in base 2   edit  PRON can determine if  2x be integer by check how close  x be to the log of an integer  if  x  log2nltepsilon for some small  epsilon for some integer n  then PRON can say that  2x be approximately an integer   PRON can use a taylor expansion of  log2x to evaluate PRON   the tricky part be know which integer n have a log close to x   this be a partial response to op s question that cover the case where  x be rational and be not be use to approximate an irrational number   if  xinmathbbq  test whether  2x be an integer be equivalent to test whether  x be a non  negative integer   proof  proposition 1  give  log2yx where  xinmathbbq  if  y be a positive integer there must be some  c such that  2m  yn  c and  xfracmn where  m  ninmathbbn   see comment of this answer for the proof   proposition 2  if  ygt2   and  m  n  yinmathbbn  if  2m  yn then  m must equal  ni where  i be some integer   consider the set of prime factor of  2m in order for  2m to have an integer  nth root  the set of prime factor must be able to be split into  n identical subset  since the prime factor be a set of  m two  this condition can only be fulfil if  m  ni where  iinmathbbn  put PRON together  take together  proposition 1 and 2 imply that if  2xinmathbbn then  xfracmnfracniniiinmathbbn PRON be take as obvious that if  xinmathbbn then   2xinmathbbn therefore   2xinmathbbnlongleftrightarrow xinmathbbn  in theory this would mean that all PRON need to do be test whether the fractional part of  x0 of course  op s application skirt this proof by virtue of the fact that PRON test be not that  2xinmathbbn but rather that  2x should be arbitrarily close to some positive integer   PRON just need to check that  x in mathbb zge 0 give  x in mathbbq  in the following  let   mathbbi   mathbbrsetminusmathbbq   en  iff n be even  lemma 1   n in mathbbzge 2   implie 21n  in mathbbi  proof   case  n  2    sqrt2  in mathbbi be a well  know fact   case  n gt 2   proof be similar to the  sqrt2  in mathbbi find in euclid s elements  suppose for a contradiction that  21n  in mathbbq then by definition   21n   a  b  where  a and  b be coprime  then  2   a  bn  an  bn implie 2bn  an implie ean  implie ea  implie a  2k  k in mathbbz  implie 2bn   2kn  2nkn implie bn  2n1kn implie ebn  because 2 mid 2n1  because n ge 2 implie eb  implie b  2l  l in mathbbz but since  2 mid a and  2 mid b   a and  b be not coprime  implies contradiction   lemma 2  if  x in mathbbq  cap  01 then  2x in mathbbi  proof  note that  2  01  and  2  1  2 since  fx   2x be strictly monotonic  PRON follow that  0ltxlt1 implie f0   lt  fx   lt  f1 but this be the same as say  1  lt  fx   lt  2 thus  fx  notin mathbbz because there be no integer between  1  and  2 since  x in mathbbq   x  a  b  where  a in mathbbz and  b in mathbbz but  b  gt  1 because fx  notin mathbbz thus  2x  2a  b   2a1b    21ba    21ba  wa  w in mathbbi by lemma 1  since  a  lt  b because x in  01   wa in mathbbi  because   fuck PRON  PRON be stick w  evs    theorem  if  x in mathbbq then  2x in mathbbz  iff x in mathbbzge 0  proof  let  x in mathbbq  case  x  lt  0    2x notin mathbbz because  0  lt  2x  lt  1  by exponential property   case  x ge 0   by rule of exponentiation   2x  2n  epsilon   2n2epsilon  n in mathbbzge 0 and  epsilon in  0  1 by lemma 2 and  2  0  1    2epsilon in mathbbz  iff epsilon  0 hence  2n2epsilon in mathbbz  iff epsilon  0  because  2n in mathbbz and  2  0  1 since  x  nepsilon by definition  PRON follow that  2x in mathbbz  iff x in mathbbzge 0 
__label__machine-learning __label__azure-ml PRON be try to experiment if an opportunity will win or lose in azure machine learning studio  however  be still in data preparation method   in PRON data base PRON have opportunity table and product table   for example  one opportunity have multiple product  should PRON deal with the many product and put PRON in one record   will PRON affect the prediction if PRON have duplicate record for an opportunity like  a  or PRON ’ good to have one record per opportunity in order to feed PRON to ml studio  and if yes which one will be good approach  b  or  c    approach a  approach b  approach c  oppid first product first technology2nd product 2nd technology  1  out  service  active directory  trn  item adobe acrobat  the simplification of the datum may make the model more stable  but PRON will also remove PRON ability to use more specific input criterion  for example  the way PRON move from approach a to approach b PRON be aggregate specific product into product category  this mean that if PRON model be successful in approach a  PRON will be able to predict base on specific product  on the other hand  if PRON model training succeed in approach b PRON will only be able to predict on product category  and PRON will have to convert product into PRON category before supply PRON to the model    so to answer PRON question  the number of datum sample PRON have determine how much PRON have to aggregate and simplify PRON datum  the datum PRON in PRON most detailed form could also fail to train the model properly  in which case the approach take in approach b be the good next step  
__label__neural-network let PRON say PRON train a neural network on input  1output  2  input  4output  8   how would PRON train a machine to recognize that PRON need to multiply the input by 2  from scratch   PRON need to limit the network model to one which naturally would generalise PRON problem   for a simple multiplication  this would be a single linear neuron  PRON can also train bias so that the network represent y  ax  b  if PRON take that very simple network give PRON just 2 or 3 example to learn from  and train PRON use least square error  PRON will quickly converge to the function PRON want  a single layer linear network like this can generalise to y  ax  b where x  y and b be vector  and a be a matrix  ie PRON can solve simultaneous linear equation provide PRON supply enough example to do so  usually number of example equal number of dimension of the vector    this be actually really trivial  and if PRON train such a simple network use any modern nn library  PRON will be very fast to learn PRON linear function  however  in the general case of have a specific non  linear function and want the network to learn PRON in a general sense  PRON be not possible  the network can be make to learn to generalise reasonably well near where PRON have provide example input and output  but will output incorrect result when PRON give PRON an x value far away from the training example   PRON be important to note that the network do not learn the math formula for PRON function  PRON find the good approximation give PRON own internal model  but by pick a network where the internal model be a good match to the function PRON want to learn  PRON be go to generalise well to PRON  and may be able to get PRON exactly if there be no noise in the example and enough of PRON  
__label__machine-learning __label__classification __label__data-mining __label__nlp __label__text-mining PRON be look for an algorithm that can deduct a set of rule base on a dataset of  train document  that can be apply to classify a new unseen document  the problem be that PRON need these rule to be viewable by the user in the form of some string representation  for example  the algorithm find that document have a minimum word count of 1000 and that there be 4 citation in each document  the key be that these rule must be deduct by a algorithm  an example of this in practice would be   document 1 contain 890 word and only 2 citation  PRON need PRON to return something like    PRON should add more word to make PRON better   add more citation to prove PRON point  PRON think a simple way be like this  convert the document to a vector like  bag of word  or  n  gram   then apply algorithm which can be use to derive rule decision tree or lem2   PRON sound like PRON have two issue  the first one be preprocess and feature extraction  the second one be how to learn classification rule   the second issue be the easy one to approach  there be a number of algorithm for learn classification rule  PRON could use a decision tree algorithm such as cart or c45 but there be also rule induction algorithm like the cn2 algorithm  both these type of algorithm can learn the type of rule PRON mention  however  rule induction base system can usually be supplement with hand craft rule in a more straight forward way than decision tree base system  while  unless PRON memory fail PRON  decision tree algorithm generally perform better on classification task   the first issue be bit hairy  to recommend the type of change PRON suggest PRON first need to extract the relevant feature  there be pre  processor which perform part  of  speech tagging  syntactic parsing  name entity recognition etc  and if the citation follow a strict format  PRON guess a regular expression could perhaps solve the problem  but otherwise PRON have to first train a system to recognize and count the number of citation in a text  and the same for any other non  trivial feature   then PRON can pass the output of this feature extraction system into the classification system  however  on read PRON question again PRON be unsure whether this problem may already be solve in PRON case  
__label__machine-learning __label__neural-network __label__deep-learning __label__randomized-algorithms give difficult learning task  eg high dimensionality  inherent datum complexity  deep neural networks become hard to train   to ease many of the problem one may   normalize  ampamp  handpick quality datum  choose a different training algorithm  eg rmsprop instead of gradient descent   pick a steep gradient cost function  eg cross entropy instead of mse   use different network structure  eg convolution layer instead of feedforward   PRON have hear that there be clever way to initialize good weight  for example PRON can choose the magnitude better   glorot and bengio  2010   for sigmoid unit  sample a uniformr  r  with  r   sqrtfrac6nin   nout  or hyperbolic tangent unit  sample a uniformr  r  with  r  4  sqrtfrac6nin   nout  be there any consistent way of initialize the weight good   as far as PRON know the two formula PRON give be pretty much the standard initialization  PRON have do a literature review a while ago  PRON copy PRON below if interested    1  address the question   first  weight should not be set to zero in order to break the symmetry when backprogragat   bias can generally be initialize to zero but weight need to be initialize carefully to break the symmetry between hidden unit of the same layer  because different output unit receive different gradient signal  this symmetry break issue do not concern the output weight  into the output unit   which can therefore also be set to zero   some initialization strategy    2  and  3  recommend scale by the inverse of the square root of the fan  in  glorot and bengio  2010  and the deep learning tutorials use a combination of the fan  in and fan  out   for sigmoid unit  sample a uniformr  r  with  rsqrtfrac6textfan  intextfan  out  fan  in be the number of input of the unit    for hyperbolic tangent unit  sample a uniformr  r  with  r4 sqrtfrac6textfan  intextfan  out  fan  in be the number of input of the unit    in the case of rbm  a zero  mean gaussian with a small standard deviation around 01 or 001 work well  hinton  2010  to initialize the weight   orthogonal random matrix initialization  ie w  nprandomrandnndim  ndim   u  s  v  nplinalgsvdw  then use u as PRON initialization matrix   also  unsupervised pre  training may help in some situation   an important choice be whether one should use  unsupervised pre  training  and which unsupervised  feature learn algorithm to use  in order  to initialize parameter  in most setting  PRON have find unsupervised pre  training to help  and very rarely to hurt  but of course that  imply additional training time and additional  hyper  parameter   some ann library also have some interesting list  eg lasagne   constantval   initialize weight with constant value   normalstd  mean   sample initial weight from the gaussian distribution   uniformrange  std  mean   sample initial weight from the uniform distribution   glorotinitializer   gain  c01b    glorot weight initialization   glorotnormalgain  c01b    glorot with weight sample from the normal distribution   glorotuniformgain  c01b   glorot with weight sample from the uniform distribution   heinitializer   gain  c01b    PRON weight initialization   henormalgain  c01b    PRON initializer with weight sample from the normal distribution   heuniformgain  c01b   PRON initializer with weight sample from the uniform distribution   orthogonalgain    intialize weight as orthogonal matrix   sparsesparsity  std   initialize weight as sparse matrix    1  bengio  yoshua   practical recommendation for gradient  base training of deep architecture   neural networks  trick of the trade  springer berlin heidelberg  2012  437  478    2  lecun  y  bottou  l  orr  g b  and muller  k  1998a   efficient backprop  in neural networks  tricks of the trade    3  glorot  xavier  and yoshua bengio   understand the difficulty of train deep feedforward neural network   international conference on artificial intelligence and statistic  2010  
__label__machine-learning __label__computer-vision __label__pattern-recognition PRON have instal opencv and intraface  now the code can detect landmark on the face  like this   now how can PRON convert this to a smile detector   PRON have the landmark point as an array  so PRON should probably first find which one belong to the mouth  how can PRON do that  be there any tutorial for intraface  PRON could not find any   any help would be appreciate  thank   intraface have an expression detector build in  however  if PRON would like to do a custom smile detector  then what PRON should do be to convert PRON spatial facial fiducial point to some kind of a feature vector  which could be input to a non  linear classifier such as svm or mlp   a trivial way to do this would be to use angular relation  if the face be fronto  parallel  or else PRON could use projective invariant such as cross ratio to find out the geometric relationship   two type of classifier can be train   two  class  PRON would first prepare a training set of smile and non  smile people  then train PRON learning machine with this input   novelty  detection  if PRON use something like gmm or a variant of svm PRON could model only the smile feature  build a hyper  envolope around those feature   and then during the runtime  PRON could query if the test feature fall into this hyper  envelope or not   finally  regard the mouth part  PRON be right that PRON can help PRON  and intraface would have that build in  in the bad scenario  PRON may not need to separate anything  if PRON have a good training set PRON could as well run a dimensionality reduction on PRON feature   of course there be more advanced method publish to accomplish what PRON like to do  just as an example here  
__label__constrained-optimization be PRON possible to solve a quadratic cost function which be subject to both nonlinear  quadratic   amp  linear constriant by sqp method  if not  what be the good iterative solution for this kind of problem   thank  yes  PRON be look for quadratically  constraint quadratic programming  qcqp    PRON would solve PRON sqp as a sequential qcqp   PRON can find result in literature for sqcqp   most solver come with a  socpsecond  order cone program  solver  socp s subsume qp s and qcqp s   for reference  as PRON know nothing about PRON actual problem domain  PRON be also often totally fair to take a first  order taylor approximation of PRON constraint  just as PRON  presumably  take a second  order taylor approximation of PRON cost function for PRON sqp   an example of a paper that do that  in robotic  be fossen s constrains nonlinear control allocation 
__label__k-means __label__apache-mahout PRON apologize that this have be ask and PRON feel that PRON may be obvious  but PRON be wonder exactly what the meaning of the numerical value below from clusterdump   top term   monkey    gt   08170868432876803  PRON believe that to the be center of the centroid   but if the term vector be create with term frequency  could one interpret this as the average occurrence of the  monkey  in the document that be consider part of the cluster  in this case   monkey  would appear in 82  of the doc in that cluster or more likely that the average count of monkey be 82   look further  PRON see word like so   top term   zebra     3432595573440644  so PRON be good to interpret this as the average count of  zebra  in the set of doc   and give the radius value  one could consider that the range of percentage of  monkey    mahout seq2sparse i out  sequence   o out  sparse  kmean wt tf maxdfpercent 100 namedvector    mahout kmean   i out  sparse  kmean  tf  vectors   c out  kmean  cluster   o out  kmean   dm orgapachemahoutcommondistancecosinedistancemeasure   x 10 k  i ow cluster  when one use tf  idf weighting  PRON may be good to normalize the output weight by create a proportion of evidence via wi  wi  sumw   be that a good idea   some python lda lib do this    thank PRON   reference  httpsmahoutapacheorgusersclusteringclusterdumperhtml  
__label__matlab PRON want to use the quad function to do the integration today  PRON write a function file   function z  pdfy1x   z100   and then  in the command line  PRON type   quadpdfy1005   the output be      error use    quad at 79  the integrand function must return an output vector of the same length as the input vector   PRON try z2x  PRON work  PRON do not know why quad be not work for constant case  PRON actually want to use PRON on some other function involve if  else statement  but PRON keep show the same message  the integrand function must return an output vector of the same length as the input vector   PRON do not know why  could somebody help PRON  thank PRON very much   try   function z  pdfy1x  z100onessizex    matlab require the output to be the same shape  dimension as the input for function pass to quad    
__label__data-mining __label__dataset __label__regression __label__correlation __label__visualization PRON want to investigate price  set behavior of airline  specifically how airline react to competitor pricing   as PRON would say PRON knowledge about more complex analysis be quite limited PRON have do mostly all basic method to gather a overall view of the datum  this include simple graph which already help to identify similar pattern  iam also use sas enterprise 94   however iam look for a more number base approach   data set  the  self  collect datum set iam use contain around 54000 fare   all fare be collect within a 60 day time window  on a daily basis  every night at 0000    hence  every fare within that time window occur  n time subject to the availability of the fare as well as the departure date of the flight  when PRON be pass by the collection date of the fare    PRON can not collect a fare for a flight when the departure date of the flight be in the past   the unformatted that look basically like this   fake datum       requestdate   price tripstartdeparture  tripdestinationdeparture  flightcarrier       14apr2015000000  72532   16apr2015105002  23apr2015215504   xa        14apr2015000000  96632   16apr2015132002  23apr2015190004   xy        14apr2015000000  91532   16apr2015132002  23apr2015215504   xh        daysbeforedeparture  be calculate via  PRON  s  c where  PRON  amp  interval  day before departure   s  amp  date of the fare  flight departure   c  amp  date of which the fare be collect  here be a example of group datum set by PRON  daysbeforedep    fake datum         daysbefdeparture  avgofsale  minofsale  maxofsale  operatingcarrier       0   88068   47799   224523   dl        0   90489   47799   253455   dl        0   104439   92099   211909   lh       what PRON come up with so far  look at the line graph PRON can already estimate that several line will have a high correlation factor  hence  PRON try to use correlation analysis first on the group datum  but be that the correct way  basically PRON try now to make correlation on the average rather then on the individual price   be there an other way   iam unsure wich regression model fit here  as the price do not move in any linear form and appear non  linear  would PRON need to fit a model to each of price development of an airline  ps  this be a long text  wall  if PRON need to clarify anything let PRON know  iam new to this sub   anyone a clue    in addition to exploratory datum analysis  eda   both descriptive and visual  PRON would try to use time series analysis as a more comprehensive and sophisticated analysis  specifically  PRON would perform time series regression analysis  time series analysis be a huge research and practice domain  so  if PRON be not familiar with the fundamental  PRON suggest start with the above  link wikipedia article  gradually search for more specific topic and read correspond article  paper and book   since time series analysis be a very popular approach  PRON be support by most open source and closed source commercial datum science and statistical environment  software   such as r  python  sas  spss and many other  if PRON want to use r for this  check PRON answer on general time series analysis and on time series classification and clustering  PRON hope that this be helpful   word of warn from a former airline revenue management analyst  PRON may be bark up the wrong tree with this approach  apology for the wall of text that follow  but this data be a lot more complex and noisy than may appear at first glance  so want to provide a short description of how PRON be generate  forewarn be forearm   airline fare have two component to PRON  all the actual fare  complete with fare rule and what have PRON  that an airline have available for a certain route  most of which be publish the airline tariff publishing company  a few special  use one be not  but those be the exception rather than the rule  and the actual inventory management perform by the airline on a day  to  day basis   fare can be submit to atpco four time a day  at set interval  and when airline do so  PRON will usually consist of a mixture of addition  deletion  and modification of exist fare  when an airline initiate a pricing action  assume PRON competitor be not try to make PRON own move here   PRON usually have to wait until the next update to see if PRON competitor follow  respond  the converse go when a competitor initiate a pricing action  as the airline have to wait until the next update before PRON can respond   now  this be all well and good with respect to fare  but the problem be that  because this be all get publish in atpco  fare be the next good thing to public information  all PRON competitor get to see what PRON have get in PRON arsenal  so attempt to obfuscate be not unheard of  such as publish fare that will never actually be assign any inventory  list all the fare as day  of  departure  etc   in many way  the secret sauce come down to the actual inventory allocation  ie how many seat on each flight will PRON be willing to sell for a give fare  and this information be not publicly available  PRON can get some glimpse by scrap web info  but the potential combination of departure time  date and fare rule be quite numerous and may quickly escalate beyond PRON ability to easily keep track of   typically an airline will only be willing to sell a handful of seat for a very low fare and the people who snag those have to book quite far in advance lest the fare rule lock PRON out  or other traveler simply beat PRON to the punch  the airline will be willing to sell a few more seat for a high fare  and so on and so forth  PRON will be quite happy to sell all of the seat for the high fare PRON have get publish  but this be not usually feasible   what PRON be see with fare get high the closer PRON get to the day of departure be simply the natural process of have the cheap seat get book farther out  while the remain inventory gradually get more expensive  of course  there be some caveat here  the rm process be actively manage and human intervention be quite common as the rm team generally strive to meet PRON revenue goal and maximize revenue on each flight  as such  flight that fill up quickly may be  tighten up  by close out low fare  flight that be book slowly may be  loosen up  by allocate more seat to low fare   there be a constant interplay and competition between airline in this area  but PRON be not very likely to capture the actual dynamic just from scrap fare  do not get PRON wrong  PRON have such tool at PRON disposal  and  despite PRON limitation  PRON be quite valuable  but PRON be just one datum source that feed into the decision  make process  PRON would need access to the hundred  if not thousand of operational decision make by rm team on a daily basis  as well as state  of  the  world information as PRON see PRON at the time  if PRON can not find an airline partner to work with in order to get this datum  PRON may need to consider alternate data source   PRON would recommend look into get access to oampd fare datum from the official airline guide  or one of PRON competitor  and try to use that for PRON analysis  PRON be sample  base  about 10  of all ticket sell  and aggregate at a high level than would be ideal so careful route selection be imperative  PRON would recommend something with plenty of airline  fly non  stop multiple time a day  with large aircraft   but PRON may be able to get a good picture of what be actually sell  average fare  and how much of PRON be sell  load factor   vs merely what be available for sale at a give point in time  use that information PRON may be in good position to at least explore the outcome of the airline  pricing strategy  and make PRON inference from there  
__label__bigdata __label__apache-hadoop  hadoop divide the input to a mapreduce job into fix  size piece  call input split  or just split  hadoop create one map task for  each split  which run the user  define map function for each record  in the split  have many split mean the time take to process each  split be small compare to the time to process the whole input  so if  PRON be process the split in parallel  the processing be good  load balance when the split be small  why   all bigdata eco system work on something call parallel processing   PRON have to process 100gig of file  if PRON do not split the file  then all the 100 gig should be process by single jvmsingle map    if PRON split the file into 1000 part each of 100 mb  then PRON can process each part with different jvm and apply the map function in less time   mpp  massively parallel processing 
__label__time-integration PRON want to compute the value of an integral of a function   this function  however  be not give by a formula  say  fx    forall x in  01  but be only know through PRON value on some give point  say   f leftfrackn  right    forall k in 0cdot  n  be the name for that scatter datum    how should PRON do so  PRON do not want to implement a quadrature method by hand  PRON know how to do so in a basic way   method refer to a scientific library would be prefer  eg gsl     PRON want to compute the value of an integral of a function   this function  however  be not give by a formula  say  fx  forallxin01  but be only know through PRON value on some give point  say  fk  n  forallkin0⋯n  be the name for that scatter datum    in general  if PRON have datum as collection of point   xk   fxk  the absolute simple method of integrate the  curve  through those point  there be many such curve  be to assume a linear interpolant between adjacent point  which give PRON a trapezoidal rule  as far as PRON know  the trapezoidal rule be not in gsl  or at least  not in the numerical integration section   however  there be subroutine available in some library or software package  trapz  amp  cumtrapz in matlab come to mind  at worst  a trapezoidal rule would not be onerous to implement by hand   high  order method assume high order interpolant  and also typically do not equispace the quadrature point due to runge s phenomenon  which be one motivation for use method with non  equispac point  for instance  quadrature base at chebyshev point  or adaptive quadrature method   if PRON have the ability to query PRON function anywhere  PRON ne not be give by a formula  but PRON must be computable for arbitrary input within PRON interval of integration   PRON should use one of these method  if not  then PRON doubt there be much advantage to try to use high  order piecewise interpolant  eg  spline   but PRON be not an expert on numerical integration  
__label__python PRON be look for information on how should a python machine learning project be organize  for python usual project there be cookiecutt and for r projecttemplate   this be PRON current folder structure  but PRON be mix jupyter notebooks with actual python code and PRON do not seem very clear     ├ ─ ─ cache  ├ ─ ─ datum  ├ ─ ─ mymodule  ├ ─ ─ log  ├ ─ ─ notebook  ├ ─ ─ script  ├ ─ ─ snippet  └ ─ ─ tool  PRON work in the script folder and currently add all the function in file under mymodule  but that lead to error load datarelative  absolute path  and other problem   PRON could not find proper good practice or good example on this topic besides some kaggle competition solution and some notebooks that have all the function condense at the start of such notebook   the university of washington have release a project template for small scientific python project  include data science project  call shablona  be that more or less what PRON be look for   httpsgithubcomuwescienceshablona  PRON do not think there be good practice in this develop area yet but  in addition to cookiecutter  there be some interesting idea show in a tutorial at the scipy 2016 conference  httpisaacslavittcom20160720datascienceissoftwaretalk  personally  PRON try to minimize the number of sub  folder in a project unless PRON have a really good way to distinguish PRON and have good reason to keep PRON separate  bad organization be nearly as bad as no organization  PRON think good practice may depend on use case  not every project need the same amount of boilerplate  
__label__linear-algebra __label__petsc below a piece of code be show to solve a linear system   ierr  kspcreatepetsccommworldampksp   chkerrqierr    ierr  kspsetoperatorsksp  a  a  differentnonzeropattern   chkerrqierr    kspsettypeksp  kspbcgs    ierr  kspsettolerancesksp1e2m1n11e50petscdefault  petscdefault   chkerrqierr    ierr  kspsetfromoptionskspchkerrqierr    ierr  pcsettypepc  pcsorchkerrqierr    ierr  kspsolveksp  b  xchkerrqierr    accord to the petsc manual  petsc use gmres with an ilu0  preconditioner to solve sle  PRON want to modify code to solve system use the bicgstab method with a ssor preconditioner   can someone help to do that   the petsc team always recommend that PRON user control solver option from the command line  the whole package be build with the idea of extreme flexibility in compose solver and preconditioner  and the only way to achieve that be to use the command line scheme  PRON could get rid of set the ksp and pc type in PRON code and use  ierr  kspcreatepetsccommworldampksp    ierr  kspsetoperatorsksp  a  a  differentnonzeropattern    ierr  kspsettolerancesksp1e2m1n11e50petscdefault   petscdefault    ierr  kspsetfromoptionsksp    ierr  kspsolveksp  b  x    and run with  ksptype bcgsl pctype sor pcsorits 10  kspsetfromoptionsksp  will override the default option with the option PRON have supply here  so  instead of gmres with ilu  PRON will use bicgstabl  with ssor  additional option to control ssor can be find on the petsc man page   httpwwwmcsanlgovpetscpetsccurrentdocsmanualpagespcpcsorhtml  if PRON insist on set the option from code  use kspsettypeksp  kspbcgs    almost all petsc run  time option have an equivalent functional interface  but PRON be only recommend when PRON application be make active decision about which method to use   PRON can either do PRON from the code by do this   ksp  mysolver    define the ksp method  pc  myprec     define the preconditioner    define the matrix to be use     kspcreatecommampmysolver    kspsettypemysolver  kspbcgs     set bicgstab as the krylov method  kspgetpcmysolverampmyprec    pcsettypemyprec  pcsor     set ssor as the preconditioner method    then continue with the settype and solve and other stuff     or by initialize the ksp method and preconditioner as do before  then set PRON use the command line by add   kspsetfromoptionsmysolver    kspsetupmysolver    the second way be more flexible of course  as PRON can specify PRON ksp method and pc regardless of what the code define  check this link to know more about how to set PRON value from the database  httpwwwmcsanlgovpetscpetsccurrentdocsmanualpageskspkspsetfromoptionshtmlkspsetfromoption 
__label__machine-learning __label__neural-network there be a million and one example and tutorial on how to train up a neural network on the sample set like the mnist datum and the cifar10 datum  but how do one go from the toy example of recognise 200x200 clip each contain a single centre object to a real problem like find cifar10 category object  the dog and the cat below  within a picture  like PRON presume google do for PRON photo annotation   can someone describe how one may approach this leap from the classroom to the real world   this be a well define problem call text spotting  there be numerous avenue to tackle this problem but most of the good one be base on deep learning  naively PRON would use a network like the one PRON train on mnist to slide over PRON input and see where PRON fire strongly to build up a string  this approach work reasonably well but to convolute this over PRON whole input image be extremely computationally expensive  a way that be actually use in practice be a two step process  first there be a network that be train in localize area of interest  which be bound box of part where there may be text and then use a more advanced network to grab the text  to PRON understanding this be also do in one network pass nowadays as oppose to query on single patch  if PRON search for text spot PRON will find a lot of nice paper and a thesis  mostly from the same guy   httpwwwmathstatdalcahguneural20comput20amp20applicpdf 
__label__algorithms __label__combinatorics PRON be look for an algorithm with the following characteristic   PRON be use to generate the set of integer vector  mathbf kk1ldot  km  where  kileq ki   kigeq 0   and  ki be give positive integer  that is  PRON be look for the cartesian product of the set  1ldot  ki  call  k  k1ldotskm the output vector be generate for increase  k so first go the vector with  k0   then the one with  k1   then those for  k2   etc   the above be do without full enumeration or use of persistent datum structure  that is  give the  ith generate vector   mathbf ki  the function should return a vector  mathbf ki1 that have not be generate before  if one exist   PRON be fairly easy to generate the  mathbf k vector without the second requirement  but PRON be not sure how to handle that one   modulo an inessential detail  PRON be ask to generate mixed radix number with certain  digit  sum  k  k1ldotskm  in particular  give  mathbfki satisfy the sum of component  k  PRON be ask to find either the next large representation have the same component sum  or if none exist  the small representation have component sum  k1   which PRON call  mathbfki1  ask for the  next   mathbfki only make sense if an ordering be impose among solution with a fix component sum  k  PRON will assume a lexicographic ascend order below  but a descending order would be as easy to arrange   the basic idea be clear enough  find the rightmost  maximum  index  j such that PRON can increment  kj by one and  in compensation  decrease by one the sum of component with index great than  j in the way that represent the small possible   m  j component  mix radix number  with the specify component sum    the  boundary  case where no such index  j be possible  so that PRON increment  k and start over with the small possible mix radix number with component sum  k1   be essentially the same as the latter subproblem of the typical step   PRON therefore bear discuss how to find the small number with a give sum  k of entry   fortunately this subproblem have an easy solution  PRON choose digit from right to leave in a greedy fashion  as large as possible subject to the constrain sum  k  this be good point to pin down the inessential detail mention at the outset   the question pose a sum of component  ki in 1ldot  ki  although confusingly  the condition  ki ge 0  be also state    if the component be all require to meet  ki ge 1   or for that matter any other nonzero low bound  possibly determine by the index  i   PRON can not achieve any sum of component less than  m  see as there be  m component to be add    therefore PRON be convenient to subtract one from each component  choose instead that  ki in 0ldot  ki1  which make the analogy with mixed radix representation more explicit   correspondingly  the digit sum can then progress from  sumi1m ki  0  to  sumi1m  ki  1  least representation of rightmost  m  j digit have component sum  c  set  ell  m  while  ell  gt  j   set  kell  max c  kell  1   set  c  c  kell  set  ell  ell  1  return success if  c  0   otherwise fail   find rightmost index  j such that  kj can be increment  scan from right to left to find an index  j gt 1  such that  kj gt 0  if no such index exist  fail   do   set  j  j1  until  kj lt kj  1  for efficiency PRON can keep a run sum  c of entry above index  j  since PRON will offset the increment of  kj by decrement the sum  c of component beyond that point  
__label__software __label__data-analysis __label__visualization to be clear   PRON calculate some frequency  which PRON want to visually match with a measure spectrum  so PRON would like to convolve PRON with gaussian or lorentz peak   example   v1  123 cm1 intensity 54  v2  222 cm1 intensity 22  v3  321 cm1 intensity 12  v4  751 cm1 intensity 2  result PRON want   a whole dataset of intensity vs frequency that represent lorentz peak center at those frequency  with the correct relative intensity  which PRON can plot to compare PRON with measure peak   
__label__r __label__databases __label__efficiency __label__tools PRON have an r script that generate a report base on the current content of a database  this database be constantly in flux with record be add  delete many time each day  how can PRON ask PRON computer to run this every night at 4 be so that PRON have an up to date report wait for PRON in the morning  or perhaps PRON want PRON to re  run once a certain number of new record have be add to the database  how may PRON go about automate this  PRON should mention PRON be on windows  but PRON could easily put this script on PRON linux machine if that would simplify the process    how can PRON ask PRON computer to run this every night at 4 be so that PRON have an up to date report wait for PRON in the morning   PRON can set up a cronjob on a linux system  these be run at the set time  if the computer be on  to do so  open a terminal and type   crontab e  and add   00 4    r sourcehome  filepath  myrscript  r   source  stack overflow  for window  use the task scheduler to set the task to run for example daily at 400 am  PRON give PRON many other option regard frequency etc   httpenwikipediaorgwikiwindowstaskscheduler 
__label__fortran __label__interpolation be there a fortran subroutine which perform linear interpolation in one  dimenional datum   PRON need something similar to matlab function interp1   there be no build  in fortran functionality to do linear interpolation  PRON could either use a library or write PRON own routine  PRON have not try compiling or testing and PRON fortran may be a bit rusty  but something like the follow should work   subroutine interp1  xdata  ydata  xval  yval    inputs  xdata  a vector of the x  value of the datum to be interpolate    ydata  a vector of the y  value of the datum to be interpolate    xval   a vector of the x  value where interpolation should be perform   output  yval   a vector of the result interpolate value  implicit none  real  intentin    xdata     ydata     xval     real  intentout    yval     integer   inputindex  dataindex  real   minxdata  minydata  xrange  weight   possible check on input could go here   thing PRON may want to check     monotonically increase xdata    sizexdata    sizeydata     sizexval    sizeyval   minxdatat  xdata1   maxxdatat  xdatasizexdata    xrange  maxxdata  minxdata  for inputindex  1  sizexval    possible check for out of range xval could go here   this will work if x be uniformly space  otherwise increment   dataindex until xdatadataindex1gtxvalinputindex   dataindex  floorxvalinputindexminxvalxrange    weight   xval  xdatadataindexxdatadataindex1xdatadataindex     yvalinputindex    10weightydatadataindex     weightydatadataindex1    end  end subroutine  take a look at the numerical recipes book  a classic  find here  look in the interpolation chapter  PRON can find an explanation  plus the code  PRON be in pdf  a simple google search also produce a github library by thomas robitaille  where a routine call interp1d exist  
__label__anomaly-detection PRON want to write PRON thesis about fraud detection in erp database  PRON be look for a industry standard processs such as crisp  dm for data mining projects  in order to justify PRON approach in solve the issue of find outlier  anomaly in the data set   be there any established standard process which would be more suitable for such an project   thank   PRON should refer this survey paper on anomaly detection  from university of minnesota    please let PRON know if this help PRON   here be a great book on fraud analytics  PRON be quite comprehensive and have a detailed list of original research reference and other textbook at the end of each chapter   fraud analytic use descriptive  predictive  and social network techniques  a guide to data science for fraud detection  bart baesens  veronique van vlasselaer  wouter verbeke  isbn  978  1  119  13312  4  aug 2015 
__label__linear-algebra __label__numerical-analysis __label__ode __label__eigenvalues __label__iterative-method what method be know for efficiently solve a large set of linear homogeneous ordinary differential equation of the follow form   beginequation   mathbfb  fracdmathbfydt   mathbfa  mathbfy    endequation   where  mathbfb  in mathbbcntim n be hermitian positive definite    mathbfa  in mathbbcntim n be hermitian   mathbfyin mathbbcn  and  n be a large integer such that not all the element of the matrix  mathbfa or  mathbfb can be store on the main memory   be there a method that avoid compute  mathbfb1  solve a linear equation  mathbfb  mathbfx   mathbfc for any  mathbfc  and diagonalize  mathbfb  there seem to be such an  inverse free  method  1  for the related eigenvalue problem   mathbfb  lambda mathbfy   mathbfamathbfy  and PRON would like to know if there be an extension of such a method to ode    1  eg gene h golub  qiang ye  an inverse free preconditioned krylov subspace method for symmetric generalized eigenvalue problems  siam j sci  comp  24  312  2002    b be know as a mass matrix  there be many method which can directly solve with mass matrix  the main one be implicit method  implicit runge  kutta method and the multistep bdf method  also rosenbrock method can handle mass matrix directly  but be not implicit method   a quick overview for  why PRON work  be because at every step these method have to solve linear equation involve the jacobian of the rhs  the linear problem be normally of the form   i  gamma jx  b where  j be the system jacobian   gamma and  b be some constant  and  x be the evolution of the system that must be solve for  well  PRON turn out that if PRON just use the mass matrix  b by change the system to   b  gamma jx  b  these directly solve the linear problem with the mass matrix  implicitly invert  b in calculation that PRON would already have be do  this be because the  i come from the lhs when take the jacobian  but with a mass matrix instead that value be just  b    do this implementation with the non  adaptive 2nd order version of the rosenbrock method be outline in shampine s paper and be not difficult  though PRON lose a lot of efficiency without adaptive timestepping here   if PRON use julia  differentialequationsjl have access to these method  just change massmatrix in the construction of the odeproblem type  the rosenbrock23   method will handle mass matrix like this  in addition  mass matrix can be pass to the implicit rk method radau   and radau5   which be high order and tend to be fast if PRON be look to solve stiff problem with high accuracy  technically sundials  cvodebdf   could solve with a mass matrix  but there be an open issue for set up the c ffi for do so  PRON be high on PRON priority list  full disclosure  PRON be a developer of differentialequationsjl    if PRON can handle the performance drop of go to matlab instead  PRON will find the field be very similar  ode23s be a 2nd  order rosenbrock method  be very similar to rosenbrock23    and can use constant mass matrix  ode15s be very similar to cvodebdf   and can take mass matrix  matlab do not have an implicit rk method  though PRON do have ode23 t which be good for problem where a symplectic integrator be need and PRON can handle mass matrix   PRON do not see option for pass mass matrix to scipy s ode solver  and have not use the scipy wrapper  those wrap some of the same code as differentialequationsjl so PRON know that in some case the mass matrix can be solve by the underlie c  fortran code that PRON use  but PRON look like that part of the api be not expose  
__label__finite-difference __label__ode __label__integral-equations PRON be try to integrate an incredibly simple ode      yx   fyquad y0   y0       from  x0  to  x1 this be a decay type of equation   f be the  variable  decay rate and  y be the abundance   both  y and  fy be positive everywhere   PRON be face with a couple of constraint   sometimes  fy  gg 1   which make this a stiff problem on the timescale PRON be interested in   PRON can evaluate  fy exactly  but PRON can be costly  so PRON would like to minimize the number of time PRON have to call PRON   luckily  the result do not have to be super accurate   this be a tiny routine that will be call often in a much large code   the current method be obviously horrible  PRON just take a single forward euler step      y1  approx y0   fy0        so literally anything suggest will be an improvement   obviously  PRON could use an rk scheme  probably with adaptive step sizing  or good yet an implicit scheme   however  give the simplicity of the equation PRON think there may be something more elegant   for instance  PRON could rewrite the problem as      inty0y1  fracdyfy    1     and attempt to solve for  y1   but PRON be not sure of the good way to approach this numerically   any suggestion appreciate   
__label__linear-algebra __label__matrices __label__eigenvalues PRON apologize if this question in a more general form have be ask before  PRON have a tridiagonal toeplitz matrix  k  whose eigenvalue and eigenvector be know analytically for any dimension  n  1   specifically  in the notation of section 22 of this paper  the matrix PRON wish to diagonalise be such that  a1  a2    2   and  b1  b2c1c21  or     k  beginpmatrix   2  amp  1  amp  0  amp  0amp  amp0   1  amp  2  amp  1  amp  0amp  amp0   0  amp  1  amp  2  amp  1amp  amp0        endpmatrix      then theorem 21 give PRON eigenvalue and eigenvector   PRON be   lambdai  4 cos2leftfraci1pi2n1right and eigenvector  vi   leftufracj2   fraclambdai22  22    ufracj21  fraclambdai22  22rightt   where  uj  x be the chebyshev polynomial of the second kind of order  j  what PRON further wish to do be to use the matrix of eigenvector  say  a  to implement similarity transform   note that  a be not sparse at all in this example   how good can PRON do PRON computationally  in python  for  n  10  4   the program take a lot of time to even calculate  a and store the matrix  presumably because of the presence of the chebyshev polynomial  on the other hand  use the sparse linear algebra module scipysparselinalgeig finish the task faster  even though PRON do not give PRON any information   so  PRON question be this how do PRON use a combination of PRON knowledge of the true eigenvalue and eigenvector and the efficient linear algebra routine in scipy to implement the similarity transform on a diagonal matrix d   even if PRON use just the information about eigenvalue  can PRON efficiently compute the eigenvector  be there routine that compute the eigenvector after take an input of eigenvalue if PRON be know   for instance  the follow code be PRON trial in python   import numpy as np  from scipyspecial import evalchebyu  from scipy import sparse as sp  n  103  def matx  j    if j20   if j0   return 1  else   return evalchebyuj2    x22  22   evalchebyuj2  1    x22  22   else   return  x2evalchebyuj12    x22  22   def norma    a  atransposenpsqrtnpeinsum  i  i   a  a    return a  def tridiaga  b  c  n    return spdiagsanponesn1   1   spdiagsbnponesn   0   spdiagscnponesn1   1   k  tridiag121n   a  npzerosnn    lam  npzerosn   for i in range0n    lami   4npcosi1nppi2n12  for j in range0n    ai  j matlamij   a  norma   tqnew  spdiags12omeganpsqrtlam0   tqold  atransposedottqnewtodensedota   gilbert strang explain the matrix square root of the second difference matrix here   in particular   import numpy  import scipylinalg  def fn  p    a  2   1p1   b  1numpytannumpypi   2p  1     4  4n    c  1numpytannumpypi   2p  1     4  4n    return a  b  c  n  10000  ktri  numpyeyen   numpyeyen  k1   k  ktri  ktri  t  s  numpyarangen   t  scipylinalgtoeplitzfn  s    h  scipylinalghankelfn  s2   fn  n1s    ksqrt   05   n1     t  h   printnumpymaxnumpyabsnumpydotksqrt  ksqrt   k     214717132963e13 
__label__machine-learning __label__convolutional-neural-networks if PRON want to train a convoluted nn on time series but PRON can not decide where to split the datum   PRON see that other people use jump window over the input  so the feed say 20 sec of observation as 1 sequence into a cnn   PRON can not do that as split the observation by fix size will most definitely break important pattern  ie first part of a pattern will go into the end of the current seq and the rest into the beginning of the next seq   PRON can however find a sensible solution by preprocess datum and find place of much small significance and make seq cut in the middle  but then the seq length will vary greatly   can PRON still use cnn  or this idea be silly   how else people extract feature from time series use cnns   
__label__neural-networks __label__convolutional-neural-networks PRON be follow andrew ng course on artificial neural networks   PRON talk about cnn s  convolutional neural networks  and rnn s  recurrent neural networks    what be the fundamental difference between PRON  where be PRON apply   basically  a cnn save a set of weight and apply PRON spatially  for example  in a layer  PRON could have 32 set of weight  also call feature map   each set of weight be a 3x3 block  mean PRON have 3x3x32288 weight for that layer  if PRON give PRON an input image  for each 3x3 map  PRON slide PRON across all the pixel in the image  multiply the region together  PRON repeat this for all 32 feature map  and pass the output on  so  PRON be learn a few weight that PRON can apply at a lot of location   for an rnn  PRON be a set of weight apply temporally  through time   an input come in  and be multiply by the weight  the network save an internal state and put out some sort of output  then  the next piece of datum come in  and be multiply by the weight  however  the internal state that be create from the last piece of datum also come in and be multiply by a different weight  those be add and the output come from an activation apply to the sum  time another weight  the internal state be update  and the process repeat   cnn s work really well for computer vision  at the low level  PRON often want to find thing like vertical and horizontal line  those kind of thing be go to be all over the image  so PRON make sense to have weight that PRON can apply anywhere in the image   rnn s be really good for natural language processing  PRON can imagine that the next word in a sentence will be highly influence by the one that come before PRON  so PRON make sense to carry that internal state forward and have a small set of weight that can apply to any input   however  there be many more application  in addition  cnn s have perform well on nlp task  there be also more advanced version of rnn s call lstm s that PRON could check out   for an explanation of cnn s  go to the stanford cs231n course  especially check out lecture 5  there be full class video on youtube   for an explanation of rnn s  go here    what be the fundamental difference between PRON   where be PRON apply   let PRON start by give example of one task each where PRON will apply or use convolutional neural network and recurrent neural network  respectively   task 1  sentiment analysis  PRON be give some review  and PRON want to predict the rating of the review    task 2  machine translation  translate a sentence from some source language to target language   now  the basic difference in term of applicability of conv  net and rnn be that conv  net  like most other machine learn algorithm  take a fix size input and generate fix  size output  rnn  on the other hand  can handle arbitrary input  output length  but would typically require much more datum compare to conv  net because PRON be a more complex model   use this insight  PRON see that task 2 can not be perform by conv  net  since input and output be not fix  length  so rnn for task 2   for task 1  however  PRON can use rnn if PRON have a lot of datum  but PRON can also use conv  net  fix the length of the input  and adjust the input length by truncate or pad the actual input   note that this will not affect the sentiment of the review much  so this be a reasonable approach  and since PRON be a 1d convolution  that be typically use in sequence  PRON be call temporal convolution  conceptually  PRON be similar to 2d spatial convolution   hope this can give PRON a glimpse   on a basic level  an rnn be a neural network whose next state depend on PRON past states   while a cnn be a neural network that do dimensionality reduction  make large datum small while preserve information  via convolution  see this for more info on convolution 
__label__machine-learning PRON understanding be that gpu be more efficient for run neural net  but someone recently suggest gpu be only need for the training phase and that once train  PRON be actually more efficient to run PRON on cpu   be this true   that would depend entirely on what software platform PRON be run on  not on any particular characteristic of neural net or PRON constituent object  for example  theano leverage gpus because PRON be optimize for matrix math  which neural net weight and activation be often express as  on the other hand  PRON can also be express as set  which make PRON ideal for set  base language like sql  PRON prefer this approach  since set be more flexible  PRON can  for example  operate easily on ragged dimensional set  unlike with matrix  and sql be a somewhat portable standard that make PRON easy to think about neural net layer conceptually  as PRON screen name suggest  PRON use sql server  which be one of many database server platform that can do some serious number  crunch on very large neural net  all without the use of any gpu power  long ago PRON also program neural net in vbnet  vb 6 and c  that efficiently make use of array  without leverage gpu at all   PRON think the choice may come down to a combination of PRON exist skill set  what architectur PRON have access to  PRON portability requirement  code maintenance issue and all that  not to any explicit linkage between specific hardware component and the inner working of neural net  there may be slight advantage to model ragged matrix in sql or to the matrix  crunch power of gpu  but PRON do not think there be much practical difference in most scenario  nor can PRON think of a way of test such a comparison without get into an apple vs orange conundrum  that be say  within a specific platform  PRON may be true that  gpu be only need for the training phase  and  once train  PRON be actually more efficient to run PRON on cpu   that PRON can not comment on  since PRON still with sql and the net stack and avoid theano etc  altogether  though PRON respect PRON capability   PRON hope that help  
__label__prediction __label__churn one of the most crucial skill of a data scientist be not only to be able to build an accurate predictive algorithm but to suggest a set of action base on that to enhance the goal ratio   PRON have build a churn rate predictor for a retail app which accuracy after testing be about 90   so now that PRON know with high accuracy which user will churn out of the app in the next day  what can PRON do to try to retain PRON  think in the overall app environment the only idea that come to PRON mind be   push notification  crm  use a recommender system to send push notification base on user profile and micro  moment   PRON could do the following   find out the most important predictor use by PRON model  the method use to identify importance depend on the model use   for each of the major pedictor try to find out how do each predictor segment the customer population of will  churn vs will  not  churn  that would be the starting point for frame hypothesis of why these set of customer be churn  generate report  info  graphic show how these predictor impact the tendency to churn  sometimes predictor be not cause but side  effect of the hidden because show up elsewhere  so be open to skepticism about the predictor   involve the business  user and have brainstorming session to go through each of these hypothesis and evolve the most practical and economic action that could be take to reduce churn  before PRON get into a discussion  plan for a sufficient amount of time  come up with PRON good thought action which PRON feel will address the situation and have all the detail glean from PRON analysis available for share in the session   this be an art more than a science and PRON ability to understand the domain as well as good communication will help PRON achieve this goal  
__label__machine-learning __label__svm __label__naive-bayes-classifier pardon PRON if PRON question be lame or not make sense  as a newbie PRON be try to grasp the concept   PRON be deal with a dataset have total 81 record  out of PRON PRON divide PRON to 54 record for training and rest for testing  PRON notice a few thing   accuracy be low for give test datum set 30     accuracy remain same no matter PRON use svm or naive bayes   if PRON test against 2  3 value accuracy shoot up to 75    be PRON normal or PRON be do something wrong   PRON be completely normal in some circumstance  if PRON consider the learning problem from a statistical perspective  learning be do by try to estimate the conditional estimate of PRON output variable give input variable  when PRON be do learn PRON basically have to main component  sample datum from training and the learning algorithm  and PRON use both to build an estimation of the aforementioned conditional probability  now consider the relation of each component with the desire estimate probability   training sample  training datum be only a sample from all the possible datum  PRON do not have all the possible datum since if PRON would have PRON PRON would not need to do learning  right  so a sample be just a fraction of the datum  this sample could cover more or less the whole domain of the possible datum  an immediate thought be that a big sample be good than a small one  as an extreme case to illustrate the idea be to have only one observation  but the size of the sample be not the whole story  PRON be possible that the probability PRON want to estimate be very complex  the complexity of that space require also more datum than a simple probability space  to give an example consider the problem of learn the mean of the height of male from usa  have a small random sample often would be enough if PRON assume a normal distribution  one the other hand take the example of learn to recognize speech  since pronounce be not the same against individual PRON would need a pretty big sample to unveil the useful relation need to predict and throw away thing which be not useful in the datum   model  learn algorithm  not any model be appropriate to express any true model of the signal PRON learn  for example if PRON true signal resemble a second degree polynomial would be impossible to fit properly with a linear model  other than some particular case  so there be a complexity in the model too  which could be compare with the complexity of the conditional probability PRON want to estimate during learning  even if PRON know the true model and PRON learn model be able to describe PRON  sometimes PRON need more datum to make the model able to fit  as another example consider a random forest which estimate a simple line  random forest be very flexible since the underlie statistical model be basically a constant give a local region in the input space  consider that  even if the model would be capable of estimate the true model  since the model be a local approximation  PRON need plenty of datum in every useful region of the space  to learn a good approximation   conclusion  PRON consider the individual component comparison with probability PRON want to estimate  but PRON have both  this make even hard to understand what be happen behind the wall of learning  thus there be no universal way of tell when datum be enough for a give model  so this be why be often the case that one algorithm can have different accuracy for different training set size and for different algorithm the required training set size to have approximate equal accuracy   one thing PRON try often in practice be give a learning model to check how accuracy evolf as the learning sample size increase  if PRON training sample be large enough PRON have typically a region in a small sample size where error be large due to not enough datum  after that PRON would have a region where error stay approximately the same and this perhaps be where PRON model do PRON be good  also be possible for even large sample size to have a decrease in performance for some model  maybe do to inability of the model to describe what be there or due to overfitt or some other case   PRON can trace this kind of behavior use some sort of bootstrap validation  where the sample use for training be take repeatedly for increase length  something like 20 sample with 5  size  20 sample with 10  size  and so on  this kind of information can give PRON an idea between the relation of the sample size with PRON learning problem for a give algorithm   PRON training and test error be affect by the size of the training  take a look to this plot  usually know as a learning curve   in this example  PRON compute the training score and the test score  cross validation score  of a naive bayes model as PRON increase the number of example in the training dataset  the high the score be  the good the model be perform   a large training set decrease the score because PRON be more difficult for the learn algorithm to learn a model that correctly represent all the training datum  however  as PRON increase the size of the training set  the test score also increase  due to an increase in the model s ability to generalise   as PRON can see  both line in the plot reach a limit on the right size  this mean that in order to learn properly  an algorithm require enough datum  just enough for PRON to get to the right side of this plot  once PRON reach that asymptote  PRON can not improve the test score by use more training datum   now  notice that this plot only refer to the size of the training set  and not to the size of the test set  but infer from PRON  PRON may assume that when PRON only use 2  3 example in PRON test set PRON do not have enough test datum to determine PRON test score  PRON may even get completely different result for other 2  3 different test example  therefore  PRON would assume that the 30  test error of PRON large test set be a most feasible value   PRON seem that  in order to increase the performance of PRON model  PRON will have to look for other reason  be PRON feature well define  do PRON have enough training datum  
__label__optimization __label__heuristics PRON be do research for academic credential on the use of metaheuristic optimisation of build envelope  enclosure  focus on the aspect of PRON thermal performance   the subject be handle with heavy emphasis on the integration of algorithm into the architectural design process by evaluate build thermal performance through energy simulation and feed back the result into the algorithm   PRON question be  have somewhat limited knowledge of mathematical optimisation  and very limited knowledge of programming language  how would PRON advise PRON to extend PRON research  ie what reference should PRON start look into  which computational programme  software would best serve PRON purpose   all within the general direction of PRON research topic mention above    PRON will typically get the most significant step forward if PRON can concisely state a problem in term of mathematic  when PRON have a concise formulation in term of  what the free variable be  what the objective function be  what the constraint be  then the next step be to find algorithm that be well  suit to the problem at hand  eg  can PRON show that the problem be convex  equality  inequality  constrain  continuous or integer    the book PRON always recommend to learn about optimization method be the one by nocedal and wright   numerical optimization   PRON do not need to understand everything in PRON  but PRON be an easy read and PRON will give PRON an idea of how to classify algorithm and how to approach problem  
__label__finite-element __label__fenics __label__boundary-conditions be there any way to implement an element wise contraction force  ie  a force which cause the fe PRON to contract onto PRON   for example this would happen when something dehydrate  preferably with first order tetrahedral element  but if the only way to do PRON be have a vertex in the centre of the element then high order will have to do  PRON just open up other issue to be solve  also PRON be only interested in the x2  direction  this would be something like a body force such as  dotbody force  displacementdxsubdomain   but instead of the body force be define in the global sense  and act in a global direction from a reference point  for example gravity act downward over the whole domain constrain by a boundary condition such as the ground stop the object fall  PRON would be within the local element with the reference frame be dependent on the element  PRON have try implement this with multiple body force on multiple subdomain act in opposite direction but this seem to because the force to effectively cancel each other out   a simple physical example would be a wet sponge sit on a table dry out  PRON have two force act on PRON  the first simple one be gravity in the vertical direction cause PRON to stay sit on the table  with the boundary condition of the table not let PRON fall  the second be the contraction force cause when the water evaporate and the sponge get small  if PRON be to model this use say 1 element  each of the vertex define the cell would need to contract to the cell local centre   this be implement in fenics   PRON simply seem that PRON want to calculate elasticity problem  with large displacement   PRON do not see a problem in this  just check hyperelasticity demo  but be careful  if PRON take an initial state as reference state notice that PRON reference state be not stress  free which be quite usual assumption in derive constitutive relation  after all PRON would calculate nothing if start from stress  free state   if PRON need  for some purpose  to deform mesh accord to calculated displacement there be facility for do this in fenics  check void meshmoveconst genericfunctionamp  displacement   then PRON be natural to use continuous piece  wise linear element  cg1  for displacement which just define displacement in all mesh vertex  PRON just need to reset PRON reference configuration after each time step  
__label__algorithms __label__computational-geometry __label__approximation-algorithms __label__convex-hull PRON have a set of point in a 2d space  PRON want to connect the outer point so PRON get the convex hull  the problem here be that there be a limit to the distance between two point  let PRON clarify that with a picture   the green line be line that be good  the distance between the two point of the convex hull do not exceed the maximum distance  the outer circle of a point   but the red line be not correct  PRON exceed the maximum distance   what PRON need be a line  through multiple point  correctly draw  eg all green line   as an alternative for the red line  while keep the path a kind of convex hull   PRON have be look in concave hull  and that may do the trick  but PRON have not be able to find a good  preferably in javascript  algorithm   
__label__machine-learning __label__nlp __label__dataset __label__text-mining PRON be look for a rather large amount of pdf file for test PRON text processing program  try look for an open site to get like some thousand pdfs  but be not able to find anything  PRON do not really know if that be the right place to ask  probably not  but maybe one have a good tip for PRON   thank in advance   a github repo with 1000 pdfs be here   another github repo have a corpus of pdf example  include edge  case  be here  
__label__machine-learning __label__nlp __label__encoding PRON be be build a text to speech system for a language call  kannada  use htk  from this page  the hled command supposedly convert the list of word level transcriptions to phone level transcriptions  ie replace each word with PRON phoneme  and put the result in a new phone level master label file  mlf    the output file phones0 be construct from   1  dict  dictionary file  ಅಂಚಿನಲ್ಲಿರು   ಅಂಚಿನಲ್ಲಿರು   ಅ ಂ ಚ ಿ ನ ಲ ್ ಲ ಿ ರ ು sp  ಅಂಚಿನಲ್ಲಿರುವ   ಅಂಚಿನಲ್ಲಿರುವ   ಅ ಂ ಚ ಿ ನ ಲ ್ ಲ ಿ ರ ು ವ sp  ಅಂಚೆ   ಅಂಚೆ   ಅ ಂ ಚ ೆ sp  ಅಂಚೆಕಛೇರಿ   ಅಂಚೆಕಛೇರಿ   ಅ ಂ ಚ ೆ ಕ ಛ ೇ ರ ಿ sp  ಅಂಚೆಗಾರ   ಅಂಚೆಗಾರ   ಅ ಂ ಚ ೆ ಗ ಾ ರ sp  ಅಂಚೆಚೀಟಿ   ಅಂಚೆಚೀಟಿ   ಅ ಂ ಚ ೆ ಚ ೀ ಟ ಿ sp  ಅಂಚೆಮನೆ   ಅಂಚೆಮನೆ   ಅ ಂ ಚ ೆ ಮ ನ ೆ sp  ಅಂಚೆಯಾಳು   ಅಂಚೆಯಾಳು   ಅ ಂ ಚ ೆ ಯ ಾ ಳ ು sp  ಅಂಚೆವೆಚ್ಚ   ಅಂಚೆವೆಚ್ಚ   ಅ ಂ ಚ ೆ ವ ೆ ಚ ್ ಚ sp  ಅಂಜದ   ಅಂಜದ   ಅ ಂ ಜ ದ sp  ಅಂಜಾಯ್ತ   ಅಂಜಾಯ್ತ   ಅ ಂ ಜ ಾ ಯ ್ ತ sp  ಅಂಜಿನಡುಗು   ಅಂಜಿನಡುಗು   ಅ ಂ ಜ ಿ ನ ಡ ು ಗ ು sp  ಅಂಜಿಸುಗ   ಅಂಜಿಸುಗ   ಅ ಂ ಜ ಿ ಸ ು ಗ sp  ಅಂಜಿಸುವ   ಅಂಜಿಸುವ   ಅ ಂ ಜ ಿ ಸ ು ವ sp  ಅಂಜು   ಅಂಜು   ಅ ಂ ಜ ು sp  ಅಂಜುಕುಳಿ   ಅಂಜುಕುಳಿ   ಅ ಂ ಜ ು ಕ ು ಳ ಿ sp  ಅಂಜುಬುರುಕ   ಅಂಜುಬುರುಕ   ಅ ಂ ಜ ು ಬ ು ರ ು ಕ sp  ಅಂಜುಬುರುಕನಾದ   ಅಂಜುಬುರುಕನಾದ   ಅ ಂ ಜ ು ಬ ು ರ ು ಕ ನ ಾ ದ sp  ಅಂಟಂಟಾದ   ಅಂಟಂಟಾದ   ಅ ಂ ಟ ಂ ಟ ಾ ದ sp  ಅಂಟಿಕೊಂಡಿರು   ಅಂಟಿಕೊಂಡಿರು   ಅ ಂ ಟ ಿ ಕ ೊ ಂ ಡ ಿ ರ ು sp  ಅಂಟಿಕೊಂಡಿರುವ   ಅಂಟಿಕೊಂಡಿರುವ   ಅ ಂ ಟ ಿ ಕ ೊ ಂ ಡ ಿ ರ ು ವ sp  ಅಂಟಿಕೊಳ್ಳದ   ಅಂಟಿಕೊಳ್ಳದ   ಅ ಂ ಟ ಿ ಕ ೊ ಳ ್ ಳ ದ sp  ಅಂಟಿಕೊಳ್ಳು   ಅಂಟಿಕೊಳ್ಳು   ಅ ಂ ಟ ಿ ಕ ೊ ಳ ್ ಳ ು sp  ಅಂಟಿಕೊಳ್ಳುವ   ಅಂಟಿಕೊಳ್ಳುವ   ಅ ಂ ಟ ಿ ಕ ೊ ಳ ್ ಳ ು ವ sp  ಅಂಟಿಸು   ಅಂಟಿಸು   ಅ ಂ ಟ ಿ ಸ ು sp  ಅಂಟು   ಅಂಟು   ಅ ಂ ಟ ು sp  ಅಂಟುಪದಾರ್ಥ   ಅಂಟುಪದಾರ್ಥ   ಅ ಂ ಟ ು ಪ ದ ಾ ರ ್ ಥ sp  ಅಂಟುಪುಡಿ   ಅಂಟುಪುಡಿ   ಅ ಂ ಟ ು ಪ ು ಡ ಿ sp  ಅಂಟುಬೇನೆ   ಅಂಟುಬೇನೆ   ಅ ಂ ಟ ು ಬ ೇ ನ ೆ sp  ಅಂಟುಮನೆ   ಅಂಟುಮನೆ   ಅ ಂ ಟ ು ಮ ನ ೆ sp  ಅಂಟುರೋಗ   ಅಂಟುರೋಗ   ಅ ಂ ಟ ು ರ ೋ ಗ sp  ಅಂಟುಸಿಹಿ   ಅಂಟುಸಿಹಿ   ಅ ಂ ಟ ು ಸ ಿ ಹ ಿ sp  ಅಂಟುಹಲಿಗೆ   ಅಂಟುಹಲಿಗೆ   ಅ ಂ ಟ ು ಹ ಲ ಿ ಗ ೆ sp  ಅಂಟುಹಿಟ್ಟು   ಅಂಟುಹಿಟ್ಟು   ಅ ಂ ಟ ು ಹ ಿ ಟ ್ ಟ ು sp  ಅಂಡಾಣು   ಅಂಡಾಣು   ಅ ಂ ಡ ಾ ಣ ು sp  ಅಂಡ್   ಅಂಡ್   ಅ ಂ ಡ ್ sp  ಅಂತ   ಅಂತ   ಅ ಂ ತ sp  ಅಂತಃ   ಅಂತಃ   ಅ ಂ ತ ಃ sp  ಅಂತಃಕಲಹ   ಅಂತಃಕಲಹ   ಅ ಂ ತ ಃ ಕ ಲ ಹ sp  ಅಂತದೃಷ್ಟಿ   ಅಂತದೃಷ್ಟಿ   ಅ ಂ ತ ದ ೃ ಷ ್ ಟ ಿ sp  ಅಂತರ   ಅಂತರ   ಅ ಂ ತ ರ sp  ಅಂತರಂಗ   ಅಂತರಂಗ   ಅ ಂ ತ ರ ಂ ಗ sp  ಅಂತರಂಗದ   ಅಂತರಂಗದ   ಅ ಂ ತ ರ ಂ ಗ ದ sp  ಅಂತರಪಲ್ಟಿ   ಅಂತರಪಲ್ಟಿ   ಅ ಂ ತ ರ ಪ ಲ ್ ಟ ಿ sp  ಅಂತರರಾಷ್ಟ್ರೀಯ   ಅಂತರರಾಷ್ಟ್ರೀಯ   ಅ ಂ ತ ರ ರ ಾ ಷ ್ ಟ ್ ರ ೀ ಯ sp  ಅಂತರಲಾಗ   ಅಂತರಲಾಗ   ಅ ಂ ತ ರ ಲ ಾ ಗ sp  ಅಂತರವುಳ್ಳ   ಅಂತರವುಳ್ಳ   ಅ ಂ ತ ರ ವ ು ಳ ್ ಳ sp  ಅಂತರಾರ್ಥ   ಅಂತರಾರ್ಥ   ಅ ಂ ತ ರ ಾ ರ ್ ಥ sp  ಅಂತರಾಷ್ಟ್ರೀಯ   ಅಂತರಾಷ್ಟ್ರೀಯ   ಅ ಂ ತ ರ ಾ ಷ ್ ಟ ್ ರ ೀ ಯ sp  ಅಂತರಿಕ್ಷಕ್ಕೆ   ಅಂತರಿಕ್ಷಕ್ಕೆ   ಅ ಂ ತ ರ ಿ ಕ ್ ಷ ಕ ್ ಕ ೆ sp  ಅಂತರ್ಗತ   ಅಂತರ್ಗತ   ಅ ಂ ತ ರ ್ ಗ ತ sp  ಅಂತರ್ಜಾತೀಯ   ಅಂತರ್ಜಾತೀಯ   ಅ ಂ ತ ರ ್ ಜ ಾ ತ ೀ ಯ sp  ಅಂತರ್ದೇಶೀಯ   ಅಂತರ್ದೇಶೀಯ   ಅ ಂ ತ ರ ್ ದ ೇ ಶ ೀ ಯ sp  ಅಂತರ್ದೋಷ   ಅಂತರ್ದೋಷ   ಅ ಂ ತ ರ ್ ದ ೋ ಷ sp  ಅಂತರ್ಧಾನವಾಗು   ಅಂತರ್ಧಾನವಾಗು   ಅ ಂ ತ ರ ್ ಧ ಾ ನ ವ ಾ ಗ ು sp  mkphones0l  ex  be sil sil  de sp  wordsmlf  which have the sorted list of word  below be just a sample     mlf     audio  kn10681wavlab   ಇಳಿ     audio  kn243wavlab   ಸ್ವಲ್ಪ     audio  kn3173wavlab   ಕೊಂಚ     audio  kn5862wavlab   ರಾಯಭಾರಿ     audio  kn202862wavlab   ಬುದ್ಧಿರೂಡ     audio  kn204645wavlab   ಸಾರಸಂಗ್ರಹ     audio  kn14909wavlab   ಹೊರಗಿನಿಂದ     audio  kn16829wavlab   ಪ್ರಾರಂಭದಿಂದ     audio  kn209884wavlab   ಮಣಿಚೌಕಟ್ಟು     audio  kn55886wavlab   ತೊರೆ     audio  kn60433wavlab   ಬಿಟ್ಟುಕೊಡುವುದು     audio  kn192154wavlab   ಜಾರುಕೀಲು     audio  kn24203wavlab   ತಗ್ಗು     audio  kn183746wavlab   ತಗ್ಗಿಸು  now  PRON execute the hled command    hled a d t 1 l    d dict i phones0mlf mkphones0l wordsmlf  the output should be basically every character on every line  since the character PRON be the phone  PRON believe PRON get the output  however  the text appear as follow in phones0mlf     mlf      kn10681wavlab   sil  340262207  340262263  340262277  sil      kn243wavlab   sil  340262270  340263215  340262265  340262262  340263215  340262252  sil      kn3173wavlab   sil  340262225  340263212  340262202  340262232  sil      kn5862wavlab   sil  340262260  340262276  340262257  340262255  340262276  340262260  340262277  sil      kn202862wavlab   sil  340262254  340263201  340262246  340263215  340262247  340262277  340262260  340263202  340262241  sil      kn204645wavlab   sil  340262270  340262276  340262260  340262270  340262202  340262227  340263215  340262260  340262271  sil      kn14909wavlab   sil  340262271  340263212  340262260  340262227  340262277  340262250  340262277  340262202  340262246  sil      kn16829wavlab   sil  340262252  340263215  340262260  340262276  340262260  340262202  340262255  340262246  340262277  340262202  340262246  sil      kn209884wavlab   sil  340262256  340262243  340262277  340262232  340263214  340262225  340262237  340263215  340262237  340263201  sil      kn55886wavlab   sil  340262244  340263212  340262260  340263206  sil      kn60433wavlab   sil  340262254  340262277  340262237  340263215  340262237  340263201  340262225  340263212  340262241  340263201  340262265  340263201  340262246  340263201  sil    PRON think every line be encode in some format that PRON do not recognise  every line should be a simple kannada character like ಅ or ಕ however  PRON get something like 340263212 instead  so  PRON try define a configuration file call configphoneconf with a single line   charset  utf8  but the output remain unchanged  what should PRON do to make phones0mlf display character in kannada   
__label__machine-learning __label__mod PRON believe that in most case  the positive case be arbitrarily define to be the case with a low frequency  if PRON instead define the negative case as the one with the low frequency  what difference would this entail for the model   for all of the algorithm  procedure that PRON know  PRON make no difference between a positive or negative interpretation of a class   the fact that one category have a low frequency  by that PRON be assume that PRON mean a low probability  be insignificant  PRON be just a matter of communication  by maintain the general designation in a specific field so that other will be able to read PRON work more easily   as an example  in anomaly detection  one designate the case with low frequency as anomaly  but even so  technically PRON do not matter how PRON label PRON case  PRON be interchangeable  
__label__optimization __label__least-squares PRON ask for clarification about a recent question about minpack  and get the following comment   any system of equation be equivalent to an optimization problem  which be why newton  base method in optimization look a lot like newton  base method for solve system of nonlinear equation   what confuse PRON about this comment  and relate negative opinion about specialize nonlinear least square solver like minpack  may be best explain on the example of the conjugate gradient method  this method be applicable to system  ax  b with a symmetric positive definite matrix  a  PRON could also be use to solve the general least square problem  operatornameminxax  b2  for an arbitrary matrix  a  but do so be not recommend  one explanation why PRON should not do this be that the condition number of the system would increase significantly   but if turn a system of equation into an optimization problem be consider problematic even for the linear case  why should PRON be less problematic for the general case  PRON seem to be somehow relate to use a state of the art optimization algorithm  instead of use a slightly age nonlinear least square solver  but be not the problem relate to throw away information and increase the condition number of the system relatively independent of the actually use optimization algorithm   if a give nonlinear system be the first order optimality condition for an optimization problem  then PRON can often produce a more robust algorithm by use that information  for example  consider the equation    fx   x2  expbig4x22 big  qquad textclick for wolfram alpha  this clearly have a unique minimum and PRON expect PRON optimization method to find PRON regardless of starting point  but if PRON only look at the first order optimality condition  PRON be look for a solution  x of  nabla fx   0   wolfram alpha   which have a unique solution  but many rootfinding method can get stick at the local minimum   if PRON reformulate a new optimization problem to minimize the norm of the gradient square  PRON be look for a global minimum  x of  lvertnabla fxrvert2   wolfram alpha  which have multiple local minima   to summarize  PRON start with an optimization problem that have a unique solution that PRON could guarantee that a method would find  PRON reformulate as a nonlinear root finding problem that have a unique solution that PRON could identify locally  but a rootfinding method  like newton  may stagnate before reach PRON  PRON then reformulate the root finding problem as an optimization problem that have multiple local solution  no local measure can be use to identify that PRON be not at the global minimum    in general  each time PRON convert a problem from optimization to rootfind or vice  versa  PRON make the available method and associate convergence guarantee weak  the actual mechanic of the method be often very similar so PRON be possible to reuse a lot of code between nonlinear solver and optimization   since one of PRON answer have be cite  PRON will try to clarify why PRON suggest use ipopt instead of minpack   PRON objection to use minpack have nothing to do with the algorithm that minpack use and everything to do with PRON implementation  PRON main objection be that the software date back to 1980  and be last update in 1999  jorge moré be retire  PRON doubt PRON or any of the other author of the software keep tab on PRON anymore  and there be no team of people actively support PRON  the only documentation PRON can find on the software be the original  1980 argonne technical report write by jorge moré and the other minpack author   chapter 1  3 can be find here  and chapter 4 can be find here   after search the minpack source code and peruse the documentation  the pdf be scan image  and can not be search   PRON do not see any option to accommodate constraint  since the original poster of the nonlinear least  square problem want to solve a constrain nonlinear least  square problem  minpack will not even solve that problem   if PRON look at the ipopt mailing list  some user indicate that performance of the package on nonlinear least square  nls  problem be mixed relative to levenberg  marquardt algorithm and more specialized nls algorithm  see here  here  and here   the performance of ipopt relative to nls algorithm  be  of course  problem dependent  give that user feedback  ipopt seem like a reasonable recommendation relative to nls algorithm   however  PRON make a good point that nls algorithm should be investigate  PRON agree  PRON just think that a package more modern than minpack should be use because PRON believe PRON will perform good  be more usable  and be support  cere seem like an interesting candidate package  but PRON can not handle constrain problem right now  tao would work on box  constrain least  square problem  although PRON do not implement the classic levenberg  marquardt  but instead implement a derivative  free algorithm  a derivative  free algorithm would probably work well for large  scale problem  but PRON would not use PRON for small  scale problem  PRON could not find any other package that inspire a great deal of confidence in PRON software engineering  for instance  galahad do not seem to use version control or any automate testing  at first glance  minpack do not seem to do those thing either  if PRON have experience with minpack or recommendation regard good software  PRON be all ear   with all of that in mind  get back to the quote of PRON comment   any system of equation be equivalent to an optimization problem  which be why newton  base method in optimization look a lot like newton  base method for solve system of nonlinear equation   a good comment be probably something to the effect of   when PRON want to solve a system of  n equation with  n unknown  gx   0   PRON can formulate this as a least square optimization problem   paraphrase of last paragraph of p102 of nonlinear programming  2nd edition  by dmitri bertsekas    this statement hold even for solve system of equation under constraint  PRON do not know of any algorithm that be consider  equation solver  for the case where there be constraint on the variable  the common approach PRON know of  perhaps jaundice by several semester of optimization course and research in an optimization lab  be to incorporate the constraint on the system of equation into an optimization formulation  if PRON be to try to use the constraint in a newton  raphson  like scheme for equation solve  PRON would probably end up with a project gradient or project trust  region method  much like method use in constrained optimization  
__label__machine-learning __label__nlp __label__svm __label__naive-bayes-classifier PRON be collect twitter tweet for sentiment analysis  PRON choose to use multinominal navy theorem for find out the sentiment   PRON find some example of svm theorem make use of stop word  PRON question be that  can PRON apply that stop word in multi nominal navy theorem  or not   the basic task here PRON get be sentiment analysis of tweet  so for this  PRON can first extract feature and then use a classifier  stop word be those which be present for grammatical purpose but do not add any value  meaning to the sentence   so  yes PRON can remove the stop word from the tweet  extract the feature and then pass PRON to the classifier  PRON can get already exist stop word from  from nltkcorpus import stopword  stopword  stopwordswordsenglish    remove the stop word do not always improve the accuracy   take a look at this as PRON be a very similar problem statement as PRON  hope PRON help  
__label__machine-learning __label__classification __label__dataset __label__image-classification be there any standard approach for detect the covariate shift between the training and test datum  this would be useful to validate the assumption that covariate shift exist in PRON database which contain a few hundred image   PRON do not give many clue about what property of the image PRON may be consider  but PRON seem that what PRON may want to measure be the difference in the distribution of the training and test set  a useful place to start would be with the kullback – leibler divergence  which be a measure of the difference of two distribution   there be method like the kullback  leibler divergence model  the wald  wolfowitz test for detect non  randomness and covariance shift   a simple test for quick analysis of covariance test would be to build a machine learning model  where the model be repeatedly test with inputt training datum and the production datum   in case  the model can make out the difference between the training and production dataset then PRON can be a sign of covariance shift   adaptive learn with covariate shift  detection for motor imagery  base brain – computer interface  httplinkspringercomarticle101007s0050001519375  ewma model base shift  detection method for detect covariate shift in non  stationary environment  httpwwwsciencedirectcomsciencearticlepiis0031320314002878   here be a simple procedure PRON can use   learn a classifier to distinguish between train  test datum  use regular x feature   compute the phi correlation coefficient to estimate the quality of the classifier  the separability of the train  test datum  set a threshold  eg 2  above which PRON can claim there be a covariate shift  and start look as correction   the problem of covariate shift ultimately result in dataset with different underlying mathematical structure  now  manifold learning estimate a low dimensional representation of high  dimensional datum thereby reveal the underlie structure  often manifold learning technique be not projection  therefore  different and more powerful  than standard pca   PRON have use manifold learning technique  for eg  isomap  mds  etc  to visualize  and  if possible  quantify  the   dissimilarity  between train and test dataset  
__label__machine-learning __label__predictive-modeling __label__survival-analysis __label__churn PRON be try to define a churn prediction model for an online service  bet  gambling   a lot of paper talk about churn analysis  prediction for telco company where define a churn user be straightforward  a churn user be a user who cancel PRON or PRON contract  PRON be wonder how to define a churn user within the context of an industry where user do not cancel any contract  account   PRON can envision a general approach  if a user who play every day for a month stop play for 2 day  this be could be flag  but a user who only play once a week and then have no activity for 2 day would not  what be the good approach to encode this logic algorithmically  be survival analysis and good approach   PRON should first define what PRON churn event be which PRON have start  be PRON global or individual  be PRON have not gamble for 3 month or have change PRON pattern  global be good to model  PRON can use survival model for this   first PRON want to say dirk be basically correct that survival analysis do not have to model death PRON be use essentially the same way for look at user group in cohort analysis   however  PRON be unreliable for specific individual behavior activity  for say fraud management  revenue analysis and trend forecasting be great us of survival model   that have be say the situation be not hopeless for individual behavior  simply not so straight  forward   have run into a similar situation in pre  pay card account for a mobile company  PRON can model an individual s behavior in the way PRON try to  by model PRON as a process control issue  and then follow good practice  by randomly sample dataimportant  of an individual s behavior  usage  and compare PRON to similar behavior of other user s  normalizing for environmental factor  and look at the control point  more than 3 sequential out of control point in a period or more than 8 overall be base set   this can then give PRON a set  suspicious  potentially  churn or fraud  activity which can then be analyze and label to train good model   hope that be helpful   another approach would be to model  churn   aka  diminish use of the service  include non  use   as a process and not an event   years ago in retention marketing this be call a  defection funnel   to mirror the  sale funnel  on the customer acquisition side  suspect   prospect   trial customer   repeat customer   loyal customer    so a defection funnel may look like this   loyal  frequent customer   disaffect customer   infrequent customer   non  customer  PRON be proper to label this a  funnel  because a subset of each stage move on to the next stage   not all  disaffect customer  become  infrequent customer   and so on   there be two key to define a defection funnel   first  define the behavioral characteristic for the funnel as a whole  and for each stage  category  within the funnel   in PRON case  PRON may be a variation of the familiar rfm   recency   frequency  and  monetary   score   second  PRON need to identify other behavioral characteristic or pattern that show a propensity to move from one stage to another   these may include  complaint  be victim of trolling  experience service interruption  experience bet loss  how long PRON have be use the service  or maybe PRON be just demographic   with these definition in place  PRON be in a position to build a predictive model and also track over time the population of customer in each category   by the way  this approach be not dependent on divide the funnel into  stage   nor do PRON matter fundamentally how many stage PRON have   however  define discrete stage have many practical benefit  include communicate PRON model and result to other people who be not quantitative or statistically minded  
__label__linear-algebra __label__optimization __label__matlab __label__matrix PRON have be try to use the matlab s pcg   function to minimize an energy functional  convert minimization problem to the solution of a linear system be straight  forward  the 1st derivative of a function  or functional  with respect to a give variable have to be zero  amp  this give a linear equation for that variable  ex  take the taylor expansion of a function  fx around  x0 if minimization condition  fracpartial fxpartial xx  x00  be to be fulfil  one need to solve  fx0fracpartial fxpartial xx−x00  for  x0   which be a linear equation of the form  ax  b for a problem in 1d  PRON be straight forward  the diagonal matrix  ax  represent as a column vector  and  b  column vector again  be pass via function handle   similarly in the 3d case  if a fast  fourier  transform discretization yield a 3d matrix  array with 3 index   can the pcg be use by pass the matrix  a and  b to PRON in any of the matrix pack format  ie reduce the 3d matrix to a column vetor   if so  be there a way to specify this format to the pcg routine  just convert a 3d matrix to column vector with reshape   be wrong  since without any format specify  pcg   assume the whole column vector to be the matrix  a  or  b  and minimize PRON give rubbish   edit  christian clason  thank for the clarification  PRON could not post a comment   PRON be right about pass a function handle ie  ax to  pcg  that be exactly what PRON do  PRON be basically use a code someone have write to minimize the gross  pitaevskii equation  a 2nd order pde  use fft method  for both 1d  amp  3d case  in 1d  PRON be a column vector and in 3d  one end up with a 3 index array  PRON suspect the code be write to store a diagonal matrix as just a column vector   so for a 3d problem  PRON be wonder how one minimize an  ax which be a 3d array   do just a  reshape do not seem to converge  PRON understand that with most linear system solver base on lapack  blas  the usual convention be to represent multiple solution as column vector  amp  to retain square matrix  one take the same number of solution as the number of grid point  so when give a square matrix  a  PRON assume pcg recognize PRON have multiple lhs  left hand side  expression  for a given single rhs ie  b be that so  please correct if PRON be wrong   when PRON reshape a 3d array into a column vector  PRON suspect  pcg recognize this as just one single lhs of the problem  amp  optimization fail  so PRON wonder if  a can a matrix packing format be use  amp   pcg be tell the packing format  or  b be there an example multidimensional optimization problem do in any publication that use  pcg  PRON could not find any after google as thoroughly as PRON could  with PRON limited knowledge   thank   
__label__data-mining __label__bigdata __label__dataset PRON be a new bee for datum analysis  i need to work on a research project in big datum analysis  first of all i search for a dataset and i find interest in stack exchange  datum dump  however i browse for research i find a lot and whatev i think about a idea base on this dataset  PRON already do by someone else   please help PRON out with a new and useful idea for PRON research   PRON may help to have a bit more information on what PRON want to do  but have a look at kaggle  PRON be a regular big data science competition  all of the data set be huge and available for download   there be also some small dataset make available by tableau   hope that give PRON some inspiration  
__label__r __label__data-mining __label__decision-trees PRON problem have three categorical variable c1c2  c3 and one continous variable x  predict a continuous outcome y PRON can visualize the problem with the follow reproducible code  apology for the badly write code    librarydatatree   i  expandgridcabccdcef     i  iorderi1i2i3      i4   c18   t  expandgridcabccdcefseq0101    t  tordert1t2t3      t  joint  i  by  cvar1var2var3     tvar5  runifnrowt    tpathstring  lt witht  pastetree   var1  var2  var3  v4  sep     plottree  lt as  nodet   plotplottree   ggplotdata  t  aesx  var4  y  var5    geomline    facetgridv4   the tree categorical variable show 8 possible path combination   and within each path there be a distribution of y depend on a continuous variable x   PRON would like to bin both the continuous y and x variable such that PRON be leave with a more concise decision tree  there be method to this madness as in PRON actual problem  some categorical path will become insignificant due to no movement in the predict y beyond the established volatility threshold of 5    PRON can manually obtain bin by brute force  but be there a bin algorithm that support bin of both predict and predictor variable  PRON have look at smbinn and PRON knowledge in this area of algorithm be limit  PRON actual problem have many categorical and continuous variable result in a more complex structure  but PRON would like to convert PRON to a decision tree result which will help PRON understand the significance and movement of variable better   
__label__constrained-optimization PRON be try to optimize a constrain  problem with a discrete  non  linear objective function  evaluate this function be also fairly expensive  nevertheless  despite the above two factor  PRON hope  that PRON can still be solve  efficiently  since the structure of the constrain parameter space should be helpful   to be more exact  the parameter space will in general be of dimension 4  150  the parameter lie on a n  simplex  ie   beginalign   sumi1n pi  1   pi geq 0   forall   i  1dotsc  n  endalign   now PRON question be  which algorithm could work best for solve such type of problem   so far PRON have try variant of the following   constrain the space by  1varepsilon leq sumi1n pi leq 1  varepsilon  gt0  and then apply an adaptive barrier method combine with the nelder  mead algorithm  r constroptim function   apply unconstrained optimization in  mathbb rn by modify the objective function  so that in the first step PRON normalize the parameter appropriately   map the simplex to the unit sphere  then perform unconstrained optimization use nelder  mead  the subplex algorithm or the covariance matrix adaptation evolution strategy  cmaes  algorithm base on the spherical coordinate   so far spherical coordinate follow by cmae show the good result  but PRON be too slow  what else could PRON try   generally speak  for derivative  free optimization  there be no one algorithm that work best for all problem  although there be some algorithm that tend to work better than other  the classic recent review reference for derivative  free optimization be by rios and sahinidis  see also this related presentation by sahinidis  and PRON suggest the mcs algorithm  the lgo algorithm  proprietary implementation   and bobyqa  newuoa  both of which be implement by nlopt    derivative  free method tend not to incorporate general constraint  if PRON be use a method that do not permit PRON linear constraint  PRON can either normalize the parameter as PRON be do right now  or PRON can use more of a barrier  type approach and penalize violation of the constraint with a penalty term and increase the penalty parameter over a number of solf   in addition to geoff s answer  PRON can also just eliminate the linear constraint  the  mollification   1varepsilon le sumi pi le 1  really create more problem than PRON solve because PRON leave PRON optimize in an  n dimensional domain that be  however  very narrow in one direction  a good option be to just replace  pn 1  sumi1n1  pi everywhere and add the constraint  sumi1n1  pi le 1  to PRON list of inequality constraint  this way  PRON can simply express everything in term of just  n1  variable  in essence  PRON be now optimize over the interior of an  n1  simplex  
__label__machine-learning __label__feature-selection with the amount of geographic diversity that be pop up over time with chipotle and the outbreak of e  coli  if PRON be investigate the source of all these issue  and where PRON could occur again in the future  where would PRON begin   while PRON initially think PRON would be very simple to narrow PRON down  the most recent cdc article indicate PRON have suspicion but nothing for sure yet to indicate how to track PRON down   PRON be relatively new to machine learn algorithm  but PRON would think a feature classification that would take into account the ingredient involve in positive case  the source of the ingredient  and possibly even the machinery use or method of cooking to help classify the input that relate to a positive or negative output would be a good start   
__label__matlab __label__machine-learning __label__modeling PRON be try to identify a non  linear plant use the identification toolbox of matlab   in particular PRON would like to identify a non  linear arx model  where the nonlinearitie be express by mean of tree partition   accord to mathworks  httpitmathworkscomhelpidentreftreepartitionhtml    treepartition be a nonlinear function y  fx   where y be scalar and  x a 1by  m vector   f be a piecewise linear   affine  function of x   fx   xl  1xca  d when x belong to da  where  l be a 1by  m vector  ck be a 1bym1  vector   and dk be partition of the x  space   the active partition da be determine as an intersection of  half  space by a binary tree as follow   first a tree with n  node and j level be initialize  a node at level j be a  terminate  leaf  and a node at a level j  lt  j have two descendant at  level j1  all level be complete  so n  2j11   the  partition at node r be base on  1xbr  0 or  lt 0  move to left or  right descendant    where br be choose to improve the  stability of least  square computation on the partition at the  descendant  node  then at each node r the coefficient cr of good  linear approximation of unknown regression function  on dr be  compute use penalize least  square algorithm       when the value of the mapping f  define by the treepartition object  be compute at x  an adaptive algorithm select the active node k of the tree on the branch of partition which contain x  in this description PRON be not clear what be the underlying algorithm use to identify the current active node  indeed  at a first glance  PRON tought that all the intermediate node be decisional node and the terminating leafs be contain the unknown coefficient that have to be use for a particular pwa representation   that approach be not correct  indeed as bold in the citation  every single node have PRON own set or parameter that good fit the unknown regression function in dr   be there a clear reference andor explanation about this   thank  
__label__numerical-analysis __label__computational-physics __label__fortran __label__runge-kutta PRON be try to calculate real speed and time in free fall of a body  PRON write a code in fortran and PRON be try to improve PRON by use rk4 method  x  time y  total free fall  purple line use   do while   k1k2   gt  y   y  y  tgmt2r  y2     sqrt2gmyrr  y      time  time  t  end do  green line usingrk4 method    do while   d1d2   gt  y   k1  tgm02r  y2     sqrt2gmyrr  y      k2   t2gmt22ryk12     sqrt2gmyk1rryk1        k3   t2gmt22ryk22     sqrt2gmyk2rryk2        k4  tgmt2ryk32     sqrt2gmyk3rryk3       y  y   1060k1  2k2  2k3k4   time  time  t  end do  all of other code be the same  stepsize  for example  and PRON be get good result without rk4  PRON guess PRON do not understand rk4 right and PRON write PRON wrong but PRON can not see PRON mistake  what should PRON change to get good result with rk4 or do PRON do PRON wrong    PRON be use t for the time step  PRON would be clear to use dt   k1   should be the derivative  not the step  so PRON should use  k1    gmt2r  y2     sqrt2gmyrr  y      to calculate the first derivative  and also for k2   do not multiply by time either   then when calculate k2  and k3  do not use yk1 but y  05dtk1 inside the formula  the first two tentative step in rk4 be only half step long   to make the code look nice  PRON should write a function that calculate the acceleration  instead of paste the same formula 4 time  less visual clutter also help think   also PRON equation be wrong  but that be a separate issue  with these correction at least PRON should be able to get the same result from PRON euler and rk version  
__label__python __label__classification __label__tensorflow __label__inception PRON be test the machine learn water and use ts inception model to retrain the network to classify PRON desire object   initially  PRON prediction be run on locally store image and PRON realize that PRON take anywhere between 2  5 second to unpersist the graph from a file and around the same time to run the actual prediction   thereafter  PRON adapt PRON code to incorporate the camera feed from opencv but with the above note time  video lag be inevitable   a time hit be expect during initial graph load  which be why initialsetup   be run beforehand  but 2  5 second be just absurd   PRON feel like with PRON current application  real  time classification  this be not the good way of load PRON  be there another way of do this  PRON know with mobile version ts recommend trim down the graph  would slimm PRON down be the way to go here  in case PRON matter PRON graph be currently 874 mb  along with this  be there a way of speed up the prediction process   import os  import cv2  import timeit  import numpy as np  import tensorflow as tf  camera  cv2videocapture0    load label file  strip off carriage return  labelline   linerstrip   for line  in tfgfilegfileretrainedlabelstxt     def grabvideofeed     grab  frame  cameraread    return frame if grab else none  def initialsetup     osenvirontfcppminloglevel     2   starttime  timeitdefaulttimer     this take 2  5 second to run   unpersist graph from file  with tfgfilefastgfileretrainedgraphpb    rb   as f   graphdef  tf  graphdef    graphdef  parsefromstringfread     tfimportgraphdefgraphdef  name    print  took   second to unpersist the graphformattimeitdefaulttimer    starttime   def classifyimagedata    print           session start            with tf  session   as sess   starttime  timeitdefaulttimer     feed the imagedata as input to the graph and get first prediction  softmaxtensor  sessgraphgettensorbynamefinalresult0    print  tensor   softmaxtensor  print  took   second to feed datum to graphformattimeitdefaulttimer    starttime   starttime  timeitdefaulttimer     this take 2  5 second as well  prediction  sessrunsoftmaxtensor    mul0   imagedata    print  took   second to perform predictionformattimeitdefaulttimer    starttime   starttime  timeitdefaulttimer     sort to show label of first prediction in order of confidence  topk  predictions0argsortlenpredictions01   print  took   second to sort the predictionsformattimeitdefaulttimer    starttime   for nodeid in topk   humanstr  labellinesnodeid   score  predictions0nodeid   prints  score   5f     humanstring  score    print           session end            initialsetup    while true   frame  grabvideofeed    if frame be none   raise systemerrorissue grab the frame    frame  cv2resizeframe   299  299   interpolation  cv2intercubic    adhere to ts graph input structure  numpyframe  npasarrayframe   numpyframe  cv2normalizenumpyframeastypefloat    none  05  5  cv2normminmax   numpyfinal  npexpanddimsnumpyframe  axis0   classifynumpyfinal   cv2imshowmain   frame   if cv2waitkey1   amp  0xff   ordq     break  camerarelease    cv2destroyallwindow    edit 1  after debug PRON code  PRON realize that session creation be a both resource and time consume operation   in the prior code  a new session be create for each opencv feed on top of run the prediction   wrap the opencv operation inside a single session provide a massive time improvement but this still add a massive overhead on the initial run  prediction take 2  3 second  afterwards  the prediction take around 05s which make the camera feed still laggy   import os  import cv2  import timeit  import numpy as np  import tensorflow as tf  camera  cv2videocapture0    load label file  strip off carriage return  labelline   linerstrip   for line  in tfgfilegfileretrainedlabelstxt     def grabvideofeed     grab  frame  cameraread    return frame if grab else none  def initialsetup     osenvirontfcppminloglevel     2   starttime  timeitdefaulttimer     this take 2  5 second to run   unpersist graph from file  with tfgfilefastgfileretrainedgraphpb    rb   as f   graphdef  tf  graphdef    graphdef  parsefromstringfread     tfimportgraphdefgraphdef  name    print  took   second to unpersist the graphformattimeitdefaulttimer    starttime   initialsetup    with tf  session   as sess   starttime  timeitdefaulttimer     feed the imagedata as input to the graph and get first prediction  softmaxtensor  sessgraphgettensorbynamefinalresult0    print  took   second to feed datum to graphformattimeitdefaulttimer    starttime   while true   frame  grabvideofeed    if frame be none   raise systemerrorissue grab the frame    frame  cv2resizeframe   299  299   interpolation  cv2intercubic   cv2imshowmain   frame    adhere to ts graph input structure  numpyframe  npasarrayframe   numpyframe  cv2normalizenumpyframeastypefloat    none  05  5  cv2normminmax   numpyfinal  npexpanddimsnumpyframe  axis0   starttime  timeitdefaulttimer     this take 2  5 second as well  prediction  sessrunsoftmaxtensor    mul0   numpyfinal    print  took   second to perform predictionformattimeitdefaulttimer    starttime   starttime  timeitdefaulttimer     sort to show label of first prediction in order of confidence  topk  predictions0argsortlenpredictions01   print  took   second to sort the predictionsformattimeitdefaulttimer    starttime   for nodeid in topk   humanstr  labellinesnodeid   score  predictions0nodeid   prints  score   5f     humanstring  score    print           session end            if cv2waitkey1   amp  0xff   ordq     sessclose    break  camerarelease    cv2destroyallwindow    edit 2  after fiddle around  PRON stumble into graph quantization and graph transformation and these be the attain result   original graph  874 mb  quantized graph  875 mb  transform graph  871 mb  eight bit calculation  22 mb but run into this upon use   PRON have PRON own project that work similarly  but much simple model   and PRON take PRON about 01s to run PRON prediction in real time  PRON do the right thing by re  use the session  that be what PRON do too  PRON guess be that PRON bottleneck be the size of the model  as far as PRON be aware  the inception model be huge  there will always be a tradeoff between model complexity and runtime prediction speed  the inception model be accurate  but no one ever say PRON be fast  
__label__experiment PRON do computational work as a phd student and PRON try to find a correct way to separate code  solver  from the computational experiment base on this solver   basically  each project that PRON do revolve around one particular solver  matlab or python code   and for each project there be a set of experiment that should be do  before  PRON come to the idea to do PRON in the following way   project  01  2016  01  01task1  02  2016  01  01task2  03  2016  01  03task3  such that each subfolder correspond to one particular task  or experiment  that PRON do  each subfolder contain all the output from the solver and the solver code PRON as PRON frequently have to modify the solver for a particular experiment   PRON have already deduce that the good way to modify the solver code be in a backward  compatible way  such that if an experiment require that PRON change the code to use  for example  a different time integrator  PRON solve pde   then a new configuration parameter should be add to the solver which define what time integrator to use in the runtime   however  PRON have a problem with this approach  keep local version of solver for each experiment   PRON have to merge all these different version together and PRON do PRON manually now  recently  an idea come to PRON that a good organization be  project  code  task  01  2016  01  01task01    27  2016  03  06task27  that is  that the code should be centralize and version control   now imagine the follow situation   experiment  n require that PRON modify the code and then run solver three time with different parameter and each run take 20 minute   PRON do PRON with a bash script which run the solver three time sequentially   in the meantime  PRON start experiment  n1  which also require code modification   PRON start to modify the code and PRON can break the experiment  n  for example  between run 1 and run 2    how do PRON handle such situation   be PRON good to fork the solver code for each experiment and keep PRON under the experiment subfolder   update 2016  09  28  accord to the answer and comment that PRON get  PRON seem that the question be not write clear enough  particularly the title   PRON question be about how to conduct experiment base on a simulation code  where the code must evolve simultaneously with the progress of the project  indeed  usually  the code admit some generality from the very start  for example  all problem parameter can be specify as command  line argument  as well as some solver parameter   however  PRON be quite possible that the code will require further generalization after a series of experiment  for example  one finite  difference approximation should be replace by another  in this case  as many point out below  the good solution be to add an option to the solver  which choose the need approximation during the runtime   therefore  PRON be good to version  control PRON as well as to isolate a copy of the code for each experiment  this be what PRON question be about  how to deal with multiple copy of the code for each experiment   if PRON do not use any version  control system  PRON can just copy the code into the subdirectory of each experiment and then manually merge any modification to the  golden  copy of the code  this be PRON previous approach    if PRON use version  control system git  PRON can make multiple clone of the repository to have isolate copy  however  as git 25  provide a feature to have multiple work copy of the repository  the answer that PRON write below propose more elegant approach   thank to everyone for PRON comment   PRON have find the follow good solution to PRON question   apparently  each task should have PRON own version of the code to isolate task from each other and to ensure reproducibility in a sense that PRON must be always clear what version of the code be use for the task   the remaining question be  how to avoid clone a git repository multiple time   git version 25  allow create multiple worktree connect to the same repository   use multiple worktree  PRON can avoid clone the repository multiple time  and at the same time track all version of the code in separate branch   so  the structure of the project will be like this   project  code  task  01  2016  01  01task01  code    27  2016  03  06task27  code  where each directory 01  2016  01  01task01 and similar will have PRON own copy of the code set to PRON own branch   to create additional worktree  the follow git command be use   cd  ltprojectgtcode  git worktree add b 01  2016  01  01task01    tasks01  2016  01  01task01code master  which create a branch 01  2016  01  01task01 from master and check PRON out into the directory  tasks01  2016  01  01task01code   then  when the work on the give task be finish  PRON can merge the correspond branch into the master branch   part of PRON question be address in a recent publication  good enough practices in scientific computing   reference 1   primarily in point 4  PRON add the whole list  though   data management  save the raw data   create the datum PRON wish to see in the world  create analysis  friendly datum   record all the step use to process datum   anticipate the need to use multiple table   submit datum to a reputable doi  issue repository so that other can access and cite PRON   software  place a brief explanatory comment at the start of every program   decompose program into function   be ruthless about eliminate duplication   always search for well  mantain software library that do what PRON need   test library before rely on PRON   give function and variable meaningful name   make dependecie and requirement explicit   do not comment and uncomment section of code to control a program s behavior   provide a simple example or test datum set   submit code to a reputable doi  issue repository   collaboration  create an overview of PRON project   create a share public  to  do  list   make the license explicit   make the project citable   project organization  put each project in PRON own directory  which be name after the project   put text document associate with the project in the doc directory   put raw datum and metadata in a data directory  and file generate during cleanup and analysis in a result directory   put project source code in the src directory   put external script  or compile program in the bin directory   name file to reflect PRON content or function   keeping track of change  back up  almost  everything create by a human as soon as PRON be create   keep change small   share change frequently   create  mantain  and use a checklist for save and share change to the project   store each project in a folder that be mirror off the the reasearcher s work machine   add a file call changelogtxt to the project s doc subfolder   copy the entire project whenever a significant change have be make   manuscript  write manuscript use online tool with rich formatting  change tracking  and reference management   include a publication file in the project s doc directory   write the manuscript in a plain text format that permit version control   reference  wilson  greg  jennifer bryan  karen cranston  justin kitzes  lex nederbragt  and tracy k teal   good enough practices in scientific computing   arxiv preprint arxiv160900037  2016   wilson  greg  d a aruliah  c titus brown  neil p chue hong  matt davis  richard t guy  steven hd haddock et al   good practice for scientific computing   plos biol 12  no  1  2014   e1001745   while greg wilson s publication in nicoguaro s answer give great advice  PRON also think that the situation PRON describe would best be handle by give PRON solver option for runtime configuration  this article by jed brown  also a scicomp user  describe some good high  level practice  and PRON can also see example of runtime configuration use petsc in httpwwwmcsanlgovuploadscelspapersp20100112pdf   the basic idea be that if PRON be change parameter  these should all be runtime option  so PRON can run PRON code with mycode parametera 17 parameterb 25 and change the parameter at runtime again with mycode parametera 26 parameterb 34  in an ideal world  PRON should never have to recompile PRON code to run experiment  in practice  of course  set up runtime option require some infrastructure  but if PRON can do PRON  the investment be well worth the effort because PRON will make run concurrent experiment with different parameter value easy  
__label__machine-learning __label__feature-selection __label__recommender-system PRON girlfriend have recently be struggle with find a new job  so PRON think PRON would make a website to help PRON out  the basic idea be that PRON will be show a list of job  rate PRON interest  and then a hopefully interesting job will be email to PRON every day  so far PRON have the frontend work out  but PRON be struggle with the engine  PRON have look into different kind of recommendation engine  and PRON seem like a content base engine be the way to go since PRON will probably be the only user   with that say  feature selection be problematic because job categorization kind of unrealistic  for instance  PRON could have job category like  administrative  or  analytical   but those would be hard to define  especially if PRON be analyze the job description for certain keyword   sorry about the wall of text  PRON basic question be how should PRON go about select feature for this job recommendation engine   thank for the help   the basic way to make a recommendation engine be   link a profile to similar one  extract characteristic of a profile  and match PRON with characteristic of  here  job  a mix of the two solution above  either way  PRON need datum  label  characteristic extractor   for a first pass at this problem  PRON suggest just use a simple document search  classifier feature set such as bag of word  or maybe tf  idf against each full job description  and see what result PRON can get from a basic classifier   PRON can train the model base on PRON girlfriend short  listing or reject each item  these feature can also be use more or less directly to find and order other document by degree of similarity   a bag of word model be not very sophisticated  however  PRON be simple to implement  and have reasonable chance of be train with limit amount of datum  with this project PRON be more interested in optimise a human s search time than PRON get the good possible accuracy  any accuracy better that random chance should be helpful  
__label__machine-learning __label__classification so PRON have potential for a machine learn application that fit fairly neatly into the traditional problem domain solve by classifier  ie  PRON have a set of attribute describe an item and a  bucket  that PRON end up in  however  rather than create model of probability like in naive bayes or similar classifier  PRON want PRON output to be a set of roughly human  readable rule that can be review and modify by an end user   association rule learning look like the family of algorithm that solve this type of problem  but these algorithm seem to focus on identify common combination of feature and do not include the concept of a final bucket that those feature may point to  for example  PRON datum set look something like this   item a  4door  small  steel    gt   sedan   item b  2door  big   steel    gt   truck   item c  2door  small  steel    gt   coupe   PRON just want the rule that say  if PRON be big and a 2door  PRON be a truck   not the rule that say  if PRON be a 4door PRON be also small    one workaround PRON can think of be to simply use association rule learn algorithm and ignore the rule that do not involve an end bucket  but that seem a bit hacky  have PRON miss some family of algorithm out there  or perhaps PRON be approach the problem incorrectly to begin with   c45 make by quinlan be able to produce rule for prediction  check this wikipedia page  PRON know that in weka PRON name be j48  PRON have no idea which be implementation in r or python  anyway  from this kind of decision tree  PRON should be able to infer rule for prediction   later edit  also PRON may be interested in algorithm for directly infer rule for classification  ripper be one  which again in weka PRON receive a different name jrip  see the original paper for ripper  fast effective rule induction  ww cohen 1995  PRON be actually even simple than that  from what PRON describe  youre just look for a basic classification tree algorithm  so no need for slightly more complex variant like c45 which be optimize for prediction accuracy   the canonical text be   httpwwwamazoncomclassificationregressionwadsworthstatisticsprobabilitydp0412048418  this be readily implement in r   httpcranrprojectorgwebpackagestreetreepdf  and python   httpscikitlearnorgstablemodulestreehtml  PRON could take a look at cn2 rule learner in orange 2 httporangebiolabsiorange2  PRON should try arule package in r PRON allow PRON to create not only the association rule but also to specify the length of each rule  the importance of each rule and also PRON can filter PRON  which be what PRON be look for  try the rhs   command of this package   
__label__architecture PRON know what vectoriz be  and PRON know what pipelining be   PRON assume  vector pipelining  may mean vectoriz in such a way that vector instruction can be pipelin   be this a new or come feature in modern architecture with a more specific meaning   update on march 5  PRON consider this one pretty well  answer   PRON indeed be unclear about whether PRON be look for  a new thing  or the vector pipelin of yore  sorry brian     PRON encounter this phrase in the context of future hardware technology and want to know if PRON be miss something   thank to everyone for PRON insight  contribution   PRON will be more careful with PRON question in the future   PRON be barely old enough to understand this  but PRON understanding be that vector pipelin in the old day allow for one vector instruction in the pipeline to use the result from the vector instruction ahead of PRON  PRON believe that now on the most common vector architecture this be not possible  vector instruction be pipelined  but only insofar as PRON be independent   instruction level parallelism  ilp  be complementary to vectorization  but both require sufficient cache and register space   PRON paper  PRON be fun to learn that PRON own paper answer PRON question  eh   analyze this in detail for stencil operation on blue gene  p  combine vectorization and unroll  jam to expose sufficient parallelism while use register effectively   modern hardware have a float point instruction latency of a few cycle  more for gpus    for peak flop  PRON need to be able to issue fp instruction every cycle   this can be do use ilp or use multiple hardware thread   hardware thread typically give PRON access to more register  but have to share cache and cross  thread communication be relatively more expensive than data dependency within a single thread   consequently  PRON be application and hardware  dependent whether hardware thread be good or bad than ilp   the term  vector pipeline  be use in the 1970 s to describe vector processing at a time when a single vector instruction may  for example  compute the sum of two vector of float point number use a single pipelined float point arithmetic unit   pairs of float point number would be bring from memory into the pipelined arithmetic unit to be add together  and once the pipeline fill  PRON would get one sum out per cycle and these sum would be stream back into memory   see this 1977 article by ramamoorthy and li   httpwebeceucdaviseduvojinclasseseec272s2005papersramamoorthymar77pdf  early supercomputer like the star  asc  and cray1 implement vector processing in this way   the basic approach can be modify by add  vector register  to store fix length section  say 64 element long  of a vector   multiple pipelin alu s can also be use to simultaneously perform the same or different operation on PRON own vector   the cray1 could  chain  together vector operation so that the output from one vector operation be feed into another operation   this be somewhat different from the sse and avx instruction on modern x86 processor from intel and amd   these processor include  single instruction multiple data  instruction   the processor have multiple parallel float point arithmetic unit that can  for example  take four pair of float point number from two vector register   add PRON together in a single cycle and then store the result in another vector register   although the modern implementation of this be vastly different from the way thing be do in the 1970 s and 80 s  from a programming point of view the idea of express PRON algorithm in term of operation like vector addition be similar   this obscure 1990 technical review from the u of sydney refer to  vector pipelining  as a means of implement simd under the hood  if PRON suppose that the processor do not have enough logic to execute a vector instruction in  full width  at one go  but that the instruction can be break down into sub  operation  at a level below instruction set level   then there be a performance gain to be have by pipelin those sub  operation  so that process n time as much datum take less than n time as long   to illustrate a little bit  suppose that the 8x88 bit operation  swizzle  take 4 clock  and can be conveniently break down into four 1clock stage  PRON chip also have eight independent swizzle unit   PRON can swizzle one pair of value in 4 clock   with simd  PRON can swizzle up to 8 pair of value  64x6464 bit  by engage all of the swizzle unit at once   without pipelin  a 16wide  128x128128  swizzle instruction would take 8 clock  and a 32wide  256x256256  swizzle instruction would take 16 clock   with vector vector pipelining  the chip can introduce the first 8 operand to the first stage of the swizzle pipeline on clock 1  to emerge on clock 4   the second set on clock 2  to emerge on clock 5   etc  assume nil overhead of break up and reassemble the operand  and assume PRON have understand this all right  then PRON can have a 16wide swizzle in 5 clock  and a 32wide swizzle in 7   whether modern architecture have  or need  anything like this  PRON be afraid PRON do not know  
__label__linear-algebra __label__eigenvalues __label__lapack PRON have encounter an odd issue with solve for the eigenvalue of the follow matrix  in matlab format    0 1 0 1 0 0 0 0 0 0 0 0 1 1   1 0 0 0 0 0 0 0 0 0 0 0 0 0   0 1 0 1 0 0 0 0 0 0 0 0 0 0   0 0 1 0 0 0 0 0 0 0 0 0 0 0   0 0 0 0 0 1 0 0 0 0 0 0 0 0   0 0 0 0 1 0 0 0 0 0 0 0 0 0   0 0 0 0 0 0 0 1 0 0 0 0 0 0   0 0 0 0 0 0 1 0 0 0 0 0 0 0   0 0 0 0 0 0 0 0 0 1 0 0 0 0   0 0 0 0 0 0 0 0 1 0 0 1 0 0   0 0 0 0 0 0 0 0 1 0 0 1 0 0   0 0 0 0 0 0 0 0 0 0 1 0 0 0   0 1 0 0 0 0 0 0 0 0 0 0 1 1   0 1 0 0 0 0 0 0 0 0 0 0 1 1   when store as a matrix of double  call dgeev on this matrix correctly find the eigenvalue   however  when store as a matrix of complex double  with all the imaginary component set to  0    call zgeev on PRON return an error value of  13   indicate that the qr algorithm fail to converge   strangely  multiply this matrix by  i  so that the real component be  0  instead  result in the correct eigenvalue be find again   what be cause the qr algorithm not to converge in the first complex case  and be there a way to get around PRON   PRON matrix be not diagonalizable  in the jordan decomposition of PRON there be a block for the eigenvalue  0  of the form    beginpmatrix0amp0amp00amp0amp10amp0amp0endpmatrix  mean a triple zero eigenvalue with only two eigenvector   if PRON compute the matrix of eigenvector numerically PRON have condition number  kappaxsim 1016  the actual value be  kappaxinfty   which immediately tell PRON something be wrong because  kappax determine the sensitivity of eigenvalue to perturbation in the original matrix   the lapack algorithm probably assume the matrix be diagonalizable  in which case PRON be sensible for PRON to give an error  whether PRON give an error or not probably depend on effect of roundoff error and do not have much to do with multiply by  i  PRON would not read much into that  
__label__machine-learning PRON be deal with problem where i have to increase the sale by product recommendation  PRON only have customer datum and product that PRON have purchase  no rating  review or feedback be present  what approach fit best for PRON problem   even if PRON do not have rating or review  PRON can use the customer purchase to help create PRON model and select the most appropriate one  if the customer have buy the product  PRON can suppose PRON have like PRON  if PRON have not  PRON can suppose PRON have dislike PRON  this be an approach which be mainly use by e  shop   PRON can find more on recommendation system selection and validation below  PRON be discuss how to choose the good recommender with both offline and online approach   httpsmediumcomrecombeeblogevaluatingrecommendersystemschoosingthebestoneforyourbusinessc688ab781a35 
__label__machine-learning __label__neural-network __label__overfitting __label__genetic-algorithms PRON recently embark on a very ambitious project and PRON have to say PRON have turn out a lot good than PRON expect  PRON succeed in cod from scratch a neural network that play checker at a very acceptable level   PRON have one problem though which prevent PRON from have a strong ai  PRON guess some people would call PRON overfitt  PRON be not sure if the term be correct in this situation   the main issue be that after a certain amout of generation the weight become so great that the small change in the state of the board change the evaluation of PRON drastically  effectively limit any kind of nuanc analysis   a bit more information   neural network structure   input layer be a series of indipendent layer which take as input a section of the board  1  05  05 and 1 be the input take from the board    PRON be use 2 hidden layer of approximately 30 and 10 node each  the output node be an evaluation of the board   genetic algorithm  30 neural networks be generate randomly with weight from 02 to 02  PRON play 20 game against random opponent and PRON be rat accordingly  2 point for a loss  1 for a draw  120 move with no winner  and 1 point for a win    the 15 good be keep and copy  paste on top of the 15 bad one  then each weight of the new network be mutate use the following equation    m   m  er09     w   w  m   r  where  m be the mutation parameter of that specific weight which start off at 005   r be a random standard gaussian number and  w be the weight   PRON hope someone here can help PRON figure out how to prevent the weight from go haywire after the 40th or so generation   
__label__death ai death be still unclear a concept  as PRON may take several form and allow for  come back from the dead   for example  an ai could be somehow forbid to do anything  no permission to execute   because PRON infringe some law    somehow forbid  be the topic of this question  there will probably be rule  like  ai social law   that can conclude an ai should  die  or  be sentence to the absence of progress   a jail   then who or what could manage that ai s state   follow on from PRON own software verification  base answer to this question  PRON seem clear that ordinary  ie physical   notion of death or imprisonment be not strong enough constraint on an ai  since PRON be always possible that a state snapshot have be or can be make    what be therefore need be some mean of move the ai into a  mentally constrain  state  so that  as per the  formal ai death  paper  what PRON can subsequently do be limit  even if escape from an ai  box or be re  instantiate   one may imagine that this could be do via a form of two  level dialogue  in which   the ai be supply with percept intend to further constrain PRON    explain the error of PRON be way   if PRON like    PRON state snapshot be then examine to try and get some indication of whether PRON be be appropriately persuade   in principle  1  could be do by a human programmer  psychiatrist  philosopher while 2  could be simulate via a  black box  method such as monte carlo tree search   however  be seem likely that this would in general be a monstrously lengthy process that would be better do by a supervisory ai which combine both step  and which could use more  whitebox  analysis method for 2     so  to answer the question of  who manage the state   the conclusion seem to be   another ai   or at least a program that be highly competent at all of percept generation  pattern recognition  ai simulation    the ai agent can be design in such a way that PRON could consist of two major component   the free  will component expand the experience of the ai agent and produce output base on artificially generate thought input   the hard  wire component that the agent can not modify by PRON  this could include a set of secured code to action sequence mapping  one of which could be temporary suspension of actuator  a punishment  another could be total suspension of operation  death   the selection of who have the right to manage this state depend on what right have be bestow upon the ai agent PRON  if the right provide be that of a human citizen  then the right to sentence to death state be as per the legislature a human citizen would follow  if the right of the ai agent be no different from that of a basic machine  then the owner of the agent would have to right to activate the death state  
__label__machine-learning __label__neural-network __label__deep-learning __label__gradient-descent __label__backpropagation can anyone explain what be saturate gradient problem  PRON would be nice if anyone can provide math detail as well  thank PRON in advance   if PRON use sigmoid like activation function  like sigmoid and tanh  after some epoch of train the linear part of each neuron will have value which be so much big or small  this mean that the linear part will have an output value which have big amount regardless of PRON sign  consequently  the input of sigmoid like function in each neuron which add non  linearity will be far from the center of these function   in those location  the gradient  derivative value be so much small  consequently  after numerous iteration the weight get update so slowly because the value of gradient be so much small  this be why PRON use relu which PRON gradient do not have this problem  saturate mean that after some epoch that learn happen relatively fast  the value of linear part will be far from the center of sigmoid and PRON somehow saturate  and PRON take to much time to update the weight  because the value of gradient be small  PRON can take a look at here as a solution for this problem   if PRON want to explain the math part  suppose that PRON be use sgimoid as the activation function  if s represent sigmoid  PRON gradient be  s1s   now suppose that PRON linear part  input of sigmoid be a positive number which be too large  then simoid which be   1   1  expx    will have a value near to one but small than that  on the other hand 1  s will be so close to zero  multiply s and 1  s will result in a small value  which mean the value of gradient be so much small  if the value of linear part be so small  then s will be close to zero and 1  s will be close to 1 but small than that  again  multiply these will lead to a small value as the gradient  
__label__python __label__predictive-modeling __label__evaluation PRON have this code in python in order to calculate the precision of PRON model and to print confusion matrix use decision trees classifier   coefgini  decisiontreeclassifiercriterion   gini   randomstate  100  maxdepth  3  minsamplesleaf  5   coefginifittrainingfeature  trainingtarget   ypr  coefginipredicttestfeature   ypr  for name  importance in ziptrainingfeaturescolumn  coefginifeatureimportanc     printname  importance   print   train accuracy use decision trees classifier be    accuracyscoretrainingtarget  coefginipredicttrainingfeature     print   test accuracy use decision trees classifier be    accuracyscoretesttarget  ypr    print   confusion matrix use decision trees classifier be   confusionmatrixtesttarget  ypr    what be the cost matrix  be this the money that company will lose for each wrong predictive target value  anyone have an example   thank    confusion matrix  a confusion matrix be an important tool to measure accuracy of a classification algorithm  PRON compare predict class of an outcome and actual outcome   scenario 1  credit risk  base on a credit risk scorecard  application for credit card be classify as  good  and  bad    good  indicate applicant pay back due on credit card and  bad  indicate customer default on the due  now  the customer be compare against actual performance of the customer payment behaviour after say 18 month  so  comparison of predicted class   good  or  bad   to actual customer behaviour state   defaulted  or  regular     there be always a trade  off between type PRON error  false positive  accept bad customers  and type ii error  false negatives  reject good customers    PRON generally require to optimize between false positive rate  type i error  and false negative rate  type ii error    so  role of cost matrix come in picture to find the optimal cut off value for a classification rule   now  go back to credit risk model  the cut off value optimize between cost of an opportunity loss  miss to accept a good customer  type ii error  and cost of accept a potential defaulter  involve in loss due to default    reference example be from a blog and image from google   cost matrix  cost matrix be similar  of confusion matrix  PRON ’ just  PRON be here more concerned about false positive and false negative there be no cost penalty associate with true positive and true negatives as PRON be  correctly identify   the goal of this method be to choose a classifier with low total cost   total cost   cfnxfn  cfpxfp  where   fn be the number of positive observation wrongly predict  fp be the number of negative example wrongly predict  cfn  and cfpcorrespond to the cost associate with false negative and false positive respectively   remember  cfn   gt  cfp    hope this helps  
__label__classification __label__svm __label__supervised-learning __label__matlab __label__accuracy while tune the svm classification model in matlab  PRON come across the rng function in matlab in which seed  stabilize the random shuffling of the datum in the algorithm  be change  when the function call be rng1  then PRON be get one accuracy value  99    when PRON be change to rng2  then PRON be get another value  57    so there be a huge change in accuracy as visible  what do this mean  be PRON train PRON wrong   the train and test set correct rate  in   that PRON be get with different run without change rng aretrain  test    96  828    946  953    96  859    96  90    95  95   the training error in this dataset have a huge difference  99  vs 57    so  maybe the one with the rng1  split have overfitt PRON dataset   so there be a huge change in accuracy as visible  what do this mean   be PRON train PRON wrong   the huge change may be due to overfitt   also  judge the model through validation curve  and then fit a model which balance the bias  variance plot   
__label__machine-learning __label__neural-network train a basic multilayer perceptron neural network boil down to minimize some kind of error function  often the sum of squared error be choose as a this error function  but where do this function come from   PRON always think this function be choose because PRON make sense intuitively  however  recently PRON learn that this be only partly true and there be more behind PRON   bishop write in one of PRON paper that the sum of squared error function can be derive from the principle of maximum likelihood  furthermore PRON write that the squared error therefore make the assumption that the noise on the target value have a gaussian distribution   PRON be not sure what PRON mean with that  how do the sum of squared error relate to the maximum likelihood principle in the context of neural network   PRON reference of bishop be not entirely accurate  what PRON state in the paper PRON link be  PRON should be note that the standard sum  of  square error  introduce here from a heuristic viewpoint  can be derive from the principle of maximum likelihood on the assumption that the noise on the target datum have a gaussian distribution  reference cite   even when this assumption be not satisfied  however  the sum  of  square error function remain of great practical importance   the important point with regard to PRON question be that there be no inherent assumption that there be gaussian noise when train a multilayer perceptron  mlp   therefore  for an mlp  the sum  of  square error function be not derive from the principle of maximum likelihood   for example  consider train an mlp to learn the xor function  there be four pair of input with correspond output but there be no noise in the datum  yet the sum  of  square error be still applicable   the relevance of use sum  of  square for neural network  and many other situation  be that the error function be differentiable and since the error be square  PRON can be use to reduce or minimize the magnitude of both positive and negative error   PRON can trace the squared error in statistic through multivariate calculus all the way to pythagorus  PRON be basically calculate the  effective length  of the error  the hypotenuse  among error from multiple variable   x1  x22   y1  y22    like in a triangle   but where do the square root go   somebody realize that calculate root of multiple variable over multiple iteration be computationally very expensive  so PRON decide to drop PRON  checkout the squared euclidean distance here for more detail  how would a cubic error or a logarithmic error affect the outcome   PRON just take more time to converge because PRON be not as accurate  but PRON do see logarithmic error over square such as logistic regression where PRON be more optimal  all in all PRON be a simple case of optimization 
__label__python __label__scipy __label__integration __label__vectorization PRON need to implement the following in python   for a give discrete time series zt  t0 to t   find small t such that    csums0t ekzt  zsmt  s      gt fracp1p     where c  k  m be constant and p  be give by   int005  frac1  2yed  y1y1dy1ddyint05p   frac2y1ed  y1y1dy1ddy   where d be another constantsometh to be optimize   PRON need to implement this for each row of an array z  input 2d array of shape  n  w   PRON implement a loop version   i implement the sum in equation 1 above utilize the fact   ekzt  zsmt  s     fracekztmtekzsms     hence   sums0t ekzt  zsmt  s     ekztmtsums0tfrac1ekzsms    which be reflect in use of cumsum in code below  def fz    return   1  2znpexpd  z1z1dz1d     lhs  integratequadf005   def rhsp    return integratequadf05  p   pstar fsolverhs  lhs075   will depend on time series only indrectly when d will be optimize  for i in nparangen    z  zi     main  npexp  kzmnparangew    cumsumt  npcumsum1main   finalsum maincumsumt  tsolution  index i where finalsumigt  pstar1pstar    not implement yet  be there any way PRON can vectorize this for z  in this case n be 400000 so vectorization would really help  the function fz  will be fine with vector input but PRON do not think the function rhs will be fine as PRON use integration   
__label__finite-element __label__domain-decomposition note  thank to comment  PRON realize that PRON have two problem  each which can be describe more clearly on PRON own  this revise question cover the first   PRON would like to solve     delta u  10quad u0y 0quad u1y0     over a rectangle use alternate schwarz  the follow be PRON mesh  where the small disk indicate the internal boundary node of the subdomain  the node that have dirichlet condition that PRON update on PRON     x  0  and  x  1  be the physical boundary of the full mesh  the local solution do not converge to the global solution  the problem be mostly in the overlapping part of the mesh  here be the difference between the actual solution and the solution give by the local solution   if PRON impose the dirichlet condition  ux0   0  and  ux1phi   0   where  1phi be the height of the mesh  then the local solution converge to the global solution   should PRON not also work with only the boundary condition that PRON mention about earlier   edit   PRON solve PRON by select the artifical boundary in this manner   
__label__python __label__classification __label__visualization PRON have be work on a classification problem and have some good result  but now PRON struggle with try to put together a good plot to illustrate the probability for each prediction   here be PRON current datum   import numpy as np  import panda as pd  import matplotlibpyplot as plt  dfvotingoutputheadn5   prob  actual  pr  correct  0  0460200  0  0  1  1  0548478  1  1  1  2  0270609  0  0  1  3  0686557  0  1  0  4  0527935  0  1  0  5  0098687  0  0  1  PRON have be able to create a bar chart with probabilite use this code   pltbarnparangelenvotingpredictions1     votingpredictions1    pltxlabelrecord number    pltylabelprobability    plttitleclassification probability    pltshow    however  PRON be think there have to be a good way to include more information and a key   PRON would like the  correct  label to be clear to see  so from a visual standpoint PRON can see how the probability relate to a correct classification   PRON be really rare that PRON would show a plot of the probability for each example in PRON set  be PRON sure PRON want to do this  a good presentation may be a confusion matrix  here be how PRON work   1  the column be the true class label  2  the row be the predict class  3  along the right hand side of the plot PRON can show the probability of correctly assign to a class  or the classification error  if PRON prefer    for example  say PRON have three class in PRON dataset   PRON have 10 example of each class  so 30 example total   PRON classification result from PRON model be   8 of the 10 example in class 1 be correctly label  1 be misclassifi to class 2 and 1 be misclassifi to class 3   7 of the 10 example in class 2 be correctly label  3 be misclassifi to class 3   9 of the 10 example in class 3 be correctly label  1 be misclassifi to class 2   then PRON confusion matrix look like this    beginbmatrix  08  amp  01  amp  01  00  amp  07  amp  03  00  amp  01  amp  09 endbmatrix  and along the right hand side of PRON confusion matrix PRON can include the classification error  02 for class 1  03 for class 2  01 for class 3   notice the row must add to 1   what piece of information be PRON try to convey by present this plot  that determine what an appropriate plot type would be   the bar chart PRON show in PRON question would be useful if the specific index of the probability be important  for example  if there could be something special about eg index 504  559 and a person would want to see what probability go with those index  but PRON suspect that be not the case  PRON be more likely that PRON be interested in how the probability correlate with actual result  to show that  PRON could start with a simple scatter plot with probability on the horizontal axis and actual result on the vertical axis  PRON also help if PRON use different color  and perhaps slightly different symbol  for actual result of 0 and 1  here be a primitive example   another option to show nearly the same information would be to group the probability into range and plot a histogram show how many 1 result fall in each range  this good convey how narrow the crossover from 0 to 1 prediction be   if all PRON want to convey be how many prediction of each type be correct vs incorrect  then the confusion matrix recommend by statssorceress may be a good choice  since PRON more directly present that information  PRON could  decorate  PRON as a heatmap  if PRON think that would enhance the impact  ie if this be target at less numerically  minded people or PRON fit with an overall graphical theme   but that probably do not make much of a difference  
__label__ai-design __label__ai-community not sure if this be the proper venue for this type of question  be there any visual element  pattern  color  graph  map  or even programming font that be use and even recur in artificial intelligence and machine learning process  as a designer  PRON be not entirely clear what these  look  like realistically  as most generally associate ai with the kind of color and texture generally see in sci  fi film   a characteristic visual element for neual  network machine  learning may be the node diagram  which show  schematically  the node in the layer of a neural network and the connection between the node  both within and between the layer  that get weight as the network be train   if PRON do a google search on neural network node diagram  PRON will see what PRON mean   PRON may also think of any sort of video or photo with computer  identify feature highlight  as in a google image search for machine vision feature detection   if PRON be look for something more concrete  PRON can think of rack of gpu card  or a spinning lidar scanner  
__label__pde __label__finite-element __label__eigensystem PRON have to approximate the small eigenvalue of the follow generalize eigenvalue problem      nabla cdot  dx  nabla px   alphax  px   lambda betax  px      over a domain like follow  many more subdomain  this be only an approximation to illustrate the problem   the eigenvalue be all positive  so to approximate the small one PRON invert the left side of the equation after the spatial discretization  and use arnoldi method  the function  dx   alphax and  betax be piecewise constant over the whole domain  be probably different over different subdomain   PRON already have a code to approximate the small eigenvalue and associate eigenvector with a continuous galerkin fem   PRON be wonder which locally conservative method can PRON use to solve the eigenvalue problem  ie  PRON want the solution to satisfy the balance equation over each subdomain    what PRON know   option 1  PRON know that use discontinuous galerkin fem the balance equation will be satisfied over each subdomain  but PRON usually deal with the mixed formulation of the problem      nabla cdot  qx   alphax  px   lambda betax  px    qx   dx  nabla px   0     and PRON introduce a lot of degree of freedom to the system   option 2  PRON have read that PRON can also be do use a hybridizable discontinuous galerkin  because the PRON can apply a static condensation technique and the degree of freedom relate to  q be the only one that PRON have to solve in a coupled way  PRON work fine for source problem  but PRON think that PRON can not apply this technique with an eigenvalue problem   edit   why static condensation will not work for eigenproblem    if deal with a source problem  PRON form the linear system for the hybridizable discontinuous galerkin as follow      leftbeginarrayc c   a  amp  b   c  amp  d  endarrayright   leftbeginarrayc   p   q  endarrayright     leftbeginarrayc   r   s  endarrayright      PRON know that matrix  a be block diagonal because the degree of freedom be completelly decouple  from element to element   so PRON perform the static condensation to solve the follow problem      d  ca1bq  s  ca1  r     and then PRON recover  p with     ap  r  bq     when PRON form the algebraic eigenvalue problem with discontinuous galerk PRON get     leftbeginarrayc c   a  amp  b   c  amp  d  endarrayright   leftbeginarrayc   p   q  endarrayright     lambda leftbeginarrayc c   e  amp  0   0  amp  0  endarrayright   leftbeginarrayc   p   q  endarrayright      then  as in a linear system  PRON try to isolate  p in the first equation  and PRON obtain     p   alambda e1b q     and after substitution in the second equation PRON get      d  calambda e1b  q  0     matrix  a and  e be block diagonal  so easy to invert  but the eigenvalue  lambda now be inside the matrix to invert  so the eigenvalue problem have be convert into a nonlinear eigenvalue problem  different linear approximation of    alambda e1  taylor expansion on  lambda  0  or  lambda  lambda0   for instance  will solve the problem of the nonlinear eigenvalue problem  but then the algorithm will converge much more slow and succesive correction will be necessary  thus static condensation be not a good idea here   option 3  there be some paper that talk about locally conservative continuo galerkin method  the continuous galerkin method be locally conservative  and locally conservative fluxes for the continuous galerkin method  the first one seem to perform some extra calculation to approximate the flux  that for PRON be equivalent to the mixed formulation  and the second one say that be posible to deal with the primal formulation  but also develop the article about a formulation that include the flux  qx  question  PRON be no clear for PRON that locally conservation can be obtain with a primal formultaion  like in the first equation PRON post   moreover take into accout that PRON be solve an eigenvalue problem  can PRON be do   any idea or advice about literature on the topic would be greatly appreciate  PRON be mainly interested in fem method  but any idea be welcome   
__label__word2vec __label__word-embeddings PRON be new to data science and PRON be try to understand the  word2vec  approach from a long time  can someone please explain PRON in simple term  moreover  what be the problem that can be solve through  word2vec  approach   what be word2vec   word2vec be an implementation of word embedding technique   word embedding try to represent relationship that may exist between the individual word  those contain in PRON processing text  by give PRON each a vector with same predefined dimension  in this vector space word that share common context may be locate closer  how to assign the word into the vector space  this work have often be do through neural network training   what problem can PRON solve   the main problem solve by word2vec  or word embedding  be that PRON create a way to represent relationship between the word in PRON processing text  as oppose to just treat the word as individual symbol  this make any follow datum mining or machine learn more effective   besides word2vec  PRON can also use many other pre  train embedding like glove  if the dataset be large  PRON be good to train PRON own word embedding which will get a good performance than use pre  train embedding  
__label__finite-difference __label__precision when one want to compute numerical derivative  the method present by bengt fornberg here  and report here  be very convenient  both precise and simple to implement   as the original paper date from 1988  PRON would like to know whether there be a good alternative today  as  or almost as  simple and more precise    PRON be not aware of anyone have improve fornberg s algorithm  see also PRON slightly more recent paper   as an aside  PRON seem to PRON that look at PRON algorithm as a way to compute numerical derivative be not right  all PRON be do be derive an efficient algorithm to compute weight for finite  difference method  the advantage of PRON method be that PRON give PRON the weight for all derivative up to the desire derivative in one go    overview  good question  there be a paper entitle  improve the accuracy of the matrix differentiation method  for arbitrary collocation point  by r baltensperger  PRON be no big deal in PRON opinion  but PRON have a point  that already be know before the appearance in 2000   PRON stress the importance of an accurate representation of the fact that the derivative of the constant function  fx1  should be one  this hold exactly in the mathematical sense  but not necessarily in the numerical representation    PRON be simple to see that this require the row sum of the n  th derivative matric  dn to be zero  PRON be common to enforce this constraint by adjust the diagonal entry  ie by set    tag 1 dnjj    sumsubstacki1i neq j   n dij      PRON be clear that this feature do not hold exactly when work on a computer due to roundoff error in float point calculation  what be more surprising be that these error be even more severe when use the analytical formula for the derivative matrix  which be available for many classical collocation point  eg gauss  lobatto    now  the paper  and reference therein  state that the error of the derivative be in the order of the deviation of the row sum from zero  the goal be therefore to make these numerically as small as possible   numerical test  the good point be that the fornberg procedure seem to be quite good in this regard  in the picture below PRON have compare the behaviour of the exact  ie analytical  first derivative matrix and the one derive by the fornberg algorithm  for vary number of chebyshev  lobatto collocation point   again  believe the statement in the cite paper  this imply that the fornberg algorithm will yield more accurate result for the derivative   in order to prove that  PRON will use the same function as in the paper     tag 2 fx   frac11x2 and evaluate the error   tag 3  en  maxiin 0ldot  n   bigg fprimexi   sumj1n dij  fxj  bigg this be do for  i  the analytically obtain derivative matrix   ii  the fornberg derivative matrix  and  iii  an adjust version of the fornberg matrix  where the above eq   1  be enforce quite straightforwardly via   tag 4 tilde djj   djj   leftsumi1n dji  right   qquad textfor all j      here be what PRON get  again for the example of gauss  lobatto abscissa    conclusion  in conclusion  fornberg s method seem to be quite accurate  in the case  n512  even by about 3 order of magnitude more accurate than result from the analytical formula  this should be accurate enough for most application  moreover  this be remarkable because fornberg do not seem to explicitly include this fact in PRON method  at least there be no mention in the two fornberg paper    another order of magnitude can be gain for this example through a straightforward inclusion of eq4   as this be a quite simple approach and apply only once for each derivative  PRON see no reason in not use PRON   the method from the baltensperger paper  which use a more sophisticated approach for evaluate the sum in eq1  in order to reduce roundoff error  yield about the same order of magnitude for the error  so  at least for this example  PRON be roughly equivalent to the  adjusted fornberg  method above   to increase the precision of numerical differentiation do the following   1  chose PRON favorite high  precision  standard  method base on some step size h  2  compute the value of the derivative with the method choose in 1  many time with different but reasonable step size h each time PRON may pick h as a random number from the interval  05h10  15h10  where h be an appropriate step size for the method PRON use   3  average the result   PRON result may gain 2  3 order of magnitude in the absolute error wrt  the non  average result   httpsarxivorgabs170610219 
__label__machine-learning __label__python __label__neural-network __label__convnet __label__theano PRON have train a simple cnn  use python  lasagne  for a 2class eeg classification problem  however  the network do not seem to learn  loss do not drop over epoch and classification accuracy do not drop from random guessing  50     questions  be there anything wrong with the code that be cause this   be there a good  more correct   way to handle eeg datum   eeg setup  data be collect from participant complete a total of 1044 eeg trial  each trial last 2 second  512 time sample   have 64 channel of eeg datum  and label 01  all trial have be shuffle so as to not learn on one set of participant and test on another   the goal be to predict the label of a trial after be give the 64x512 matrix of raw eeg datum  the raw input datum  which PRON can not show here as PRON part of a research project  have a shape of  1044  1  64  512   train  validation  test split be then create at 602020   with such a small dataset PRON would have think overfitt would be a problem  but training loss do not seem to reflect that  code  network architecture   def buildcnninputvar  none    lin  inputlayershapenone  1  64  512   inputvar  inputvar   lconv1  conv2dlayerincom  lin  numfilter  32  filtersize   1  3    stride  1  pad   same   w  lasagneinitnormalstd  002    nonlinearity  lasagnenonlinearitiesrectify   lpool1  pool2dlayerincom  lconv1  poolsize   1  2   stride   2  2    lfc  lasagnelayersdenselayer   lasagnelayersdropoutlpool1  p5    numunits256   nonlinearity  lasagnenonlinearitiesrectify   lout  lasagnelayersdenselayer   lasagnelayersdropoutlfc  p5    numunits2   nonlinearity  lasagnenonlinearitiessoftmax   return lout  note  PRON have try add more conv  pool layer as PRON think the network be not deep enough to learn the category but 1  this do not change the outcome PRON mention above and 2  PRON have see other eeg classification code where a simple 1 conv layer network can get above random chance  helper for create mini batch   def iterateminibatchesinput  target  batchsize  shuffle  false    assert leninput    lentarget   if shuffle   index  nparangeleninput    nprandomshuffleindice   for startidx in range0  leninput   batchsize  1  batchsize    if shuffle   excerpt  indicesstartidx  startidx  batchsize   else   excerpt  slicestartidx  startidx  batchsize   yield inputsexcerpt   targetsexcerpt   run the model   def mainmodelcnn   batchsize500  numepochs500    inputvar  ttensor4input    targetvar  tivectortarget    network  buildcnninputvar   prediction  lasagnelayersgetoutputnetwork   loss  lasagneobjectivescategoricalcrossentropyprediction  targetvar   loss  lossmean    trainacc  tmeanteqtargmaxprediction  axis1   targetvar    dtype  theanoconfigfloatx   param  lasagnelayersgetallparamsnetwork  trainable  true   update  lasagneupdatesnesterovmomentumloss  param  learningrate001   testprediction  lasagnelayersgetoutputnetwork  deterministic  true   testloss  lasagneobjectivescategoricalcrossentropytestprediction   targetvar   testloss  testlossmean    testacc  tmeanteqtargmaxtestprediction  axis1   targetvar    dtype  theanoconfigfloatx   trainfn  theanofunctioninputvar  targetvar    loss  trainacc   update  update   valfn  theanofunctioninputvar  targetvar    testloss  testacc    printstart training     for epoch in rangenumepoch     full pass over the training datum   trainerr  0  trainacc  0  trainbatch  0  starttime  timetime    for batch in iterateminibatchestraindata  trainlabel  batchsize  shuffle  true    input  target  batch  err  acc  trainfninput  target   trainerr   err  trainacc   acc  trainbatch   1   full pass over the validation datum   valerr  0  valacc  0  valbatch  0  for batch in iterateminibatchesvaldata  vallabel  batchsize  shuffle  false    input  target  batch  err  acc  valfninput  target   valerr   err  valacc   acc  valbatch   1   after training  compute the test prediction  error   testerr  0  testacc  0  testbatch  0  for batch in iterateminibatchestestdata  testlabel  batchsize  shuffle  false    input  target  batch  err  acc  valfninput  target   testerr   err  testacc   acc  testbatch   1   run the model  mainbatchsize5  numepochs30   1your input layer seem off  the first dimension be for channel  please try  with the datum format correctly   lin  inputlayershapenone  64  512  1   inputvar  inputvar   a more clean way would be to replace the conv2dlayer by a conv1dlayer  which be what PRON be replicate   2there be no correct way to handle eeg  but people often also use spectrogram and feature extraction  there be a lot of possible reason why PRON setup may not work  however  one very good start be to try to overfit PRON model on a very small subsample of PRON entire dataset just to see if the problem be in the code   PRON have the same problem when PRON use tensorflow to build a self drive car  the training error for PRON neural net bounce around forever and never converge on a minimum  as a sanity check PRON could not even intentionally get PRON model to overfit  so PRON know something be definitely wrong  what work for PRON be scale PRON input  PRON input be pixel color channel between 0 and 255  so PRON divide all value by 255  from that point onward  PRON model training  and validation  error hit a minimum as expect and stop bounce around  PRON be surprised how big of a difference PRON make  PRON can not guarantee PRON will work for PRON case  but PRON be definitely worth try  since PRON be easy to implement  
__label__linear-algebra __label__iterative-method __label__preconditioning for the solution of large linear system  ax  b use iterative method  PRON be often of interest to introduce precondition  eg solve instead  m1ax  b  where  m be here use for left  preconditioning of the system  typically  PRON should have that  m1approx a1 and provide the basis for  much more  efficient solution or reduction in computational resource  eg memory storage  in comparison with solution of the original system  ie when  m  a   however  what guideline should PRON use to choose the preconditioner  how do practioneer do this for PRON specific problem   PRON originally do not want to give an answer because this deserve a very long treatment  and hopefully someone else will still give PRON  however  PRON can certainly give a very brief overview of the recommend approach   perform a thorough literature search   if that fail  try every preconditioner that make sense that PRON can get PRON hand on  matlab  petsc  and trilinos be nice environment for this   if that fail  PRON should think carefully about the physics of PRON problem and see if PRON be possible to come up with a cheap approximate solution  perhaps even to a slightly change version of PRON problem   example of 3 be shift laplacian version of helmholtz  and jinchao xu s recent work on precondition the biharmonic operator with laplacians   jack have give a good procedure for find a preconditioner  PRON will try an address the question   what make a good preconditioner    the operational definition be   a preconditioner m accelerate the iterative solution of  a x  b  and  m1 can be apply cheaply compare to  a1  however this do not give PRON any insight into design a preconditioner  most preconditioner be base upon manipulation of the operator spectrum  generically  krylov method converge faster when eigenvalue be cluster  see matrix iterations or meromorphic functions and linear algebra  sometimes PRON can prove a preconditioner result be just a few unique eigenvalue  eg a note on preconditioning for indefinite linear systems   a common strategy be exemplify by multigrid  relaxational preconditioner  here smoother  like sor remove high frequency component in the error  when the residual be project onto a coarse grid  low frequency error component become high frequency and can again be attack by sor  this basic strategy underlie more complicated version of mg  such as amg  note that at bottom  the solver must resolve the low frequency in the error accurately   another strategy involve solve the equation in small subspace  which be exactly what krylov solver be do  in the simple form  this be the kaczmarz method or the additive schwarz method  the advanced strain of theory here  domain decomposition  concentrate on spectral approximation of the error on the interface  since the domain be assume to be solve fairly accurately   then there be a bunch of thing produce operator close to  a be some sense  which be easy to invert  but give no guarantee of actually improve convergence of the krylov iteration  as far as PRON know  these preconditioner include incomplete factorization  icc  ilu   sparse approximate inverse  spai   and some low rank approximation   other have already comment on the issue of precondition what PRON will call  monolithic  matrix  ie for example the discretiz form of a scalar equation such as the laplace equation  the helmholtz equation or  if PRON want to generalize PRON  the vector  value elasticity equation  for these thing  PRON be clear that multigrid  either algebraic or geometric  be the winner if the equation be elliptic  and for other equation PRON be not quite so clear  but something like ssor often work reasonably well  for some meaning of  reasonable     to PRON  the big revelation have be what to do about problem that be not monolithic  for example for the stokes operator     beginpmatrix  a  amp  b  bt  amp  0 endpmatrix     when PRON start with numerical analysis some 15 year ago  PRON think people have the hope that the same technique could be apply to such matrix as above  and the direction of research be to either try multigrid directly or to use generalization of ssor  use  point smoother  like vanka  and similar method  but this have fade since PRON do not work very well   what have come to replace this be what be initially call  physics  base preconditioner  and later simply  and maybe more accurately   block preconditioner  like the one by silvester and wathen  these be often base on block elimination or schur complement and the idea be to build a preconditioner in such a way that one can reuse preconditioner for individual block that be know to work well  in the case of the stokes equation  for example  the silvester  wathen preconditioner use that the matrix     beginpmatrix  a  amp  b  0  amp  bt a1  b endpmatrix1      when use as a preconditioner with gmres would result in convergence in exactly two iteration  since PRON be triangular  the inversion be also much simple  but PRON still have the problem of what to do with the diagonal block  and there one use approximation      beginpmatrix  widetildea1    amp  b  0  amp  widetildebt a1  b1   endpmatrix      where the tilde mean to replace the exact inverse by an approximation  this be often much simple  because the  a block be an elliptic operator   widetildea1 be well approximate by a multigrid v  cycle  for example  and PRON turn out that here   widetildebt a1  b1 be well approximate by an ilu of a mass matrix   this idea of work with the individual block that comprise the matrix and re  use preconditioner on individual one have prove to be enormously powerful and have completely change how PRON think of precondition system of equation today  of course  this be relevant because most actual problem be  in fact  system of equation  
__label__visualization __label__paraview __label__vtk PRON be model a soil medium in plane strain and would like to determine if paraview support the 15noded triangular element as show on this link   on the documentation PRON be only see element up to second order for an unstructured grid  please see vtk user guide page 480 and 481  PRON also find some indication on this link that other element type may be support  but nothing specifically regard the element type PRON be after   furthermore the model PRON be use have additional stress point  9 of PRON  in addition to the geometric node on the model  PRON be not terribly worried about these for the time be although PRON will eventually have to include PRON in the rendition   unfortunately  unless thing have change very recently  vtk do not fully support anything high than second order cell in term of mesh visualisation and filtering  various common workaround base on upscal have be attempt  see for example this relate question  
__label__data-mining excuse the potentially dumb question  PRON have only just start learn about datum science   how do PRON find out how much datum PRON should collect before use PRON to start make decision   be there a way of know if PRON should wait to collect more datum  or if the quantity of datum can lead to meaningful result   background  PRON have a mobile app and some stat about PRON  show below  PRON be early day and the datum set be small  PRON want to start use this datum to make good decision about improve the product   short answer be  as much as possible   the more datum PRON have  the more likely PRON can make correct inference  in the bad case  PRON could simply discard anything that PRON do not like  PRON be like money  the more the good  PRON always have an option to spend or use less   statistic could be doggy without sufficient sample size  PRON statistical power would be affect and any model PRON have could be bias   have say that  PRON do not want everything from PRON user  use PRON common sense  and ask PRON what exactly PRON want to do   base only on the datum in PRON question  PRON would say   PRON could be wrong because PRON do not understand where PRON get the datum   how many total user PRON have that do not drop out  if PRON total user base be like 10 k  28 user drop out be not a problem  but if PRON only have 30 user   PRON sample size be small  be that one  day installation  in any case  28 user for a mobile app be a tiny datum set  PRON may want to focus more on marketing   edit   how many respondent PRON need be hard to tell  apple probably need good datum quality than PRON  if PRON would like to do PRON statistically  PRON may calculate the minimum sample size and PRON be relate to statistical power  
__label__finite-element __label__condition-number in computation of the solution PRON achieve a solution with nice precision and small number of node but the condition number of the matrix be very large  PRON be confused  be PRON possible to have a large condition number with a good precision  the condition number be  10  4  and PRON increase as the number of node incres and also the precision increase  how do PRON explain this condition   of course PRON can  the condition number give PRON an upper bound for the error  suppose PRON want to solve  ax  b where  a be a matrix with large condition number  the error depend on the structure of  a and on  b  as an example  if  a be a  2 time 2  diagonal matrix  with element  1  and  1020  solve  ax  b will still be accurate  despite have a huge condition number of  1020 
__label__computational-geometry __label__linear-programming __label__complexity be check the equivalence of two convex polytope  ps and  pt np  hard    ps ch  cup  ltps  a1     ps  amgt       ch be convex hull compute on union of a polynomial number of polytope  ps  ai   pt ch  cup  ltpt  b1     pt  bngt     ch be compute on union of a polynomial number of polytope  pt  bi  and   ps  ai xi1  xik     lij  leq xij  leq uij   j1    k    sumj1k xij   1   lij   uij  textare non  negative rational number for    j1  k  and   pt  bi yi1  yir     lij  leq yij  leq uij   j1    r    sumj1r yij   1   lij   uij  textare non  negative rational number for    j1  r  that heavily depend on the representation   if PRON be give  p1  and  p2  as system of linear inequality  or  dually  as the convex hull of a finite set of point  with finite precision  PRON can reduce each linear system  or finite point set  to an irredundant system of linear inequality by solve linearly many linear program   then scale each inequality so that PRON have a canonical representation  sort PRON  and check whether the two representation be equal   if PRON be give  p1  and  p2  as system of linear inequality with real number coefficient  and PRON be use a complexity theory that have a notion of  np  hard  that handle real number computation  like the one derive from the real ram model   then check polytope equivalence be exactly as hard as linear programming in say model    which be to say that PRON be still  to PRON knowledge  an open problem    if PRON allow other primitive  such as  intersect with the integer lattice then take the convex hull   PRON can make the problem np  hard  
__label__nlp __label__word2vec PRON be read tensorflow documentation now and PRON can not understand what do  batch  and  batchsize  mean in explanation skip  gram model  please  can someone explain PRON   here this paragraph   recall that skip  gram invert context and target  and try to predict each context word from PRON target word  so the task become to predict  the  and  brown  from  quick    quick  and  fox  from  brown   etc  therefore PRON dataset become   quick  the    quick  brown    brown  quick    brown  fox     of  input  output  pair  the objective function be define over the entire dataset  but PRON typically optimize this with stochastic gradient descent  sgd  use one example at a time  or a  minibatch  of batchsize example  where typically 16  lt batchsize  lt 512    a minibatch  be a group of  input  output  pair that PRON present to PRON neural net in one pass  or epoch   without compute the stochastic gradient descent between two pair  PRON only do PRON at the end of the minibatch  sum the error over the pair   this improve the speed and prevent over learn on single element  thus improve learning  PRON also do not need to much memory since PRON batch be  mini    some people call stochastic training when PRON mini  batch have a size of 1  and batch training when PRON have the size of PRON training dataset   PRON can get some nice explanation here  
__label__python __label__statistics in the attached image  PRON be plot actual vs predict along with confidence interval for each of the predict point  the black star represent the average of all the predict point  what formula can PRON use to combine the confidence interval of all the predict data point to get a confidence interval for the average  also  please assume that the average could be a weighted average   PRON be use python  numpy   ok  PRON have two dozen prediction  with  uncertainty  and actual point value  PRON be still not sure PRON know enough to answer  so PRON be go to just outline some point and hope someone can improve upon this  a quick search find three similar stackexchange question   combine 13 independent regression seem the most similar  note comment warn this may be meaningless  the good  answer suggest a sample approach because the result have no close form  use a normal approximation would have a close form  but PRON think PRON plot suggest the error be not normal  or PRON 95  ci be much too narrow   combining 2  see esp  the reply discuss meta  analysis and whether PRON have a fix effect or a random effect model  this may be what PRON be do  in which case re  formulate the question for that community may be good   delta method  seem to PRON the same question  but take a direct maximization approach  assume approximation hold   a couple of observation suggest any simple average be not go to give PRON what PRON want  and PRON need to be very clear what be be combine  and what question the result be go to answer   if these be 95  confidence interval  then PRON error model be much too conservative  nearly all of PRON should include the true value  gray dash diagonal  but only about 5 do  therefore any parametric combination will be at least as unreliable  but suppose PRON have a black box with uncalibrated estimate  and PRON just want to know the  average  from that black box  however uncalibrated   if these be two  dozen unrelated estimate from a crowdsourcing platform  the  average  be not meaningful  on the other hand  if PRON be performance measure of the crowd  then the average be something like the average performance of the crowd across all question type  in that case   1st approximation would just average the two  dozen point and use the standard deviation of that   each point can be weight    2nd approximation would treat each estimate and ci as normal  and use one of the formula for combine normal ci  note however that PRON seem error bar be wide towards the edge  that may require special handling   3rd approximation  sample say 100 forecast from the two  dozen distribution and then combine those 2400 sample   weighting can be do by vary sample size for each forecast   work even if the distribution be not assume to be normal   if instead PRON have two  dozen forecast from a single statistical model on different input datum  and PRON want to know the average forecast across all input combination  do not combine PRON  just run the model with no factor specify  and use that result   hopefully a bona fide statistician can supply a more definitive answer   PRON could also try a bootstrapping method in which PRON make the fit to only a subset of datum point  this subset be create by randomly pick some of the data point  by repeat this many time PRON can take the uncertainty from the distribution of the fit result  
__label__machine-learning __label__classification __label__bioinformatics __label__data PRON be well know that science have give PRON large amount of free accessible datum  such as httpwww1000genomesorg and httpwwwncbinlmnihgovgenbank  how can PRON play around with the datum and apply data science  machine learn to PRON  what could be some idea   PRON own idea   biological datum visualisation  gene prediction use hide  markov  model  any more    determine the function of gene and the element that regulate gene  throughout the genome   find variation in the dna sequence among people and determine PRON  significance  the most common type of genetic variation be know as a  single nucleotide polymorphism or snp  pronounce  snip     these  small difference may help predict a person ’s risk of particular  disease and response to certain medication   discover the 3dimensional structure of protein and identify PRON  function   explore how dna and protein interact with one another and with the  environment to create complex living system   develop and apply genome  base strategy for the early detection   diagnosis  and treatment of disease   sequence the genome of other organism  such as the rat  cow  and  chimpanzee  in order to compare similar gene between specie   develop new technology to study gene and dna on a large scale and  store genomic datum efficiently   continue to explore the ethical  legal  and social issue raise by  genomic research   source  PRON may build model to classify genome by population   run unsupervised learning  clustering  to see if population be reconstruct in the model   build model to infer miss genotype  to do a scalable dna analysis PRON may check adam software base on apache spark 
__label__convex-optimization PRON want to minimize a complicated objective function  and PRON be not sure if PRON be convex   be there a nice algorithm that attempt to prove that PRON be not convex   of course the algorithm could fail to prove this  in which case PRON would not know if PRON be convex or not  and this be ok  PRON just want to try to rule out convexity before PRON spend a lot of time try to analytically determine whether the objective function be convex  for example by try to rewrite the problem in a standard form know to be convex   one quick test would be to try to minimize from various starting point and if multiple local minimum be find in this manner then PRON be not convex   but PRON be wonder if there be a good algorithm that be design with this goal in mind   a function can be non  convex without have multiple minimum  there be a variety of optimization method that apply  linear or nonlinear  conjugate gradient iteration  truncate when a negative operator norm be compute  the negative value indicate a direction of negative curvature  which can not happen for convex functional   if negative curvature be rarely encounter  these method converge in a small number of optimization iteration   if a quality preconditioner be available  the inner step should also converge quickly    a function that be convex need to satisfy  falpha x   1alphay  le alpha fx    1alphafy for all  alphain01 and  x  y in the domain of definition  PRON could simply try to verify this formula for a large number of pair  x  y and a couple of value  alpha  eg  alpha141234  for a number of practically useful convexity  nonconvexity verification test  see  self  disclaimer  PRON be the third author on this paper    r fourer  c maheshwari  a neumaier  d orban  and h schichl  convexity and concavity detection in computational graphs  tree walks for convexity assessment  informs j computing 22  2010   26  43   note that there be many function that be convex in the domain of interest but acannot easily be   disciplined   ie  write in one of the form require by structured convex solver such as cvx  
__label__machine-learning PRON be work on a project to predict the setting of the iot devicesfan  light  ac  the user be use  base on PRON  location  outside temperature  humidity  time of the day etc   training datum be not available and the model should start build  adapt PRON every time the user use the iot device  to predict PRON preferred setting in any new environment PRON go  this model will be unique for each user  possibly PRON can start off with a demo model but PRON should adapt PRON as per each user s preference   which  machinelearn algorithm can be useful here  any link to the same and some tip on implementation would be appreciate   this sound like a fun problem but PRON very open end   PRON will provide some link from the scikit  learn user guide   there be many more reading option  but the skl userguide have lot of example and usually link to academic publication for more in  depth reading   another great resource be  introduction to statistical learning  and if PRON be good at math  element of statistical learning  model selection  the first big question be whether the setting PRON be try to predict be ordinal value or continuous value   continuous value will allow PRON to use regression method  where as ordinal value will give PRON a choice between regression method and classification scheme   PRON would suggest employ a couple of different model to start and then select the good one move forward   some possible candidate be   linear regression  with nonlinear feature as appropriate  because linear model be usually insightful   support vector classification  regression  svc  svr  as these be often very accurate classifier  and either  naive bayes or random forests as these often give good result where other model fail   initial training data  PRON will need to prime PRON solution with some deterministic datum or datum from another dataset that PRON have adapt to fit PRON model   PRON suggest prim the system with common sense value and then employ some sort of algorithm to forget those value as the system become well  train   move forward  PRON will have to decide whether each user s model will use other user s datum or not   this be sometimes solve via cluster method like k  mean or dbscan   where by user be cluster into group each cluster have a different model associate with PRON   PRON would suggest retrain  fit  the model on regular time frame initially  and then investigate the possibility of move to an online learning system   PRON absolutely suggest use a well know library for PRON problem  scikit  learn be great for the reason mention above and for prototyp  but do not scale particularly well  h20 and mahout be high quality scalable library that PRON would recommend for a big datum production system   mahout in action be a great book to learn from  also   hope this help  
__label__scikit-learn __label__xgboost __label__evaluation PRON be experiment with xgboost   PRON run gridsearchcv with scorerocauc  on xgboost  the good classificator score 0935  this be what PRON read from gs output   but now when PRON run good classificator on the same datum   rocaucscorey  clfbestxgbpredictx    PRON give PRON score 0878  could PRON tell PRON how the score be evaluate in both case  thank  try use predictproba instead of predict as below  PRON should give PRON the same number   rocaucscorey  clfbestxgbpredictprobax1    when PRON compute auc  most of time people will use the probability instead of the actual classs  
__label__machine-learning __label__data-cleaning __label__preprocessing can PRON really be call generalization if PRON remove  modify datum point to suit PRON model   PRON depend  actually there be research paper find that neural network can sometimes cope very well with sparse or noisy label  but basically when PRON apply any machine learn approach  PRON want PRON model to pick up pattern in PRON datum  in order to be a pattern something need to be repetitive and predictable  outlier and error in PRON datum may not adhere to that  PRON rather break the pattern  PRON can still work but PRON be hard to recognize PRON   make a model  learn  outlier  miss value etc  will only work  if these incidence also appear in a pattern which make PRON unlikely to be outlier  as a rule of thumb PRON be therefore a good idea to remove this noise   find pattern in exception  amp  error  as PRON mention some research try to address the noise issue for example by classify if there be random   white   noise or structured noise  PRON not only predict the target value but also the type of noise and then try to make another prediction base on both piece of information  so there be some active approach to address noise but usually this be due to the fact that PRON would be to cumbersome or even impossible to remove the noise  
__label__word2vec __label__word-embeddings concern the notion of word embedding  skip  gram method aim for compute the probability of a word give PRON neighborhood  PRON do not understand the rationale behind PRON  since PRON be possible to infer this information by look directly into the co  occurrence matrix   in general  PRON can not understand those method aim to capture as much relevant information from the original co  occurrence matrix as possible  be not PRON easy to work on the co  occurrence matrix directly   thank PRON very much in advance   basically  skip  gram aim for compute the probability of a context give a word  PRON be cbow that do the opposite   but  PRON do not simply learn a co  occurrence matrix  PRON compress the information in a low dimensional space  for example 300 for a 100000 original dimension   by do this  PRON learn continuous low dimensional representation for the word   PRON be prove that word2vec actually factorize a word  context pointwise mutual information matrix  close to the co  occurrence matrix   PRON think this article may give PRON a good understanding of the underlie process  
__label__linear-algebra __label__matlab __label__algorithms __label__matrix do anyone know any matlab algorithm that can help PRON generate a random matrix with real eigen value   thank   PRON do not mention what kind of distribution PRON want for the entry of the random matrix  so the simple recipe that PRON can think of be to first create a random  n time n matrix  mathtta and to then obtain  mathttb   frac12   mathtta   mathttamathttt       that give PRON a symmetric matrix  mathttb with random entry   since symmetric matrix have real eigenvalue  the matrix  mathttb will satisfy PRON requirement   matlab code   a  randn   b  05   a  transposea    here be another approach  build on bill barth s answer  first  create a random diagonal matrix  mathttd  then obtain a random orthogonal matrix  mathttq and  finally  compute the similarity transformation  mathttb   mathttq    mathttd    mathttqmathttt  the matrix  mathttb will have the same eigenvalue as  mathttd but will not be diagonal in general   a simple way to come up with a random orthogonal matrix  mathttq be to compute the qr decomposition of a random matrix   matlab code   d  diagrandn  1     q    qrrandn    b  q  d  transposeq   if PRON want to specify the eigenvalue  then PRON can just put PRON on the diagonal of an otherwise empty matrix  
__label__matlab __label__nonlinear-equations __label__random PRON would like to solve an equation that look like this  update   er1gammarktheta  rz0   where  rphi rz1phirktheta and  phiin01    theta  be a random variable normaly distribute with zero mean  and some variance  sigma2 thus  the  e stand for the expectation operator over this random variable   an alternative version of the problem  solve a similar equation  but now  theta be lognormally distribute   the computational task  be to find  phi  such as the condition  equation above hold  for some give number  value for  rz  rk and  gamma all of which be positive real number   can someone help on how PRON can calculate this  ideally in matlab   since  phi be a scalar between  0  and  1   the easy method for find a root be bisection  if PRON can not calculate the expectation of the nonlinear function    fphitheta   leftphi rz   1phirkthetaright1gammarktheta  rz in term of  phi analytically  PRON can use quadrature to approximate  for the first variant  this amount to    efphitheta    frac1sigmasqrt2piintinftyinfty  efract22sigma2   fphitdt approx sumi1n fracwisqrtpi   fphisqrt2 sigma ti  where  ti and  wi be the gauss  hermite quadrature node and weight of order  n if  theta be log  normally distribute  then  theta  ebeta  where  beta be normally distribute  so PRON can just replace  ti by  sieti in the above to get a quadrature rule  with the same weight  for the log  normal variant   this procedure be straightforward to implement in matlab  PRON can download function to calculate the quadrature weight and node here or here  
__label__neural-network __label__r PRON be use nnet package in r for train artificial neural  network  some tell that weight be assign to each case in training set   and some say that no  of weight be equal to the no  of connection in  structure of neural network  PRON do not understand this  can anybody help PRON  out   PRON look at the r code of nnet ie  what be the difference between weight  amp  wts in the nnet code below   nnetx  y  weight  size  wts  mask  linout  false  entropy  false  softmax  false  censor  false  skip  false  ring  07  decay  0  maxit  100  hess  false  trace  true  maxnwts  1000  abstol  10e4  reltol  10e8     from the documentation PRON can find out that   weight   case  weight for each example – if miss default to 1   wts  initial parameter vector  if miss choose at random   though PRON be no r expert  PRON understanding be   weight be something like sampleweight in python  which control the weight of each sample  PRON be useful when deal with imbalanced class issue   wts be the initial weight of neural network  random weight will be initiate if this parameter be not pass  
__label__genetic-algorithms __label__game-ai __label__minimax PRON have create a gomoku5 in a row  ai use alpha  beta pruning  PRON make move on a not  so  stupid level  first  let PRON vaguely describe the grading function of the alpha  beta algorithm   when PRON receive a board as an input  PRON first find all repetition of stone and give PRON a score out of 4 possible value depend on PRON usefulness as an threat  which be decide by length  and PRON will return the summation of all the repetition score   but  the problem be that PRON explicitly decide the scores4 in total   and PRON do not seem like the good choice  so PRON have decide to implement a genetic algorithm to generate these score  each of the gene will be one of 4 score  so for example  the chromosome of the hard  cod score would be   5  400001000000050000   however  because PRON be use the genetic algorithm to create the score of the grading function  PRON be not sure how PRON should implement the genetic fitness function  so instead  PRON have think of the following   instead of use a fitness function  PRON will just merge the selection process together  if PRON have 2 chromosome  a and b  and need to select one  PRON will simulate a game use both a and b chromosome in each ai  and select the chromosome which win   1is this a viable replacement to the fitness function   2because of the characteristic of the alpha  beta algorithm  PRON need to give the max score to the win condition  which in most case be set to infinity  however  because PRON can not use infinity  PRON just use an absurdly large number  do PRON also need to add this score to the chromosome  or because PRON be insignificant and do not change the value of the grading function  leave PRON as a constant   3when initially create chromosome  random generation  follow standard distribution be say to be the most optimal  however  gene in PRON case have large deviation  would PRON still be okay to generate chromosome randomly   
__label__data-mining __label__reference-request PRON be look for paper concern mining rule from datum that have ordinal attribute  PRON aim be to create an efficient way to incorporate this kind of datum without binarization  which require expert knowledge of topic or an assumption about distribution of example   ideally  PRON want to come up with a technique to include these ordinal datum in opus algorithm  which offer efficient pruning   PRON be aware that this question may be consider  too broad   PRON be hop for point out at least a direction in research   
__label__random-forest __label__image-classification __label__mnist mnist dataset be handwritten image which contain 60000 training and 10000 testing sample  can PRON use only a fraction of dataset for instance 10000 for training and 4000 for testing  PRON want to know that whether PRON result will be comparable to other claimed result  like PRON use random forest classifier on 10k training and 4k testing and get accuracy of 93   would this result be equivalent when PRON use 60k training and 10k testing   and in some publish paper PRON read that PRON only use 40 image per class and evaluate PRON accuracy and performance use PRON do PRON conform to the standard that use 60k as training and 10k as testing   or PRON can say that the choice of selection of training and testing datum be dependent on the method use to classify  this be actually a pretty complex question that have to do with many facet of how information entropy be carry in a dataset and how PRON learn algorithm will make use of the information   machine learning be optimization  machine learn at PRON core be just find the ideal parameter value in order for a specific function to be represent  this mean PRON will use the datum in PRON dataset and some framework in order to tune these parameter  learning be an iterative process  as PRON can see provide one instance to PRON algorithm will not result in very good result  however  there come a point when PRON have provide sufficient datum that the information contain in that subset be representative of the information contain in the entire dataset   PRON can see this in practice  when train PRON model PRON will notice that performance start to converge to a specific value  if PRON have infinite datum this value would not get any good  this be cause by limitation in PRON datum and not PRON model  machine learn algorithm   this be the good the specific model PRON be use can do with that kind of datum   how to get good performance   assume PRON still have more datum but performance be no longer improve  PRON will want to consider a more complex model  this model will likely be able to extract information from PRON dataset that be too subtle for PRON simple model  this be why deep learning outperform most classical machine learn algorithm when big datum be available  deep learning make use of very high complexity to approximate function very accurately  however  the tradeoff be that a lot of datum be need   actually PRON dataset be quite limited  can PRON improve PRON performance   PRON actually can  this go back to the core of machine learning  the dataset PRON be use  the dataset PRON have construct have a certain information entropy base on the feature PRON select  by increase the information entropy in PRON dataset  the complexity of PRON model can be reduce and this will lead to good performance   intuitively  if PRON be try to classify dog  cat  but PRON dataset contain information about the weather the day the measurement be take  this will reduce the entropy of the information for classify dog  cat  thus obscure PRON other feature and reduce performance   furthermore  sometimes PRON may overlook the importance of a certain feature  or overlook the potential of perform transformation to PRON feature to better suit PRON model  for example  if PRON be use linear regression to determine point on 2 circle  radius1  radius4   perform linear regression on the cartesian plane will lead to very poor result  essentially 50   however  transform PRON  x  y coordinate into polar coordinate will lead to 100  performance as PRON will be perfectly separable  this be a simple application of the kernel trick  
__label__machine-learning __label__statistics __label__reinforcement-learning __label__probability __label__randomized-algorithms PRON be read the follow paper on the epoch greedy algorithm for the contextual bandit problem  PRON have two question  httphunchnetjlprojectsinteractivesidebanditsbanditpdf  PRON be unsure how PRON have use the bernstein inequality on page 6 to conclude  munmathcalh1  leq c1  sqrtk mathrmlnmn could someone please elaborate on this as PRON seem bernstein inequality seem to measure whp the deviation of a sum of random variable from PRON be mean  where as the regret bind  munmathcalh1 be define as the expect regret from the empirically good policy from the absolute good policy  could someone fill in the detail   can PRON get any reasonable estimate for the constant  c if i be to try and implement this in practice   PRON would really appreciate any help  thank   
__label__machine-learning __label__dataset __label__algorithms __label__monte-carlo PRON be experience an efficiency problem with PRON app and PRON wish to solve PRON with machine learning  use scikit  learn   the problem  a monte carlo simulation be run over  n iteration  the resultant datum set that PRON produce be a  m element array  m  with each element be an array  n of size  n  where each  n be sort  for each element in  m  PRON select the middle element  at index n2  and store PRON in the result array  r the sorting of  n take a very long time  and PRON wish to bypass this completely by use machine learn to train a model to output  r  what PRON want to do  PRON want to use machine learn to produce a model that take in the input that would go into the monte carlo simulation but then output  r PRON plan on train the model use the input and result from the monte carlo simulation   for example  an entry in the dataset PRON would use to train the model with will look like this   25684  300  1000  18  12  r   32511  33564  34786  35765    100544  102888   so if PRON graph  r for  k different set of input  for the  k1th input PRON should see a graph along the line of the previous graph   PRON question  what machine learn algorithm can identify pattern in a dataset such that for a give set of input parameter  PRON will produce an  r similar to the  r that the monte carlo simulation would produce  so in essence  how can PRON generate a model that  when give 25684  300  1000  18  12  PRON will output  32511  33564  34786  35765    100544  102888    
__label__machine-learning __label__nlp __label__clustering __label__text-mining __label__lda PRON have a graph which be already separate into cluster  each node in the graph have a label  typically  PRON be a function s name like orgjavasomepackagevalidatelogin   what PRON want to do be to give a representative label for each cluster   for the sake of simplicity  let PRON assume PRON be able to clean the datum  ie break  validatelogin  to  validate  and  login    PRON have do a little research on the subject of topic model and cluster labeling and encounter a few algorithm  such as  lda  nmf and tf  idf  not quite an algorithm on PRON   basically  many algorithm be document orient which be reach of word and not short  text  label orient   PRON be worth mention   PRON can use the fact that different cluster may have different label  so maybe a proper label for a cluster could be unique word in the overall bag of word  that can be do with tf  idf PRON guess   labels can be one word but can also imply a hierarchy  ie packageapackagebpackagecfuncname   PRON would be glad if PRON could give PRON PRON insight on this problem and what approach could fit here   a possible approach may be to use the most predictable and predictive words  in each cluster as PRON names   the follow be inspire by fisher s category utility use by the cobweb algorithm   an attribute be highly predictable in a cluster  cl if most element in that cluster have the same value  a for PRON  thus a word represent by attribute  ai have a predictability of  pai  acl  an attribute be highly predictive for a cluster  cl if know PRON be value  a imply PRON can say with high certainty to which cluster PRON belong  express as  pclai  a  assume now PRON process each label orgjavasomepackagevalidatelogin as a sentence   org java somepackage validate login  and apply one  hot encoding to all PRON sentence in the dataset  the occurrence of a word be now represent by a value of 1 for PRON be correspond attribute   the task of represent each cluster by a word can now be formulate as find for each cluster  cl the word represent by attribute  ai that equal 1 and have the high predictability and predictiveness  weight by the total probability of this word be in a sentence     pai1cltime pclai1  time pai1  which can be calculate by count occurrence of a word per cluster and cluster per word   PRON research have yield result like lda  nmf  etc  but since PRON want to keep PRON concentration to short text and generate short label  PRON would recommend PRON to check this algorithm call  bi term topic model for short texts   here be the paper from the author  httpciteseerxistpsueduviewdocdownloaddoi10114024032ampreprep1amptypepdf  and here be the code from the author again  httpsgithubcomxiaohuiyanbtm  PRON have use this personally and can very vouch for this for PRON performance  
__label__r be there an r build tool  like maven or gradle for java  to get the dependency and package an r project   the packrat package be what PRON be look for   PRON bypass r s native package and allow PRON to build and deploy a bundle of package and dependency   however  PRON do not do maven  style dynamic dependency resolution  PRON need to specify a list of package that PRON believe will work together  then the bundle be instal  deploy as a single unit  
__label__optimization __label__regression __label__partitioning PRON be all familiar with traditional least  square method for construct a straight line through a set of datum point  the question be  suppose PRON show PRON a scatter plot which clearly be suggestive of more than one line  suppose two   PRON need to split the data point into two group and build a regression line for each   how do PRON split the dataset into two so group so as to make optimal split  what if PRON allow to leave out some data point entirely from either dataset  be there a formal name for this problem in research  be there any know solution   thank  
__label__feature-engineering __label__encoding as the title describe  PRON have use likelihood encoding for some category and one  hot encoding for the other  but the result seem worse than the method which only use one hot encoding method   anyone know why   and be there actually exist any effect between different method   
__label__keras __label__rnn __label__lstm __label__kernel PRON be try to understand how the weight matrix in an lstm cell be use  an lstm unit have several weight matrix  wf  wi  wc  wo like below    from httpcolahgithubioposts201508understandinglstms   at the same time  PRON be play with the keras lstm and study PRON source code   httpsgithubcomkerasteamkerasblobmasterkeraslayersrecurrentpyl1871  in the source code  there be only one kernel mention  PRON be wonder be PRON refer to wc only  then where be the other weight matrix wf  wi  wo initialized and use  thank   PRON use this variable to save all the weight matrix by concatenate PRON   in the call function of the lstmcell PRON can see  how PRON be unpacked   selfkerneli  selfkernel     selfunits   selfkernelf  selfkernel    selfunit  selfunit  2   selfkernelc  selfkernel    selfunit  2  selfunit  3   selfkernelo  selfkernel    selfunit  3   
__label__predictive-modeling PRON have get survey datum that resemble    q1a  q1b  q1c  q2a  q2b  q2c  classification   respondent   1   0   0   1   0   0   red   respondent   0   0   1   1   0   0   green   respondent   0   1   0   0   0   1   yellow  PRON be try to predict the classification for new respondent  currently PRON be use a naive bayes  and get pretty bad accuracy  20    PRON do not have much training datum  and the training data be hand scrap from non  standard source  internal company procedure be a mess here    PRON be look for other way to predict the classification   PRON be think about assign weight to each question  and magically predict the result base on those  somehow  although PRON do not really know where to start learn about how to do that  and whether PRON be appropriate for this datum  PRON have very little background in this   any idea or tip on predict the classification column with no training datum   can PRON give a bit more information on the size of the datum PRON be train on  and if PRON be really 6 parameter PRON be base the prediction off of    if PRON be really 6 question with binary answer  1  0 as PRON suggest   then there be 2  6  ie 64  unique answer combination  and to determine a probability for PRON PRON will want a multiple entry per combination   standard error scale like 1sqrtn  so for 10  accuracy PRON will need roughly 6400 input which give PRON description  sound like more datum than PRON may have   PRON may want to invest time into automate datum collection   if on the other hand  PRON have a reasonably large datum set and be hop for some alternative model  both boost decision tree and random forest model sound like good candidate for this problem  
__label__machine-learning __label__classification __label__time-series problem  there be several event  eventa  eventb    represent by waveform  for each event there be several csv file  eventa1csv  eventa2csv  eventancsv  have the pointsx  y  from which the correspond waveform can be generate   use these waveform as training datum  waveform segment on  test datum  large datum should be identify ie the interval during which an event occur be need to be label on the test datum  see image for reference    what would be the good strategy  tool and algorithm to address this problem   if classification model be to be generate then   the point be just to describe the nature of the waveform ie the test waveform will not be on same point but with similar nature eg the concavity  convexity  slope etc   different event in training graph be represent use different number of point ie event a can be determine by 20 point  while event b have 200 point   so   how do PRON create feature vector to include all these thing for classification   how would the test datum be analyse  as PRON have large number of point on the large waveform ie how to segment PRON to generate feature and give PRON to classification model   statistical data analysis  PRON would start with the following   statistically analyse PRON training event and see the mean and the variance of length and width and other property to have an overall image of what PRON be   for each event take the mean and the standard deviation   walk through the test datum and compute the error between test signal and learn event and plot PRON   at the end for each event type  PRON will have an error curve whit many local minimum   choose a threshold base on standard deviation of training event and detect minimum behind that threshold as event   pattern recognition  more advanced option be to learn a model to classify event  at the beginning PRON should say the last question be theoretically wrong because PRON do not generate feature from test datum but only compute PRON  feature have be extract  generate  from training datum  PRON assume PRON mean how to feed the model with test datum for classification accord to PRON size  well  PRON would say PRON can segment PRON to chunk which may lead to lose some event because of cut point but if test datum be large enough that would not be a problem   the point be that time  series be a special kind of datum and there be method specifically for PRON so the classic feature extraction and learning may lead PRON to an ocean of confusing method  PRON suggest to search the term time  series mining instead to see more direct solution   time  series mining  the very first approach PRON propose be kind of this  there be many more algorithm for detect similarity base on two time  series mostly base on euclidean distance  for example look at lcss or dtw  these method be what PRON be look for and both be advanced version of PRON first suggested algorithm  please note that for all of PRON normalize PRON training and test datum play an important role   see this answer as well however that be about unsupervised segmentation of time  series  
__label__bigdata __label__similarity PRON be use minhash to find similar vector in a matrix  PRON be use lsh to band minhash value  now PRON would like to find item with 2 or more similar band between PRON   what be the most efficient way to go about this problem    for a matrix with 1000000 column   
__label__data-mining __label__statistics __label__algorithms __label__distribution say PRON have an organization that require employee to participate in a qampa site similar to stackoverflow  question and answer be vote upon  select answer get extra point  certain behavior boost PRON score etc   what PRON need to do be assign a rating from 1  100 to these user with even distribution   the behavior that add point   ask a question  fix   answer a question  fix   receive an upvote on a question  determine by relative ranking   receive an upvote on an answer  determine by relative ranking   have PRON answer select  determine by relative ranking   respond to a comment  etc  fix   likewise  there be behavior that subtract point   if a user with a high ranking upvote a question ask by a lower  rank user  more point should be award than the inverse situation   likewise if a lower  rank user downvot a higher  rank user s question  the impact should be minimal compare to the inverse   there should be a limit to this impact though so that a high  rank user do not unintentionally destroy any momentum of a low  rank user by issue a powerful downvote   PRON have a few challenge here   how do PRON determine how many point to assign to each type of behavior  with actor  recipient relative rank take into account   PRON be think PRON just assign a flat number to each behavior  that number decide relative to the importance of the other behavior  and then have a variable score that can alter the score if there be a wide variance between the user   the mechanic of this  do the score double at most   be unclear   how to PRON assign this rank  this one be a little easy  PRON be think PRON just order the user accord to score and then split the dataset into 100 section  assign each  chunk  a number 1  100   should PRON be worried about these number get  very big    the scenario describe above have be trivialize  action take by these user may happen hundred of time per day so the score can become very high  very quickly  be there a way PRON can keep this under control while avoid a large number of duplicate score   how do PRON define the  fix  score as the total score become very big   over time PRON may have user with hundred of thousand of point  but the fix  score behavior should still reward PRON   PRON should reward lower  rank user more than higher  rank user   PRON do not know if there be some standard practice  algorithm  or terminology that PRON should be aware of when face a problem like this  any input would be appreciate   to solve challenge  3 and  4  let PRON limit the overall available rank volume  for example  sum of this rank for all the user will be 1  100     from challenge  2 PRON understand  that PRON accept 2 different rank   1  place from 1 to 100  and  2  simple sum of all earn point  fix and relative   do PRON get PRON right  PRON so  there be no need to worry about unlimited growth  or fix score inflation  let PRON just use percentage  not 1  100 rank   these percentage rank could be calculate base on interaction behavior  vote  selecting answer  etc   use pagerank  like algorithm  such algorithm will consider all previous reaction  and rank of act user   obtain by an exact user  unfortunately  PRON can not use pagerank algorithm  as be   because PRON support only  positive  link  but PRON can look for PRON s extension  for example  look at this paper with pagerank extension  for both positive and negative link  as user can down vote   PRON can iteratively estimate percentage rank  trustrank  tr  use this algorithm   the second task be to calculate reward  penalty rate in point for each single action  let PRON determine  predefine  maximal reward  penalty rate  x  for each type of action  and will use coefficient to discount PRON  base on trustranks of act user  eg  author and voter   slightly modify sigmoid will map this ratio from  infinf  range to  01    here for peer user PRON will have 05 of predefined maximal rate  if  voter  have tr twice more than  author    author  will recieve 075 of predefined value  and so on  PRON can tune steepness with additional parameter  or try to find any other mapping transformation function   anyway  now simply multiply maximal penalty  reward by this coefficient  and PRON will get the number of point  PRON need to deduct or add  the only issue  PRON see  be a user with zero tr  such user as a voter will  give  nothing  and as an object of voting  will recieve the maximal amount of point regardless voter s rank  to avoid this  PRON can predefine minimal tr  like 1e10   and do not let user s tr to fall beyond this value  
__label__python __label__data-cleaning __label__pandas in one field PRON have entry like  u 1920   work on panda  how PRON ignore non numerical datum and get only the numerical part   use strstrip if the prefix be fix or strreplace if not   datum  panda  seriesu 1920     datastrreplaced        astypefloat   this remove all the non  numeric character to the left of the number  and cast to float  
__label__dataset PRON have a 3 gb dataset but PRON computer only have 4 gb ram  be PRON possible to run some simple analysis in r  studio  like anova  use some build in package   etc  with that one 1 gb of additional ram   if PRON be impossible to run  be there any solution to this problem   PRON want to know before anything crash and PRON lose PRON work   thank   this depend on several factor   whether PRON operating system be 32 or 64 bit  if PRON a 32bit operating system  most certainly PRON will not be able to do everything at the same time in ram   the version of r there be a historic limit on vector  and so on matrix also  length of 2  31  1  however  r 300 incorporate long vector which allow for large vector   the specific processing PRON be do and the amount of memory PRON use  eg temporary buffer   and the memory access pattern PRON follow   the amount of time PRON be willing to wait  PRON operate system s virtual memory mechanism will almost certainly enable PRON processing to take place  but swap to disk may take time if PRON swap partition be on a magnetic drive  PRON may get much good if PRON be on an ssd drive   the amount of swap space be also relevant here in the case PRON be not enough to fit the need   sum up  PRON depend   anyway  what PRON can do to avoid lose PRON work be save PRON r object to file by mean of save so that PRON be able to restore PRON later by mean of load in the case anything go wrong  after save PRON datum  just launch PRON processing without fear  PRON will be able to retry as many time as PRON want   PRON can try ff or bigmemory package to handle large dataset  
__label__deep-learning __label__hardware-evaluation before PRON start PRON want to let PRON know that PRON be completely new to the field of deep learning  since PRON need a new graphic card either way  gaming PRON know  PRON be think about buy the gtx 1060 with 6 gb or the 1070 ti with 8 gb  because PRON be not rich  basically PRON be a pretty poor student   PRON do not want to waste PRON money  PRON do not need deep learning for PRON study PRON just want to dive into this topic because of personal interest  what PRON want to say be that PRON can wait a little bit longer and do not need the result as quick as possible   so here be PRON question   can PRON do deep learning with the 1060  6 gb seem to be very limit accord to some website  or the 1070 ti  be the 1070 ti overkill for a person hobby deep learner   or should PRON wait for the new generation nvidia graphic card   thank PRON very much in advance   regard specific choice PRON can not recommend  but if PRON be completely new  PRON should probably learn  code some more until PRON get a gpu  there be a lot to learn in machine learn before gpu speedup make a significant difference  and until then do the computation on any old cpu would be just fine  especially if PRON be just start since PRON will not be do anything too complex  PRON will know when computational resource be PRON main bottleneck  and until then PRON should not really matter too much   or  PRON could also rent compute power from say  aws or google  PRON do not think PRON need to invest in any kind of gpu unless PRON be familiar with the computation require for the task PRON want to achieve use deep learning   also  by the time PRON have sufficiently master deep learning to a point where PRON can actually make the most of PRON gpu  there will be new product in the market   so until then PRON suggest PRON use PRON cpu for do little task such as regression etc   PRON can always use the free credit offer by the various cloud company for PRON task  give that PRON be a student do this out of personal interest and want to do some gaming on the side  PRON would suggest the gtx 1060 6 gb since at present the gtx 1070ti be overprice due to crypto miner  this will date the answer  but for reference the 1060 be go for gbp340  the 1070ti for gbp600  two other option be the 1050ti 4 gb for gbp160 or the vanilla 1080 at gbp650     which gpu   by tim dettmers be very helpful  as be  pick a gpu   by slav ivanov  especially the summary at the end for different use case  as PRON be not look at spend a huge amount of money  the 1060 seem like a good compromise as the 1050ti may just leave PRON with a disappointing gaming experience  find a use 1070 be also suggest  but PRON would need to be comfortable with that   other answer have mention the cloud  but that do not help with PRON gaming  if PRON want to save some cash while PRON be wait for the next gen of card  take advantage of PRON student status on aws educate or azure on ms imagine  the github student dev pack be a good package  
__label__monte-carlo PRON recently read something that talk about dist distribution string   PRON appear to be a way to take a long string of previously generate number and somehow compress PRON into a string that can then be use by other   PRON believe PRON be lossy   PRON be have a great deal of trouble find out anything other than some generality about PRON   PRON may be proprietary  PRON do not know   do anyone know where PRON may be able to find something out about the algorithm use to create such a dist string   also if anyone know anything about this  be this something that be be use anywhere and how useful do PRON appear to be   online  there do not seem to be much information  although PRON be supposedly describe by PRON inventor  sam savage  in PRON book the flaw of averages   apparently  PRON be a lossy xml standard  accord to this website  PRON look like a proprietary standard manage by vector economics and probability management  if this standard be a key piece of intellectual property for sam savage  PRON doubt the standard be publicize anywhere   PRON appear to be use in the financial service industry  but for scientific purpose  the lack of openness may be a big problem when PRON come to check numerical result  
__label__machine-learning __label__nlp the explanation from the official doc2vec paper  distribute representations of sentences and document  for pv  dbow be as follow   another way be to ignore the context word in the input  but force the model to predict word randomly sample from the paragraph in the output  in reality  what this mean be that at each iteration of stochastic gradient descent  PRON sample a text window  then sample a random word from the text window and form a classification task give the paragraph vector   accord to the paper the word vector be not store and pv  dbow be say to work similar to skip gram in word2vec   skip  gram be explain in word2vec parameter learning  in the skip gram model the word vector be map to the hidden layer  the matrix that perform this mapping be update during the training  in pv  dbow the dimension of the hidden layer should be the dimension of one paragraph vector  when PRON want to multiply the word vector of a sampled example with the paragraph vector PRON should have the same size  the original representation of a word be of size  vocabulary size x 1   which mapping be perform to get the right size  paragraph dimension x 1  in the hidden layer  and how be this mapping perform when the word vector be not store  PRON assume that word and paragraph representation should have the same size in the hidden layer because of equation 26 in 1  PRON have not read the paper PRON link  but PRON have follow a lecture note by richard socher   so  basically that mapping matrix be call weight matrix  there be two weight matrix w1 and w2 for input and output mapping  thus for each word  vector in both the matrix be update via backpropagation   to answer PRON question  a word be represent by one  hot sparse vector which have the size of vx1  v be size of vocabulary   with a value of 1 in one of the position  and when this vector be multiply with the weight matrix w1 with size nxv  the correspond embed vector of size nx1  n be size of the require embedding vector  be use in the hidden layer   document have no one  hot represent vector  so  basically there be a document matrix d of size nxd  d be number of document  where each column represent a document  in other word  the matrix w1 and w2 need the one  hot representation only for mathematical step  other than that PRON be represent each word in each column just as the document matrix d 
__label__multiclass-classification __label__evaluation PRON be try out a multiclass classification setting with 3 class  the class distribution be skewed with most of the datum fall in 1 of the 3 class   class label be 123  with 6728  of the datum fall in class label 1  1199  datum in class 2  and remain in class 3   PRON be train a multiclass classifier on this dataset and PRON be get the follow performance   precision  recall  f1score  micro average  0731  0731  0731  macro average  0679  0529  0565  PRON be not sure why all micro avg  performance be equal and also why macro average performance be so low   original post  httprushdishamsblogspotin201108microandmacroaverageofprecisionhtml  in micro  average method  PRON sum up the individual true positive  false positive  and false negative of the system for different set and the apply PRON to get the statistic   tricky  but PRON find this very interesting  there be two method by which PRON can get such average statistic of information retrieval and classification   1  micro  average method  in micro  average method  PRON sum up the individual true positive  false positive  and false negative of the system for different set and the apply PRON to get the statistic  for example  for a set of datum  the system s  true positive  tp1    12  false positive  fp1   9  false negative  fn1   3  then precision  p1  and recall  r1  will be  5714 frac  tp1tp1fp1 and  80frac  tp1tp1fn1  and for a different set of datum  the system s  true positive  tp2    50  false positive  fp2   23  false negative  fn2   9  then precision  p2  and recall  r2  will be 6849 and 8475  now  the average precision and recall of the system use the micro  average method be   textmicro  average of precision   fractp1tp2tp1tp2fp1fp2   frac12  5012  50  9  23   6596    textmicro  average of recall   fractp1tp2tp1tp2fn1fn2   frac12  5012  50  3  9   8378   the micro  average f  score will be simply the harmonic mean of these two figure   2  macro  average method  the method be straight forward  just take the average of the precision and recall of the system on different set  for example  the macro  average precision and recall of the system for the give example be   textmacro  average precision   fracp1p22   frac5714  68492   6282    textmacro  average recall   fracr1r22   frac80  84752   8225   the macro  average f  score will be simply the harmonic mean of these two figure   suitability  macro  average method can be use when PRON want to know how the system perform overall across the set of datum  PRON should not come up with any specific decision with this average   on the other hand  micro  average can be a useful measure when PRON dataset vary in size   micro and macro  average  for whatev metric  will compute slightly different thing  and thus PRON interpretation differ  a macro  average will compute the metric independently for each class and then take the average  hence treat all class equally   whereas a micro  average will aggregate the contribution of all class to compute the average metric  in a multi  class classification setup  micro  average be preferable if PRON suspect there may be class imbalance  ie PRON may have many more example of one class than of other class    to illustrate why  take for example precision  prfractptpfp let PRON imagine PRON have a one  vs  all  there be only one correct class output per example  multi  class classification system with four class and the follow number when test   class a  1 tp and 1 fp  class b  10 tp and 90 fp  class c  1 tp and 1 fp  class d  1 tp and 1 fp  PRON can see easily that  pra  prc  prd  05   whereas  prb01  a macro  average will then compute   prfrac05  01  05  05404   a micro  average will compute   prfrac1  10  1  12  100  2  20123   these be quite different value for precision  intuitively  in the macro  average the  good  precision  05  of class a  c and d be contribute to maintain a  decent  overall precision  04   while this be technically true  across class  the average precision be 04   PRON be a bit misleading  since a large number of example be not properly classify  these example predominantly correspond to class b  so PRON only contribute 14 towards the average in spite of constitute 943  of PRON test datum  the micro  average will adequately capture this class imbalance  and bring the overall precision average down to 0123  more in line with the precision of the dominating class b  01     for computational reason  PRON may sometimes be more convenient to compute class average and then macro  average PRON  if class imbalance be know to be an issue  there be several way around PRON  one be to report not only the macro  average  but also PRON standard deviation  for 3 or more class   another be to compute a weighted macro  average  in which each class contribution to the average be weight by the relative number of example available for PRON  in the above scenario  PRON obtain    prmacro  mean025·05  025·01  025·05  025·0504    prmacro  stdev0173    prmacro  weighted00189·05  0943·01  00189·05  00189·050009  0094  0009  00090123   the large standard deviation  0173  already tell PRON that the 04 average do not stem from a uniform precision among class  but PRON may be just easy to compute the weighted macro  average  which in essence be another way of compute the micro  average  
__label__neural-networks __label__convolutional-neural-networks __label__recurrent-neural-networks PRON be a php developer learn python for one reason  i wanna learn ai and i think that python would be good than php at that  PRON try find tutorial on how to build a neural network but PRON all use library  PRON be very interested in build the algorythm PRON to understand how PRON actually work completly  PRON would use library once i have full understanding of how neural network work  sorry if this be too broad  but any explanation of neural network or example  without library  be much appreciate  thank  neutral network require advanced mathematic to understand  if PRON do not mind the complexity  goto   httpneuralnetworksanddeeplearningcomchap2html  the section the code for backpropagation have what PRON want  the github link be httpsgithubcommnielsenneuralnetworksanddeeplearn 
__label__parallel-computing __label__reference-request PRON write code in fortran and c for various matrix algorithm  however  when PRON profile PRON code use vtune  PRON usually run into some terminology that PRON can not fully appreciate  be there a good resource for learn profile from scratch to a fairly advanced level   although PRON be look forward to profile numerical code  PRON do not think  PRON may be wrong  that profile other code be any different  PRON want a tutorial on profiling and PRON would prefer an online  free  pdf but would not mind book or handbook   PRON have try read vtune s handbook but that s like try to learn chinese by read a chinese language book   further  be vtune the good way to go  PRON really like a gui and since PRON use intel mkl  PRON figure PRON would be good than valgrind   another common choice be to use papi httpiclcsutkedupapi or tau httpwwwcsuoregoneduresearchtauhomephp  valgrind be great for find memory error   some slide from tacc   httpwwwtaccutexaseduuserservicestrainingcoursematerial  httpwwwtaccutexaseducdocumentlibrarygetfileuuidfc609b77b7274bff81a4d30caa4013d4ampgroupid13601  if PRON be interested in performance analysis of parallel code PRON like the book   scientific parallel computing  l ridgway scott  terry clark  babak bagheri   also  here be some cool slide use in a summer school course give  pasi  valparaíso  chile  january 2011  by one of the author  the material be base on the book   PRON tool PRON have use extensively for profile be valgrind  in combination with the graphical interface kcachegrind   valgrind be a set of dynamic analysis tool  memory error detection  thread bug hunting and profiling   some reference on valgrind   valgrind 33  advanced debugging and profiling for gnu  linux application  valgrind user manual  unfortunately  depend on where PRON come from    valgrind do not run on windows  only on linux and darwin base machine  
__label__machine-learning __label__text-mining __label__topic-model if this be a problem of text classification  be a similar dataset available or i have to make one on PRON own    PRON have a dataset of email for classification into spam  ham   topic modeling be usually use in the context of unsupervised learning  while classification be use for supervised learning  so PRON more depend on PRON dataset  do PRON have label that PRON can learn from  classification  or do PRON need to figure out topic PRON  topic modeling   
__label__linear-algebra __label__linear-solver __label__sparse __label__krylov-method PRON have matrix  a and  g  a be sparse and be  ntime n with  n very large  can be on the order of several million    g be an  ntime m tall matrix with  m rather small   1 lt m lt 1000   and each column can only have a single  1  entry with the rest be  0  s  such that  gtg  i  a be huge  so PRON be really tough to invert  and PRON can solve a linear system such as  ax  b iteratively use a krylov subspace method such as  mathrmbicgstabl  but PRON do not have  a1 explicitly   PRON want to solve a system of the form    gta1gx  b  where  x and  b be  m length vector  one way to do PRON be to use an iterative algorithm within an iterative algorithm to solve for  a1 for each iteration of the outer iterative algorithm  this would be extremely computationally expensive  however  PRON be wonder if there be a computationally easy way to go about solve this problem   introduce the vector  y  a1gx and solve the large coupled system  aygx0    gty  b for   y  x simultaneously  use an iterative method  if  a be symmetric  as seem likely though PRON do not state PRON explicitly  then the system be symmetric  but indefinite  though quasidefinite if  a be positive definite   which may help PRON to choose an appropriate method   relevant keyword  kkt matrix  quasidefinite matrix    edit  as  a be complex symmetric  so be the augment matrix  but there be no quasidefiniteness  PRON can however use the  ax routine to compute  axoverlineaoverline x  therefore PRON could adapt a method such as qmr ftpftpmathuclaedupubcamreportcam9219pdf  design for real system  but PRON can easily rewrite PRON for complex system  use the adjoint in place of the transpose  to solve PRON problem   edit2  actually  the  01structure of  g mean that PRON can eliminate  x amd the component of  gty symbolically  thus end up with a small system to solve  this mean mess with the structure of  a  and pay only when  a be give explicitly in sparse format rather than as a linear operator   but PRON know  g   gt and  a  so   gt a1  g x  b   g gt a1  g x  g b  since  gt g  i  then  gt  g1  so  g gt  i    a1  g x  g b   a a1  g x  a g b   g x  a g b   gt g x  gt a g b   x  gt a g b  unless PRON have miss something  PRON do not need any iteration  or any solver to calculate x give  g   a and  b  follow arnold s reply  there be something PRON can do to simplify the problem  specifically  rewrite the system as  aygx0  gty  b then note that from the statement that  g be tall and narrow and each row have only one 1 and zero otherwise  then the statement  gty  b mean that a subset of the element of  y have a fix value  namely the element of  b  let PRON say that for simplicity that  g have  m column and  n row and that exactly the first  m row have one in PRON and that be reorder the element of  x PRON can make PRON so that  g have the  m time m identity matrix at the top and a  n  m time m zero matrix at the bottom  then  PRON can partition  yyc  yf into  m  constrain  and  n  m  free  element so that  yc  b PRON can also partition  a so that  abeginpmatrix  acc   amp  acf   afc   amp  aff  endpmatrix from the equation  aygx0  PRON then get the following      acc  yc  acf  yf  x  0    afc  yc  aff  yf  0     and use what PRON know about  yc PRON have from the second of these equation     aff  yf  afc  b     and consequently     x  acc  b  acf  aff1  afc  b     in other word  the only matrix PRON have to invert be the subset of  a whose row and column be not mention in  g  the null space of  g   this PRON can easily do   i  compute   z  afc  b   ii  use whatev solver PRON have to solve  aff  h  z   iii  compute  x  acc  b  acf  h  in other word  give the structure of  g  solve the linear system PRON have be really not more difficult than solve a single linear system with  a  
__label__pde __label__finite-difference __label__parabolic-pde PRON be solve the follow equation  ut  x2uxxfracx  yt  tuy with an initial datum  u0maxy  c0 for some  c in the domain of the numerical method  in time PRON be solve that on   0t before PRON implement a numerical method PRON have to make sure the solution exist and unique  assume PRON be do and the next question be the regularity since the error analysis be base on the taylor expansion and to do so PRON need enough regularity in the solution  so  PRON would like to know what PRON can have in this case   in  y dimension this equation be just transport and thus this non differentiability may propagate  but do the diffusion in the first variable help to smooth PRON at all  if not  then PRON do not even have  uy in classical sense and only in weak  so why any numerical method would ever converge because if the local truncation error include  uyy or any high order term PRON can never have boundedness of PRON and at the point of the kink  and thus no mesh refinement can achieve decrease error at that bad point   thank   PRON diffusion only act in  x direction and so  consequently  any kink in the solution will only be smooth away if the be along line in  x direction  but  in PRON case  the discontinuity be only in  y direction  and consequently  the kink will never disappear  PRON will only be transport  the easy way to see this be to think of the solution as the limit  delta t rightarrow 0  of an operator splitting approach in which PRON first apply the diffusion step in  x direction and then the transport step in  y direction   base on this hypothetical operator splitting scheme  PRON will realize that any numerical method will only converge if PRON make the transport part converge  there be of course many way of do this  most notably thing like supg or artificial diffusion  to understand why convergence for these method do indeed happen  local truncation error analysis be not a very useful approach  PRON will need to read the literature on discretization of transport equation and adapt the reasoning there  
__label__finite-difference __label__fluid-dynamics __label__finite-volume __label__method-of-lines so PRON inherit from some people a code that solve the advection  diffusion  reaction equation for a particular system  the original code be first implement in 1d which work fine in cartesian coordinate  however  now that PRON be try to implement PRON in 2d PRON be have a few problem   the equation be     fracpartial upartial t   nabla cdot left  boldsymbolv  u  dnabla u right   f  where  u be the primitive variable be evolve   d be a diffusion coefficient  and  f be sourcesink   by separate the spatial discretization from the temporal through a method of line  integrate time with a runge  kutta 4th order scheme  and use a cartesian coordinate discretation PRON become relatively straightforward to extend a model from 1d   2d because the spatial component be additive  however  the problem be that the genuine solution to the equation PRON be solve tend be somewhat spherically symmetric  but not completely   but the grid be cartesian so the solution tend to show grid orientation error in the cartesian grid  ie the solution look kinda  square  instead of look like a  circle   a cpu costly solution to this grid orientation error be to make the grid fine  but PRON be get to the point the computation become very slow   PRON be able to solve the 2d grid orientation error for the diffusion part of the advection  difussion  reaction equation by discretiz the laplacian operator into a 9point stencil instead of the original 5point stencil  the latter 5point stencil be acquire by just naively add the 1d laplacian discretation of the x and y component  the 9 point stencil add spatially  transverse  component to the discretization  however  the advection part still have some pretty bad grid orientation problem   PRON be wonder if there be a  9point stencil  analog to the 2d advection problem  maybe a stencil that take into account the  transverse  cross  term spatial component somehow  PRON know of corner transport upwind  ctu  which have a transverse component   beginalign   qijn1ampqijnfracudelta tdelta xleftqijn  qi1jnright  fracvdelta tdelta yleftqijn  qij1nright     ampqquaduvfrac2delta t2delta xdelta yleftleftqijn  qij1nrightleftqi1jn  qi1j1nrightright   ampqquadqquadqquadqquadleftleftqijn  qi1jnrightleftqij1n  qi1j1nrightright   endalign   however PRON be not sure if PRON can be use with mol and runge  kutta 4th order because PRON depend explicitly on time eg  delta t  while PRON seem that cell discretization scheme use for mol only depend on the spatial component so that time can be integrate separately  however PRON may be wrong and ctu could be perfectly appropriate  PRON would be really grateful for some guidance here because PRON be a noob   PRON could use a stencil to do a multidimensional least  square fit which will account for transverse component   PRON have see this do in finite volume before   weller et al  2009  weller  amp  shahrokhi 2014  give detail of use upwind  biased stencil on unstructured two  dimensional mesh  but the same technique can be straightforwardly apply to uniform cartesian mesh   PRON have see this technique use with flux  form advection that give stable result  but PRON own test with advective form suggest that such a scheme be not stable  likely due to godunov s theorem   reference   weller et al  2009  voronoi  delaunay  and block  structure mesh refinement for solution of the shallow  water equation on the sphere  monthly weather review  137  4208  4224  dxdoiorg1011752009mwr29171   weller  amp  shahrokhi 2014  curl  free pressure gradient over orography in a solution of the fully compressible euler equation with implicit treatment of acoustic and gravity wave  monthly weather review  142  4439  4457  dxdoiorg101175mwrd14000541 
__label__machine-learning __label__neural-network PRON have be read through neat  neuroevolution of augmenting topologies  s whitepaper  and one thing be particularly confusing   innovation number be supposedly use to track the origin of a mutation  so that crossing over be easily do  from what PRON understand  two identical connection should always have the same innovation number  because PRON number be index  however  accord to this illustration   when the two genome be cross over share connection with the same innovation number  the result innovation be take from the more fit parent  or random if PRON be equal fitness    if the two connection be identical  PRON have the same innovation number  then why do PRON matter which parent PRON come from  since the connection be the same either way  or be there something about PRON that be not the same and PRON be understand this wrong   the gene show in the diagram have be simplify so that PRON only show topology and innovation index number  in addition  the full genome contain weight for each connection and bias for each node   the innovation number be not track change with connection weight or bias  just the rough time of insert a specific connection between two node  so each parent can have matching connection  innovation  but with different weight associate  the full genome be still subject to cross  over and mutation  the excerpt PRON quote help explain how the genome be align between parent with different architecture  so that ga crossover and mutation can occur in each connection and node   which parent be use to create each offspring gene matter because of the additional connection weight datum associate with each parent s gene  
__label__feature-selection __label__random-forest __label__decision-trees __label__xgboost __label__gbm PRON be try to find a cutoff value  in the feature importance space to eliminate spurious feature  so PRON be inject a completely random generate feature  as one of the input feature  and PRON cut the feature that be score below the value of the random feature in the importance ranking  but once PRON remove the feature  the performance drop  PRON do not understand what be happen  PRON check this behavior in a gradient boost and in a random forest  someone have a clue of what be happen   
__label__nonlinear-equations __label__convergence PRON be well know that for certain linear system jacobi and gauss  seidel iterative method have the same convergence behavior  eg stein  rosenberg theorem  PRON be wonder if similar result exist for nonlinear iteration  where at step  k the jacobi iteration on  say  mathbbrn  be define as    xik1fix1k1cdot  xi1k1xikcdot  xnk  and the gauss  seidel iteration be define as    xik1fix1kcdots  xnk  for  i1cdot  n and PRON have a set of nonlinear function  ficdot   mathbbrntomathbbr  
__label__machine-learning __label__neural-network __label__deep-learning PRON understand how recurrent neural network work  however PRON be try to build a deep intuitive understanding of PRON behavior which be difficult for PRON because PRON exhibit such complex behavior   the most difficult thing for PRON to understand be how a reccurrent neural network can exhibit an oscillatory behavior along with the notion of explode weight  for one  PRON be guess that oscillatory behavior be only possible for certain activation function and configuration  here be PRON follow question   1  can sigmoid activation function exhibit oscillatory behavior  PRON have convince PRON that PRON do not since PRON have a positive range between 0 and 1 which do not allow for negative derivative  but maybe PRON be wrong  be there a formal proof out there for whether PRON can or can not   2  which activation function can exhibit oscillatory behavior and be there proof out there for PRON  PRON believe tanh may have this behavior but PRON be not sure   3  what be explode weight derivative and how do PRON occur  there be polar opposite of explode weight s which seem to be where the weight do not learn and stay stagnate  what cause these issue  PRON imagine this be dependent upon the activation function as well   lastly  PRON begin read this article   httpwwwgooglecomurlsatamprctjampqampesrcsampsourcewebampcd2ampved0ccsqfjabampurlhttp3a2f2fwwwresearchgatenet2fprofile2fchristianoreilly2fpublication2f52004955permanentoscillationsina3noderecurrentneuralnetworkmodel2flinks2f0c96052464be171d68000000pdfampeixe1vf7ulngsogso9yhibqampusgafqjcne0fex0s7hy2wupmnkmaxiseukwwampbvmbv91386359dcgu  but get confuse around page 10 because the author omit s explain opaque assumption such as why PRON be define the matrix measure as be and so forthit be a good paper nonetheless   be there a more transparent paper or lecture out there that can shed light on this subject  or maybe simple explanation for what the author be try to get at   
__label__finite-element __label__boundary-conditions __label__navier-stokes __label__variational-calculus PRON be work on a rectangular domain where  gammain be the left hand side inlet boundary and  gammaout be the outflow boundary  to the right  the top and and bottom be rigid wall with no  slip condition   the flow equation be a stationary navier stokes problem which be solve use finite elements method with a non  homogeneous inlet condition  uy PRON want to know how to correctly implement this boundary into the weak form  or more precisely be there a possibility that PRON can eliminate face term which be a result of the weak form    rhoucdotnabla u   div   sigmarho f   div   sigma  0   bc   u  uy on  gammain  
__label__optimization __label__complexity __label__stochastic PRON have a reference value  r and a model value  m  m be generate use a stochastic algorithm with parameter  a and  b  the objective be to tune  a and  b so that  m be as close as  r possible  heuristically   but each run of  m take around 30 second  any  good  algorithm will be jam by this computation  seemingly  even so  what be good  than other  algorithm of choice   if PRON be to allow  vert m  rvert converge quickly initially  and accept PRON to slowly converge in later stage   what type of algorithm should PRON look for  PRON colleague suggest particle swarm optimisation  pso   be that a good choice   note  PRON need  good  fit but not  good  fit  PRON tolerance be fairly large   response surface model  a kind of surrogate model  be often use in situation like this   the idea be to sample value of the parameter  a and  b  compute  ra  b at each point  and then build a regression model  typically quadratic or even high order  of the function   PRON can then optimize over the the fitted model   active learning  aka experimental design  strategy be suitable for this  one of PRON be the response surface modeling suggest by brian borchers  the idea be to choose the next point to evaluate to learn about the optimal m as fast as possible  here be some old and new paper to start from   harold j kushner  a new method of locate the maximum of an arbitrary multipeak curve in the  presence of noise  journal of basic engineering  page 8697–106  march 1964   j mockus  the bayesian approach to global optimization  lecture notes in control and information sciences  38473–481  1982   julien villemonteix  emmanuel vazquez  and eric walter  an informational approach to the global optimization of expensive  to  evaluate function  journal of global optimization  september 2008  cs0611143  
__label__neural-network __label__image-classification this be a major confusion for PRON  PRON have always think filter  those small slide window size 3x3 or 5x5  be strictly 2d  this among other be specify in caffe api   kernelsize  kernelh  kernelw  the filter dimension  give by  kernelsize for square filter or kernelh and kernelw for  rectangular filter  now  PRON have never be 100  clear on what these convolution mean  take from matt zeiler s article  visualizing and understanding convolutional networks  and how PRON be obtain   after search PRON turn out that filter can be 3d   how do PRON work then  PRON find a bit of information on cs 231n course  but PRON be not explain in detail anywhere   the diagram below example 2 in this cs231 lecture be a good visualization of how filter work  httpcs231ngithubio  convolutional  networks since each layer of a convnet have a depth correspond to the number of filter  in an input rgb image  for example  the depth of the input layer would be 3   each filter of size  ktime k apply to a layer of delth  d slide over a volume of  ktime ktime d  note that the filter stay at a constant depth   
__label__pde __label__finite-element PRON be a beginner with fe  PRON application be the pricing of financial derivative where the space be five dimensional  so  add time  the problem have six dimension   PRON try to look around  fenic  escript  deal  ii     but PRON understanding be that those software be limit to 3  1  3d space  1d time   be this correct   PRON targeted language be python or c   description of PRON problem  PRON would like to price an investment product where  each month  the investor have the freedom to re  invest or not   PRON would like to do so with stochastic volatility  stochastic interest rate  and stochastic mortality   the stochastic pde look like this  beginalign   dst  amp must dt  sqrtsigmat  dbst   amptextstock  dsigmat  amp musigmat dt  nusigmat dbsigmat  amp  textvolatility     drt  amp murt dt  nurt dbrt  amp  textinter rate     dqt  amp muqt dt  nuqt dbqt  amp  textmortality    endalign   where  must be a time  dependent constant associate to the stock price  s  and  bst be an independent levy process which create noise in the stock price  s similarly for the other quantity   nusigmat be a time  dependent quantity associate to the volatility  sigma  let  ctau denote the admissible investment at time  tau  the stochastic control problem look like     vtau  max left  c in ctau  ptextdeathertau fstau1     paliveertau vtau1right     the above pde be continuous  but the value of the product  vtau be solve only at predefin  tautime  say each month   PRON guess monte  carlo can always brute force PRON problem  but PRON be very slow   deterministic form of the stochastic pde  for this part  assume that the value of the option     v   t  st  sigmat  rt  qt  ct  mapsto   t  vt       be define on the natural time  t  not the  tautime  with  ct the investment at time  t  define the differential operator  beginalign   lt  amp partialr  s   partialrsigma   partialsigma  s    lst  amp sigmat partial  rt partials  s    lrt  amp partialr  partialr  r    lsigmat  amp partialsigma  partialsigmasigma    lqt  amp partialq  partialq  q   endalign   where time  dependent constant  mustldots be ignore   the deterministic pde be then     partialt  vt  leftlt lst  lsigmat  lrtlqtrightvt  0      which can adapt to the optimal control problem on the  tautime   ok  so PRON look like what PRON have be a coupled set of ode  since as far as PRON can tell  there be only derivative with respect to time  and no derivative with respect to anything else  there be quite a few package out there for solve system of ode of arbitrary dimension  matlab have stuff like ode45   for python  look at this question for some suggestion  finally  there be old fortran code on netlib that can be interfac with c pretty easily  ease of use be another matter   there be probably good alternative out there since PRON be be a while since PRON have look  other should chime in    assume PRON want to solve the black  scholes equation or a variant on a portfolio of 5 asset  then PRON indeed have 5 spatial plus one time dimension  PRON do not know any fem package that can do that off the top of PRON head  deal  ii can not readily do PRON  but see below  but PRON think PRON remember that some people from chris schwab s group at eth zurich solve such problem use sparse mesh  PRON may get lucky look around PRON publication   there be other equation that have extra dimension  one example be the radiative transfer equation that have 3 space  1 time  2 angular  1 energy dimension  the way this be typically solve be to discretize 3dimensional space as usual  then discretize the angular and energy dimension on separate 2 and 1dimensional mesh and at each nodal point of the spatial mesh simply have a lot of variable  one each for each node of the angular mesh time the number of node in the energy mesh   PRON use this scheme in deal  ii implementation successfully  this make sense for the radiative transfer equation  and PRON may be emulate for PRON equation even if PRON be not natural there   dune  the distributed and unified numerics environment httpwwwduneprojectorg  feature some structured grid of arbitrary dimension  sgrid  and yaspgrid   see dune s feature  currently  there be a branch that turn yaspgrid  one of the above grid  into a tensor product grid  if that be of interest  since release 20  the current one be 221  and 23 be soon to come  PRON do have reference element for various finite element method that support arbitrary dimension  therefore PRON should be possible set up finite element discretization of arbitrary dimension with eg the disrectization module dune  pdelab  albeit this may not be test to often   have say that  there be still the curse of dimensionality as wolfgang point out   for further information PRON refer PRON to the dune mailinglist  
__label__neural-network __label__beginner __label__convnet __label__computer-vision background  PRON be study cnn s outside of PRON undergraduate cs course on ml  PRON have a few question relate to cnn   1  when train a cnn  PRON desire tightly bound  crop image of the desire class  correct  ie if PRON be try to recognize dog  PRON would use thousand of image of tightly crop dog  PRON would also feed image of non  dog  correct  these image be scale to a specific size  ie 255x255   2  let PRON say training be complete  PRON model s accuracy seem sufficient  with no problem  from here  let PRON have a large  hd image of a non  occlude dog run through a field with various obstacle  with a typical nn and some datum  PRON just take the model  cross PRON with some input  and bam PRON be go to output some class  how will the cnn view this large image  and then  find  the dog  do PRON run some type of preprocess on the image to partition PRON  and feed the partition   though there can be a very detailed explanation for this question but PRON will try to make PRON understand much minimal word   1  crop the image to a particular size be not a necessary condition and neither be scale  but put this way  PRON do not matter whether a dog be represent in a bampw image or  rgb image because a convolution network learn feature in the image which be independent of color  scale and resize help to limit the value of pixel between 0 and 1   2  once PRON have train PRON cnn model  PRON have learn all the feature like edge  etc  to recognize a dog in the image  because the model have learn the feature  PRON acquire certain property like translation invariance which mean that no matter where PRON position a dog in the image  PRON be still a dog and have the same feature  how the model recognize PRON  PRON check for the feature of a dog  learn during training  no matter what the size of the new image be or where the dog be in the image or what the dog be do   for get a in  depth understanding PRON can refer to the follow resource   httpneuralnetworksanddeeplearningcomchap6html  httpcs231ngithubioconvolutionalnetworks 
__label__bigdata __label__apache-hadoop __label__research PRON be look for a thesis to complete PRON master m2  PRON will work on a topic in the big datum s field  creation big datum application   use hadoop  mapreduce and ecosystem  visualisation  analysis    please suggest some topic or project that would make for a good master thesis subject   PRON add that PRON have base in datum warehouse  database  datum mining  good skill in programming  system administration and cryptography   thank  since PRON be a master s thesis  how about write something regard decision tree  and PRON  upgrade   boost and random forests  and then integrate that with map  reduce  together with show how to scale a random forest on hadoop use m  r  
__label__numerical-analysis __label__fluid-dynamics __label__numerical-modelling PRON need to discretize the surface of prolate spheroid give by the equation    fracx2l2   fracy2d2   fracz2d2   frac14  the surface have to be divide to 500 equal panel to calculate the velocity potential on PRON  this mean PRON will need to calculate the normal vector at each panel  can anyone suggest any method   a spheroid be really just a sphere that have be squash in the different coordinate direction  so to get a mesh for a spheroid be the same as get a mesh for a sphere  start with the latter  then do the obvious and trivial transform   so how do PRON get a mesh for a sphere  if PRON want quadrilateral  PRON start with six quad that cover the surface of the sphere and that be arrange like the face of a cube  then refine these by recursively replace each quad by PRON four child  where PRON pull the new point at the midpoint of the edge and the face out onto the sphere  this give PRON 24   96   384   1536 quadrilateral that cover the surface of the sphere   if PRON want triangle  then PRON can either start with the four side of a tetrahedron  or the 8 side of an octahedron  or the 20 side of a icosahedron  PRON would then recursively refine each triangle into PRON four child in the same way as above  until PRON be satisfied that PRON have enough triangle  
__label__predictive-modeling __label__word-embeddings __label__matrix-factorisation __label__text-generation __label__generative-models text generation be well study use markov chain or nn  but PRON be not aware of any work to word sequence prediction in term of subspace learning   treat phrase or sentence as temporal datum such as time series  PRON be possible to represent word sequence as a tensor  t  ws time w time k  where  ws be the set of word sequence present in the corpus   w represent the set of segmented word  and  k be the maximum length of observed sequence  for instance  for a phrase  ws  word sequence prediction  then  tws   sequence   2   1   for an incomplete tensor  where entry st  prediction be miss  the reconstructed tensor after decomposition can then be use to generate text  in term of the observed word space   PRON question be as follow   1  be there any work use tensor factorization or factorization machine for word sequence generation   2  how subspace learning model differ from those generative model  such as recurrent neural networks or belief networks  what be the downside of use subspace method as compare to other establish method   2  how to establish the threshold for the length of the predict sequence  for example  can one look at the  wsr time kr space  and use cross  validation to find the threshold for each word sequence   any pointer or answer to any of the above question be highly appreciate   tensor factorization would not work for text generation as a stand  alone technique  there be no way for the decomposition to model long  term dependency in language  without model long  term language dependency  PRON result would be similar to low  order markov chain   tensor factorization could be use as another signal in a large natural language generation system  for example improve word  embedding  
__label__pde __label__finite-difference PRON be probably a student level question but PRON can not exactly make PRON cleat to PRON  why be PRON more accurate to use non  uniform grid in the numerical method  PRON be think in the context of some finite  difference method for the pde of the form  utx  tuxxx  t and assume PRON be interested in a solution at the point  xast so  PRON can see that if PRON approximate the second derivative  for example on a uniform grid use three point approximation  the error be second order  oh2 then PRON can construct non  uniform grid via a mapping and find coefficient for the three point that be use to approximate the derivative  PRON can do the taylor expansion and again obtain a bound for the derivative to be a second order  oh2  where  h be the distance on a uniform grid from which PRON obtain map to a non  uniform grid  both estimate contain derivative and PRON be not clear to PRON why the solution would be more accurate on non  uniform grid as PRON depend on the magnitude of the correspond derivative in error estimate   the typical reason why a non  uniform can lead to high accuracy be that the pde to solve be not of the form  utx  tuxxx  t  but of the form  utx  tdxuxx  tx if PRON non  uniform grid allow PRON to represent the real  dx more accurately  PRON will get a more accurate solution  because  dx be normally determine by material property  PRON be likely constant within each material  so PRON normally have a piecewise constant function and should really align the grid accordingly   a different reason may be that  ux0 have more variation in certain region than in other  one can try to compensate this a bit with an adaptively refine non  uniform grid   however  in PRON opinion there be other technique which can handle this situation even better than a non  uniform grid    the rationale for non  uniform mesh go like this  all equation understand to be qualitative  ie  in general true but without the pretense to be provably so in all circumstance and for all equation or all possible discretization    when solve an equation with  say  linear finite element  then PRON typically have an error estimate of the kind     u  uhl2omega   le c htextmax2 nabla2 ul2omega        or  equivalently but in a form better suit to the follow      u  uhl2omega2 le c htextmax4 nabla2 ul2omega2      however  this be an overestimate  in fact  one can in many instance show that the error be actually of the form     u  uhl2omega2 le c sumk in  mathbb t   hk4 nabla2 ul2k2      here   k be the cell of the triangulation  mathbb t this show that in order to make the error small  PRON be not actually necessary to reduce the maximal mesh size  htextmax rather  the most efficient strategy will be to equilibrate the cellwise error contribution  hk4 nabla2 ul2k2   in other word  PRON should choose     hk propto nabla2 ul2k12     in other word  the local mesh size  hk should be small where the solution be rough  have large derivative  and large where the solution be smooth  and the formula above provide a quantitative measure for this relationship   prove PRON to PRON with this example   what be the maximum error when interpolate sqrtx  on the interval  01  with piecewise linear interpolation on a uniform mesh   what be the maximum error when interpolate on a mesh in which the ith of n point be give by  i  ns  and s be a carefully choose mesh grading parameter   kamil   differential equation solve be global  interpolation be local   in piecewise polynomial interpolation  the accuracy far from the singularity will not be bother by the singularity   unfortunately  this be not at all true for solve an elliptic equation  such as a two  point boundary value problem   the singularity will pollute the approximation globally   here be something to try   solve dsqrtx  du  on  01  with homogeneous dirichlet bc   d be the differentiation operator   use finite element or finite difference on an n  point uniform mesh   compare to a mesh in which the ith point be  1n15   note that the bad error for the uniform mesh be far from the singularity  and much large than for the grade mesh  
__label__matlab __label__regression PRON find r function ridgecv very useful   PRON would like to implement the equivalent function in matlab  as a starting point  PRON use matlab function b0  ridgey  x  k  scale   however PRON give completely different result  why do this may happen  what value should PRON set for variable  scale   1 or 0 and what be PRON difference    and how could PRON implement PRON from the scratch in matlab   one form of ridge regression be the problem    nbspnbspnbspnbsp  minimize ax  b  and keep x near a give point x0  often 0    if PRON cast this as    nbspnbspnbspnbsp  minimize ax  b2  w2 x  x02   PRON can solve PRON by run least square on these extended input    a    b    w i    w x0   here w be a weight factor that balance minimize ax  b  and keep x near x0   how should one choose PRON if one have no idea   a rule of thumb be to make ax  b and x  x0 roughly equal  print those value   add   weighted least square   with different weight wi  be a powerful method   for example  wi sim frac1sqrt textxi  push small xi towards 0  think iphone finger    sparsifie   mdash   a poor man s approach to l1norm regularization   see lasso   not matlab  but hope this help   PRON can get the source and documentation for the parcor package which include the  ridgecv function on cran   PRON can implement the function in matlab by look at the source  alternatively  PRON think PRON should be possible to call r from matlab  
__label__ai-design __label__intelligent-agent recently mark get some attention from the medium by state that PRON have create jarvis  not that PRON be against PRON or anything  but this jarvis seem to have be do a hundred time before  PRON be do something which most developer would classify as a home automation system  to PRON PRON be more like PRON do PRON for the attention  PRON be kind of take back by the amount of medium attention PRON get  if PRON have hear of jeremy blum  maybe PRON may understand what PRON be try to imply here   PRON be just curious as to why PRON get so much attention  be there anything technically novel about PRON system that set PRON so much apart from previous one   no  there be nothing novel about this system  the main hurdle PRON have to pass be problem that PRON will face when PRON system have a lot of integration point across various api provide by different vendor with messy and often outdated documentation   as far as attention be concern PRON live in a world where so call celebrity get attention for anything that PRON do  remember that medium have only one goal  more and more eyeball aka more money and nothing else  
__label__statistics __label__data-cleaning __label__hive PRON be try to find some outlier on PRON database use hive and PRON be use standard deviation technique  PRON query be   select id  from datum  where id  lt   avgid   stddevid    and id  gt   avgid   stddevid     when PRON run this code PRON be get the follow error   error while compile statement  fail  semanticexception  error 10128   line 312 not yet support place for udaf  avg   how to solve this problem  many thank   seem like hive do not let PRON use avg in a where clause  PRON can solve this with a subquery   select PRON would  from   select PRON would  avgid  as avgid  stddevid  as stddevid from datum   where PRON would  lt  avgid  stddevid and PRON would  gt  avgid  stddevid 
__label__python __label__mesh-generation PRON have a dataset of 3dimensional point for which PRON would like to construct a mesh  use python  all the software PRON have see require that PRON provide the edge  be there a program in python which take as the input a set of point in 3d and output a mesh  if possible  PRON would like the meshing to be uniform   PRON could try vtk which have a python api  PRON would first try paraview and bring PRON point into that and then try some of the filter  eg delaunay   if the filter work in paraview with PRON datum  which be vtk base  then PRON can use vtk to do the job  how successful PRON will be will depend on what the point look like and how nicely PRON suit the filter   if vtk look like PRON will work  have a look at the kitware tutorial on vtk as PRON know there be one that walk through use python  vtk   cgal  httpwwwcgalorg  have a number of module for triangulate point in 3d  surface mesh from point  triangulation of point in 3d  etc   python wrapper for a subset of cgal module be available  httpscodegooglecompcgalbinding   include for 3d triangulation  PRON have use the cgal c interface for triangulate point in 3d  but PRON have no experience with the python interface   if an unrestricted triangulation be ok  PRON can do PRON with scipyspatialdelaunay which use qhull  
__label__sage PRON be a bit of sage newb  so PRON apologize if this be a simplistic question   PRON want to use sage to compute some invariant of elliptic curve define over ring which be not the integer  mathbbz or the rational  mathbbq more specifically  PRON be interested look at elliptic curve define over the ring     mathbbz16c4c6      or     mathbbzpc4c6      where  p be some prime  can anyone tell PRON how produce a localized ring such as  mathbbz16 or  mathbbzp in sage  or know where in the manual PRON could find such information   thank   
__label__b-spline __label__curve-fitting PRON be try to use b spline curve fitting  the order of b  spline curve be 4  when PRON have many control point  PRON work well   however if the number of control point be small such as two  PRON  program will crash  PRON realize that the number of control point be relate to number of knot and the order   can anyone help PRON clarify the relationship or give some link on PRON   the formula be    m  n  p  1    m number of knot    n number of control point    p degree   PRON can check the nurb book chapter 2 for a complete set of definition  the shumaker s book  be a more readable reference   there be a paragraph on interpolation also on tom lyche and knut mørgens s lecture note  here PRON shall find that all the entry in PRON matrix be positive  so the linear algebra should be ok  donno what about the rest of PRON code  
__label__r __label__predictive-modeling __label__logistic-regression __label__glm PRON have a problem with output the term for a logistic regression model in r  for a give list of independent value  say list l of term  w  y  z  to determine dependent variable  x   PRON want to find out what the big regressor be when PRON pair two term together   PRON want to be able to group multiple independent variable together and say  when a record have this combination of value  then PRON have a very strong chance of predict x    PRON try to just add the interaction when call the glm function like glmx  y  w  z  w  z  y  z  y  w  data  l    but the result come out very hard to explain  because PRON of how PRON be measure between PRON and not just measure against the mean  do anyone know a way to do this   
__label__machine-learning __label__r __label__predictive-modeling __label__performance __label__training PRON be run the follow model  logistic regression  decision trees  svm  naive bayes  random forest  on the same datum set   PRON be use caret package in r  PRON PRON dream to plot training error and testing error curve  PRON know those plot  be pretty beautiful as well as other performance measure curve use caret  package  any help would be really appreciate   here PRON be create datum to for a give model   the main advantage with this code snippet be PRON be able to see how the model performer over various size of the training set  the learingcurvedata function can be find in the  60  71 version of caret1   of course  learning curve be useful in machine learn for several reason which include comparison of algorithm  choose model parameter  optimization  right data size to use for training   setseed1412   classdat  lt twoclasssim1000   setseed29510   ldadata  lt learingcurvedatdat  classdat   outcome   class    testprop  14      train  argument   method   lda    metric   roc    trcontrol  traincontrolclassprobs  true   summaryfunction  twoclasssummary    ggplotldadata  aesx  trainingsize  y  roc  color  datum     geomsmoothmethod  loess  span  8    themebw   
__label__classification __label__sequence PRON dataset be comprise of vector sequence  each vector have 50 real  value dimension  the number of vector in a sequence range from 3  5 to 10  15  in other word  the length of a sequence be not fix   some fair amount of the sequence  not vector   be annotate with a class label  PRON task be to learn a classifier that give a sequence of vector  the class label for the whole sequence be compute   PRON can not tell the exact nature of the datum but the nature of sequence be not temporal  nevertheless  a vector  xi can not be interchange with a vector  xj without change the label   i neq j   in other word  the order of vector be important  the vector PRON be comparable  for example PRON make sense to compute a dot product and use this similarity value   PRON question be  what be the tool  algorithm that can help to classify such datum   update   the datum have such a property that one or very few vector influence strongly the class label   possible solution   after some research PRON look like the recurrent neural networks  rnn  fit the bill pretty naturally  the overarch idea be to pick a context size  k  concatenate word vector  do max pooling and feed that through classical nn  at each possible context window position in a sentence  a feature vector be build  the final feature vector be build use max pool for example  the backpropagation be do to adjust the network s parameter  PRON already get some positive result  gpu be a must    as PRON can not disclose much detail  PRON be force to be a bit generic in PRON answer  PRON hope PRON will be helpful nevertheless  first of all  PRON would only consider reduce the sequence before classification  be PRON by use the dot product or something else  if PRON can make sure that PRON do not lose information PRON need for classification afterwards  so this approach be only feasible if PRON have some insight into the nature of the classification  to give a simple example  if the class label be just the number of vector in PRON sequence  PRON will not be very successfull in predict the class label from the dot product   hence  PRON would take the full sequence as an input for classification  and impose a maximum on the sequence length PRON want to consider  PRON may do this by first find the maximum sequence length m in PRON training set and then turn each sequence of 50dimensional vector into one vector of dimension 50m  possibly with some miss value at the end if PRON sequence do not have maximum length  PRON will probably want to get rid of these miss value and PRON may want to simply replace PRON by zero   there be two road PRON can go from here   1   PRON directly apply classification method know to be suitable for high dimension  try something simple that do not need much tuning like naive baye  this way PRON can see whether this approach be feasible without lose too much time if PRON be not   2   PRON try first to reduce the dimension and understand the nature of the classification better  PRON may want to use something like principal component analysis or analyse correlation  association between each vector component and the class label  if PRON be successful  PRON know how to properly reduce the dimension of PRON input before apply classification   if PRON would like to follow any of these idea  please keep in mind that the concrete detail of PRON datum and the classification may render any of the idea propose above infeasible  so please be careful to check against any detail PRON know but can not post here before try to make sure PRON be not waste PRON time    the datum have such a property that one or very few vector  influence strongly the class label   the good  and easy  approach would probably be to simply train a classifer on each vector and then average the prediction across the vector for a give sequence  the important vector would be strongly influential in PRON prediction  whereas the prediction for the unimportant vector would be close to 05  or similar for a non  binary classification problem   
__label__neural-network PRON have the network architecture from the paper  learn fine  grain image similarity with deep ranking  and PRON be unable to figure out how the output from the three parallel network be merge use the linear embed layer   the only information give on this layer  in the paper be   finally  PRON normalize the embedding from the three part  and combine PRON with a linear embed layer  in this paper  the dimension of the embedding be 4096    can anyone help PRON figure out what exactly be this layer  the author be talk about   
__label__algorithms __label__stability __label__computational-physics PRON be write a mod for a game that model orbital physics  kerbal space program  or ksp   PRON be attempt to model the effect of thrust on spacecraft in certain state where the game only model PRON as a keplerian orbit  the ship be  on rail    and PRON be have some difficulty with numerical stability  even if PRON do not have any thrust apply PRON end up constantly increase eccentricity for the orbit   in the  on rail  state  a spacecraft in the game follow a keplerian orbit exactly  the orbit be calculate when the spacecraft be put on rail  and then when PRON go off rail the game calculate the expect position and velocity at the current time and set up the spacecraft to do normal newtonian thing from there   at the moment  all PRON code do be take the calculated position and velocity of the spacecraft at the current time  use the game s own code  and then call a function already present in the game to calculate a keplerian orbit for an object with that position and velocity around the currently  dominant body  that is  PRON should be a no  op  PRON be actually see the orbit s eccentricity drift rapidly  certainly quickly enough that this will not work for gameplay purpose  this code be be call every physics timestep in the game  which be rather often  something like 50 time a second  so PRON suspect what PRON be see be that the code in the game for calculate position and velocity from orbit and vice  versa be not stable  PRON have verify that in actual gameplay take a vessel off rail and then put PRON back on rail can because observable orbital difference  and PRON have confirm that the calculated position  velocity of the craft change after PRON recalculate the orbit  relative error in position be about  108  relative error in velocity be about  102  PRON be not sure what the good solution be here  and PRON only have very limited experience with numerical computation  PRON would like to know what a good approach to this problem may look like  as far as PRON can see there be essentially three class of solution   calculate a new orbit directly from the old orbital parameter and the thrust  PRON do not know how plausible calculate a delta on a keplerian orbit be  though  and PRON can not find much online about do that   write code with good precision or good numerical stability for do orbit   pos  vee   orbit conversion  PRON do not know how plausible any stability be when PRON be do that fifty time a second  though  PRON have find some code for calculate that  but PRON do not know where PRON sit on the fast  stable  good spectrum   keep PRON own position  velocity value for the ship  update PRON continually  recalculate the orbit at interval just so the player get some feedback  that would require implement a general two  body problem solution  which sound like a lot of work   what be a good option here   relative error of  108 for what should be an identity operation seem okay  but not great   but  102 be absolutely terrible  with a relative error per time step of  epsilon  the relative error after time  t at  50  time step per second will be     1epsilon50 t  approx e50epsilon t      so the time  scale for accumulation of error be  002epsilon for  epsilon102  this be  2  second  from PRON drift in eccentricity PRON sound like that relative error be not only large  but also bias in one direction  and that bias be what would because the drift  PRON be not completely clear whether this be specifically numerical stability  or if this be just an inaccurate computation method be use   0   implement PRON own function to compute position  velocty  leftrightarrow orbit parameter transformation be certainly feasible  a good textbook on orbital dynamic would explain the necessary formula  there be also this helpful site  a book such as accuracy and stability of numerical algorithms by higham will explain what numerical stability be and how to design a stable algorithm for these computation   the main difficulty be cover all the corner case correctly  since the equation be a little more complicated for inclined orbit  if PRON remember correctly PRON be all just coordinate transformation  with just one non  linear kepler equation that be quite easy to solve   also note that add a small amount of acceleration at the beginning of every time step may or may not be accurate enough  this would need to be experimentally check  this be only first  order accurate  or second  order if the thrust vanish at end  point of integration   so if PRON be not enough  then write PRON own integrator be the way to go   1  implement PRON own ode integrator be quite feasible  and easy than PRON sound  the integrator need to be symplectic  so something like the stormer  verlet method be a good start     dot x  v  qquad dot v  fx        vfrac12   v0  tfrach2  fx0   qquad x1  x0  h vfrac12   qquad v1  vfrac12   tfrach2  fx1      however  PRON probably need much more accuracy that just one stormet  verlet step per in  game time  step  so PRON be probably worth look at a high  order method   the relevant reference be geometric numerical integration by hairer  lubich  wanner  and solve ordinary differential equations by hairer  norsett  wanner  accord to that  implicit gauss method  butcher  kuntzmann method  be symplectic  so for example if PRON do not mind solve a system of 6 nonlinear equation  then the 6th  order butcher  kuntzmann method may be worth try  PRON be  nu3  on p56 of butcher implicit runge  kutta processes  1964   
__label__machine-learning __label__deep-learning __label__feature-selection __label__feature-extraction __label__feature-engineering some observation be far too voluminous in PRON raw state to be model by predictive modeling algorithm directly   common example include image  audio  and textual datum  but could just as easily include tabular datum with million of attribute   feature extraction be a process of automatically reduce the dimensionality of these type of observation into a much small set that can be model   for tabular datum  this may include projection method like principal component analysis and unsupervised clustering method  for image datum  this may include line or edge detection  depend on the domain  image  video and audio observation lend PRON to many of the same type of dsp method   what about generate new feature with high predictive value from the raw datum and concatenate PRON to the raw data   for example PRON have datum about student wealth  health  family status and PRON want somehow generate a new feature PRON can call social status whic be generate from the raw datum and with high predictive value  be this possible  linear regression can be a good way PRON need to discover    high predictive value  be only define if PRON a target which PRON be try to predict upon  PRON seem that PRON do not  and PRON goal be to cluster datum point accord to some scale define by a variety of factor  these can undoubtedly be use to cluster datum point  and PRON would advise PRON to look into the various method available  some that may be interesting for PRON be agglomerative and hierarchical clustering   now to answer the question  PRON can surely generate new feature from the one present in PRON dataset that may or may not help PRON achieve PRON goal  PRON can   bin PRON datum  define some category such as  rich    average    poor  with specify range and create a new feature that map a numerical value  wealth  to a bin  one  hot  encode categorical variable  after these pre  processing step be do  PRON could go ahead an apply the clustering method PRON mention to group datum together into PRON respective  social status   of course  lot of tweaking and experimentation will be need  as far as what PRON have encounter  there be not really automagic way of generate new feature  and mostly the available method will depend greatly on the type of datum and problem PRON be work on  
__label__classification PRON have four different category which the number of variable in each category be 7  3  8  and 9  in total 27   the value of each variable change from 1 to 5 but some of the variable be not applicable for some observation  in total  there be 20 observation which PRON datum be available for 27 variable in the mention context  from 1 to 5 plus not applicable   how can PRON cluster these four category for 20 observation to get three cluster for each category   edit  there be 4 category  a  b  c  and d   each category have a different vector include 7  3  8  and 9 value  from 1 to 5 plus na   now  PRON be 18 observation for each category of a  b  c  and d PRON need to make three group for each category  how can PRON find the centroid of each cluster for each category  PRON mean PRON need three centroid in each category  totally 124 category  3 group   any feedback would be appreciate  thank PRON   first  be PRON category independent of each other  if this be really four parallel problem  solve one at a time and PRON will simplify thing a bit   second  the heart of clustering be the distance function between any two observation  because PRON datum have miss value  PRON need to explicitly decide how to handle PRON  a common approach be to ignore the problem by throw away any row that have any null value  which will not work at all for PRON datum   a simple approach would be to ignore any na column  but keep the rest of the column  for category c  the difference between row 1 and 2 would be 4  column 11  14  15  and 16 be all off by one   there be an issue with this approach that PRON will get to later  but let PRON implement PRON for now   for each category  PRON need three row of centroid position  PRON may as well start off with randomly select row that have no miss value in PRON   PRON then calculate distance to each centroid use a bunch of summed ifs  ifmatrix cell   na  matrix cell  centroid cell  0  across all the column   if PRON be do this computation in a spreadsheet  PRON would use intermediate cell to calculate the piece instead of deal with one huge formula    PRON then optimize the centroid with a standard iterative procedure  each row get a group membership correspond to the close centroid  then each centroid become the mean of PRON group member  and PRON look like PRON mean function automatically exclude nas   then PRON repeat until satisfied   why do PRON specify that PRON have to seed with a row without nas  if PRON allow na measurement in the centroid  this will fail because a row of all nas be as central as PRON get  if PRON update step never let PRON move to nas  the na column will be transient  but what happen when PRON take the mean of a miss column   in PRON sample datum  consider what happen if row 4  12 be a group in the b category    so if PRON want to allow nas in the centroid  PRON need to modify PRON distance function to something like the follow   if both cell be number  the distance be the difference  if both cell be na  the distance be 2  if only one cell be na  the distance be 3   those number be choose such that for any full column of number  a numerical value in the centroid position will be close  but if the column have sufficiently many na row  a na value in the centroid will be close  PRON may want to adjust the value so that the na threshold look like what PRON want PRON to   in order to find the centroid of a set of row use PRON distance function  PRON should use an optimization procedure  but PRON can cheat  if the number of na in the group be above the threshold  PRON set PRON to na  and otherwise PRON set PRON to the mean  
__label__python __label__pandas suppose PRON have such a json file        PRON would    0     name    name0     firstsent    date0     analytic        a   1           a   2             and PRON want to parse PRON with pandas  so PRON load PRON with  df  pdreadjsonfileson    PRON be all good until PRON try to access and count the number of dictionary in the  analytic  field for each item  for which task PRON have not find any good way than  for i in rangedfshape0     num  lendfi  i1analyticsi    but this look totally non  elegant and PRON be miss the point of use pandas in the first place  PRON need to be able to access the field within  analytic  for each item   the question be how to use pandas to access field within a field  which map to a series object   without revert to non  pandas approach   a head of the dataframe look like this  only field  PRON would  and  analytic  report    0    ua   00  ub   1    ua   001  ub   2    ua   04  ub   3    ua   02  ub   name  analytic  dtype  object  0   0   1   1   2   2   3   3   the first number be obviously the index  the string be the  PRON would   and PRON be clear that  analytic  appear as a series   multi  indexing may be helpful  see this   httppandaspydataorgpandasdocsstableadvancedhtml  but the below be the immediate solution that come to mind  PRON think PRON be a little more elegant than what PRON come up with  few obscure number  more interpretable natural language    import panda as pd  df  pdreadjsontestfilejson    df  dfappenddf   just to give PRON an extra row to loop through below  dfresetindexinplacetrue   not really important except to distinguish PRON row  for   row in dfiterrows     currnumbdict  lenrowanalytic     printcurrnumbdict  
__label__probability consider  n integer  value and independent random variable  e1  e2  dot  en with know distribution function  m1  m2  dot  mn let PRON denote with  e1  n the random variable give by the sum of  e1  e2  dot en  that be   e1  n   displaystylesumiei the pdf of  e1  n   m1  n  be give by the convolution of  m1  m2  dot  mn  PRON would like to calculate  pe1  n   1  e1  1  PRON intuition tell that that this be equal to  pe2  n   0  pe11 but PRON just can not figure out the math behind PRON   as say in PRON comment of the question  a good place for ask this kind of question would be on the math part of stackexchange   however  PRON can find   pe1  n1e11   pe2  ne11e11   pe2  n0e11   pe2  n0   the last equality be the result of the independance of the  ei 
__label__machine-learning ok  maybe this be a really dumb question  but have anyone ever consider extend some of the statistical learning method to live in some space other that  mathbb rn  PRON guess be that PRON be just shuffle parameter around  but PRON wonder if there be any intuition to be gain   this question come up as PRON be work through some course note on neural net  and realize that computer scientist may be even more cavalier with tensor index than physicist   essentially  each non  linear layer in a neural network be a map from  mathbbrn input to a  mathbbrm output  there be no requirement for these dimension to be separate and uncorrelated  nor for the map to be reversible   the good performance be usually find if each layer be roughly scale to approximate a normal distribution with mean 0 and standard deviation 1  however  that be an independent issue to any kind of space  like mapping   if PRON know a suitable mapping that relate to PRON problem and would simplify PRON expression  then PRON can apply a spacial transform between co  ordinate system  typically by map the input feature  and use PRON within the neural network  PRON could map co  ordinate to any curved space if PRON think PRON may help  choose between polar or cartesian co  ordinate for feature be one choice that may crop up for example   however  even without make this helpful mapping  PRON can analyse a neural network layer  by  layer  and see that PRON can effectively learn these kind of spacial mapping from the training datum anyway  this be an intuition that have be call the  manifold hypothesis   the link have some image show how a neural network may learn to separate class depend on how PRON be arrange in feature space  and this article from a darpa researcher have a nice series of image show successive transformation of a 2d spiral that allow the last layer to perform linear separation  
__label__optimization __label__matlab __label__convex-optimization __label__constrained-optimization __label__cvx PRON have the underdetermined outer optimization problem    textminxgeq 0quad ax  b12  2atxb22  2  with  ainmathbbrmtimes n and  mltltn64  2  or in correspond cvx matlab code  variable xn  nonnegative  minimizesum   ax  b12   atx  p1p2p3p4p5b22    where p1  p5 be fix parameter require by the function t inside t there be another linear minimization problem    beginalignedtextminc  q  quad  amp1top qamptextstquadampdc  x leq qampdc  x leq qendaligned  with  dinmathbbrntime n or in matlab syntax  function value  tx  p1p2p3p4p5    something happen   cvxbegin  variable cn   variable qn   minimize sumq   subject to  dc  x  lt q  dc  x  gt q  cvxend   something else happen and calculate return value   unfortunately PRON get the error  undefined function  newcnstr  for input argument of type  cvx    error in cvx  lt  line 22   b  newcnstr  evalin   caller    cvxproblem         x  y    lt      after the cvxend line in the function t  PRON encounter this error several time before when deal with other problem  but this time PRON can not replace the outer optimization with a build in matlab function  since fmincon be far to slow  replace the inner problem by linprog be too slow as well  unfortunately   be PRON possible to nest cvx optimization problem like that  if yes  how   be there any other idea what PRON could try    be PRON possible to nest cvx optimization problem like that  if yes  how   PRON do not think PRON can  no  cvx be design to model convex program  as such  PRON can not model nonconvex program  but could model PRON convex relaxation   in general  the bilevel linear programming case can yield a nonconvex feasible set  see this presentation by chris fricke  slide 40  44   which would make PRON impossible for cvx to solve these correctly  so this construction be probably not allowable for cvx   be there any other idea what PRON could try   in order to try other idea  PRON would help to know if  t be a smooth function   sorry if PRON comment be way too late but PRON just join this community   PRON may consider bender s decomposition or dantzig  wolfe decomposition  PRON have do similar thing   min  cx  e  qx x0   where qx min  qywy  txd  y0   PRON be able to solve PRON efficiently use bender s decomposition   good luck  
__label__optimization incomplete low  rank matrix can be exactly recover in most case  so long as the rank be low enough relative to the number of know entry  this result be famously prove by candès and recht in the paper  exact matrix completion via convex optimization   algorithmically  the nuclear norm minimization that recover the full matrix require keep the entire data matrix in memory  and only after convergence can the matrix be reduce to PRON low  rank part   suppose PRON know that PRON matrix  xinmathbbrntime n be rank one   x  uvt   u   vinmathbbrn when  n be large  keep  x in memory may be intractable  even if PRON be very tractable to keep  u and  v a nice identity about the nuclear norm be that  x    minu  vstuvt  x  frac12u22  frac12v22   so PRON be possible to recast nuclear norm minimization as a minimization over  u and  v constrain such that  uvt agree with know entry of  x PRON question be this  can PRON carry out this minimization tractably  will PRON still converge to the exact solution  how do PRON perform this minimization   PRON realize that specifically in the rank one case  this can be turn into a convex program by a change of variable  let  i1ldot  ik and  j1ldot  jk be the row and column index of the observed entry in  x  PRON problem be to minimize  frac12u2  2  frac12v2  2  under the constraint  uikvjk   xik jkforall k  1ldot  k then let  pi  mathrmlogui and let  qj  mathrmlogvj  the transform problem become    minp  q  frac12sumi mathrmexppioverlinemathrmexppi    frac12sumi mathrmexpqjoverlinemathrmexpqj  with linear equality constraint  pik   qjk   mathrmlogxik jk  which be a convex problem over complex number  or over real number if  x be nonnegative   of course this do not extend to anything above rank one  as the constraint on  u and  v go from be product to sum of product  however  PRON do extend to arbitrary rank one tensor   for anyone interested in go beyond the rank one case  here be some resource  courtesy of rahul mazumder    spectral regularization algorithms for learn large incomplete  matrices  describe an algorithm for matrix completion that allow PRON to work with the reduced singular value decomposition of  x instead of the entirety of  x  there be also some result suggest that  when work with  u and  v rather than  x  even if the convergence be to a local minimum in  u and  v  this correspond to the global minimum in  x  for instance  local minima and convergence in low  rank semidefinite programming  
__label__neural-network __label__deep-learning __label__reinforcement-learning __label__q-learning __label__dqn PRON be try to gain an intuitive understanding of deep reinforcement learning  in deep q  network  dqn  PRON store all action  environment  reward in a memory array and at the end of the episode   replay  PRON through PRON neural network  this make sense because PRON be try to build out PRON reward matrix and see if PRON episode end in reward  scale that back through PRON matrix   PRON would think the sequence of action that lead to the reward state be what be important to capture  this sequence of action  and not the action independently  be what lead PRON to PRON reward state   in the atari  dqn paper by mnih and many tutorial since PRON see the practice of random sampling from the memory array and training  so if PRON have a memory of     actiona  state1  rightarrow  actionb  state2  rightarrow  actionc  state3  rightarrow  actiond  state4  rightarrow reward  PRON may train a mini  batch of     action c state 3    action b  state 2   reward    the reason give be   second  learn directly from consecutive sample be inefficient  due to the strong correlation between the sample  randomize the sample break these correlation and therefore reduce the variance of the update   or from this pytorch tutorial   by sample from PRON randomly  the transition that build up a batch be decorrelat  PRON have be show that this greatly stabilize and improve the dqn training procedure   PRON intuition would tell PRON the sequence be what be most important in reinforcement learning  most episode have a delay reward so most action  state do not have a reward  and be not  reinforce    the only way to bring a portion of the reward to these previous state be to retroactively break the reward out across the sequence  through the futurereward in the q algorithm of reward  reward  learningratefuturereward    a random sampling of the memory bank break PRON sequence  how do that help when PRON be try to back  fill a q  reward  matrix   perhaps this be more similar to a markov model where every state should be consider independent  where be the error in PRON intuition   the de  correlation effect be more important than follow sequence of trajectory in this case   single step q  learning do not rely on trajectory to learn  PRON be slightly less efficient to do this in td learning  a qlambda  algorithm which average over multiple trajectory length would maybe work better if PRON be not for the instability of use function approximator   instead  the dqn  base learning bootstrap across single step  state  action  reward  next state   PRON do not need long trajectory  and in fact due to bias cause by correlation  the neural network may suffer for PRON if PRON try  even with experience replay  the bootstrapping  use one set of estimate to refine another  can be unstable  so other stabilise influence be beneficial too  such as use a frozen copy of the network to estimate the td target  r  textmaxa   qs   a  sometimes write  r  textmaxa   hatqs   a   thetabar      where  theta be the learnable parameter for  hatq function   PRON may still be possible to use long trajectory  sample randomly  to get a td target estimate base on more step  this can be beneficial for reduce bias from bootstrapping  at the expense of add variance due to sample from large space of possible trajectory  and  lose  part of trajectory or alter predict reward because of exploratory action   however  the single  step method present by dqn have show success  and PRON be not clear which problem would benefit from long trajectory  PRON may like to experiment with option though    PRON be not an open  and  shut case  and since the dqn paper  various other refinement have be publish  
__label__convolutional-neural-networks __label__gaming __label__game-ai this be a question transfer from stackoverflow   PRON be develop a game ai which try to master racing simulation  PRON already train a cnn  alexnet  on ingame footage of PRON play the game and the press key as the target  as the cnn be only make prediction on a frame  to  frame basis  and PRON resize the image input to 160x120 due to gpu memory limitation  PRON can not read the speedometer  thus seem not to have a feeling for PRON current velocity  PRON think of different way to fix this issue   crop the capture image down to the size of the speedometer  which display the current speed in mph  and feed the low resolution game image  as well as the relatively high  re image  70x30  of the current speed into the neural network  which make prediction base on the two image   as PRON do not know whether alexnet can serve as an ocr as well  PRON second thought be to use an exist one  like tesseract  oct  pytesser  on the crop image and feed PRON output to the fully connect layer   PRON already try to implement an optical  flow algorithm  but sadly  non of the python one seem to output good real  time result  PRON wonder whether PRON can input the current frame as well as the last one  and let alexnet figure out the movement   as the processing have to happen in real time  and the only performance review of pytesser PRON find report a processing time of 100ms  never test that   PRON question be  what method would work best   thank   edit  optical flow would have the advantage of the ai know in which direction other car be move as well   option 1 would be a very interesting one from a research perspective  PRON can not imagine that cnn have to capacity at the moment  to learn the concept of number and apply PRON in a useful way  if PRON be an analog speedometer  thing would be different  but PRON would be really interesting to try PRON and see what PRON can achieve with this approach  PRON have not read any research paper yet  where such a challenge would have be master   if PRON be less concerned with research but rather get this project to work  PRON would propose an approach similar to option 2  the ocr tool PRON mention be design to identify symbol  even if distorted or otherwise hard to read  in PRON case  the number will always look the same and be most likely always in fix position  therefore  use fancy ocr algorithm or neural net be overkill for the problem at hand  PRON can write a simple algorithm that crop the speedometer  search for the 10 possible pattern  0 to 9  with 10 specific kernel and calculate the speed from the result  this can be implement efficiently without the need to train a cnn or some other complex algorithm   method for implementation   to answer the first question in the comment  the algorithm for this be really easy  PRON image will be represent by a 2d array  or maybe a 3d array if PRON be work with color   all PRON need to do be find out  what PRON number look like and store the appropriate array  PRON should end up with 10 different array for 10 different number   to find out if a number be in PRON current picture  PRON only have to check if the array for this number be a subset of the array of PRON current picture  the position  where the array be match  will also indicate the position of the number  PRON do this for all 10 number and can calculate the actual value display on the speedometer afterwards from the result   this be very easy if the number always look exactly identical  in this case PRON can simply look for an exact match  if there be slight variation  PRON can look for part of the image array that be similar to PRON template array  just sum the absolute value of the diff of each pixel value and if PRON be below a certain threshold  PRON have PRON match  that be all there be to PRON   in python PRON like work with pil for this kind of image manipulation  but there be other framework out there as well  
__label__machine-learning __label__neural-network __label__q-learning PRON be currently program a q learn neural network tha do not work  PRON have previously ask a question about input and have sort that out  PRON current idea to why the program do not work be to do with the threshold value  this be a neural network  q learn specific variable   basically the theshold be a value that be between 0 and 1  PRON then make a random number between 0 and 1  if this random number be large than the threshold then PRON pick a completely random choice  otherwise the neural network choos by find the large q value   PRON question be that with this threshold value  i be currently implement PRON as start at almost 0  then increase linearly until PRON reach 1 by the time the program have reach the final  iteration  be this correct   the reason i suspect this be incorrect be that when plot an error graph from train the neural network  the program do not not learn at all  but when the threshold reach almost 1  PRON start to learn very fast  and if PRON run more iteration after PRON reach 1  the all the game set in the replay memory become the same and the error be basically 0 from PRON on in   any feedback be greatly appreciate and if this question in unclear in anyway just let PRON know and i will try and fix PRON  thank PRON to anyone who help out   PRON be effectively implement  epsilongreedy action selection   the usual way to represent this in rl  at least that PRON be familiar with  be not as a  threshold  for probability of choose the best estimate action  but as a small probability   epsilon  of not choose the best estimate action   for consistency with rl literature that PRON know  PRON will use the  epsilongreedy form  so instead of consider what happen as PRON threshold rise from 0 to 1  PRON will consider what happen when  epsilon drop from 1 to 0  PRON be the same thing  PRON hope PRON can either adjust to use  epsilon or mentally convert the rest of this answer so PRON be about PRON threshold     when monitor q  learning  PRON have to be careful how PRON measure success  monitor the behaviour on the learning game will give PRON slightly off feedback  the agent will make exploratory move  with probability  epsilon   and the result from a learning game may involve the agent lose even though PRON already have a policy good enough to not lose from the position where PRON start explore  if PRON want to measure how well the agent have learn the game  PRON have to stop the training stage and play some game with  epsilon set to  0 PRON suspect this could be one problem  that PRON be measure result from behaviour during training  note this would work with sarsa   in addition  choose value that be too high or low for PRON problem will reduce the speed of learning  high value interfere with q  learning because PRON have to reject some of datum from exploratory move  and the agent will rarely see a full game play use PRON preferred policy  low value stifle learn because the agent do not explore different option enough  just repeat the same game play when there may be good move that PRON have not try  for tic tac toe and q  learn PRON would suggest pick a value of  epsilon between  001  and  02   in fact  with q  learning there be no need to change the value of  epsilon PRON should be able to pick a value  say  01   and stick with PRON  the agent will still learn an optimal policy  because q  learning be an off  policy algorithm  
__label__svm __label__unbalanced-classes __label__cross-validation PRON have an extremely unbalanced datum set  around 200 positive sample and 70000 negative sample  to overcome this problem PRON have try to over  sample the minority class as suggest in previous question here   since train the classifier  in PRON case PRON be use svm  take a long time on PRON computer PRON first train a classifier on a small subset of PRON data pick randomly  the classifier give solid result when test on the test set  yet when PRON train a classifier use the full training datum the classifier produce very poor result  do anyone have insight into why this be happen and what should i do  PRON have also try use weight but PRON do not produce a different result   for the 200 positive sample  be there many different  type   in other word  will future positive sample look nothing like the current positive sample  if this be the case  PRON may try anomaly detection in machine learning   from a lecture by andrew ng in coursera  if PRON have small number of positive sample  eg  20   large number of negative sample  eg  10000   and PRON be hard for any algorithm to learn from the positive sample  PRON should use anomaly detection  otherwise  PRON can use supervised learning  
__label__topic-model __label__lda PRON have be try to extend the lda and want some help  direction and insight   can author  topic lda be use as a document  category  model  the premise of the author  topic model be that multiple author influence the topic from which every word be derive  can PRON instead treat  author  as category to which document belong   PRON have be try to slightly alter this   httpyaowucodocssigir12pdf  and this  httppapersnipsccpaper2994modelinggeneralandspecificaspectsofdocumentswithaprobabilistictopicmodelpdf  with author  topic model   what PRON be try to do be find out document  level word association in the post  datum of a forum on a specialized site  for example  this site be about candy  PRON member post about sweet  candy and toffee  obviously there be category of candy  then brand and then the product PRON  PRON reasoning be that if PRON use author  topic and treat the observe node  author  as observe  node  candy category   then PRON can introduce that candy category feature into recommender system down the pipeline   like the paper above suggest  a  word stream   switch by a multinomial  x  decide which stream a word come from  background  document or  candy category     do PRON think this make sense   furthermore  be there a book or resource that  very thoroughly and step  by  step explain the collapsed gibbs derivation for lda   
__label__python __label__text-mining __label__random-forest __label__sampling PRON have get a dataset of 15 million and be look to train 7 different classifier  for each classifier PRON have up to 10 class to predict  the total sample have 20 k text feature  more if PRON include bigram   like most distribution of text feature  only 20  of PRON account for 80  of occurrence in the sample  PRON be go to manually label 10 k for each prediction category  and use that to predict against the remain 15 million as well as new document that come through   PRON question be  how would PRON choose the subsample base on the feature and distribution  should PRON just choose a random sample  ie try to match the distribution   or should PRON try to find the 10 k that maximize the number of feature represent in the sample  what s the benefit and drawback of each   PRON have only one shot to label these 10 k so PRON want to make sure PRON choose the right sample that maximize PRON accuracy for each of the prediction category   ideally PRON will end up with a dataset where each class be fairly represent  with enough datum for each class to enable suitable predictive performance  as PRON currently have no label  any kind of stratified sampling be unavailable to PRON   random sampling will get PRON the 10 k sample PRON need  but there be no guarantee PRON will get fair representation for each of PRON class  assume PRON be to use this approach PRON would make sense to continue to label the sample until PRON have decent representation for all class  there be no guarantee regard balance and also no guarantee that PRON will cover the majority of the variance of PRON dataset in feature space either   an alternative sample approach that be able to capture a majority of the variance of the datum in feature space should hold up with good performance  generalisation  there be a few variation of how this could be do but PRON could try a clustering approach   cluster PRON point in feature space and then instead of sample randomly  sample from each cluster in turn  so draw a random point from the first cluster  then second  then third etc until the last cluster  and repeat if necessary  label PRON in this order and keep an eye on the count for each class as there be always the chance a class be not well represent by this approach  another potential drawback in this approach be select the number of cluster  PRON could aim for the number of class in the dataset or go with anything up to 10 k cluster   an additional variant on this would be the strategy for select a sample from the cluster  instead of choose a random point  choose the sample close to the centroid  or if choose multiple point from a cluster  order the sample via distance from centroid and sample uniformly from this   active learning be a semi  supveris approach that may be the most useful choice in this instance  when label  once PRON have get some decent coverage for each of PRON class try an active learning approach to select which sample to label next  
__label__numerical-analysis __label__parallel-computing __label__software PRON be accept to computational science master program  the master program just start to accept student in PRON university and today each group of 3 student be assign to an advisor teacher  PRON be only 13 student   PRON advisor ask PRON to research for valuable topic in computational science  so PRON think why do not PRON ask to experienced scientist in this forum   PRON group be combine of 1 electronic engineer  PRON   1 mathematician and 1 computer engineer  so  PRON strength be math and software programming   
__label__linear-algebra __label__krylov-method __label__preconditioning __label__condition-number question   suppose that PRON have two different  factored  preconditioner for a symmetric positive definite matrix  a     a approx btb  and    a approx ctc  where the inverse of the factor  b  bt  c  ct be easy to apply   when be PRON possible to use information from both  b and  c to build a good preconditioner than either  b or  c alone   PRON can use additive    pa1  x   bt b1  x   ct c1  x     multiplicative    pm1  x   bt b1  x   ct c1  bigx  a  bt b1  x big      or symmetric multiplicative   methods of this class be available in petsc use pccomposite in petsc   for example   petsc  src  ksp  ksp  example  tutorials ex2 m 100 n 100 kspmonitor   pctype composite pccompositetype multiplicative   pccompositepcs ilu  gamg  0 ksp residual norm 7088415699389e01  1 ksp residual norm 1271768323411e01  2 ksp residual norm 1529853612054e00  3 ksp residual norm 1214841683459e01  4 ksp residual norm 8341606406485e03  5 ksp residual norm 6471990946051e04  6 ksp residual norm 8082672366030e05  7 ksp residual norm 6111138513482e06  norm of error 693786e06 iteration 7  the user manual have a section on  combining preconditioners    in addition to jed s excellent answer  a method PRON have find recently be to switch between the preconditioner every other step in flexible gmres  fgmres   as be do  for example  in  tezduyar  t e  et al   a new mixed preconditioning method for finite element computation   computer methods in applied mechanics and engineering 991  1992   27  42  httprepositoryiasacin246801320pdf 
__label__parallel-computing __label__fortran in fortran  give array a1000000  and b1000000   and to compute the sum PRON simply write   c  a  b  however  when PRON want to implement openmp  PRON have to write an explicit loop     omp parallel do  do PRON  1  1000000  ci   ai   bi   end do    omp end parallel do  be there a good way to do this  PRON know there be parallel compiler flag but PRON give less performance   for the record  PRON may try put and openmp workshar region around PRON array operation syntax     omp workshare  c  ab  do not forget to build with openmp enable  openmp for the intel compiler  and to set ompnumthreads   short answer  no  there be not a good way to do this   long answer  workshare be much hard to implement than explicit loop parallelism   compilers have be know to do correct but useless  aka  no  op   implementation of workshare that map PRON to single   this be correct but obviously provide no performance benefit   when compiler support workshare  PRON may do so selectively  eg intel fortran 15  which mean that depend on PRON may lead to mixed result   even when compiler do a good job  in parallel computing in general  PRON will find that explicit parallelization win  because the user be require to make good design choice up front and compiler  which be much less intelligent than human  do not have to make important decision  
__label__research what be the top artificial intelligence journal   PRON be look for general artificial intelligence research  not necessarily machine learning   this link include various journal for artificial intelligence apply to various domain   some of those be   1  ieee transactions on human  machine systems  2  journal of the acm  3  knowledge  base system  4  ieee transactions on pattern analysis and machine intelligence  5  journal of memory and language   there be lot more  PRON can refer to any of those journal and explore the research do by ai enthusiast and researcher   PRON most often reference   httpdblpunitrierde  PRON be not a journal but PRON get PRON where PRON need to go   a couple of other   journal of artificial intelligence research  jair   httpjairorg  ieee transactions on knowledge and data engineering  ieee computational intelligence magazine  the journal  artificial intelligence  ai    httpswwwjournalselseviercomartificialintelligence  be not list  yet  although be consider the top  level journal on ai  although this be a journal for ai  just be name  artificial intelligence    PRON be not to be confuse with another top  level ai journal  call  journal on artificial intelligence research  jair    httpwwwjairorg   which be already list in one of the other answer   further  there be a german journal on ai  call  ki  künstliche intelligenz   german for ai   but almost always the article be in english as well  httpwwwkuenstlicheintelligenzdeenkijournal   while be internationally recognize  PRON be not regard a top  level journal  a nice feature of that journal be that every special issue have an editorial  a special  article  at the beginning of each journal   in which there be a section call  service   this service section list publication medium  like journal  and conference etc  that be relate to the give special issue  so  in case PRON be interested in journal of a special field of ai  like human  computer interaction   just search for a special issue that be relate to that topic and read the editorial s service part  
__label__neural-network have PRON ever see a neural network without matrix   PRON be ask  because PRON be currently build one for educational purpose   matrix multiplication be just a simplify notation for a particular set of addition and multiplication operation  PRON can absolutely represent a neural network without invoke matrix notation  PRON will just be really tedious  and PRON will run slower    start with a single perceptron and build PRON way up from there  
__label__recommender-system __label__similarity __label__matrix-factorisation when PRON be build a recommendation system and have a very sparse matrix  PRON use model base collaborative filtering   be there any specific reason as to why PRON do not use  a classification model base recommendation system  b item similarity base recommendation system  c user similarity base recommendation system  PRON know PRON be difficult to calculate value in a sparse matrix  but be PRON any more specific reason   
__label__deep-learning __label__training PRON be a newbie in neural network   see this article object detection with deep learning and opencv   these three type of neural network be shortlist in the article  faster r  cnns  PRON only look once  yolo   single shot detectors  ssds   find a lot of online resource that help in understand how the neural network actual work   as build a neural network from scratch be time consume and not entirely foolproof to get the desire efficiency   PRON come across these article transfer learn  amp  the art of use pre  train models in deep learning  transfer learning and transfer learning  where PRON transfer the learning from an exist pre train network to train the object which PRON would desire the network to detect   all of these pre train network have be train on a data from either coco or imagenet or pascal voc which contain different category image   example case   PRON want to train one of these to count the number of banana here in this image  how should PRON training set of banana image be   PRON need a fix resolution of the image PRON feed into the network  so can PRON even feed a half banana like this for the training  PRON do not have to remove the background for training  please correct PRON if PRON be wrong   PRON should read this post from the same author  PRON be about how to build a learning dataset with google s search  some javascript and some python   the idea there be to collect as many picture of the object as possible  object  banana  and then let the deep learning network to compare and learn to differentiate PRON from bulk random image from some free database   PRON suppose background or partial image variation be all ok  PRON can not know which part of the object be visible when the whole bunch be evaluate   data need to be fit to same size  at least adobe have some tool for that   hope this help  after do a bit of research into how the pascal visual object class challenge 2007 image datum be store  this be what PRON could find   PRON follow an annotation of each image which create an xml file of that particular image which contain these detail pascal visual object class challenge 2007  voc2007  annotation guidelines  here be an example of xml file from voc2007 of one of these image  image  xml   ltannotationgt    ltfoldergtvoc2007ltfoldergt    ltfilenamegt000001jpgltfilenamegt    ltsourcegt    ltdatabasegtthe voc2007 databaseltdatabasegt    ltannotationgtpascal voc2007ltannotationgt    ltimagegtflickrltimagegt    ltflickridgt341012865ltflickridgt    ltsourcegt    ltownergt    ltflickridgtfri camelsltflickridgt    ltnamegtjinky the fruit batltnamegt    ltownergt    ltsizegt    ltwidthgt353ltwidthgt    ltheightgt500ltheightgt    ltdepthgt3ltdepthgt    ltsizegt    ltsegmentedgt0ltsegmentedgt    ltobjectgt    ltnamegtdogltnamegt    ltposegtleftltposegt    lttruncatedgt1lttruncatedgt    ltdifficultgt0ltdifficultgt    ltbndboxgt    ltxmingt48ltxmingt    ltymingt240ltymingt    ltxmaxgt195ltxmaxgt    ltymaxgt371ltymaxgt    ltbndboxgt    ltobjectgt    ltobjectgt    ltnamegtpersonltnamegt    ltposegtleftltposegt    lttruncatedgt1lttruncatedgt    ltdifficultgt0ltdifficultgt    ltbndboxgt    ltxmingt8ltxmingt    ltymingt12ltymingt    ltxmaxgt352ltxmaxgt    ltymaxgt498ltymaxgt    ltbndboxgt    ltobjectgt   here be a link for annotation tool labelimg that can help PRON generate the similar xml file for an image in PRON own dataset  for more detail on how the image datum be divide into train  trainval  test and val check this the pascal visual object class challenge 2007  voc2007  development kit 
__label__linear-algebra __label__matlab __label__finite-difference __label__condition-number how do PRON approximate the condition number of a large matrix  g  if  g be a combination of fourier transform  f  non  uniform or uniform   finite difference  r  and diagonal matrix  s   the matrix be very large and not store in memory and be only available as function   in particular  PRON have the follow matrix    gmu  shfhfsmu rhr  PRON want to investigate the relation between  mu and the condition number  kgmu  PRON assume one need some kind of iterative approach   optimally there would be some matlab code availabe   matlab have a couple of  exact  function for this  cond and rcond  with the latter return a reciprocal of the condition number   matlab approximate function condest be more fully describe below   often estimate of the condition number be generate as by  product of the solution of a linear system for the matrix  so PRON may be able to piggyback the condition number estimate on other work PRON need to do anyway   see here for a brief description of how estimate be compute   also sandia labs aztecoo documentation remark  see sec  31  that optional condition number estimate be available from iterative solver  use the generate tridiagonal lanczos matrix with conjugate gradients or the generate hessenburg matrix with restarted gmres    since PRON matrix be  very large  and  only available as function   the logical approach would be a method that piggyback on a conjugate gradient solver or variant   a recent arxivorg paper non  stationary extremal eigenvalue approximation in iterative  solution of linear system and estimator for relative error propose such an approach and have a few citation to the early literature   now that PRON look  this forum have a number of closely related previous questions  not all with answer  but check comment    estimate extreme eigenvalue with cg  estimation of condition number for very large matrix  fast algorithm to compute the condition number of a large matrix in matlab  octave  since availability of matlab code be part of the question  here be some information about condest  a build  in function that estimate the 1norm condition number  a1 a11  the idea be from hager1984   with a 2010 write  up and extension here  to explicitly compute  a1   find maximum 1norm of a column  and estimate  a11  by a gradient method   see also john burkardt s condition  a matlab library  other language available   for compute or estimate the condition number of a matrix    since PRON matrix be apparently hermitian and positive definite  perhaps the 2norm condition number be of great interest   the problem then amount to estimate the ratio of large to small  absolute  eigenvalue   the challenge be somewhat parallel to the 1norm case in that generally a good estimate for the large eigenvalue can be easily obtain  but estimate the small eigenvalue prove more difficult   although aim at non  spd  and even non  square  case  this recent arxivorg paper  reliable iterative condition  number estimation  give a good overview of the small eigenvalue estimation problem and a promising line of attack by a krylov  subspace method  lsqr  that amount to conjugate gradients in the spd case  
__label__machine-learning __label__python __label__keras detail   gpu  gtx 1080  training  11 million image belong to 10 class  validation  150 thousand image belong to 10 class  time per epoch  10 hour  PRON have setup cuda  cudnn and tensorflow  tensorflow gpu as well    PRON do not think PRON model be that complicated that be take 10 hour per epoch  PRON even check if PRON gpu be the problem but PRON be not   be the training time due to the fully connect layer   PRON model   model  sequential    modeladd    modeladdconv2d64   3  3   paddingsame   strides2    modeladdactivationrelu     modeladddropout025    modeladdconv2d64   3  3   paddingsame   strides2    modeladdactivationrelu     modeladddropout025    modeladdconv2d32   3  3     modeladdactivationrelu     modeladdmaxpooling2dpoolsize3  3   strides2    modeladdflatten     modeladddense256    modeladdactivationrelu     modeladddense4096    modeladdactivationrelu     modeladddense10    modeladdactivationsoftmax     modelsummary    opt  kerasoptimizersrmsproplr00001  decay1e6   modelcompilelosscategoricalcrossentropy    optimizer  opt   metricsaccuracy      because there be a lot of datum PRON use the imagedatagenerator   gen  imagedatagenerator   horizontalflip  true    traingen  genflowfromdirectory    train    targetsize512  512    batchsize5   classmodecategorical     validgen  genflowfromdirectory    validation    targetsize512  512    batchsize5   classmodecategorical     that be about expect  if PRON divide the number of second by the number of image PRON process  PRON get 33 millisecond per image  which seem about right for such a small network  large network usually take in the ballpark of 50 to 200 millisecond per image   yes  a large dense layer be likely to hurt PRON performance  since that be a huge matrix  256 by 4096  and a large matrix multiplication to go along with PRON every time PRON run the network   as shimao say  that be about what PRON would expect  despite not have many layer  an input size of 512x512 be a large image to be convolve over  the large computation time be likely more due to convolve 64 filter over the large image  instead of the fully connected layer   the network PRON have put together have a funny information bottleneck in PRON though  PRON start off with 64 filter on the original sized image  only decrease as PRON image size reduce  as the image pass through PRON network  the feature PRON be learn become more and more abstracted and complex  PRON conv2d32   3  3   layer essentially limit the network to learn a 128x128 map of 32 feature   most network architecture double the number of feature every time PRON pool  and most recent imagenet architecture actually ditch the fully connected layer in favour of an average pool over the final feature map  and basically perform logistic regression on the output of that pool   try start off with few filter  say 16 in PRON first convolution layer  double each time PRON stride or pool  do this a few more time than PRON be  to increase receptive field and decrease feature map size  do this down to 64x64 or 32x32  which would be 128 or 256 filter  PRON can use keras  global avg or max pool to eliminate fully connect layer as well  that should about double the speed of the network  and PRON would expect an increase in accuracy at the same time  
__label__predictive-modeling PRON be not sure how to formulate this problem clearly into a machine learn task yet  so hope PRON guy can chime in and give PRON some help   problem  to predict whether someone will pick up PRON phone during office hour in week n2 by look at customer s behaviour in week  n  datum   PRON have call record for about 3 month  which be aggregate on customer level  the various attribute include  num of call  duration of call  time of call  amount of datum traffic  but of course  these main attribute be further split into at about 20 attribute   current approach  very manual   PRON look at datum at week n2 and get the group of guy who pick up the phone during office hour  duration of call  5s and time of call   this be the target group  t  PRON look at datum at week n and manually try all possible combination of the attribute to get as close to t as possible  but try manually seem tiring after some time  the baseline be of course use the same condition as at week n2  but the whole idea will be to increase this number   question  be there any way PRON can transform this dataset so that PRON would be able to do accomplish PRON as a machine learn task   PRON can try to build some kind of  slide window table   let PRON say PRON have follow attribute   call duration  x1   time of call  x2   pick the phone  x3   let PRON further assume that PRON have datum from past 3 week  which allow PRON to set following table  the row contain individual call  the column the attribute  the appendix  1 tell PRON the time  so for example x11 be call duration previous week  x22 be time of call two week before etc   client  x11  x21  x31  x12  x22  x32    x33  PRON can train PRON model use historical datum  where x33 be last week  then  PRON will feed the model with current datum   3 be current week   and try to predict x33  whether the customer will pick the phone    PRON be assume that PRON know who PRON be go to call  hence PRON have  3 attribute  but PRON don  know yet whether PRON respond or not  the aim be to give the model the opportunity to learn the time dependency  maybe time of call week before together with call duration strongly correlate with future chance of pick up the phone again   what can also help be perform feature selection  the assumption be that some attribute be strongly correlate with other  whereas other be not  PRON can simply use x11  x21 and see the relationship to x31  but PRON would suggest recalculate these often as the preference may change in time  
__label__machine-learning __label__autoencoder PRON want to develop a new autoencoder  once the network PRON be develop  the customer want to train and deploy the PRON all by PRON  due to datum protection PRON can not give PRON anymore information  as advise PRON about the usage in transactional erp datum and the number of input  output  which will be 10   usually PRON would start a project by analyse the datum and check the distribution by run histogram  the initial check on the datum usually give PRON insight about skewness  max  value  min  value  average and median   particularly the skewness worry PRON  as in PRON experience neural  network such as auto  encoder perform extremely bad on such input  usually PRON would offset a strongly skewed datum distribution by use logarithm  root function or power transformation   PRON wonder if PRON can just PRON any of these  anti  skewness  method just blindly  in order to offset  bad  datum  but not worry about destroy the performance of PRON network   
__label__machine-learning __label__python __label__classification PRON would like to perform a simple binary classification  train only with a known set of like and unlike pair   ie with some training sample  aiin s  PRON have pair which give know value of the pairwise function   pai  aj   s2to  01     rm unlike  pair  like  pair  but PRON do not know any individual classification  only by training on these pair PRON produce PRON binary classifier   cai   sto01  can any of the vanilla python ml package do this   
__label__neural-networks __label__implementation __label__computer-programming PRON be wonder  instead of implement new web browser over and over again with million line of code which be very difficult to manage  would PRON be possible to use ann or ga algorithm to teach PRON about the render process  how the page should look like    so as an input PRON would image the html source code  output be the render page  maybe in some interactive image like svg  some library or something  PRON be not sure    the training datum can be dataset of website provide input source code and PRON render representation by use other browser for the guidance as expect output   which approach would PRON take and what be the most challenging thing PRON can think of   the render process for browser be very well define  and have a very rigid definite ruleset where  virtually  every accountability be note and handle  this be not optimal for machine learning  which work when PRON have a large pool of example  and PRON do not know the ruleset  PRON will figure PRON out  even if PRON be to train an neural network to process that input  there be several thing PRON must account for   1  variance in datum   not all webpage be equal in length or complexity  and make a neural network to generate output from html would produce garbage most of the time   2  training time   the time PRON would take for a neural network to understand html tag  attribute  the dom tree  and each and every element  include new one be add every few year  and how each one render and behaf  would take an extremely long time  most likely several year on a fast computer  if PRON even be possible  3  interactivity   web page be not just static  PRON change accord html  css and javascript  not only would PRON have to design PRON system to account for the render step  PRON would also make PRON have to understand the turing complete scripting language javascript  as well as the less complicated  but inherently intertwine with html  css stylesheet language  if PRON think the render process be easy  try train a neural network to handle complicated scripting pattern   4  new standards  not all html be equal  because of different standard  whatwg begin work on html5 in 2004  and browser start to implement not long after  in 2004  there be very few example of html5 site to train PRON network to begin with  sure  now PRON be standardized and every website use PRON  but what about html6  when the first specification be release  probably 2017  2025   virtually no website will use PRON  because no one will support PRON  only when PRON finally become standard  probably in the late 2020 or early 2030s  will PRON have enough datum to train PRON monstrous system of neural network  as for ai in general  one could argue that browser already use ai in PRON rendering process  PRON intelligently decide what to render  take css into account   when in order to get the most efficient render time  PRON selectively use different javascript parser on different section of the code to optimize the speed  the whole system have be optimize on another ruleset to make render and interact with a webpage as seamless and easy  to  use as possible  PRON system will never be as good as what hundred of human have optimize over 20 year   try to solve html render with neural networks be akin to try to nail a nail with a screwdriver  PRON be just not go to work  hope this be helpful  
__label__neural-network __label__deep-learning __label__convnet __label__reinforcement-learning __label__computer-vision after read the state of the art of object detection use the cnnsr  cnn  faster r  cnn  yolo  yolov2ssd  PRON be wonder if there be an efficient method that use deep learning with reinforcement learning for object detection   to PRON extent of PRON knowledge  rl be use as a model for attention mechanism in object detection field  particularly the reinforce algorithm which be a  flavor  of policy gradient method   PRON can take a look at one of the paper that first propose the method  ram 
__label__machine-learning __label__r __label__neural-network __label__predictive-modeling __label__performance PRON construct a neural network in r use neuralnet package   PRON want to test that use cross  validation  that be a technique base on use 45 of the dataset to train the network and the fifth one as the test set   PRON wonder about what measure PRON should use to measure the neural network performance in term of predictability   could PRON suggest what measure be commonly use in the field and explain PRON why   any hint and idea about that will be appreciate   typical predictive performance measure use to compare accuracy of ann model be   rmse   root mean squared error  measure the distance between estimated and actual outcome  other metric that measure the same concept be mse  mae or mpe   r square   r2 or coefficient of determination  measure the reduction of variance when use the model   when compare two different ann model for performance  metric that take into account the complexity of the model may be use  such as aic or bic  
__label__finite-element __label__time-integration when solve semi  discrete equation  originate from finite element model  for example   which be second  order in time of the form  beginequation   mddot d  cdot d  kd  f   endequation   where  d be the solution vector   m   c and  k be matrix  and  f be a vector  one can make use of method that damp out spurious high  frequency oscillation such as damped version of the newmark method  hhtalpha  etc   if one wish to solve instead a system of the form  beginequation   mdot d  kd  f   endequation   the obvious choice would be to use a generalize trapezoidal method  however  PRON be look for a method that exhibit damp out of spurious high  frequency oscillation  as in the second  order case  PRON do not need a highly accurate method  but PRON would preferably be an explicit one   there be an excellent family of time integration scheme that fit PRON description call generalized single step single solve  gs4    the original work on the implicit method for first order system can be find in  1    here be the implicit algorithm   beginequation   c widetildedotd     k widetilded     widetildef   endequation   where the variable have be approximate between the  nth and   n1th timestep as  beginalign    amp  widetildedotd    dotdn  lambda6 w1   dotd     n1  dotd    n      amp  widetilded     dn   lambda4 w1delta t dotdn  lambda5 w2delta t   dotd     n1  dotd    n      amp  widetildef     1w1qnw1 fn1   endalign   now one can solve for  deltadotd     dotd     n1  dotd    n use  beginalign    lambda6 w1 c  amp lambda5 w2 delta t k   delta dotd       amp  c  dotd    n  k   d    n   lambda4 w1 delta t  dotd    n      amp  1w1fnw1 fn1   endalign   with the update  beginequation   dotd     n1    dotd    n   delta dotd   endequation   beginequation   d    n1    d    n   lambda4 delta t  dotd     n   lambda5 delta t  delta dotd   endequation   where  beginalign    amplambda4 w1  frac11rhoinfty   nonumber    amplambda5 w2  frac11rhoinfty1rhoinftysnonumber    amplambda6 w1  frac3rhoinftyrhoinftys   rhoinftyrhoinftys21rhoinfty1rhoinftys        ampw1  frac11rhoinfty   nonumber    amplambda4  1  quad nonumber  lambda5  frac11rhoinftys   nonumber  endalign   admittedly  this may seem like a whole load of parameter and nonsense  but all parameter only depend upon two choose value   rhoinfty   rhoinftys  the beauty be once PRON have PRON program PRON can choose from a whole family of algorithm just by choose PRON value for   rhoinfty   rhoinftys  these parameter correspond to the eigenvalue of amplification matrix of the single dof problem   thus  PRON can choose the amount of damp  numerical dissipation  simply by choose PRON   some noteable choice    rhoinfty   rhoinftys     11 give the crank  nicolson method  no damping  not for PRON  and   rhoinfty   rhoinftys     00 give an algorithm equivalent to the highly dissipative gear s method  aka two  step backwards difference formula    any choice will give PRON a second  order  unconditionally stable algorithm   note the restriction on PRON choice   0 leq rhoinftys  leq rhoinfty  leq 1  now if PRON want an explicit algorithm  some algorithm have be develop use the same approach that lead to the mess above   PRON do not think PRON have be publish any place highly visible yet but early work can be find in a master s thesis here  2    the easy thing to do to get an explicit scheme  with the nice dissipation property of the above algorithm  be to turn PRON into a predictor  corrector method   PRON  of course  lose the unconditional stability  but PRON will still have a second  order time integrator   to do so PRON can replace the  widetilded above with   beginalign   widetilded     dn   lambda4 w1delta t dotdn  endalign   and lump the  c matrix  then march to PRON heart content   everything else stay the same but the restriction to the  rho parameter above be lift  PRON can be anything    the stability of the algorithm and the amount of dissipation still depend upon this choice   hover around 0 and PRON should be okay    1  httponlinelibrarywileycomdoi101002nme3228full   2  httpconservancyumneduhandle11299162393  these differential equation appear to be linear  so PRON should be able to solve PRON analytically  in order to do this PRON first have to write PRON as one matrix differential equation of the form      dotvecx    avecx   b vecu     this can be do by rewrite the differential equation to an expression for  ddotd      ddotd   m1  f  m1  c dotd   m1  k d     the vector  vecx can be define as  beginbmatrixd  amp  dotdendbmatrixt  such that  dotvecx be equal to  beginbmatrixdotd   amp  ddotdendbmatrixt by use this definition then the matrix  a can be write as      a   beginbmatrix   0  amp  PRON   m1k  amp  m1c  endbmatrix       where  0  and  i be the zero  null and identity matrix receptively  both of the same size as  m   c and  k and similar the matrix  b can be write as      b   beginbmatrix   0   m1   endbmatrix       where  0  be again the zero  null matrix of the same size as  m   c and  k and the vector  vecu be equal to  f  the homogeneous or transient solution   f0   can be find with      vecxtt   sumcivecvielambdait        where  lambdai be eigenvalue of  a  with  vecvi the correspond eigenvector and  ci be constant which can be find with the help of the initial condition   the nonhomogeneous or steady state solution can be find by take the laplace transform      mathcalldotvecxsst   a mathcallvecxsst   b mathcallvecuts          s xss   a xss   b us          xss    s PRON  a1  b us          vecxsst   mathcall1s PRON  a1  b mathcallvecutst       if the  vecu be constant in time  then this simplifie down to      vecxsst   a1  b vecu     the steady state solution can also be find by take the laplace transform of the original differential equation  the only difference between this and use  a be that the later also give the derivative of  dss because PRON solve for  vecxsst the total response can be find by add the two solution      vecxt   vecxsst   vecxtt       where the constant  ci should only be solve for by use the initial or boundary condition when  vecxsst have also be add  
__label__optimization __label__fourier-analysis __label__matrix __label__cvx when PRON use the cvx matlab toolbox  PRON meet a puzzled problem  the function of fft  or dct  wavelet  etc   can not be recognize by the type of  cvx   for the 1d fft  PRON can be construct to an equivalent matrix  but for the 2d fft  how to transform PRON to the matrix form   yes  PRON be true  the cvx package  disclosure  PRON be the author  do not support the use of the fft   command in constraint and objective expression  however  PRON be relatively simple to get the equivalent result  in both the 1d and 2d case   first  1d give a vector  xinmathbbrn  the dft of  x be equal to  wx  where  w be the so  call dft matrix  here be a few way to construct the dft matrix in matlab   w  ffteyen    w  exp1j2pi0n10n1n   w  expbsxfuntimes1j2pi0n10n18    now try this with random vector to verify correctness   x  randnn11jrandnn1   fftx   wx  what about 2d fft  give a square matrix  xinmathbbrntime n  the 2d dft can be obtain by apply the 1d dft to each column of the matrix  and then apply the 1d dft to each row of the matrix  express in term of the dft matrix  w  the 2d dft be simply  wxwt go ahead  try the follow matlab expression and see that PRON be equivalent   x  randnn  n1jrandnn  n   fft2x   fftfftx21   fftfftx        wxw   note the use of the non  conjugate transpose w  above  that be important  if PRON use the standard hermitian transpose instead    PRON will get a scramble result   in summary  give a square matrix variable x  PRON can perform a 2d fft on a square cvx variable as follow   w  ffteyesizex     wxw   again  note that PRON be use a non  conjugate transpose above   please note that 2d dft get big fast  that is  PRON have n2 variable here if the dimension of PRON matrix be  n n   so even a 512 by 512 image  not that large by matrix standard  be still over 250k variable  and that can be a lot by cvx standard  once PRON be satisfied with PRON model s performance on small problem  PRON may need to develop PRON own numerical engine to solve large case   give that PRON be relatively simple to work around cvx s limitation here  PRON be reasonable to ask  why not just add support for fft   in cvx  use one of technique describe here  well  PRON may do that  however  PRON be important to remember that fft stand for fast fourier transform  PRON be a fast algorithm for generate the discrete fourier transform  dft   specifically  PRON can compute the dft in only  onlog n operation   the matrix workaround  on the other hand  construct a full dft matrix at a cost of  on2 operation  so much for  fast   PRON do not want to deceive user with a false sense that cvx be really use the fast fourier transform  the fact be  PRON can not  the underlie solver do not exploit the  nlog n efficiency of the fft operator   the cvx example library contain a number of problem that utilize a manually generate fourier transform  in particular  look at the filter design example   say a be a m by n matrix  then fft2a   dftmtxm   a  dftmtxn   also  PRON have   textvecabccdagger  otimes atextvecb  so PRON can use kronecker product to construct the 2d dft matrix  see here for detail  
__label__linear-regression __label__gradient-descent __label__online-learning PRON have a 2 m instance dataset with million of very very sparse dummy variable create use the hashing trick  hashorigfeaturename  origfeaturevalue1  note that the data be sparse both on row  every instance have only a limited  lt100 features1  and on column  most feature be relevant only to very few instance  lt  1    PRON discover that in such sparse scenario the follow  the  regularize  leader ftrl proximal gradient descent be very popular   paper  reference implementation   but PRON be not sure why should not PRON prefer a batch gradient descent algorithm  ftrl for all PRON merit be still an online  learn algorithm that see one instance at a time   so what be the advantage and disadvantage of use ftrl vs a well know sparse least square algorithm such as lsqr  paper  reference implementation    PRON intuition be that if possible to use all the datum for each iteration  PRON should do PRON  but PRON be not sure   
__label__predictive-modeling __label__recommender-system PRON be try to look for an idea to create a predictive model have the follow datum   customerid  integer  catalogid  integer  countrycode  integet  year  integet  month  integer  day  integer  quantitypurchas  integer  productpurchas  double  PRON be try to look for a use case that create a predictive model than can give PRON the ability to propose a product for a customer the next time that PRON come to PRON website   be this a collaborative filtering use case  if yes  PRON only can use the last two field of PRON dataset  right   thank   recommender system be usually either collaborative   other people who buy x also buy y   or content  base   item y have similar property as item x    give PRON very few datum feature  collaborative filtering seem appropriate  in the basic setting PRON could use only the purchase of PRON user  ideally  PRON would also have some sort of rating of PRON purchase   also  nothing stop PRON from use a hybrid approach with additional datum if available  
__label__petsc PRON just instal the petsc library   this be what PRON do from the home directory   gunzip c petsc32p6targztar xof   cd petsc32p6  petcdirpwd  export petscdir  configure download  f  blas  lapack1  make all  make petscdirexport  home  myusername  petsc32p6 petscarch  arch  linux2c  debug test   note  PRON do not install mpich since PRON be already instal on the system   then  PRON try to run an example from the follow this tutorial  slide 46 of 197  which ask PRON to do the following   cd petsc32p6src  sne  example  tutorial  make ex5  in the process  PRON get the following error   makefile24  conf  variable  no such file or directory  makefile24  conf  rule  no such file or directory  makefile24  conf  test  no such file or directory  be the problem in the way PRON setup petsc  or be there something PRON need to modify in the makefile   PRON most likely have not set the petscdir and petscarch environment variabl correctly   for instance  PRON misspell petscdir as petcdir in the third line of PRON build example  
__label__rbm why explain away concept be not applicable in restricted boltzmann machine  PRON hide unit form a v structure from which probabilistic influence can flow give the observed visible variable  why be this a problem in deep belief net   
__label__linear-algebra __label__dense-matrix PRON have an upper trapezoidal matrix store in column major format   that is  PRON matrix look like this   PRON would like to rq decompose PRON  and store q in the rectangular part of PRON upper trapezoidal matrix   for qr decomposition  PRON can store q in the low portion of the matrix  as a sequence of column vector   v  which form householder transformation   i  2 v vt    however  the same approach take here would have PRON store the vector for the householder transformation as row in the rectangular portion of the upper trapezoidal matrix   however  because the matrix be not store row major  that be not go to be cache efficient   be there a way to store q from the rq decomposition in the rectangular portion of the upper trapezoidal matrix in a way that let PRON use PRON in a column major way   if not use householder transformations  maybe a different method   right now PRON thought be to require the column vector in the rectangular portion to be pad with extra space so that the rectangular portion be square  and then PRON store the householder vector as column   but that can potentially be a lot of extra memory   be there a good way   PRON can try an lq factorization  similar to what PRON have in mind  PRON be mathematically equivalent to qr on  at httpwwwnetliborglapacklugnode41html  
__label__machine-learning __label__python look for data science training with python in bangalore location with real time scenario  amp  project preparation   sorry for answer PRON as PRON do not have require reputation to comment   this site be mean for problem discussion and possible approach to solve PRON  PRON be not portal to ask the question PRON have ask  PRON can google for the problem that PRON have ask and PRON be sure PRON will get an answer  PRON should have read the instruction and read community guideline before ask this question   there be a ton of great resource out there online  many free  PRON personally have experience with and recommend edx and datacamp  httpswwwdatacampcomcoursesintrotopythonfordatascience  httpswwwedxorgcourseintroductionpythondatasciencemicrosoftdat208x5 
__label__optimization __label__statistics __label__heuristics if PRON optimize some parameter use 4 optimization algorithm  2 of which be population base  say a and b  and 2 trajectory method  single point searchsay c and d   what statistical test can be use for compare 4 of these  the initial population of a and b be the same  so be for c and d however  population size of a and b be  n and the population size of c and d be 1  so  PRON think  PRON should use pair t  test for compare  a and b  and  c and d   and should use unpaired t  test for all other combination with bonferroni correction  but what should actually be do   establish methodology for benchmark optimization software can be find in publication such as benchmarking optimization software with performance profiles  benchmarking derivative  free optimization algorithms  and derivative  free optimization  a review of algorithm and comparison of software implementation   generally speak  algorithm be benchmark against a suite of problem  and then performance profile be construct base on whether or not the algorithm successfully solve each problem to within a give tolerance and time limit   ttest be not typically use   an interesting method to find  good optimization algorithm  for a class of test instance be the irace  resp  f  race  approach  for which an r  package exist  
__label__machine-learning __label__computer-vision PRON want to have some idea that what be scale space decomposition and how to perform be use different conductivity function  PRON want to implement this use matlab   please point to the right resource   
__label__r __label__feature-extraction __label__feature-engineering nrowdf1v1   63849  nrowdf2v2   3244  ifelsedf1v2   dfv1  1  0   PRON know this be an easy question but PRON try different procedure but none of PRON be useful   fori in 1nrowdf2     forj in 1nrowdf1     ifdf2v1i    df1v2j     df1v2lt 1    df1v2  lt 0      this do the job but PRON take quiet sometime to get the job do   other methods   method 1   df1v2  lt ifelsedf2v1  in df1v1  10   error in    ltdataframetmp    v2  value  c1  1  1  1  1  1     replacement have 3244 row  datum have 63849  method 2   df1v2  lt ifelsedocallpaste0  df2v1   in  docallpaste0  df1v1   10   summarydf1v2   min  1st qu   median  mean 3rd qu   max   0  0  0  0  0  0  do suggest PRON if PRON have any good solution   here be a way use dplyr   requiredplyr   x  dataframea  c1234    y  dataframeb  c5467    x  lt x   gt  mutatea  a  in yb   a  a10   convert boolean to numeric  or use base r   xa  lt 10xa  in yb  
__label__machine-learning __label__classification __label__clustering __label__dimensionality-reduction PRON have a question about classify document use supervised learning and unsupervised learning   for example   PRON have a bunch of document talk about football   as PRON know football have different meaning in uk  usa and australia  therefore  PRON be difficult to classify these document to three different categorization which be soccer  american football and australian football   PRON approach try to use cosine similarity term which be base on unsupervised  after PRON use the cluster learn  PRON be able to create a number of cluster base on cosine similarity which each cluster will contain similar document term  after PRON create the cluster  PRON can use a semantic feature to identify these cluster depend on supervised model like svm to make accurate categorization   PRON goal be to create more accurate categorization because if PRON want to test a new document PRON want know if this document can be relate to these categorization or not   PRON sound as if PRON want to use unsupervized learning to create a training set  be PRON right  PRON use PRON cluster analysis to determine which doc come from uk  us or oz  or which doc be talk about soccer  football or australian football respectively  then feed those tag doc into a supervized learn algorithm of some sort   how well this work will depend entirely on how well PRON can distinguish uk  us and oz  PRON would have think PRON would be fairly straightforward to find document where national origin be know  so that PRON could build a supervized algorithm for detect language variant  PRON would not even need a corpus that talk about football  since dialectical difference show up in other way that be subject matter independent   for example  PRON be clearly from north america  since PRON just write  in way that be subject matter independent  rather than  since dialectical difference do not depend on subject matter     however  the answer to PRON question   can PRON use unsupervized learning and then superviz learning  be no  if PRON be look for supervized learning  if the result of an unsupervized learning algorithm be feed to a supervized learning algorithm  the net result be unsuperviz  there be still no grow  up in the room  and the classification error of the result process will contain error term from both stage  PRON will not get the same performance as PRON would if PRON do a svm with properly tag training datum  this do not mean PRON should not use the method PRON propose  PRON may still work well  but PRON will not be a supervized learn algorithm   PRON can definitely try to first cluster PRON datum  and then try to see if the cluster information help PRON classification task   for example if PRON datum look like this  in 1d    aa a aa a a  bbb b b b bb bb bb  aa aa a a aaa  then PRON may be reasonable to run a clustering algorithm on each class  to obtain two different kind of a  and learn two separate classifier for a1 and a2  and just drop the cluster distinction for the final output   other common unsupervised technique use include pca   as for PRON football example  the problem be that the unsupervised algorithm do not know what PRON should be look for  instead of learn to separate american football and soccer  PRON may just as well decide to cluster on international vs national game  or europe vs us  which may look like PRON learn about american football and soccer at first  but PRON put american soccer into the same cluster as american football  and american football team in europe into the europe cluster  because PRON do not have guidance on what structure PRON be interested in  and the continent be a valid structure  too   so usually  PRON would not blindly assume that unsupervised technique yield a distrinction that match PRON desire result  PRON can yield any kind of structure  and PRON will want to carefully inspect what PRON find before use PRON  if PRON use PRON blindly  make sure PRON spend enough time on evaluation  eg if the clustering improve PRON classifi performance  then PRON probably work as intend    use unsupervised learning to reduce the dimensionality and then use supervised learning to obtain an accurate predictive model be commonly use  see for example bhat and zaelit  2012 where PRON first use pca to reduce the dimension of a problem from 87 to 35  then  PRON use l1 regression to obtain the good predictive model  this method beat non  linear tree base model build on the entire dataset and also PRON subset   if PRON goal be to create more accurate classification of datum into cluster  then a commonly use technique be to use supervised learning as a method to accurately pick the number of cluster see pan et al  2013 for a recent example  the basic approach here be to choose the number of cluster such that a supervised multi  class method can learn these cluster and predict the cluster with the high out of sample accuracy  this be one way to convince PRON that the cluster be both meaningful and predictable   another approach  if PRON goal be to classify document as be from us usa australia or for that matter discuss  soccer american football australian football could be to solve three binary classification problem that independently predict if the document talk about soccer  american football or australian football  combine the result from these three classifier  know as binary relevance   PRON could also have the ability of tag a document as both soccer or american football or any combination of the above three tag  
__label__intelligence-testing which objective and measurable test have be develop to test the intelligence of ai   the classical test be the turing test  which have objective criterion and be measurable since PRON can be measure what percentage of the jury be fool by the ai   PRON be look for other  more modern test   
__label__machine-learning __label__deep-learning __label__dataset __label__convnet __label__image-classification if PRON look at one of the many source for the imagenet class on the internet PRON can not find a single class relate to human being  and no  harvestman be not someone who harvest  but PRON be what PRON know as a daddy longleg  a kind of spider   how be that possible  PRON would have at least expect a person class  and even something more specific such as man  woman  toddler  etc  nothing of the sort  why  do fei  fei li and PRON team make a conscious choice not to have people image in the database  be PRON look at the wrong file  for the sake of the question  PRON can consider the imagenet version from 2014 onwards   PRON can also take a look at here for the label in the imagenet  PRON guess PRON be right  there be no label for human in the data  set but there be something to notice  there be label in imagenet like cowboy or some specific hat and other related thing to human like shirt and t  shirt  PRON can take a look at here and also here  in the latter link yosinski et al  have try to show that the popular alexnet have learn to recognize human face although there be no label as human face in the imagenet datum  set  in PRON paper  PRON have investigate that convolutional neural network may try to learn thing that be distribute among layer or maybe not and PRON may not have special label in the training datum  as an example  the face of cat and human can be refer to  moreover  as PRON can see here maybe the aim be attribute learn in large  scale dataset  as quote in the last line of the page  as the reference   PRON find the class 7846  namen00007846   be for person  to access to class description  read httpimagenetorgdownloadapi  even better  the follow text file contain everything PRON ever need to understand the class in imagenet dataset  class  wordnet id    httpimagenetorgarchivewordstxt map between wordnet id and word for all synset  httpimagenetorgarchiveglosstxt  map between wordnet id and gloss for all synset  n00007846 map to person  individual  someone  somebody  mortal  soul  the correspond gloss be  a human being   there be too much for one person to do   
__label__machine-learning __label__dataset PRON main use case be object detection in 3d lidar point cloud ie data be not in rgb  d format  PRON be plan to use cnn for this purpose use theano  hardware limitation be cpu  32 gb ram intel 47xx 4th gen core i7 and gpu  nvidia quadro k1100 m 2 gb  kindly help PRON with recommendation for architecture   PRON be think in the line of 27000 input neuron on basis of 30x30x30 voxel grid but can not tell in advance if this be a good option   additional note  dataset have 4500 point on average per view per point cloud  first  cnn be great for image recognition  where PRON usually take sub sampled window of about 80 by 80 pixel  27000 input neuron be too large and PRON will take PRON forever to train a cnn on that   furthermore  why do PRON choose cnn  why do not PRON try some more down to earth algorithm fisrst  like svm  or logistic regression   4500 data point and 27000 feature seem unrealistic to PRON  and very prone to over fitting   check this first   httpscikitlearnorgstabletutorialmachinelearningmap 
__label__matrix __label__fortran PRON have a matrix a which be of size  n2  n1  and PRON be multiply PRON by a matrix  b  of size  n1  n0   PRON have identify this single matrix multiplication as the bottleneck in PRON fortran code  out of 2000 line of code  this single line take about 77  of the runtime   a be a double precision matrix with float point value  b be  currently  a double precision matrix contain only value 10 and 00  PRON can easily make this integer  or even binary  but PRON be use PRON as real so that PRON could preserver precision in matmula  b    what be a good way to perform this matrix multiplication to cut down on runtime   before anyone suggest PRON  PRON be use dgemm and compile with o3 and mavx for gfortran  and o3 with xhost on ifort   the large datum PRON have implement this program on so far  n  5000  result in n2  1668  n1  1701  and n0  1631  this algorithm be implement in matlab and have short runtime  matlab version be about 25 second  while this fortran program be about 7 second  since this single matrix multiplication be so large  PRON be think that matlab be do something interesting with the variable type   PRON have compile this with ifort use mkl and be current linking against lblas and use fexternal  bla  rely on matmul to perform the underlie blas routine  the result of ldd on PRON binary executable be   linuxvdsoso1   gt    0x00002aaaaaacb000   liblapackso3   gt  usr  lib64atla  liblapackso3  0x00002aaaaaccd000   libblasso3   gt  usr  lib64libblasso3  0x00002aaaab4f0000   libgfortranso3   gt  usr  lib64libgfortranso3  0x00002aaaab747000   libmso6   gt  lib64libmso6  0x00002aaaaba39000   libgccsso1   gt  lib64libgccsso1  0x0000003f78c00000   libcso6   gt  lib64libcso6  0x00002aaaabcbe000   libf77blasso3   gt  usr  lib64atla  libf77blasso3  0x00002aaaac052000   libcblasso3   gt  usr  lib64atlas  libcblasso3  0x00002aaaac272000   lib64ld  linux  x86  64so2  0x00002aaaaaaab000   libatlasso3   gt  usr  lib64atla  libatlasso3  0x00002aaaac492000   libpthreadso0   gt  lib64libpthreadso0  0x00002aaaacaee000   b be structure in the way that PRON have zero and one  the lower left portion  not truly lower triangular  have one and the upper right portion  not triangular  be zero   PRON appear that the matlab code be treat the b matrix as logical   what be the percentage of nonzero entry in  b   if a high percentage of the entry be 0 s  then PRON may well be good off treat  b as a sparse matrix in the multiplication   PRON have not tell PRON the size of the matrix   what be n0 and n1   or at least PRON order of magnitude   PRON have not say what implementation of the blas PRON be currently use   PRON specific comment about compilation flag suggest that PRON may be use the reference blas implementation  which would be a very poor choice in comparison with optimized cache  aware and multithread implementation of the blas routine such as atlas  openblas  acml  and mkl  
__label__python __label__tensorflow __label__linux PRON have instal tensorflow on linux  anaconda  by follow the documentation which state that one should create and activate a virtual environment tensorflow  so far  so good  albeit PRON be not entirely clear why be this virtual environment be necessary when PRON want to incorporate tf to PRON exist env    but when PRON activate the tensorflow environment PRON observe that several package be unavailable in the new environment while available outside the environment at the same time    source activate tensorflow   python   gtgtgtimport h5py   no module name h5py   source deactivate tensorflow   python   gtgtgtimport h5py   gtgtgt   no problem in this case   PRON guess that PRON should install the missing package in the tensorflow environment as well  but when PRON try to  PRON get inform that the package in question have already be instal and nothing happen    source activate tensorflow   pip install h5py  requirement already statisfi   the same inconsistency occur with several other package  what be the problem here   PRON can not offer any suggestion as to why PRON virtual environment can not find the package  PRON be not familiar with anaconda   the only thing that come to mind be a path problem   perhaps try instal the package with the follow command   conda install n yourenvname  package   or  in PRON case   conda install n tensorflow h5py  to install additional package only to PRON virtual environment  enter  the follow command where yourenvname be the name of PRON  environemnt  and  package  be the name of the package PRON wish to  install  failure to specify  n yourenvname  will install the package  to the root python installation   source  httpsuoaeresearchgithubioeresearchcookbookrecipe20141120conda  PRON believe that there may be a conflicting version of python  try to see which python PRON be use when PRON be in the virtual environment   which python  if the python version PRON be use be not the one in the conda environment  try set PRON   export path  path  to  python  PRON can try two more thing  first try the conda pip install to see if this work   conda install pip  PRON may be possible that PRON have two conflict version of pip  to see if this be the case try this and set PRON to the right version with export   which a pip 
__label__programming __label__excel PRON be in the process of prepare to teach  an introductory course on datum science use the r programming language   PRON audience be undergraduate student major in business subject   a typical business undergrad do not have any computer programming experience   but have take a few class which use excel   personally  PRON be very comfortable with r  or other programming language   because PRON major in computer science   however  PRON have the feeling that many of PRON student  will feel wary of learn a programming language  because PRON may seem difficult to PRON   PRON do have some familiarity with excel   and PRON be PRON belief that while excel can be useful for simple datum science   PRON be necessary for student to learn  a serious programming language for data science  eg  r or python    how do PRON convince PRON and the student  that excel be insufficient  for a serious business student study datum science   and that PRON be necessary for PRON to learn some programming   edit in response to comment  here be some of the topic that PRON will be cover   data processing and datum cleaning  how to manipulate a data table   eg  select a subset of row  filter    add new variable  mutate    sort row by column  sql join use the dplyr package  how to draw plot  scatter plot  bar plot  histogram etc    use the ggplot2 package  how to estimate and interpret statistical model such as  linear regression  logistic regression   classification tree  and k  near neighbor  because PRON do not know excel very well   PRON do not know whether all of these task can be do easily in excel   first of all check out this post  PRON have many reason why excel be inferior to other solution  regard datum science task  excel also can not handle large dataset  hundred of thousand of record  not to mention anything in the vicinity of big data   image and sound datum   excel be good for simple task concern spreadsheet  PRON emphasize more on presentation and ease of use  while have minimal support for actually analyse the datum  unless all PRON want to do be to calculate simple statistical measure  mean  average  etc  or build a very simple model  eg linear regression   excel be inefficient  that be say  99  the work a company have to deal with concern datum be simple enough to be manageable through excel   however data science mainly deal with regression  classification and complex model that excel be not equip to handle  if PRON student want to have a look at datum science PRON need to teach PRON a tool that will be useful to PRON  r  python  etc    these language also have library with ton of build in model to  play with    another really huge reason PRON would go with the latter option be that PRON be open source  PRON personally feel that open source software should be prefer from an educational standpoint to proprietary solution  this be also why PRON suggest python and r over matlab    PRON just get do with a masters in business analytics and be face with the same problem PRON be describe  luckily PRON be a technical person and be able to teach PRON r and python  but PRON be stick teach the rest of the class how to use r and python  the class PRON have that use r  python be handicap by the lack of technical understanding by the student and so too much time be spend cover how to just open r  python  the class that go the other route be underwhelming and not very practical  PRON want to do for a class project something that end up not be able to be do in excel because of PRON limitation but the teacher would not accept any other tool   PRON may not be something PRON can do right away but PRON would highly recommend that PRON try and get the department to require a programming course prior to take PRON course  data science and business analytics imho should be cross discipline degree path that require a good bit of computer science  but until the program mature and the university system get better PRON may not happen for a while   PRON think PRON need to be teach PRON a popular data science language like python or r excel be not go to help PRON in a real job  and be not practical for datum science purpose   PRON would probably say python would be most valuable to PRON in the long run  and with package like scikit  learn PRON regression and classification can be demonstrate in very few line of code which PRON can read and understand more easily  PRON be not always easy to understand what r be do by just read PRON   another word of advice  do not waste time force PRON student to set up an ide and download the necessary package  if PRON use python create a virtual environment for PRON with all the necessary package  and set up an ide like pycharmthey can get this and most other ide under a student  academic license  where then can develop and run PRON code through ui rather than console which PRON may find daunting and confusing  if PRON go down the r route then make sure PRON have an ide like rstudio set up for PRON and make sure all of the include and package install be either include in PRON example code or fully describe   excel and data science  sound really strange to PRON  maybe excel and  data analysis    anyways  PRON think a good compromise between excel and r be  knime  httpwwwknimeorgknimeanalyticsplatform   PRON be free on the desktop and much easy to get start  PRON can import  export to excel but also use r  python or java if the  1000 node miss some functionality that PRON need  since the workflow be visually create  PRON be also much easy to show PRON to someone who do not know any programming language  which be quite an advantage in some company  
__label__deep-learning __label__feature-selection __label__feature-extraction __label__feature-engineering __label__autoencoder PRON have a dataset with n record and d numerical attribute belonign to c different class  PRON use a stacked autoencoder for feature extraction for a classification task   PRON take an input vector  x in mathbbrd  and map PRON to a hidden representation  y in mathbbrd   d     lt  d     PRON question be  if PRON consider output of the middlemost layer  ie  y in mathbbrd  as PRON feature set  for each class of datum   ci in c     how can PRON realize which subset of this feature set have the great impact on identify the class  mi  as an example  for mnist PRON have n60000  d784  and c10  an autoencoder with this architecture   d784  inp  inputshaped     x  densed  activationreluinp   x  densed2  activationrelux   x  densed8  activationrelux   y  densed128  activationrelux   x  densed8  activationreluy   x  densed2  activationreludecoded   x  densed  activationsigmoidx   model  modelinputimg  z   produce a  yin mathbbr6 for example  here PRON see the output of layer y for digit 5 and 9   class  y1  y2  y3  y4  y5  y6  9  109  959  158  847  114  725  9  213  134  400  859  153  136  5  719  752  458  504  109  535  5  980  155  146  506  649  351  if PRON connect layer y to a softmax dense layer   out  densenumclass  activationsoftmaxy   encode  modelinputimg  out   the new model encode give PRON a good accuracy about 98   so  if PRON consider new representation  y  as an efficiently extract feature vector  be PRON reasonable to assign a subset of this feature vector to each digit   or  what be the right way to relate the vector y to the c different class   
__label__linear-solver __label__conjugate-gradient __label__gmres PRON be interested in case where conjugate gradient work much good than gmres method   in general  cg be preferable choice in many case of spd  symmetric  positive  definite  because PRON require less storage and theoretical bind on convergence rate for cg be double of that gmres   be there any problem where such rate be actually observe  be there any characterization of case where gmres perform good or comparable to cg for same number of spmvs  sparse matrix  vector multiplication    one thing that cg have in PRON favor be that PRON be not minimize the discrete  l2  norm for PRON residual polynomial  what gmres do   PRON be minimize a matrix  induce norm instead  and very often this matrix  induce norm end up be very close to the energy norm for discretization of physical problem  and frequently this be a much more reasonable norm to measure error in because of conservation property come from the physics   PRON can actually achieve this sort of effect with gmres too if perform a cholesky factorization of a mass matrix be not too expensive  PRON can force the inner product to be the energy inner product PRON want   then the case where one should expect cg to perform very differently from gmres then be when the constant imply in norm equivalence be very different  this can be true for example in a high order spectral  galerkin method where the discrete  l2  norm use in gmres treat all degree of freedom as be equal  when in reality polynomial gradient be sharp near boundary  hence node clustering   and so the norm equivalence constant between that norm and say the continuous  l2  norm give by the mass matrix can be very large   one thing be that gmres be never use wherever cg can be apply  PRON do not think PRON do make sense to compare these two  for spd matrix  cg be definitely the winner because of the storage requirement and the reason PRON mention above  a question that would be interesting be  to find an extension of cg  that be applicable to problem where cg can not be apply  there be method like bicg  stab that do not require linearly increase memory like gmres  but the convergence be not as good as gmres  some time even with restarted gmres    PRON suspect there be in general not much difference between gmres and cg for an spd matrix   let PRON say PRON be solve  ax  b  with  a  symmetric positive definite and the starting guess  x0  0  and generate iterate with cg and gmres  call PRON  xkc  and  xkg    both iterative method will be build  xk  from the same krylov space  kk    b  ab  a2b  ldot      PRON will do so in slightly different way   cg be characterize by minimize the error  ekc  x  xkc  in the energy norm induce by  a   so that  beginequation    a ekc  ekc    a  x  xkc   x  xkc   miny in k   a  x  y   x  y    endequation   gmres minimiz instead the residual  rk  b  a xgk   and do so in the discrete  ell2  norm  so that  beginequation    rk  rk    b  a xkg  b  a xkg   miny in k   b  ay  b  ay    endequation   now use the error equation  a ek  rk  PRON can also write gmres as minimize  beginequation    rk  rk    a ekg  a ekg    a2 ekg  ekg   endequation   where PRON want to emphasize that this only hold for an spd matrix  a    then PRON have cg minimize the error with respect to the  a  norm and gmres minimize the error with respect to the  a2  norm   if PRON want PRON to behave very differently  intuitively PRON would need an  a  such that these two norm be very different   but for spd  a  these norm will behave quite similarly   to get even more specific  in the first iteration with the krylov space  k1    b     both cg and gmres will construct an approximation of the form  x1  alpha b    cg will choose  beginequation   alpha  frac   b  b     ab  b    endequation   and gmres will choose  beginequation   alpha  frac   ab  b     a2b  b     endequation   if  a  be diagonal with entry   epsilon111ldot   and  b   11000ldots   then as  epsilon rightarrow 0  the first cg step become twice as large as the first gmres step   probably PRON can construct  a  and  b  so that this factor of two difference continue throughout the iteration  but PRON doubt PRON get any bad than that  
__label__matlab __label__statistics __label__data-analysis PRON have a data  set that have four column  x y z c   PRON would like to find all the c value that be in a give sphere center at  x  y  z  with a radius r what be the good approach to address this problem  should PRON use the clusterdata command   to collect the require c value PRON would make a single pass through the data  set  array   with some quick check to eliminate most of the point outside the sphere  and then  only for the point that seem eligible  a check of actual radius   that is  for a candidate point  xi  yi  zi  ci   test   1  be xi between x  r and xr   2  if so  be yi between y  r and yr   3  if so  be zi between z  r and zr   only if all three test be true be PRON possible for the point to be in the desire sphere   for the final test  use precomput r2 and ask   4  be  xi  x2   yi  y2   zi  z2 less than or equal r2   if the point pass the final test  include ci in PRON collect c value  
__label__deep-learning __label__ai-design __label__game-ai __label__combinatorial-games there be more information recently that alphazero have be train to be the good chess program after 4 hour of learning  in chess   PRON be wonder how the ai network could have be model for this program   PRON be not familiar with ai so PRON idea could be basic   input  list of boolean indicate if give field contain give checker  eg isa1myrook  isa1myknight  isa1mybishop    isa2myrook  isa2myknight    output  list of all possible move include forbid one  eg ra1a2  ra1a3 etc    PRON think that this be far from optimal but PRON think that this be a good starting point   this question be good on httpschessstackexchangecom PRON have answer some of PRON  there be no reason to repeat here   please ask a specific question if PRON have any  PRON have read the paper  but PRON can not do anything unless PRON make PRON clear what PRON want   httpschessstackexchangecomquestions19353understandingalphazero  httpschessstackexchangecomquestions19366hardwareusedinalphazerovsstockfishmatch  httpschessstackexchangecomquestions19525alphazerolearningvsplayingmode  httpschessstackexchangecomquestions19500whystockfishplayedsopoorlyagainstalphazero  httpschessstackexchangecomquestions19385lecturebookonalphagoalphazero 
__label__floating-point __label__stability __label__numerical PRON be try to implement the follow function in double  precision floating point with low relative error     mathrmlogsumx  y   logexpx   expy  this be use extensively in statistical application to add probability or probability density that be represent in log space  of course  either  expx or  expy could easily overflow or underflow  which would be bad because log space be use to avoid underflow in the first place  this be the typical solution     mathrmlogsumx  y   x  mathrmlog1pexpy  x  cancellation from  y  x do happen  but be mitigate by  exp bad by far be when  x and  mathrmlog1pexpy  x be close  here be a relative error plot   the plot be cut off at  1014 to emphasize the shape of the curve  mathrmlogsumx  y   0   about which the cancellation occur  PRON have see error up to  1011 and suspect that PRON get much bad   fwiw  the  ground truth  function be implement use mpfr s arbitrary  precision float with 128bit precision    PRON have try other reformulation  all with the same result  with  log as the outer expression  the same error occur by take a log of something near 1  with  mathrmlog1p as the outer expression  cancellation happen in the inner expression   now  the absolute error be very small  so  expmathrmlogsumx  y have very small relative error  within an epsilon   one may argue that  because a user of  mathrmlogsum be really interested in probability  not log probability   this terrible relative error be not a problem  PRON be likely that PRON usually be not  but PRON be write a library function  and PRON would like PRON client to be able to count on relative error not much bad than round error   PRON seem PRON need a new approach  what may PRON be   the formula    mathrmlogsumx  ymaxx  ymathrmlog1pexpoperatornameabsx  y  should be numerically stable  PRON generalize to a numerically stable computation of    log sumi exi   xi logsumi exixiximaxi xi  in case the logsum be very close to zero and PRON want high relative accuracy  PRON can probably use    mathrmlogsumx  ymaxx  ymathrmlexpx  y  use an accurate  ie  more than double precision  implementation of    mathrmlexpzlog1ez  which be nearly linear for small  z 
__label__machine-learning __label__nlp __label__clustering in nlp  people tend to use cosine similarity to measure to document  text distance  PRON just want to hear out what do PRON think for the following two case  cosine similarity or euclidean   the task be to compute context  leave and right word of an expression  similarity of multi  word expression  ie  put up  rain cat and dog   mathematically  to calculate simcontext1mwe  context2mwe   the contextnmwe feature vector be buit from word embedding  assume the embed dimension be 200   two way to represent contextnmwe   concatenate leave and right 2 context word and then PRON have a new embed vector of 200  4800 dimension  in other word  a feature vector of  lc1  lc2  rc1  rc2  where lc  leftcontext and rc  rightcontext   take mean of the sum of left and right 2 context word and then PRON get a vector of 200 dimension  in other word  a feature vector of  meanlc1lc2rc1rc2     personal speaking  PRON think euclidean distance be a good fit for both case  cosine similarity be specialize in handle scale  length effect  for case 1  context length be fix  4 word  there be no scale effect  in term of case 2  the term frequency matter  a word appear once be different from a word appear twice  PRON can not apply cosine   
__label__dataset like above  PRON would like to know  what exactly a skewed dataset be   the explanation from statssecom sound to PRON more like what PRON call an imbalanced dataset   what be the distinction   in the context of the link  a skewed datum set be refer to a dataset with a class imbalance problem  PRON be try to build a classifier  but PRON have many more negative example than positive example  PRON be not a very precise term  but PRON have hear to use in this context a few time  
__label__apache-spark __label__scala PRON have recently face problem in access text file that PRON have upload from PRON local computer to the ibm dsx cloud object storage so that PRON can use PRON in PRON spark  scala notebook  PRON appear to PRON that the general configuration of the platform be focus on csv format  so be most of the help material  in fact  PRON figure a way around when PRON change the extension of PRON txt file to csv  through this post and the answer follow  PRON be try to share PRON experience with other   this post be useful  if   PRON have create a ibm dsx notebook with  language  scala  and  spark version21 or 20″did not try with other version   and PRON be try to access a text file  do not try other format  that PRON have upload from PRON personal computer to the ibm cloud object storage  in order to access file that be in cloud object storage  PRON need to provide the path of the file in the form of a cloudobjectstorage instance  add follow line  after the file credential  to PRON code   var soc  new cloudobjectstoragesc  credential  null   bluemixco   var myfile  sctextfilesocurl“yourbucket””datatxt     in the second code line above   yourbucket  be the value of  bucket  in the file credential   there be one last thing PRON need to do before PRON be good to go  the cloudobjectstorage constructor accept credential in a way that be for the csv file  luckily the main difference between the two format be the hashmap key name  hence  PRON can easily manage this problem by change the above credential manually  as mention below    ibmapikeyid  as  apikey    iamserviceid  as  serviceid    endpoint  as  endpoint    ibmauthendpoint  as  iamserviceendpoint   for further detail  PRON can visit the link below   httprehmansorg20180131howtoaccessatextfilefromibmdsxscalasparknotebook 
__label__apache-spark __label__sql PRON be explore spark sql  but struggle to find the optimal way to achieve something that look like this below   PRON can imagine that the client side pivot grid display the first 3 column as hierarchy which can be collapse and expand  PRON be currently show as expand   notice the population for city in broward add up to total population in broward   assume all county be list  PRON would add up to the total population in state of florida      state    county city  population       florida      15000000      broward     5000000        city c  2000000        city d  1500000        city e  1000000        city f  500000     
__label__linear-algebra __label__lapack first   ax  b be solve  so PRON have the lu factorization of  a compute already   now PRON need to solve  bx  a be there any way to reuse this information  lapack gesv be use to compute lu of  a    PRON question amount to ask which be the optimal solution strategy for compute  x  y such that     left   beginalign   ax  amp b  by  amp a  endaligned   right      where  a  b be generic square dense matrix   apart from the observation that if  a and  b be non singular  then  y  x1  PRON do not see any particular reason for which the knowledge of  x should help in the computation of  y but PRON would love to be contradict on this  
__label__statistics __label__data-analysis PRON be work on historical datum look for anomalous pattern that PRON would not expect to occur at random  PRON would like to create a scheme to analyze datum and market to test for statistical significance and consistency over time   question  be there some tool PRON could use to evaluate the nonrandomness of a database  what do PRON use usually to say that something be completely random or not  statistically   figure PRON work with change in some value over time   
__label__pde __label__fourier-analysis __label__boundary-conditions __label__poisson PRON have hear that a fast fourier transform can be use to solve the poisson problem when the boundary condition be all one type  sine series for dirichlet  cosine for neumann  and both for periodic   consider a 2d rectangular domain  suppose two opposite side have periodic boundary condition  and the other two have dirichlet condition   can a fast fourier transform be apply to solve this problem efficiently   if so  would not the exponential form be sufficient   if not  what solver would PRON recommend for this situation   PRON can separate the problem along the direction with dirichlet condition and then solve the 2d periodic problem  exactly yours combination of boundary condition be cover by wilhelmson  ericksen  jcp 1976 and PRON be easy to implement  PRON could also use fishpack  but PRON be old and buggy   PRON be work on a small solver for similar case  but PRON be not ready for release yet and PRON will not be a big mpi thing  just for share memory machine    actually  PRON code be now an mpi thing and PRON solve this problem too  httpsgithubcomladafpoisfft 
__label__declarative-programming what specific advantage of declarative language make PRON more applicable to ai than imperative language   what can declarative language do easily that other language style find difficult for this kind of problem   the advantage of a declarative language like prolog be that PRON can be use to express fact and inference rule separately from control flow   this allow the developer to focus on the datum and inference rule  the knowledge model   and allow the developer to extend the knowledge model more easily   PRON should add that in practice  this dichotomy between fact  rule on the one hand  and control flow on the other  be not strict  a knowledge engineer who write a code base in prolog do sometimes have to consider control flow  the    operator be use so that the developer can influence the evaluation of the rule   there be no objective reason to state that declarative language be better suit for ai development  however  there be indeed a bias towards PRON in practice  although most functional language be impure  that is  PRON allow side effect   and such can not count as  declarative   a few language be purely functional  that is  PRON do not allow side effect   most prominently haskell  purity be key here  in haskell  even PRON  o be pure   the key difference between imperative language and  purely  functional language be in the way PRON describe the program  an imperative program describe how to do stuff  that is  algorithm  PRON specify the specific instruction that the machine must carry on in order to perform the computation  otoh  purely functional language describe what be to be compute  that is  the relationship between the input and the output  in mathematic   function  be just a fancy name for a relationship between an input and an output   again  in the mathematical sense  the only variability be that of the function s argument  that is  the function s output depend solely on PRON input  argument   this be know as referential transparency  referential transparency state that   where ϕ be the set of all function  and δf  be f s domain  for the typical imperative language s definition of  function   the above do not hold  for instance  c s getchar   do not always return the same value   let PRON say PRON want to calculate the set of the ten least prime number whose least significant digit be 3  first  in mathematical notation   where gn  s  be the set of the less nth element from s in mathematic  PRON do not worry about how be the set s suppose to be compute  but rather about s s definition PRON   now  in python  in imperative style    def isprimen    for x in range2  n    if n  x   0   return false  return true  def foo     s  set    n  2  while lens   lt  10   if isprimen  and n  10   3   sappendn   return sets   in python  PRON care about  and be responsible for  the algorithm be use to compute the set  PRON specify  pretty much in recipe  style  how to build the set from scratch  if there be an algorithm that may be better suit for check whether a number be prime or odd  but PRON do not use PRON  PRON be PRON fault  not python s   finally  haskell step in   import qualify data  set as set  isprime   integer gt  bool  isprime n     1   length  filter    0   map  n  mod     2  n   s  setfromlist  take 10  filter     3     mod  10    filter isprime   1    haskell s version be a lot more like the mathematical model than python be  PRON define the isprime function in term of the constraint that a prime number must obey  not by describe a step  by  step algorithm to do such a check  moreover  s  the set PRON have be define so far  be define in term of the constraint PRON member must obey  rather than in term of an algorithm to compute s PRON  the compiler  more often than not the glorious glasgow haskell compilation system  aka ghc   be the one responsible for generate an algorithm  ghc s optimizer be know to be one of the strong in the world  not because PRON the good compiler of PRON all  but because haskell s nature allow for this   haskell  and other functional language   in summary  have several feature that make PRON look  taste  and behave like pure math   referential transparency and purity   haskell be lazy   a very strong type system  with such exotic  but very useful   stuff as recursive  and algebraic  data type   so  the bottom line be that ai researcher often prefer functional language over imperative language  or  more assertively  pure over impure language   because PRON be attempt to define artificial intelligence PRON  by mean of function  relationship between an input and an output   at the end  PRON do this because PRON have no real algorithm for human  level intelligence to raise from a handful of transistor  also  there be be a historical bias towards these kind of language  start with john mccarthy  lisp s creator and a pioneer in early ai research  
__label__deep-learning __label__training __label__autoencoder in a variational autoencoders  z can not be simply sample from the output of the autoencoder directly since the network would not be differentiable  instead  PRON have to sample from a normal distribution who be parameter be the output of the autoencoder  mu and stddev   PRON so can not understand how the network be not differentiable  let say PRON take z  mu  srd   so the reparameterization be not clear to PRON   any help would be much appreciate    have PRON read doersch s tutorial on vae  in paragraph 11 the author show main intuition behind the latent model and in the following section PRON go deeper into the explanation of how a vae work  PRON think PRON be a good starting point to approach to vae  
__label__deep-learning __label__tensorflow __label__convolution __label__ocr PRON have code in tensorflow use convolution neural network to recognize the character in street view text  svt  datum   since the label type be string  what should PRON use instead of tfnnsparsesoftmaxcrossentropywithlogit   in the loss function   PRON can not use tfnnsparsesoftmaxcrossentropywithlogit   because the label PRON must be an int dtype    the loss function be correct  PRON just need to convert categorical variable into numerical representation use one  hot vector encoding  please take a look at httpscikitlearnorgstablemodulespreprocessinghtmlpreprocessingcategoricalfeature  
__label__machine-learning __label__deep-learning __label__cnn can dropout be apply to convolution layer or just dense layer  if so  should PRON be use after pool or before pool and after apply activation   also PRON want to know whether batch normalization can be use in convolution layer or not   PRON have see here but PRON could not find valuable answer because of lack reference   in short  yes   batch normalization batch normalization layer can be use in between two convolution layer  or between two dense layer  or even between a convolution and a dense layer  the important question be do PRON help  well  PRON be recommend to use bn layer as PRON show improvement generally but the amount of improvement PRON will get be more problem dependent   dropout  convolution layer  in general  be not prone to overfitt but PRON do not mean that PRON should not use dropout  PRON can  but again this be problem dependent  for example  PRON be try to build a network where PRON use dropout in between conv block and PRON model get good with PRON  PRON be good if PRON apply dropout after pool layer  
__label__machine-learning __label__deep-learning when PRON augment datum for training be PRON also change the distribution of datum and if PRON a different distribution why do PRON use PRON to train a model for original distribution   yes PRON do change the distribution of PRON training datum if PRON modify PRON  for example by augment PRON with rotate version of image that be in an original training set of image    this be fine because  typically  PRON goal of training be not to get a model with high performance on the dataset PRON happen to collect as training datum  eg a bunch of natural image   PRON goal be to train a model that generalize well to new datum outside of the training datum s distribution   typically  the training datum be only a sample of the distribution PRON be actually interested in  for example  PRON will be interested in make accurate prediction for all natural image in the entire world  that be a distribution that would likely include rotate variant of all image in PRON training set  so  if PRON augment PRON training set by add such rotate variant  PRON expect to modify PRON training datum distribution in such a way that PRON actually get a little bit closer to the distribution PRON be interested in  all natural image   
__label__neural-network __label__gradient-descent __label__backpropagation PRON know that a gradient checking should be perform as the back propagation implementation be very error prone  that be compute    fracpartial jpartial wi  j    fracjtheta  epsilon   jthetaepsilon2cdot epsilon  so basically PRON perturb each  wi  j and check how j change   this could be even use to compute the gradient  without use bp   give a nn with the follow parameter   m   feature  n   train example  h   neurons in the hidden layer  k   hidden layer  can PRON please PRON give PRON advice on how to compute the complexity  for a single iteration  in the case of bp and in the case of gradient numerically compute   
__label__linear-solver PRON have a large number of system of the form    ax  bi  to solve for a large number of such  bi1leq i leq k  but where  a be fix  a be a rank  p general  ie  non sparse  non psd matrix    PRON can solve PRON individually use an lu decomposition   cost  op3  but be wonder whether there be a  more efficient way to get all of the  k vector of  solution  xi than solve these  k system independently   the typical range of value of  p   k PRON be consider be in the  10  100  typically  PRON be expect  kapprox p    a pointer to a c implementation of whatev method be propose  would also be greatly appreciate   good regard   first  PRON need to use lu factorization for the first system of equation   this be an  thetap3 operation   but since PRON be not change the matrix coefficient and only change the rhs vector  PRON can reuse the factor l and u to  resolve  the new system in  thetap2 operation   algorithm   find the lu factorization of a  for i  1 to n  solve the system  lyi  bi  forward substitution   use the solution  yi to solve the system  uxi  yi  the solution to the system with rhs vector  bi be then  xi  end for  if PRON know all right hand side in advance  PRON can combine PRON into a matrix and solve  ax  b routines for this  in c and fortran  can be find in the lapack library  which represent the state of the art   if  kgg p PRON be probably more efficient  than use paul s standard way of proceeding  to compute the inverse matrix and then multiply each right hand side with PRON  this vectoriz  and parallelize  much good  and the small initial overhead in compute the inverse may be ignore  unlike in the case  kle p  for example  if  k  p then the overhead in arithmetic operation be 50   and the gain from vectorization may not be sufficient to make up for this   if  kltp then PRON may be worthwhile to use one of the  subspace recycling  method PRON get by record the iterate of a krylov subspace method such as gmres  if  kapprox p or  kgtp PRON think PRON should be simple to show that these subspace recycling method will not be effective  
__label__predictive-modeling __label__training __label__model-selection PRON be use the darknet convolutional neural networks to detect people  as in  human  and furniture in a single image   if PRON train the model twice  one for people  one for furniture  PRON seem to get good result by lower the threshold oppose to train the model once on both people and furniture   now PRON question be this  would this be consider an anti  pattern  or be PRON common practice to solve PRON problem like this   
__label__convex-optimization PRON be look for a simple method to find a linear transform that minimiz    textargmint ft   t in mathbbrm time n     t ge 0    t mathbb1   c mathbb1     mathbb1t t  mathbb1     ie over  m time n matric  ge 0  with row sum all the same and column sum all 1    if there be a standard name for this simplex or this problem  please edit     ft be minx f  t x   with  f convex but slow  the gradient at the current minimum should be available    m  20 and  n  100  so the method should be  o  m  n    not  o  mn     perhaps line search that move  clip  t along row  column    the problem come from try to optimize a filter bank aka  overlap basis   another application area with many heuristic be feature reduction  in machine learning    the problem appear relatively simple  PRON have a few linear equality constraint and a few linear inequality constraint  together with a convex objective function  any optimizer should not have great difficulty with this  even the one in matlab  if PRON be look for specific one  PRON believe that lancelot have way that specifically deal with constraint like PRON   in general  PRON would help to know what  ft be  
__label__newton-method be there any reason to always solve the kepler equation for the eccentric anomaly   e  instead of the more meaningful  at least to PRON  true anomaly   theta   solve for the eccentric anomaly usually would mean calculate the true anomaly from PRON solution and maybe even the initial guess of the eccentric anomaly from an initial guess of the true anomaly  so this would require more calculation  however the kepler equation express in the eccentric anomaly be simple and thus less expensive to calculate  when PRON search for solve kepler s equation PRON only have some approach which use the eccentric anomaly  however PRON can imagine that when the error tolerance of the solution be not very tight or the initial guess be good PRON may be cheap to solve for the true anomaly  or be there other reason for solve for the eccentric anomaly   the relevant equation  use newton s method  be      e  2 tan1leftsqrtfrac1e1etanfractheta2right          m  e  e sine       where  m be the mean anomaly and  e be the eccentricity  such that use newton s method result in     en1   en  fracen  e sinen   m1  e cosen     and for the true anomaly      m  2tan1leftsqrtfrac1e1etanfractheta2rightfracesqrt1e2sintheta1ecostheta           thetan1   thetan  left1  e costhetanright2 frac2tan1leftsqrtfrac1e1etanfracthetan2rightfracesqrt1e2sinthetan1ecosthetan    msqrt1  e23     the most robust way of answer this be to benchmark PRON  fail  that  there be several thing to note  roughly in order of  importance    first  the most cheap float  point operation on a modern cpu be  addition and multiplication  both be equally fast  same as fuse  multiply  add when available   division be much slow  by a factor of  20   trigonometric function be also slow  200   square root and  log similar  in fact  sometimes  depend on architecture  trig  function  root  log be implement in a library  the good  reference for this be usually the optimization guide publish by the  whichev company make PRON cpu   so as a rule of thumb PRON want to  minimize special function first  then division  then multiplication  and addition   so by this measure the second formula be much bad  the there be one  inverse trig function  one square root  PRON would be wasteful to  implement PRON formula as write  with three square root   and one  sinco  since PRON be expensive PRON be good to evaluate sinco once  and then write thing like  tanfractheta2  in term of those   PRON  first formula involve the other special function  apart from sinco   only once at the end   second  PRON should not stop at newton s method  for example  if PRON  compare newton s method with halley s method     frace sin  xe x cos  xm1e cos  x    qquad  xfrace cos  x1   e sin  xm  xe2e  m  x  sin  x2 e cos   x1      both evaluate the expensive trigonometric function at the same  argument  but with a little more algebra halley s method can converge  faster  the extra algebra may not outweigh the saving of  evaluate trigonometric few time  so halley s method  and other  iterative method  also need to be check   third  PRON can precompute some thing  for example  if PRON start by  reduce the argument to the range  0ltelt2pi  PRON can experimentally   in advance  find the maximum number of iteration take by the  method  since the number of iteration be fairly small  depend on  precision  and mispredicted branch can be expensive  PRON may also  make sense to unroll the iteration loop by hand to a fix sufficient  number of iteration  unrolled code can also more easily benefit from  vectorization   fourth  PRON be difficult to intuitively predict which optimization  will do the good  and since this be just one fairly simple equation   PRON be probably good to benchmark many different approach and find  the good  when do this  also consider what optimization flag PRON  compiler support for float  point arithmetic  some of PRON be  interesting and important  many people know about ffast  math  but  PRON actually decompose into different helpful or harmful optimization  flag  here be gcc s  list for example   another thing to do be to look at the assembly  output of PRON compiler to see which cpu instruction PRON actually end  up use   fifth  if PRON need to solve this equation many time for the same  eccentricity  e  PRON be possible to rewrite the problem  if PRON  consider the function  e  em  on the range   02pi  give by  the solution of the equation for fix know  e  PRON can approximate  the function  em use  for example  chebyshev series  which take  only a small number of evaluation of  em  which can be do with  any root  find method   once PRON have a sufficiently close  approximation  which may be a 20term chebyshev series or something  like that  PRON can evaluate that later without need to solve the  equation again   this may be of interest to PRON  a paper recently  december 2014  publish present a method for compute solution to kepler s equation analytically   httpwwwscirporgjournalpaperinformationaspxpaperid52772  PRON have not work through the paper PRON  PRON involve a two  dimensional laplace transform   but PRON may be relevant to PRON work   this question now have quite some age  but PRON subject crop up so repeatedly   that perhaps the following answer and reference may still be helpful and not out of place    a    be there a reason to  solve for  the eccentric anomaly   when  the true anomaly be more meaningful     practically all of the numerical difficulty be in  kepler s  equation   ie the part of the calculation for an undisturbed elliptical orbit that start with the mean anomaly and compute the eccentric anomaly  the onward calculation of true anomaly from eccentric  by  contrast  can be simple enough and numerically well  behave when properly  arrange   a better  behave formula than the traditional  tangent half  angle  formula quote in the question be give by r broucke  amp  p cefola  1973     httpadsabsharvardeducgibinnphdataquerybibcode1973cemec7388b    direct series evaluation of true anomaly do exist  of course  but the coefficient be usually laborious to compute  except where that can be do once for all  eg when only a few predetermined eccentricity value will be use   there be also a few iterative procedure that seem to go direct from mean to true anomaly  but PRON do not entirely avoid the eccentric anomaly  and PRON be arguable PRON only mix and mask difficulty without gain simplicity or efficiency   so PRON be usual to tackle separately the kepler  equation part of the problem    b   there be many publish good procedure for solve kepler s equation   thus arguably little point now in devise one afresh  the subject be  entirely 400 year old  but PRON receive renew interest in the 1960 on  when astrodynamic develop increase demand for procedure for orbit  tracking and control  these demand  as intermediate  unattended automatic computation of many thousand of instance of kepler s equation  what emerge then be need not only for computational efficiency  but also for the avoidance of add kind of numerical misbehavior  such as misconvergence or failure to converge from a minority of pathological input value  here be a review and a brief selection of solution from that recent era      there be a good explanatory survey of method up to the mid80  and of the problem to be meet   plus some cod fortran function   in   a w odell  r h gooding   procedure for solving kepler s equation    httpadsabsharvardeducgibinnphdataquerybibcode1986cemec38307o      a very useful paper be that of a nijenhuis  1991    solve kepler s  equation with high efficiency and accuracy    substitute the following bibcode in the url give above  bibcode1991cemda  51  319n    PRON be accompany by a complete and workable implementation  in pascal   from 1991  but easily portable eg to c   method of generate a starting  approximation be select accord to the input value of m  radian  and  e     and a recently  devise cubic  approximation  mikkola  1987  be use  for start value in the specially  problematic area near to the singularity  at  m  e    0  1     another modern method  perhaps even potentially good than the foregoing  be describe  without code example  by f l markley  1995    kepler equation solver   substitute the following bibcode in the url give above  bibcode1995cemda  63  101 m      there be various method that claim to avoid the calculation of trig function  one class avoid PRON at run  time  but the cost be a precomput lookup table  eg s a feinstein  c a mclaughlin   dynamic discretization method for solve kepler s equation    substitute the following bibcode in the url give above  bibcode2006cemda  96  49f    another method ingeniously do avoid compute trig function  PRON achieve that by evaluate sin e  via sin e3  rather than e PRON   s mikkola   a cubic approximation for kepler s equation     substitute the following bibcode in the url give above  bibcode1987cemec  40  329 m    hopefully there be something here to respond to most variant of this oft  recur question  
__label__deep-learning __label__tensorflow when run the object detection tutorial  PRON can use trainpy which be supply  on the console  the follow be print   info  tensorflow  global step 1595  loss  25473  46045 sec  step   info  tensorflow  global step 1596  loss  29730  37213 sec  step   info  tensorflow  global step 1597  loss  24274  26814 sec  step   info  tensorflow  record summary at step 1597   info  tensorflow  global step 1598  loss  22615  38410 sec  step   info  tensorflow  global step 1599  loss  25232  26713 sec  step   info  tensorflow  global step 1600  loss  23982  28633 sec  step   info  tensorflow  global step 1601  loss  24807  29261 sec  step   info  tensorflow  record summary at step 1601   this information be also display on tensorboard while monitor the performance of training   at first glance  PRON believe that one step be one epoch  but apparently this be not true   what exactly be a step in tensorflow  tensorboard   example come from here   
__label__machine-learning __label__classification __label__naive-bayes-classifier __label__training what should be the min or max size of train dataset should be use to feed a classifier  can PRON use 1 gb or more datum as train datum to feed a classifier for jvm related ml framework   instead of think PRON in term of gb  unless PRON be ask about the physical limit of jvm  PRON suggest approach towards this question would be to plot the cross  validate accuracy vs the number of row use in the model dataset through stratified sampling  PRON may observe a point of diminish return where additional row do not contribute towards high accuracy   in addition  breadth  wise in term of the number of variable use  PRON could plot a variable importance plot and plot the cross  validate accuracy vs the top x important variable  in this case  PRON may even observe that accuracy increase when PRON use few variable as the noisy variable be discard  PRON could then select the top x variable that give PRON the high cross  validate accuracy  
__label__neural-networks __label__image-recognition PRON be wonder how feasible PRON be to create a machine that can separate clothing from a basket   at the most basic level PRON would distinguish between top  pant  button down and sock  programmatically  PRON would image this would require train a neural network to recognize these item  but in real time PRON become exponentially difficult to do this in a small space at a fast rate   pick up an item  lie PRON in such a way that be recognizable  deduce whether PRON be a top  button down  etc   sort PRON accordingly  if this sound ridiculous please let PRON know   if PRON be possible   would this be base on some sort of computer vision   or only a well train neural network   any insight be much appreciate   peter abbeel do work in deep learning for robotic  and one of the project PRON have tackle be manipulate clothe  here be a video from 2011 of PRON robot fold laundry  one piece at a time    there be also company attempt to market this  seven dreamer make the laundroid and foldimate claim that PRON will start take pre  order in 2017  
__label__neural-networks PRON have be read a lot about td  gammon recently as PRON be explore option for ai in a video  game PRON be make  the video game be a turn  base positional sort of game  ie a  unit   or game piece s  position will greatly impact PRON be usefulness in that board state   to work PRON way towards this  PRON think PRON prudent to implement a neural network for a few different game first   the idea PRON like be encode the board state for the neural network with a single output neuron which give that board state relative strength compare to other board state  as PRON understand  this be how td  gammon work   however  when PRON look at other people s code and example  tutorial  there seem to be a lot of variance in the way PRON represent the board  state  even for something as simple as tic  tac  toe   so  specifically for tic  tac  toe  which be a good  or what be the correct representation for the  board state  PRON have see   9 input neuron  one for each square  a 0 indicate a free  space  1 the opponent and 1 PRON   9 input neuron  but use different value such as 0 for the opponent  05 for free and 1 for PRON   could PRON use large value  like 0  1 and 2   27 input neuron  the first 3 be square 1  the next 3 be square 2 etc  every neuron be 1 or 0  the first of the set of three indicate whether this square be free or not  the second indicate whether the square be occupy by PRON opponent or not  in the end  only one in every 3 neuron will have a 1  the other two will have a 0   18 input neuron  the first be 1 for the x player  the second be 1 for the o player and both be 0 for a blank  then  when branch into game where the specific piece ability come into play  like in chess  how would PRON represent this   would PRON be as simple as use high input value for more valuable piece  ie 20 for an opponent queen and  20 for PRON own queen  or would PRON need something more complex where PRON define 10  value for each square  one for each unit  type and player combination   when PRON be work with neural network  as long as the data be there  the network be usually able to learn how to process PRON into a useful result  PRON usually also want to keep the amount of weight to a minimum  when PRON PRON use extra weight  PRON will take longer to train the network because PRON need to tune even more value for an optimal network  so  for tic  tac  toe  any of PRON solution involve 9 input should work just fine  also  PRON help if PRON keep the input between 0 and 1 if PRON be use log sigmoid  and 1 and 1 if PRON be use hyperbolic tangent for PRON activation function  PRON can probably easily figure out what to use for other activation function  PRON can take PRON datum and transform PRON into another dataset with value within a specific range through a process call range normalization  for chess  PRON can simply encode every piece in several different way  and PRON will probably not make that much of a difference  the general rule of thumb be PRON want to minimize the amount of weight while still give the most possible variable to the network   the basis of reinforcement learning method be to give each  game  state  or action  a value that somehow represent how good that state  or action  be  to store these value PRON could use something as simple as a table  hashmap  however complex game like chess or go have so many state PRON can not fit into the memory  as a remedy PRON think of the hashmap as a function and try to approximate PRON with a neural networknn   luckily nn be universal approximator  which mean PRON can learn any function  include an arbitrary mapping from a chess board to a number   now the question be how to represent a game board and feed PRON to the neural network  in the case of tic  tac  toe all 5 method PRON list can be consider correct   theoretically PRON do not matter what  w  b  v  number be assign to  white  black or vacant tile   if PRON teach the nn that fw  w  v     1 enough time  PRON will learn this association whether PRON be  w  b  v    0  1  1  or  w  b  v    0  05  1    now PRON first three example use this method  however a small flaw here be that PRON assign number to nominal thing  that be number can be order  yet PRON can not really say that black  gt  white  gt  vacant  PRON last two example try to fix this by use one  hot  vector   so for a game like chess  if PRON use number simply to represent the figure the nn may mistakenly mix up two figure type  eg   pawn be 19  queen be 20 and PRON think that a queen be try to attack PRON king whereas PRON be just a pawn  and make a bad decision  however PRON will learn that the decision be bad and will assign the correct value to the state and the decision in the long run   one last note  choose the correct state representation for a problem be a crucial part of reinforcement learning  similar to pick the right feature for a classification problem  and sometimes one may be too afraid to pick a very high dimensional state space  but remember  that chess be not a simple game so a large state space may not be unreasonable  also for reference  atari game be train with an input dimension of 84  84  4  
__label__human-like since PRON project be go to be of a purely fictional nature  PRON be not sure PRON pick the right forum for this  if not  PRON apologize and will gladly take this to where PRON point PRON to   the premise  a full  fledged self  aware artificial intelligence may have come to exist in a distribute environment like the internet  the possible ai in question may be quite unwilling to reveal PRON   the question  give a first initial suspicion  how would one go about to try and detect PRON presence  be there any scientifically viable way to probe for the presence of such an entity   in other word  how would the turing police find out whether or not there be anything out there worth police     a full  fledged self  aware artificial intelligence may have come to exist in a distribute environment like the internet   the question imply that this artificial intelligence have surpass human intelligence  full fledged  and therefore  due to the concept of the intelligence explosion result from such a state  the ai PRON be look for be no doubt superintelligent   the question state that this ai be   unwilling to reveal PRON   and therefore do not intend to be discover   strong  or superintelligent  ai  give these two factor  PRON superior intellect and PRON unwillingness to be discover   PRON can conclude that there be no way in which PRON would be able to detect such an ai under conventional condition   a possible solution may involve employ a second superintelligent ai system  but this be precarious in more way than one   weak ai  detection of a simple ai would rely on track the pattern of activity and the trace PRON leave behind in this distribute network in order to find PRON  then subject PRON to some form of testing to verify PRON intelligence   there be a very large number of possible indicator and these would vary widely depend on the specific ai concern  this would depend especially on how the ai exist within the framework  here  the web    note  a superintelligent ai would not only be able to disguise PRON activity  but also find a way around any test PRON can develop  these ability be what render this level of ai perhaps the great threat to mankind  but also PRON great asset if PRON do develop PRON and find a way to gain PRON alliance   this question should probably be move to worldbuildingstackexchange …  that be say  in the context of a story  PRON would look at something like the neural correlate of consciousness  in this book by stanislas dehaene the experiment be describe that lead to the realisation that conscious perception require information integration in certain part of the prefrontal cortex  basically the brain process many different interpretation of a percept in parallel  to become conscious of the percept this interpretation have to be collapse into a single  truth  which be propagate to a specific part of the brain   in PRON story PRON could have a researcher of consciousness stumble upon a chart of information flow in a part of the internet and realise that the same kind of information integration be go on  
__label__libraries __label__fourier-analysis PRON be look for reasonably fast implementation of the discrete fourier transform  dft  on a 2d triangular or hexagonal lattice   PRON would appreciate pointer to such implementation  especially one easily usable from python or mathematica   and also to description of how to reduce this problem to the 1d dft  which be already build into many system   there be several paper by markus püschel on PRON web site here that discuss cooley  tukey  like  so PRON be guess  fast   algorithm for lattice transform  such as dft on triangular and hexagonal 2d lattice  in the triangular case  PRON call the dft the discrete triangle transform  dtt   markus have a code call spiral that automatically generate code for transform  but PRON appear that this dtt work be not part of spiral  and there be no implementation on PRON web site that PRON can find  PRON be begin to think that jm be right and that PRON may need to roll PRON own implementation   one thing that the abstract note be that for 2d triangular and hexagonal lattice  the transform be not separable into 1d component  so PRON will not be able to reduce the problem to two 1d transform  
__label__ethics __label__decision-theory __label__robots would PRON be ethical to implement ai for self  defence for public walking robot which be expose to danger such as violence and crime such as robbery  of part   damage or abduction   what would be pro and con of such ai behavior  be PRON realistic  or PRON will not be take into account for some obvious reason   like push back somebody when somebody start push PRON first  ai will say  PRON push PRON first   or run away on crowded street in case algorithm will detect risk of abduction   the question mention  walk robot   but PRON may be illustrative to re  frame the discussion in term of self  drive car  because   PRON give a common point of reference  rather than everyone have PRON own separate vision of how vulnerable  powerful a kung  fu walking robot may be   PRON already know a lot about societal attitude to car theft   give that autonomous vehicle will soon be mainstream  the morality of the question be then more of a pressing issue   so  should a self  drive car run someone over  likely kill PRON  if PRON try to steal PRON  PRON be hop that few people would argue that PRON should   should PRON attempt to do a less amount of damage  say  calculate to hopefully only break a leg    again  PRON would argue not  the main reason for say this be that PRON decision  make algorithm be simply not sufficiently context aware to be able to decide whether theft or harm be the intent  to concretely illustrate this  a recent fatality arise because a self  drive tesla be oblivious to context to the extent that PRON could not distinguish between a high  sided van and empty space   under those circumstance  PRON be probably good not to allow commercial autonomous system to because physical damage  even to inanimate object     run away   or rather   drive away   in the case of the car  be another matter  driving be what PRON be design to do   PRON depend on whether the loss of the robot would end up cause harm to human   if the robot be suppose to be watch for a suspect terrorist attack to start take place  so PRON could alert authority or halt the attack   PRON would be very bad if somebody dismantle the robot or otherwise stop PRON from carry out PRON mission  in that case  the device would be certainly justify in stop human from injure PRON in any meaningful way   a robot carry classified information should probably be similarly willing to protect PRON  since the spread of such datum could bring harm to a state or a lot of people   if an ai  enable device be just walk the street in the course of carry out some mundane task  PRON think PRON would be hard to justify allow the robot to incapacitate a human attacker  after all  PRON be make  presumably  to serve human   no matter whether the ai be program to defend PRON  people could not just impede or damage PRON with impunity  intentional destruction of another person s property  include public property  be almost certainly a crime  as be intentional obstruction of law enforcement  PRON would not have to be up to each robot to defend PRON  PRON could just send information about the perpetrator to campc before PRON demise  
__label__optimization __label__convex-optimization PRON be approximate the expression  leftax  bright1  by the expression    textminimizesumisqrtaitx  bi2varepsilon  where  ai be the  ith row of  a   this function be convex  and PRON be wonder if there be a way to convert PRON into an socp  or if PRON be hopelessly nonlinear   the original 1norm minimization problem can easily be convert to an lp  socp  use standard technique   PRON be quite odd to convert the smoothed problem into an socp  but PRON could do this as follow   let   uiaitx  bi    sqrtepsilont  and   ti  geq  ui  2    then    min sumi  ti 
__label__natural-language PRON seem that most project attempt to teach the ai to learn individual  specific language   PRON occur to PRON that there be relation in write and speak word and phrase across language  most of use have a much easy time learn more language after PRON learn a second language  and PRON start to understand the relation between word and phrase in different language   have anyone attempt to train an ai to learn all language   would not this potentially be a much simple problem than try to teach an ai a single  specific language with all of the specific and detail of that single language  since PRON be actually omit a lot of related datum in other language from the training set   there be approach in machine translation that try to capture this kind of synergy between language  the idea be that if PRON train PRON architecture to be able to translate english  japanese  japanese  english  korean  english  english  korean PRON will also be able to translate from japanese to korean without ever have see a single such training example  here  PRON can read about this so  call zero  shot translation   PRON be also possible to train wordvector on several language at once  which may give PRON good wordvector for a language with few training example   of course for true language understanding PRON have to solve the ground problem and PRON be not sure use several language will help with that  
__label__matlab the issue PRON have be have to compute the derivative  in real time  of the solution produce by ode45 within the event function   some pseudo  code to explain what PRON be mean be   function dx  myfunct  x  xevent   persistent xevent   xevent be the solution at the event  dx1   x2    dx2   complicated function of x1x2   and xevent   end  function  value  isterminal  direction   myeventfcnt  x   position  function of x1   x2   and dx2    isterminal  1   direction  0   end  PRON know that if PRON do not need to use the solution at the event within myfunc PRON could just compute dx  myfunct  x  within PRON event function and use dx2   yet since xevent be use within myfunc PRON can not input xevent   PRON know there be a way of inputt constant parameter within the event function  but since PRON be the solution at the event location  which also change  PRON be not sure how to go about do that   PRON work around to this be to approximate dx2  use the solution x what PRON want to know be if PRON will be satisfactory to just use a finite difference approximation here  use a small fix step size relative to the step size od45 take  which be a variable step size   thank for any help   
__label__finite-element __label__helmholtz-equation PRON be try to solve    nabla2u  ku  ftext  in  omega    u0 text  on  partialomega  where  omega be a unit square and  k  f have real and imaginary component as  k  krkij and  fx  yfrx  y   jcdot fix  y  respectively   PRON approach to solve this problem be to assume the solution have both a real and imaginary component as well    u  ur  jui  substitute this int the pde above  PRON get     nabla2ur  jnabla2ui  krur  kiuijkiurkrui   frjfi  equating real with real and imaginary with imaginary  PRON obtain a system of two couple pde s     nabla2ur   krur  kiui   fr    nabla2ui   kiurkrui   fi  multipl each equation by a separate test function  vr   vi  respectively  and integrate to get the weak form  PRON get    nabla urnabla vr   krur  vrkiui  vr   intomega frvr    nabla uinabla vi   kiur  vikrui  vi   intomega fivi  add the two equation PRON get a single equation     nabla urnabla vr   krur  vrkiui  vr   nabla uinabla vi   kiur  vikrui  vi   intomega frvrintomega fivi  PRON then choose a finite dimensional space of piecewise polynomial function v such that  ur  vr  ui  viin v  that be  PRON choose all trial and test function to come from the same space   PRON have try use method of manufactured solutions to test PRON implementation in two case   in case 1  PRON assume that the solution be real and of the form  u  sinpi xsinpi y  which yield a source term  f   2pi2  2jsinpi xsinpi y  in case 2  PRON assume the solution have both real and imaginary part and be of the form  u  sinpi xsinpi y   j sinpi xsinpi y  which yield a source term  f   2pi2  2   j2pi2  2   sinpi xsinpi y  for case 1  the error decrease as PRON refine both the mesh size and the polynomial order  as expect   however  in case 2  PRON do not get convergence at all   clearly  PRON could be a programming error  but PRON want to make sure that PRON approach be sound   in particular   be PRON ok to separate the complex equation into a system of real pde s   be PRON wrong for multiply real test function into the separate equation   should PRON have multiply the complex equation by a complex test function of the form  vrjvi   be PRON ok to add the two weak form together to come up with a single variational expression   PRON have see this do when use mixed finite element space for the stoke equation  but PRON be not entirely sure that this be ok for this problem   any help would be greatly appreciate   let  a and  b be real matrix  and  b and  c be real vector  then if PRON define  z as the complex solution to a complex system of equation      ajbz   bjc  then  z be equivalently give as  z  xjy  where    beginbmatrixa  amp  b  b  amp  aendbmatrixbeginbmatrixx  yendbmatrix   beginbmatrixb  cendbmatrix  these simple fact extend to linear operator equation  that is  suppose that  ajb be the complex helmholtz differential operator that PRON be try to invert  then PRON can follow these identical step to obtain two set of real  couple operator equation   to answer PRON original equation   yes  this be simply the matrix representation of complex number extend to linear operator   this step be fine  because complex testing  basis function also have real representation  and can be split in the same way that the equation PRON be split   this be where PRON have make a mistake  to give a finite  dimensional analogy  PRON have effectively collapse the 2x2 system show above  with  2n real equation and  2n real unknown  into   abxa  by  bc  which be an underdetermined system with  n real equation and  2n real unknown  PRON then make this problem well  define by set  y0 so PRON be really solve   abx   bc  and this be right only when  b0  and  c0  if PRON remove that last step where PRON have add the two equation together  the system should converge for all complex case  
__label__xgboost so from algorithm 3 of httpsarxivorgpdf160302754v3pdf  PRON say that an optimum default direction be determine and the miss value will go in that direction  however  or perhaps PRON have misunderstood  miss the explanation from the article  PRON do not say what exactly be the input   for example  PRON have  parent  node a  with 50 input  splitting into node b and node c now  say of the 50 input there be 7 miss value   the other 43 input be split into b and c accordingly  what PRON seem to be understand be that  PRON will allocate the remain 7 into b and c and determine which one give a high gain score  that will be the optimal direction   however  give the 7 value be miss  which mean PRON do not know what be these 7 value   how do allocate miss value into any of the child node change the gain score  or rather minimize the loss function  this seem to suggest that xgboost be inputt something for the miss value  PRON can not seem to find out what be xgboost inputting for these miss value  PRON hope this question be not too vague  general and easy   edit  PRON think  miss value  may be a vague term  what PRON mean here be  from wiki   in statistic  miss datum  or miss value  occur when no data value be store for the variable in an observation    from the author PRON  httpsgithubcomdmlcxgboostissues21   PRON say   tqchen comment on aug 13  2014  xgboost naturally accept sparse feature format  PRON can directly feed datum in as sparse matrix  and only contain non  miss value   ie feature that be not present in the sparse feature matrix be treat as  miss   xgboost will handle PRON internally and PRON do not need to do anything on PRON    and    tqchen comment on aug 13  2014  internally  xgboost will automatically learn what be the good direction to go when a value be miss  equivalently  this can be view as automatically  learn  what be the good imputation value for miss value base on reduction on training loss    the procedure be describe in PRON paper  section 34  sparsity aware split  finding   assume PRON be at PRON node with 50 observation and  for the sake of simplicity  that there be only one split point possible   for example  PRON have only one binary feature  x  and PRON datum can be split in three group   group  b  20 observation such that  x  b   group  c  20 observation such that  x  c   group  m  10 observation such that  x  the algorithm will split base on  x  but do not know where to send the group  m PRON will try both assignment      b   m   c qquad textand  qquad b   c  m  compute the value to be assign to the prediction at each node use all the datum  and choose the assignment that minimize the loss   for example  if the split   b   m   c be choose  the value of the left node will have be compute from all the  b and  m sample   this be what be mean by  automatically  learn  what be the good imputation value for miss value base on reduction on training loss   comment question   how can group m be calculate if PRON do not know what be the value for PRON   the value PRON compute be not base on the feature  x  which PRON do not know  but base on the know label  target of the sample  let say PRON be do a regression and try to minimize the mean square error  and let say PRON have the following   for group  b  the mean of the target be  5  for group  c  the mean of the target be  10  for the miss value group  m  the mean of the target be  0  note that even if the value of  x be miss  PRON know the value of target of the sample  otherwise PRON could not use PRON to train PRON model   in this case  the split would be make on   b  m   c  the value assign to the right node contain sample  c would be  10   and the value assign to the left node contain sample in   b  m would be the mean of the target for the whole group  in PRON example  the mean would be    fracmb  mtextmeanm   fracbmbtextmeanb   frac10300  frac20305  3overline3 
__label__correlation specifically  PRON be wonder if PRON can determine this when each factor explain only one of the variable well   here be an illustration of a factor pattern to clarify   be PRON possible to say that variable 1 and variable 3 be positively correlate  if not  what other information would be need   
__label__linear-algebra __label__approximation-algorithms PRON have a linear system of equation of size mxm  where m be large   however  the variable that PRON be interested in be just the first n variable  n be small compare to m    be there a way PRON can approximate the solution for the first m value without have to solve the entire system   if so  would this approximation be fast than solve the full linear system   the long answer be  sort of   PRON can re  arrange PRON system of equation such that the farth right  k column be the variable which PRON wish to solve for   step 1  perform gaussian elimination so that the matrix be upper triangular   step 2  solve by back substitution for only the first  last   k variable which PRON be interested in  this will save PRON the computational complexity of have to solve for the last  n  k variable via back  substitution  which could be worth PRON if  n be as large as PRON say  keep in mind that a fair amount of work will still have to be do for step 1   also  keep in mind that restrict the order in which PRON be go to perform back  substituion may restrict the form of the matrix  PRON take away the ability to exchange column  which could possibly lead to an ill condition system  but PRON be not sure about that  just something to keep in mind   form the schur complement  suppose that PRON have permute and partition PRON matrix into the form    aleftbeginarraycca11   amp  a12   a21   amp  a22endarrayright  such that  a22 contain PRON degree of freedom of interest and be much small than  a11  then one can form the schur complement    s22    a22   a21  a111  a12  either through a partial right  look lu factorization or the explicit formula  and then  s22 can be understand in the following sense     s22  x  y rightarrow  leftbeginarraycca11   amp  a12 a21   amp  a22endarrayright  leftbeginarraycstar xendarrayrightleftbeginarrayc0 yendarrayright  where  star represent the  uninteresting  portion of the solution  thus  provide a right  hand side which be only nonzero in the degree of freedom of the schur complement  s22  PRON need only solve against  s22 in order to get the portion of the solution correspond to those degree of freedom   computational complexity in unstructured dense case  set  n to the height of  a and  n to the height of  a22  then the standard method for compute  s22 be to first factor  l11  u11    a11  let PRON ignore pivot for now  in roughly  23  n  n3  work  then to form    s22    a22    a21  u111l111  a12    a22   a21  a111  a12  use two triangle solf require  nn  n2  work each  and then perform the update to  a22 in  2n2  n  n work   thus  the total work be roughly  23  n  n3  2nn  n2  2n2  n  n when  n be very small   n  n approx n  so the cost can be see to be roughly  23 n3   which be the cost of a full factorization   the benefit be that  if there be a very large number of right  hand side to be solve with the same system of equation  then  s22 could potentially be reuse a large number of time  where each solve would only require  2n2  work  rather than  2n2  work  if  s22 be factor   computational complexity in the  typical  sparse case  if PRON sparse system arise from some type of finite  difference or finite  element approximation  then sparse  direct solver will almost certainly be able to exploit some of the structure  2d system can be solve with  on32 work and  on log n storage  while 3d system can be solve with  on2 work and  on43 storage  the factored system can then be solve with the same amount of work as the storage requirement   the point of bring up the computational complexity be that  if  n approx sqrtn and PRON have a 2d system  then since the schur complement will likely be dense  the solve complexity give the factored schur complement will be  on2   on  which be only miss a logarithmic factor versus solve the full system  in 3d  PRON require  on work instead of  on43  PRON be thus important to keep in mind that  in PRON case where  nsqrtn  there will only be significant saving if PRON be work in several dimension and have many right  hand side to solve   the model reduction approach  since paul ask  PRON will talk about what happen if PRON use projection  base model reduction method on this problem  suppose that PRON could come up with a projector  mathbfp such that the range of  mathbfp  denote  mathcalrmathbfp  contain the solution to PRON linear system  mathbfax   mathbfb  and have dimension  k  where  k be the number of unknown for which PRON wish to solve in a linear system   a singular value decomposition of  mathbfp will yield the follow partitioned matrix     mathbfp   left  beginarrayccmathbfv   amp   endarray  rightleftbeginarrayccmathrmdiagmathbf1k    amp  mathbf0   mathbf0   amp   mathbf0endarrayrightleftbeginarrayc  mathbfwt    endarrayright  the matrix obscure by star matter for other thing  like estimate error  etc    but for now  PRON will avoid deal with extraneous detail  PRON follow that    mathbfp   mathbfvwt  be a full rank decomposition of  mathbfp  essentially  PRON will solve the system    mathbfpax   mathbfpb  in a clever way  because  mathbfv and  mathbfw also have the property that  mathbfwtmathbfv   mathbfi multiply both side of  mathbfpax   mathbfpb by  mathbfwt and let  mathbfy   mathbfvwidehatmathbfx be an approximation for  mathbfx yield    mathbfwtmathbfawidehatmathbfx    mathbfwtmathbfb  solve for  widehatmathbfx  premultiply PRON by  mathbfv  and PRON have  mathbfy  PRON approximation for  mathbfx  why the schur complement approach be probably good  for starter  PRON have to pick  mathbfp somehow  if the solution to  mathbfax   mathbfb be in  mathcalrmathbfp  then  mathbfy   mathbfx  and  mathbfy be not an approximation  otherwise   mathbfy  neq mathbfx  and PRON introduce some approximation error  this approach do not really leverage at all the structure PRON mention want to exploit  if PRON pick  mathbfp such that PRON range be the standard unit basis in the coordinate of  mathbfx PRON want to calculate  the correspond coordinate of  mathbfy will have error in PRON  PRON be not clear how PRON would want to pick  mathbfp PRON could use an svd of  mathbfa  for instance  and select  mathbfp to be the product of the first  k leave singular vector of  mathbfa and the adjoint of the first  k right singular vector of  mathbfa  assume that singular vector be arrange in decrease order of singular value  this choice of projector would be equivalent to perform proper orthogonal decomposition on  mathbfa  and PRON would minimize the l2error in the approximate solution   in addition to introduce approximation error  this approach also introduce three extra matrix multiplie on top of the linear solve of the small system and the work need to calculate  mathbfv  and  mathbfw unless PRON be solve the same linear system a lot  only change the right hand side  and  mathbfp be still a  good  projection matrix for all of those system  those extra cost will probably make solve the reduced system more expensive than solve PRON original system   the drawback be much like jackpoulson s approach  except that PRON be not quite leverage the structure that PRON mention   as other have point out  this be difficult to do with a direct solver  that say  PRON be not that hard to do with iterative solver  to this end  note that most iterative solver in one way or another minimize the error with respect to some norm  oftentimes  this norm be either induce by the matrix PRON  but sometimes PRON be also just the l2 vector norm  but that do not have to be the case  PRON can choose which norm PRON want to minimize the error  or residual  in  and PRON could  for example  choose a norm in which PRON weigh the component PRON care about with 1 and all other with 1e12  ie for example something like    x 2    sumi15 xi2    1e24sumi6n xi2   and correspond scalar product  then write all step of the iterative solver with respect  to this norm and scalar product  and PRON get an iterative solver that pay significantly  more attention to the vector element PRON care about than to the other   the question of course be whether PRON need few iteration than with the norm  scalar product that weigh all component equally  but that should indeed be the case  let PRON  say PRON only care about the five first vector element  then PRON should need at most  five iteration to reduce the error by a factor of 1e12 since five iteration be what  be need for the 5x5 system that describe PRON  that be not a proof but PRON be pretty certain that PRON should indeed get away with a far small number of iteration if the weight in the norm  1e12 above  be small than the tolerance with which PRON want to solve the linear system iteratively  
__label__machine-learning PRON be wonder whether PRON could list machine learn win method to apply in many field of interest  nlp  image  vision  medical  deep package inspection  etc  PRON mean  if someone will get start a new ml project  what be the ml method that can not be forget   the question be very general  however  there be some study be conduct to test which algorithm perform relatively well in a broad range of problem  PRON will add link to paper later   concern regression and classification   lately random decision forests  support vector machines and certain variation of neural networks be be say to achieve the good result for very broad variety of problem   this do not mean that these be  the good algorithm  for any problem  that do not exist  and actually be not very realistic to pursue  also PRON must be observe that both rdf and svm be rather  easy method to initially grasp and obtain good result  so PRON be become really popular  nn have be use intensively since couple of decade  after PRON revive   so PRON appear often in implementation   if PRON be interested in learn further PRON should look for an specific area and deal with a problem that can be solve nicely by machine learn to understand the main idea  and why be impossible to find  the method    PRON will find common the task to try to predict the expected behavior of something give some known or observable characteristic  to learn the function that model the problem give input datum   the issue relate to deal with datum in high  dimensional space  the need for good quality datum  the notable improvement that can give datum pre  processing  and many other  
__label__machine-learning __label__gaussian PRON be currently take PRON first step in machine learning use andrew ng s  coursera course  the follow octave  matlab script optimize a svm use a give kernel function on training set x  y   svmtrain  at the bottom PRON return some value alpha which seem to be correlate to the weight  vector w these alpha be later use to predict  the hypothesis   if the svm be train base on the gaussian kernel  should not the prediction of y on the cross  validation set be independent of what kernel PRON use in the first place  be not the hypthesis just wxgt0 gty1 and wxlt0  gty0 base on the train w  PRON have a hard time grasp what thos alpha value mean   thank in advance   
__label__natural-language __label__unsupervised-learning while conduct research  PRON recently stumble upon the deep learning and natural language processing concept  in this question PRON say that the ‘ grammar induction’ be a  supervised learning’ mode  so PRON be wonder   let PRON say that there ’ a way  more  than  human intelligent alien probe orbit PRON planet  PRON can receive  decode and analyze all the broadcasting signal leave the earth  for all PRON know at this moment  how could PRON learn the basic of a language with nothing else but PRON broadcasting signal  so without the help from a ‘ supervisor’  how would an artificial intelligence human engineer  theoretically   face the problem   PRON be interested on the more  technical  side of the issue   PRON think PRON understand where PRON confusion come from  and the reason for PRON question   when PRON talk about a supervised problem  PRON mean that there be some feedback of how well the machine do at a certain task  of course  PRON always need this kind of feedback in some way or another  otherwise  the machine would never learn  because PRON would have no incentive to change anything that PRON do   but the feedback can be classify into  supervised  if PRON have a clear way of tell when something be right  wrong  or  unsupervised  if PRON just roll out with the result and try to make PRON better each time   for example  an ai perform translation like in grammar induction in the other question will take as input both the phrase and the syntactic tree  after perform the prediction of what the syntactic tree for the phrase should be  PRON can be compare against the real syntactic tree give as input and base on the accuracy of the prediction  the weight  think of PRON as  knob  that adjust the result  can be tweak a little bit to give good prediction next time   this type of learning be consider supervised  not because of the existence of a supervisor  but rather because there be label datum that PRON can use to test prediction and good PRON next time   unsupervised problem do not have this kind of label datum  and just work with what PRON have  PRON can not tell what be right and what be wrong in term of prediction  PRON only have raw datum and try to make sense out of PRON by extract correlation or common property   for the case of a language  PRON will go on a limb here but PRON will say that this be mostly likely a task for supervised algorithm  unsupervised technique could properly analyze language and determine that PRON actually have a particular structure  but this be also true for lot of other source of datum that be not language  an alien race  know nothing about humanity  could end up deduct that car honk be some sort of language too  albeit much simple than human  produce noise   also  the natural flexibility that language have make PRON extra complex  because PRON do not follow a correct structure completely  and to make PRON bad  PRON be constantly change   as leitasat mention in PRON comment  if even human scientist  which PRON certainly deem intelligent  could not decipher ancient egyptian on PRON own without any context  PRON be very unlikely that a machine would do PRON   finally  notice that translator machine or inference machine do not actually derive meaning in a way that be meaningful  without go much in much detail  PRON should think of PRON as  correlation detection machine   so a machine may notice a high enough correlation between how  hola  be use in spanish as to how  hi  be use in english  and by give PRON that information  PRON call PRON a translator  however  the internal learnt structure be actually just a probability distribution for an output give a set of input   not to say that any of this be not useful  PRON definitely be  but with something as separate as an alien race of super  human intelligence and without any way to derive meaning from just a bunch of sound  PRON be very unlikely that a machine would prove useful in such correlation   and still  PRON can not find any hard stop to PRON idea  if these alien have a language and PRON express the same concept that human express  then there be correlation  there just need to be a way to find PRON   sorry for the wall of text and the huge rambling  PRON hope PRON provide some context on the point PRON be look clarification for   PRON be hard to imagine whether an alien specie will reason similarly to mankind and PRON be consiousness similarily process the world PRON be throw into with the same vocation or whether PRON interpretation be limit to PRON human nature or if PRON be an absolute determination of interpretation and alien will fall into a similar consequence of this absolute  the progress and accumulation by this consiousness be a function of the length of time PRON be be subject to and the share knowledge through PRON experience be fundamental to the civilization PRON be a part of and PRON age  if this absolute be a function of the universe PRON may share  then PRON think an alien specie that PRON share PRON with would be able to interpret communication from PRON because PRON share a similar mental progression as PRON do and PRON be conclusion would mirror our dependent upon the length of time of there civilization and depth of knowledge that PRON be aquir be equitable too or great than PRON  
__label__neural-network PRON be use an elman neural network in r to train the datum   while the iterative error of any neural network model go on decrease constantly until convergence  why be PRON elman model show haphazard trend in iterative error  after apply the elman neural network in r  the graph of iterative error show   plotiterativeerrorelmanmodel   the graph show   what be the weighted sse  why be PRON not decrease constantly   how do PRON find rmse of each iteration of the elman model   how do PRON plot the iterative rmseroot mean square error  of each iteration of the elman neural network against the number of the iteration   ie rmse on y  axis and number of the iteration on x  axis    
__label__neural-network __label__deep-learning be very new to neural net PRON have a question regard utility   suppose PRON want to learn the function  fx   4x  2 this be a very simple function of course  but PRON have discrete weight namely the pair   42 now further suppose that PRON have a lot of datum that be scatter approximately accord to this line  to within some reasonable margin of error   can PRON  use some neural network architecture  learn the function  fx   note PRON do want to learn exactly the weight   42 and not approximation to PRON   PRON real problem obviously be much more complicated  but still have discrete weight  but far far more numerous in dimensionality   thus  PRON wonder where to start from a nn point of view   thank  
__label__stochastic __label__probability PRON be currently write PRON master thesis and PRON topic also touch on stochastic tomography for volume reconstruction present in this  paper  now i understand most of the process describe  but i just do not understand one aspect of calculate the objective function  on page 4 under  evaluate the objective function  the author mention   this be essentially a density estimation  problem in 2d  where new sample need to be add on the  fly  a suitable algorithm be kernel density estimation  parzen 1962    also know as splatt  how be the kde use for calculate the new residual value here  be the input value for the kde calculation actually all sample emissitvitie which be already incorporate into the residual image at the pixel position the sample get project to  do someone know how PRON be use here  sadly the author do not get any more specific about that   thank PRON    how be the kde use for calculate the new residual value here   assume that PRON have already process  n sample  x1x2ldot  xn in mathbbr3 then the residual image  rj  mathbbr2 rightarrow mathbbr   j12ldots  take the form    rjx   mjx   sumi1n ei kx  projjxi  where  mj be the  jth projection image   where  ei in ed   ed be the emissivity for the  ithe sample  xi   where  k be the kernel PRON have choose   and where  projj be the coordinate of the projection of  xi into the  jth projection image   to incorporate a new sample  xn1  define new residual image    widetilderjpmx    rjx    pm ed  kx  projjxn1  for positive and negative emissivity   be the input value for the kde calculation actually all sample emissitvitie which be already incorporate into the residual image at the pixel position the sample get project to   in the end  the kde calculation depend on all sample emissivitie  yes  
__label__matrices __label__random __label__projection PRON be perform series of random projection ie project the input matrix onto randomly generate orthonormal base  of much low dimensionality   the projection be just a matrix multiplication like  a cdot b  p  where  b be the basis compose of several mutually orthogonal column vector of unit length   PRON have come to conclusion that there be no need to generate new basis before each projection because simple permutation of PRON row should be sufficient to keep the randomness of the whole process  if this be true  do so would be very profitable in term of computational complexity  since row  permutation also keep orthonormality of the basis   the problem be that PRON be a programmer and no matematician  so PRON be not sure if indeed projection over such base be random and thus independent of each other  or PRON be somehow similar and thus limit  be PRON idea right or wrong  and why   edit   the matrix  a consist of  n row vector of binary value   0  or  1    where the vector dimension be close to  n which be between  10  5  and  10  7  the process of repeat random projection aim to find well  clusterable projection  or at least avoid bad case  currently the clusterability be define as variance   in order to minimize complexity of row permutation PRON be just rotate row index  so the original basis be not alter at all   the final goal be to reduce dimensionality of the input vector and then cluster PRON in this low  dimensional space with some distance or density base algorithm      b  beginpmatrix   1  amp  0   0  amp  1   0  amp  0   endpmatrix      the above counterexample clearly show potential risk of generate a basis which be insensitive to certain permutation  exchange row 1 and 2  and the result be the same as change the order of basis vector  which do not change the projection at all   question 1  what be the probability of generate a random basis with such an inherent flaw   PRON see that row permutation somehow reflect the basis  but PRON do not know which base do not reflect well  PRON do not expect exact number  since PRON would require precise definition of word  have    flaw  and  well   any help in express the problem more profesionally be greatly welcome   question 2  be PRON the only sort of flaw that can reduce randomness of the entire process   
__label__r __label__xgboost everytime i use the same code that do the training on the same set of datum with no change in parameter  PRON expect the output to be same so that PRON can be reproduce  but i find the prediction change  why be PRON so   
__label__pytorch PRON be incorporate adversarial training for semantic segmentation from adversarial learning for semi  supervised semantic segmentation   the idea be like this   the discriminator take as input a probability map  21x321x321  over 21 class  pascal voc dataset  and produce a confidence map of size 2x321x321   a real  fake decision for each pixel   for an input image  3x321x321   the segmentation network  generator  produce  fake  probability map  21x321x321   the  real  probability map come from the ground truth segmentation label  21x321x321 use one  hot encoding   the loss for both generator and discriminator be easy to understand through the code   with PRON current implementation  the discriminator loss quickly go to 0  which be a failure more for gan trainingmention here    PRON be new to pytorch  as well as to adversarial training   so  PRON be not sure if there be an issue with PRON network architecture  hyperparameter or simply PRON training scheme  PRON would really appreciate some pointer   this be how PRON training look    10  ld  1402573823928833 lg  30447137355804443   11  ld  08725658655166626 lg  2544170618057251   12  ld  06969347596168518 lg  21177046298980713   13  ld  0611475944519043 lg  17778557538986206   14  ld  049319764971733093 lg  21366050243377686   15  ld  030195319652557373 lg  17873120307922363   16  ld  014412544667720795 lg  1045764684677124   17  ld  004816107824444771 lg  15864180326461792   18  ld  0012304163537919521 lg  1370680332183838   19  ld  00035684951581060886 lg  13428194522857666   110  ld  00011156484251841903 lg  1145486831665039   111  ld  000045744137605652213 lg  1371126651763916   112  ld  00001588731538504362 lg  1378540277481079   113  ld  4844377326662652e05 lg  1504058837890625   114  ld  3028669743798673e05 lg  15484118461608887   115  ld  15183023606368806e05 lg  1584553837776184   116  ld  20302868506405503e05 lg  14818311929702759   117  ld  10679158549464773e05 lg  12976796627044678   118  ld  15313835319830105e06 lg  12631664276123047   119  ld  4273606009519426e06 lg  1770961046218872   120  ld  11575384633033536e06 lg  15112217664718628   121  ld  2138318961897312e07 lg  12034248113632202   122  ld  10100056897499599e06 lg  1581740140914917   123  ld  56876764631397236e08 lg  10763123035430908   124  ld  1475878548262699e07 lg  16125952005386353   125  ld  6919402721905499e07 lg  16719598770141602   126  ld  13498377526843797e08 lg  11914349794387817   127  ld  13576584301233652e08 lg  11994632482528687   128  ld  3087819067104647e08 lg  12909866571426392   129  ld  3416153049329296e07 lg  2143049478530884   130  ld  4477038118011478e08 lg  17709745168685913   131  ld  21782324832742006e09 lg  12023413181304932   132  ld  10589346999267946e07 lg  14242452383041382  PRON training code be this   generator  deeplabv2resdeeplab    optimizerg  optim  sgdfilterlambda p  prequiresgrad    generatorparameterslr000025momentum09  weightdecay00001nesterov  true   discriminator  disinchannels21   optimizerd  optim  adamfilterlambda p  prequiresgrad    discriminatorparameterslr00001weightdecay00001   for epoch in rangeargsstartepoch  argsmaxepoch1    for batchid   img  mask  ohmask  in enumeratetrainloader    img  mask  ohmask  variableimgcudavariablemaskcudarequiresgrad  false  variableohmaskcudarequiresgrad  false    ohmask  mask  hxw  convert to one  hot encode probability map for each class  21xhxw   outimgmap  generatorimg   outimgmap  nn  logsoftmaxoutimgmap                            adverarial training                           if argsmode    adv    n  outimgmapsize0   h  outimgmapsize2   w  outimgmapsize3    generate the real and fake labels  targetfake  variabletorchzerosn  h  wlongcudarequiresgrad  false   targetreal  variabletorchonesn  h  wlongcudarequiresgrad  false                              discriminator training                              train on real  confmapreal  nn  logsoftmaxdiscriminatorohmaskfloat      optimizerdzerograd    ldreal  nn  nllloss2dconfmapreal  targetreal   ldrealbackward     train on fake  confmapfake  nn  logsoftmaxdiscriminatorvariableoutimgmapdata     ldfake  nn  nllloss2dconfmapfake  targetfake   ldfakebackward     update discriminator weight  optimizerdstep                            generator training                         confmapfake  nn  logsoftmaxdiscriminatoroutimgmap    lgce  nn  nllloss2doutimgmap  mask   lgadv  argslamadv  nn  nllloss2dconfmapfake  targetreal   lgseg  lgce  argslamadv  lgadv  optimizergzerograd    lgcebackwardretainvariable  true   lgadvbackward    optimizergstep    print           ld    lg     formatepoch  ildreal  ldfakedata0lgsegdata0     PRON be use resnet101 as segmentation network  which be PRON generator  and PRON discriminator be as follow   class disnn  module        discriminator network for the adversarial training       def   initself  inchannel  negativeslope  02    superdis  selfinit      selfinchannel  inchannel  selfnegativeslope  negativeslope  selfconv1  nn  conv2dinchannel  selfinchannel  outchannels64kernelsize4stride2padding2   selfrelu1  nn  leakyreluselfnegativeslope  inplace  true   selfconv2  nn  conv2dinchannels64outchannels128kernelsize4stride2padding2   selfrelu2  nn  leakyreluselfnegativeslope  inplace  true   selfconv3  nn  conv2dinchannels128outchannels256kernelsize4stride2padding2   selfrelu3  nn  leakyreluselfnegativeslope  inplace  true   selfconv4  nn  conv2dinchannels256outchannels512kernelsize4stride2padding2   selfrelu4  nn  leakyreluselfnegativeslope  inplace  true   selfconv5  nn  conv2dinchannels512outchannels2kernelsize4stride2padding2   def forwardself  x    x selfconv1x   161161  x  selfrelu1x   x selfconv2x   8181  x  selfrelu2x   x selfconv3x   4141  x  selfrelu3x   x selfconv4x   2121  x  selfrelu4x   x  selfconv5x   1111   upsample  x  fupsamplebilinearx  scalefactor2   x  x11     2121  x  fupsamplebilinearx  scalefactor2   x  x11   4141  x  fupsamplebilinearx  scalefactor2   x  x11   8181  x  fupsamplebilinearx  scalefactor2   x  x11   161161  x  fupsamplebilinearx  scalefactor2   x  x11   321321  return x  thank in advance   
__label__neural-network __label__deep-learning __label__gradient-descent PRON do the coursera deep learning course where as an assignment PRON have to complete a few function of a neural net  everything work great so PRON try to implement PRON from scratch   do PRON all and when PRON test PRON PRON behave really strange  PRON remove the hidden layer and leave only one neuron on the output one so PRON basically logistic regression   PRON be train PRON with the following   x    3    2    3    2    y    0    0    1     1     but when PRON use that same x to check  PRON predict aprox    1     1     0    0    the bias be close to 0 and the single weight be 27  after each iteration the cost increase from 07 to 299 where PRON stay for the last 7 epoch   if PRON change y to  1  1   0    0   the prediction be 05 for all instance  and both bias and weight be close to 0  with this y configuration the cost stay at 069 through all iteration   if PRON change the update line from w  alpha  grad to   the situation be the same but all the way round   this be PRON gradrient function  def gradientsself  x  y    dws     dbs     h  activation  zs  selfforwardx   da    npdividey  h   npdivide1  y  1  h    dj  doutput  for d in nparange1  lenselfweights1    a  activationsd   output for layer d  z  zsd   linear for layer d  w  selfweightsd   weight for layer w  dz  da  a   1a   m  wshape1   dw  1m  dzdotactivationsdt   db  1m  npsumdz  axis1  keepdim  true   da  w  tdotdz   dwsinsert0  dw   dbsinsert0  db   dwsinsert0  none   dbsinsert0  none   return dws  dbs  PRON be use an empty weight at 0 so PRON easy to match the layer s activation  zs and weight   for all layer the activation function be sigmoid  once PRON work PRON will see if PRON change the hide to relu   here be the whole code   github  after be all day try to figure this out PRON have to post this question to find clarity   the problem be here  dw  1m  dzdotactivationsdt   PRON should be  dw  1m  dzdotactivationsd1t  
__label__machine-learning __label__neural-network __label__logistic-regression __label__optimization __label__octave PRON have download and build octave library and PRON work fine  but PRON can not call function minimizer like fminunc    fmingc   etc to minimize PRON function for perform logistic regression or use PRON in neural network  can these function be access from c  if yes  then how   PRON can use a native library in c for optimization  check out this one   dlib 
__label__convex-optimization PRON need to solve the follow problem   for a give px0y0z0w0  and arbitrary t for example  let p0801006004  and t12  PRON need to find a vector qx  y  z  w  with the minimum distance from p  under the give constraint   PRON need a numerical solution  PRON try use matlab but PRON have a lot of problem with that  PRON will be glad for some help  idea   thank   PRON guess PRON could well use cvx package to minimize that in matlab  httpcvxrcomcvx  PRON try PRON  use yalmip  disclaimer  develop by PRON    x0   0801006004     x  sdpvar41    objective   x  x0x  x0    constraint   sumx1  xgt0  sumxlogx    lt 12    solvesdpconstraint  objective  sdpsettingssolverfmincon     doublex   an   5356398659440911e01  1666760280554157e01  1522838063170345e01  1454002996834589e01  absolutely no problem for fmincon to solve the problem  PRON statement  a lot of problem  be a bit vague  PRON guess be that PRON be not be careful with the definition of the entropy at zero  ie  PRON can not compute xlogx  directly  PRON have to check for zero and return the analytic value there  as PRON will get 0infnan otherwise  if x0  y  0   else  y  xlogx    end  some version of fmincon may even require a safeguard against negative value of x  but the version above be an absolute minimum  
__label__linear-algebra __label__finite-element __label__geometry PRON have a 2d triangle which deform with each vertex move by some small   sinx  approx tanx  approx x  displacement vector  the displacement of any point in the triangle be linearly interpolate from the displacement at the vertex   how can PRON find the rotation angle of any point in the triangle   PRON want the rotation angle as a linear function of the vertex displacement and PRON feel that PRON should be  again  small displacement only   PRON try the follow approach but get bogg down in the enormous expression that appear  hundred or thousand of term  and PRON be not sure if PRON be even a correct way  be there an easy way or do this simplify somehow   1  find the deformation gradient  f  2x2 matrix  use the displacement and the derivative of the interpolation function   2  find the polar decomposition  f  qs where  q be orthogonal and  s be symmetric  3  treat  q as a rotation matrix and extract the rotation angle from PRON  PRON also try more intuitive geometrical way like average the rotation angle of all 3 vertex about the point but PRON do not seem to give correct looking result   PRON approach seem to be correct  but PRON should not be get catch up in  hundred to thousand  if PRON be do PRON in a reasonable way  in case PRON be catch up in the mechanic of PRON  here be the key part of PRON process lay out in a little bit more detail  PRON be assume that PRON can compute the deformation gradient  since PRON have not provide any detail about how PRON be interpolate   1  compute deformation gradient  mathbff  2  compute the polar decomposition   mathbff   mathbfru  2a  find eigenvalue and eigenvector of  mathbfc   mathbfft mathbff this tensor have the same eigenvector  mathbfv1  mathbfv2 as  mathbfu  and PRON eigenvalue  lambda1  2  lambda2  2 be the square of the eigenvalue of  mathbfu  PRON seem to be work with 2x2 deformation gradient  so this should again be pretty easy   2b  reconstruct  mathbfu use the spectral theorem  httpsenwikipediaorgwikispectraltheorem       mathbfu   sumi lambdai mathbfvi otimes mathbfvi     3  solve for  mathbfr   mathbfu1mathbff again  2x2 matrix make this pretty easy   4  profit  aka  post  process  mathbfr however PRON would like    as nicoguaro have note in a comment above   mathbff will be constant within the triangle if PRON have linear interpolation function  so PRON would only need to compute this angle of rotation multiple time if PRON use high  order interpolation   a much simple formula be     theta   1 over 2  nabla time u     where  u be displacement  the derivative of  u be obtain multiply element of the strain  displacement matrix  p8 here  by component of  u  edit clarification    u be a 3d field but since the question be 2d  one component be always zero    nabla times mean curl   theta be a vector whose magnitude be the rotation angle  in 2d  PRON must be parallel to the plane normal so only that component be the angle and the other two be zero   give a small  infinitesimal  2d displacement field  u   ux  uy the infinitesimal counterclockwise rotation angle  theta be simply     theta  frac 12  left  fracpartial uypartial x   fracpartial uxpartial y  right      in general  for  n dimensional displacement field  uinmathbbrn PRON can be show that the infinitesimal rotation be describe by the skew symmetric part of the jacobian      mathbfj   fracpartialu1dot  unpartialx1dot  xn       ie     mathbfomega   frac12 left  mathbfj   mathbfjt right      this be link to the well know fact that  mathrmson  the skew symmetric square real matrix  form the lie algebra associate to the special orthogonal group  mathrmson  real orthogonal matrix with determinant 1    as already note in another answer   fracpartial uipartial xj can be easily compute from the derivative of the shape function and the nodal displacement value   edit  for linear shape function  the jacobian be constant  this mean that  delta u  mathbfj  delta x  where  delta u   delta x be the difference of displacement and coordinate  respectively  evaluate at two distinct point inside the element  if PRON choose two node  then  delta u be the difference of the nodal displacement  while  delta x be the vector represent the corresponding edge of the element  for triangle and tetrahedra be PRON therefore possible  explicit formula leave as an exercise  to easily compute the request quantity directly from nodal coordinate and nodal displacement  
__label__linear-algebra let  f  mathbbr  rightarrow mathbbr be a linear map  PRON want to evaluate an expression of the type   faxbk in term of  fx for some fix value of  x  PRON already know  fxr for  r1  k     x be typically small   0ltxlt1    and  k be typically about  15  a be always positive  about  30   and  b be always negative  about  30   so that  axb be always between  0  and  1 further PRON be also know that the range of the map  f be   01  use the binomial theorem to expand   axbk involve very large number  which because error due to overflow  in matlab    since PRON be already know that the argument   axbk as well as PRON image  faxbk be always small  PRON be interested to know if there be method to evaluate  faxbk in term of  fx stably or without involve large number  any help will be much appreciate   edit  the actual problem PRON want to solve be not quite the same  PRON be describe PRON below   PRON have two function  f  gmathbbr  to mathbbr  g be positive and unit normalize  ie  intinftyinfty  gxdx1  the function  f be not know  but PRON be know that  fx  in  01 for all  x also the quantity  intinftyinfty  gx  fxr dx be know for  r1  k  which be  of course  in   01   PRON want to calculate the quantity   PRON  intinftyinfty  gx   a fx   bk dx where  a  b in mathbbr PRON be know that  agt0blt0  and  a  b be choose such that  afxb in  01 hence  PRON in  01  a be typically about  30   and  k be about  15  since only  intinftyinfty  gx  fxr dx be know PRON have no option but to expand   a fx   bk  sumr0k binomkr  ar fxr bk  r use the binomial theorem  however this involve the product of large number like  binomkr and power of  a and  b since  PRON in  01  compu  i by add and subtract such large nos  do not seem like a good idea   in the original question PRON intend the linear map  f to correspond to the map  f mapsto intinftyinfty  gx  fx  dx  but that be clearly not a good example   this be probably not an answer  more like random thought  PRON know that    int gx  fxkdx  fk  let PRON assume PRON can invert  y  fx  PRON get    int gf1yf1primey  yk dy  fk  hence  PRON can interpret  fk as the moment of the function     gf1yf1primey   sumk fracykkfk  which be   fracgxfprimex    sumk fracfxkkfk    now  let PRON do the same exercise with PRON shift form     int gx   a fxbkdx with  yxa fxb and  xyf1leftfracy  baright hence   fracdxdyf1primeleftfracy  barightfrac1a  again PRON get     int gleftf1leftfracy  barightrightf1primeleftfracy  barightfrac1a  yk dy  hatfk  with the same interpretation     gleftf1leftfracy  barightrightf1primeleftfracy  barightfrac1asumk fracykkhatfk    fracgxa fprimex    sumk fraclefta fxbrightkkhatfk  since both series should be equal in power of  fxk     a sumk fraclefta fxbrightkkhatfksumk fracfxkkfk PRON have a relation between  hatfk and  fk  be this easy to compute  be this even correct  PRON do not know  but maybe PRON be helpful   PRON think PRON problem be ill  condition and that this be not a numerical issue  the condition number with respect to  fj be proportional to  binomkjaj bk  j  which with yours number can be huge  eg   3015 for  j  k   think for example about how the answer would change if  fk be change by  1016—this have nothing to do with the exact way in which PRON compute that number  only how PRON define PRON  so since the number PRON be try to compute be so sensitive to minute change in the input datum  this be not a well  pose problem   any correct algorithm  however numerically accurate  will be just as unstable because PRON be a property of the problem as pose  in principle  this could change if PRON pick a different but similar problem with a different parametrization  but not as PRON stand  
__label__linear-algebra __label__linear-solver __label__iterative-method __label__conjugate-gradient what be a good way to check if the any numerical error be occur in conjugate gradient algorithm  additionally why be PRON not suggest to check error by check a  orthogonality of search direction or check orthogonality of residual   note  here by error PRON mean error from float point unit of cpu because of incorrect computation  which can be due to corrupt datum in cache etc   not due to round error  in some case the error can be due incorrect computation of matrix vector product  in case where matrix a be not explicitly available    if PRON try to use the cg to seek the least square solution of  ax  b  where  a be not symmetric or even square  PRON will solve the normal equation  atax  atb  in this case  from PRON experience  PRON be very important to check if  at be indeed the adjoint of  a by use the definition of adjoint  inner product   especially when PRON implicitly compute  a and  at every time PRON find PRON cg do not converge  PRON be due to the error from  at  ie PRON be not the strictly adjoint of  a   what PRON be look for be call fault tolerant computing  for highly sensitive routine  one traditionally use redundant computation  perform every elementary step twice  and if the result agree  continue  otherwise repeat again and choose the result which occur twice  often on independent machine    there be more recent approach which have less overhead  but except for very specific type of error  some sort of extra computation  checksum  probabilistic repeat together with checkpoint  be require to catch error  this be especially the case for iterative linear solver  since these be quite vulnerable  PRON could fail to converge or even calculate wrong solution   for conjugate gradient  this be address in detail in httpwwwemclkitedupreprintsemclpreprint201110pdf  here  the conjugate gradient  in defect correction form  be frequently restart  and iteration where the residual have fail to decrease be discard as corrupt by error   on the other hand  and since PRON refer to this case in PRON question   the error due to incorrect computation of the matrix vector product have also be investigate  the keyword be inexact conjugate gradient  see  eg   theory of inexact krylov subspace methods and applications to scientific computing  simoncini  v and szyld  d  siam journal on scientific computing 2003 252  454  477  inexact krylov subspace methods for linear systems   van den eshof  j and sleijpen  g   siam journal on matrix analysis and applications 2004 261  125  153 
__label__recommender-system a recommendation system keep a log of what recommendation have be make to a particular user and whether that user accept the recommendation  PRON be like  userid itemid result  1  4  1  1  7  1  5  19  1  5  80  1  where 1 mean the user accept the recommendation while 1 mean the user do not respond to the recommendation   question  if PRON be go to make recommendation to a bunch of user base on the kind of log describe above  and PRON want to maximize map3 score  how should PRON deal with the implicit datum  1 or 1    PRON idea be to treat 1 and 1 as rating  and predict the rating use factorization machine  type algorithm  but this do not seem right  give the asymmetry of the implicit datum  1 do not mean the user do not like the recommendation    edit 1  let PRON think about PRON in the context of a matrix factorization approach  if PRON treat 1 and 1 as rating  there will be some problem  for example  user 1 like movie a which score high in one factor  eg have glorious background music  in the latent factor space  the system recommend movie b which also score high in  glorious background music   but for some reason user 1 be too busy to look into the recommendation  and PRON have a 1 rating movie b if PRON just treat 1 or 1 equally  then the system may be discourage to recommend movie with glorious bgm to user 1 while user 1 still love movie with glorious bgm  PRON think this situation be to be avoid   PRON system be not just train on item that be recommend right  if so PRON have a big feedback loop here  PRON want to learn from all click  view  PRON hope   PRON suggest that not  look at an item be a negative signal  PRON strongly suggest PRON do not treat PRON that way  not interact with something be almost always best treat as no information  if PRON have an explicit signal that indicate a dislike  like a down vote  or  maybe watch 10 second of a video and stop   maybe that be valid   PRON would not construe this input as rat  like datum   although in PRON case  PRON may get away with PRON   instead think of PRON as weight  which be exactly the treatment in the hu koren volinsky paper on als that trey mention in a comment  this let PRON record relative strength of positive  negative interaction   finally PRON would note that this paper  while be very likely to be what PRON be look for  do not provide for negative weight  PRON be simple to extend in this way  if PRON get that far PRON can point PRON to the easy extension  which exist already in two implementation that PRON know of  in spark and oryx  
__label__stability __label__discretization __label__numerical PRON want to discretize and numerically solve the follow pde   beginequation   vkdfracpartial fpartial x   exdfracpartial fpartial k   sf   endequation   use finite volume  box integration  method   since the direct discretization result in an oscillatory solution  PRON have use the upwind discretization scheme  this eliminate the oscillation and the solution satisfy the maximum principle  however  if PRON have PRON  xgrid as different  xi point with  i12  n  the upwind discretization scheme lose the connection between  fx1  the boundary value  and  fx2 because PRON be always calculate the right  sided difference on the second grid node   when PRON try to use the central difference on  x2   oscillation and instability return to the solution  and when PRON use the upwind scheme  the boundary point be disconnect from PRON adjacent node   any suggestion on how to solve this issue   
__label__c PRON be try to perform a calculation which involve the follow c  function   long double complex  tridiagthomaslong double complex  a  long double complex  b  long double complex  c  long double complex  f  int n    long double complex  v  v   long double complex   mallocsizeoflong double complex   n    long double complex  y  y   long double complex   mallocsizeoflong double complex   n    long double complex w   int k   for  k  0  k  lt  n  k    yk   0   vk   0     w  a0    y0   f0   w   for  k  1  k  lt  n  k    vk  1   ck  1   w   w  ak   bk   vk  1    yk    fk   bk   yk  1    w     for  k  n  2  k  gt 0  k    yk   yk   vk   yk  1      return y     when work with real value  and make the necessary change of long double complex   long double   this function work as expect  when use PRON in the above form with complex argument  however  the result diverge to infinity very quickly   can anybody enlighten PRON as to why that may be  PRON be not new to programming  but PRON be new to c  thank  
__label__xgboost PRON be try to use the xgboost model to perform a multi  class classification over 40 class   the code be as follow   xgbparam  listcolsamplebytree 07   subsample  07   eta  005   objective  multi  softmax    maxdepth 5   minchildweight 1   evalmetric  mlogloss   numclass  categoryclassno   nthread4   fitxgb  xgbtrainparam  xgbparam   datum  dtrain   nround  500   watchlist  listtrain  dtrain  test  dt    printeveryn  50   however  PRON be get the follow error   check fail   infolabelssize       0  label set can not be empty  PRON have reproduce the dataset and the r script here   httpsgithubcomabhishekkanthealthcarequeryclassifier  any help pointer be deeply appreciate   cheers  abhishek kant  in python PRON need to set the label param in  dtrain  xgb  dmatrixxtrain  label  ytrain  featurename  cfgcolx   ps PRON be go to comment but miss rep point  
__label__strong-ai __label__control-problem __label__legal for example  would an ai be able to own property  evict tenant  acquire debt  employ  vote  or marry  what be the legal structure in place to implement a strong ai into society   there be a legal difference between a  person   which include body corporate  corporation  incorporated association  etc  and actual people  vs  natural person   which be specifically a human being    for an ai to marry  PRON would need to get the legal definition of  natural person  change  and depend on the jurisdiction possibly also the definition of  man  or  woman    for other thing  such as own property  evict tenant  enter into contract  etc  an ai would simply use a corporation  PRON may be that the corporation may need to have a minimum number of director who be natural person  but PRON could just be pay professional  so no issue there   with credit card  PRON would depend on the policy of the issue bank  there be no legal impediment to corporation have credit card in PRON own right  but in practice bank often require a director s guarantee from a natural person that PRON can sue if the bill be not pay  PRON want to be sure PRON will get PRON money  even if the corporation be wind up   yes  to some of what PRON propose   no to some   today corporation be grant right  to own property  earn income  pay tax  contribute to political campaign  offer opinion in public  ad more   even now PRON see no reason why an ai should not be eligible to incorporate PRON  thereby inherit all these right   conversely  any corporation already in existence could become fully automate at any time  and some plausibly will    in do so  PRON should not lose any of the right and duty PRON currently employ   however PRON suspect certain right would be unavailable to an ai just as PRON be unavailable to a corporation now  marriage  draft or voluntary service in the military  right due a parent or child or spouse  estate inheritance  etc   could this schizoid sense of human identity be resolve at some point   sure   already there have be numerous law introduce and some pass elevate various nonhuman specie to high level of civil right that only human heretofore enjoy  chimpanzee  cetacean  parrot and other have be identify as  high functioning  and longer live  and so  be now protect from abuse in way that food animal  pet  and lab animal be not   once ai  being  arise that operate for year and express intelligence and emotion that approach human  level and lifetime  PRON would expect a political will to arise to define  establish  and defend PRON civil right   and as human become more cybernetically augment  especially cognitively  the line that separate PRON from creature of pure silicon will begin to blur   in time PRON will become unconscionable to overlook the right of being simply because PRON contain  too little flesh    murray shanahan  in PRON book the technological singularity  make the case that the right of any being be determine by PRON intelligence   for instance  PRON value the life of a dog above that of an ant and likewise value human life above that of other animal   from here one could argue that a general artificial intelligence of equal intelligence to a human should have equal right to a human and a superior artificial intelligence should have more right   the question  of course  be whether PRON anthropocentric society would be willing to accept this fundamental shift in human right and this idea of remove humanity from PRON pedestal of importance   when PRON come to legal framework  PRON really be enter into uncharted territory as ai be go to have to revolutionise the way PRON define many of the term PRON take for grant today and question many of PRON usual assumption   ai be go to drive an important shift in PRON mindset well before PRON exceed human intelligence   not only would not a strong ai which come into existence today have the right a human have  or any right  see these discussion of the implementation of regulation for weak ai at  the white house and the american bar association    but PRON seem unlikely the first one will   observe that   have right imply that there be restriction  which mean there would have to be a system of control  however the control problem in ai be still unsolved   even assume that problem be solvable  an agi would then have to appear equivalent to natural human  PRON do not yet  see turing test pass    and even after pass equivalence test  be unlikely to remain that way  per the singularity hypothesis   further  if one or more agi be to be human  equivalent long enough to desire right  lawmaker  in the us  would have to re  interpret the definition of personhood and grant PRON right  as PRON do for corporation in 1886   no matter what right PRON get  as a company   PRON will still lack the right of not get liquefy and all PRON property transfer back to natural person   this be of course if no law be change   to change the law PRON will need to convince people that this machine be more  life  worthy than intelligent animal  and hope that people will deal with PRON good than PRON do with dolphin and chimp   as PRON see PRON  machine can easily get the same or good right then company  but will always be under the mercy of the less intelligent man   that be if thing go peacefully    a sufficiently clever agi  if self  interested  would pre  empt or co  opt exist legal structure  to seize whatev juridical right PRON desire  as the opportunity arise   thus PRON would render PRON opinion on the subject entirely moot   another way of put this point   while current legal framework would not provide any right to an artificial agent  current legal framework foreseeably will no longer be current  once an ai exist have attribute which imply the transformative change of those framework  
__label__machine-learning __label__neural-network __label__deep-learning __label__time-series __label__keras PRON be try to use lstm neural network in order to make a song composer  basically this be base of a text generator  try to predict the next character after look at a sequence of character  but instead of character  PRON try to predict note   structure of the midi file that serve as the input  y  axis be the pitch or note value while x  axis be time    and this be the predict note value   PRON set an epoch of 50  but PRON seem that the lstm s loss rate do not decrease  most of the time PRON loss rate do not improve   PRON suspect this be because there be an overwhelming number of a particular note  in this case  note value 65  which make the lstm lazy during train phase and predict 65 each and every time   PRON feel like this be a common problem among lstms and time  series base learn algorithm  how would PRON solve a problem like this  if what PRON mention be not the problem  then what be the problem and how do PRON solve that   try use a recurrent neural network and use a rprop minus trainer without weight backtracking  aka rprop  cf   igelamphuesken  neurocomput 50  2003   and without ponderation  ie  all training sample have the same weight   PRON need more detail besides what PRON have provide here   what do PRON datum look like   be PRON sure PRON do not have a bug in PRON datum   what do PRON lstm architecture look like   be PRON sure PRON be output the softmax for PRON output layer   what be PRON hyperparameter   do PRON set the learning rate too high  if everything else be correct  PRON be probably this   also  be the note PRON have for PRON prediction map to keras as category  index  need to make sure PRON be not treat PRON as real  value number  
__label__finite-element __label__libraries PRON question can be rephrase as  fem library like deal  ii but for simplex element    PRON scientific group work with very complicated 3d geometry  so usually PRON prefer tetrahedral mesh for PRON model  and PRON use gmsh to build such grid   programming language that PRON use be c  PRON have never use third  party fem library  PRON work with PRON own matrix format  solver  etc  but PRON think that apply fem library  like deal  ii  for example  could be increase the productivity of PRON investigation  deal  ii have a lot of advantage  but PRON main disadvantage  for PRON  be support of only hexahedral mesh  PRON read that deal  ii can work with mesh from gmsh through the interface describe here  maybe PRON be a good choice for what PRON be look for  but PRON be not sure   anyway  maybe anybody know c fem library as well document  good test and actively maintain as deal  ii  but with support of simplex finite element   PRON would like to add a little bit about capability that such library can possess  PRON would very like to have an opportunity to work with a  vector nedelec and b  discontinuous element among other standard  scalar  continuous  one   PRON would suggest  getfem httpdownloadgnaorggetfemhtmlhomepage  or  dolfin httpfenicsprojectorgdocumentationdolfindevcpp  kind regards  tom  PRON be go to repeat nathan s comment as an answer  PRON probably want libmesh  PRON do have support for some nedelec element  though PRON first search find the 2d one  if PRON look around or make a request  PRON be sure PRON will find the 3d one  too  PRON have a huge element library   PRON have nothing else to offer but the project the other have already suggest  PRON do like that deal  ii be apparently consider the  gold standard    disclaimer  PRON be one of the principal author of deal  ii   dune httpwwwduneprojectorg support various structured and unstructured grid  if PRON use the underlying alugrid httpaammathematikunifreiburgdeiamresearchalugrid library PRON get a grid with hexahedral element via PRON grid interface  PRON support gmsh httpwwwduneprojectorgexternallibrariesinstallgmshhtml   there be a course about dune in march httpconaniwruniheidelbergdeduneworkshopindexhtml  registration deadline be february 24 already  if PRON want to learn more about   disclaimer  PRON be one of the dune developer  but try to be not too subjective in PRON answer   two month after post PRON question PRON realize that PRON still want to try work with deal  ii  and implement the tethex converter to work with complex geometry  so if someone be interested  welcome to tethex wiki page  
__label__text-mining what be the tool  practice and algorithm use in automate text writing   for example  let assume that PRON have access to wikipedia  wikinew and similar website api and PRON would like to produce article about  data science with python    PRON believe that this task should be divide into two segment  first would be text mining and second would be text building  PRON be more or less aware how text mining be perform and there be lot of material about PRON in internet  however  amount of material relate to automate text building seem to be low  there be plenty of article which say that some company be use PRON  but there be lack of detail  be there any common idea about such text building   PRON should probably do some reading in the field of  natural language generation   since this seem to relate most directly to PRON question   but the way PRON have describe the process   text mining  text building   lead PRON to wonder if PRON be aim for something much more ambitious   PRON seem as though PRON aim to automate the process of 1  read natural language text  2  understand the meaning  and then 3  generate new text base on that semantic knowledge   PRON be not aware of any general  purpose end  to  end system that can do that  not even specialize system by the like of palantir   what PRON be aim for would probably pass the turing test for fully capable artificial intelligence  
__label__neural-network PRON think both  prelu and leaky relu be    fx   maxx  alpha x  qquad text  with  alpha in  0  1  keras  however  have both function in the doc   leaky relu  source of leakyrelu   return kreluinput  alpha  selfalpha   hence  see relu code     f1x   max0  x   alpha max0  x  prelu  source of prelu   def callself  input  mask  none    pos  kreluinput   if kbackend      theano    neg   kpatternbroadcastselfalpha  selfparambroadcast     input  kabsinput    05   else   neg  selfalpha  kreluinput   return pos  neg  hence    f2x   max0  x   alpha max0  x  question  do PRON get something wrong  be not  f1  and  f2  equivalent to  f  assume  alpha in  0  1    straight from wikipedia   leaky relus allow a small  non  zero gradient when the unit be not active   parametric relus take this idea further by make the coefficient of leakage into a parameter that be learn along with the other neural network parameter  
__label__performance __label__architecture __label__exascale PRON be search for the most important literature and slide reference for model current and future energy cost of float  point operation and datum transfer across the cpu  memory  network  and storage   PRON have mark this question as a community wiki  and PRON would prefer that PRON limit each answer to the following format   title  authors  location  conference  journal  year  doi  url  summary of what information the paper or presentation provide relevant to PRON request   there have be numerous panel and report about exascale computing that should have these sort of number  PRON would look for report that come out from doe  or be write for doe   also  there may be relevant stuff at httpwwwexascaleorg   there be a lot of information online that search engine be capable of find   here be three hit to get the ball roll   httpdxdoiorg101109tc2010121  httpsmartechgatechedubitstreamhandle185345737gtcse201201pdf  httpdlacmorgcitationcfmdoid24256762425691  ieee micro 2011 paper  gpu and the future of parallel computing  show that state  of  the  art cpu and gpu incur 1700pj and 225pj  respectively  for each float  point operation   for datum storage  access cost in different memory  eg sram  pcm  edram   one have to use tool like cacti and destiny  PRON be co  developer of destiny   also  mcpat tool can be use for model energy consumption of entire processor  while dramsim2 be for memory alone  what PRON may be look  may be achieve through combination or modification of these  PRON be open  source   there be many other  but from these  PRON can find connected reference through google scholar  
__label__data-mining __label__statistics __label__time-series PRON be pretty new for time series analysis and PRON would like to share one of PRON research question   here be the graph   apparently  both 1 and 2 be time series datum  1 represent monthly humidity and 2 represent a morbidity rate   PRON would like to analyze if the morbidity rate  2  be associate with seasonal humidity  1   what method be the good in this scenario  also  from the visualization  PRON seem like the morbidity peak be about 3 month after the peak of humidity   can anyone give PRON some suggestion of this question   thank PRON for any reply   here be some way what PRON would try   shift humidity for n day  which make pearson correlation coefficient minimize   extract frequency use fourier transform etc   do power spectral density estimation use welch s method etc  
__label__random-forest __label__linear-regression __label__categorical-data in the linear regression  when PRON have a categorical explanatory variable with  n level  PRON usually remove one level and call PRON a baseline level and fit the model on the remain level  and the final intercept be the intercept plus the coefficient of baseline level  now PRON question be   do PRON matter which level PRON choose to remove  PRON be work on a dataset to predict house price  when PRON use linear model with intercept on the testset for prediction  PRON get negative value for some house which be not correct  as house price can not be negative  but when PRON fit a regression without an intercept all the prediction value be positive as expect  PRON guess PRON have to do something with the baseline level PRON choose   do PRON have to remove one level from all PRON categorical variable in the dataset before fit a random forest  knn  ridge  lasso   too     for linear regression  PRON have to do one hot encoding and PRON create one less number of variable then level of the categorical variable  in late tool PRON do not have to do PRON manually PRON automatically do PRON have try in r one hot encoding have no impact on which level PRON choose  PRON be basically a binary way of represent the level true or false  this will create whole new variable which with just 1 and 0   for tree  base algorithm  PRON basically do not need to apply the one hot encoding  these algorithm be capable enough to handle the categorical variable   reference  httpsgerardnicocomwikidataminingdummy 
__label__boundary-conditions __label__solid-mechanics in structural mechanic  be the boundary condition  free surface    traction free    stress free  all equivalent neumann boundary condition   
__label__python __label__iterative-method __label__monte-carlo PRON be write a vmc simulation for hydrogen and helium atom  but in both PRON code PRON variational energy for certain wavefunction be not only statistically different from PRON expectation value  but PRON be also low than PRON ground state energy   for the hydrogen atom  PRON know that the ground state energy be  05ha if PRON use the trial wavefunction  e12r  where  r be the distance between PRON nucleus and electron  PRON be get an energy low than PRON ground state energy  PRON have be tell that PRON ground state energy be a low bind for PRON variational energy   if PRON use the exact wavefunction   er  PRON get  05ha exactly   PRON be quite confused by this  PRON be guess since PRON variational energy be low than PRON ground state energy for some trial wavefunction PRON must be do something very wrong  but PRON do get the exact energy if PRON use the exact wavefuntion  why may that be   for reference here be PRON code   import math  import random  import numpy  b  numpyfloat12   def psitr    PRON trial wavefunction  return  mathexpbr    def elocalr    return 05bb2br1r   def rx  y  z    return mathsqrtmathpowx2mathpowy2mathpowz2    ne  3  a  0  e  numpyarrayrandomrandom   for i in range  ne     dr  105   equilibriate  for n in range  10000     move PRON electron  dt  numpyarraydrrandomuniform11  for i in range  ne     ep  edt   be this move good   r1  re0e1e2    rp1  rep0ep1ep2    psip  psitrp1   psi  psitr1   w  mathpowpsippsi2   if w  gt randomrandom     e  ep  a  a1   accumulate  el  0  q  0  srt  0  for n in range  10000     move PRON electron  dt  numpyarraydrrandomuniform11  for i in range  ne     ep  edt   be this move good   r1  re0e1e2    rp1  rep0ep1ep2    psip  psitrp1   psi  psitr1   w  mathpowpsippsi2   if w  gt randomrandom     el  elelocalrp1   q  q1   ignore srt and s  PRON use PRON for error analysis which PRON have not  include here  srt  srtmathpowelocalrp12   s  mathpowelocalrp1elq2   e  ep  print el  q  print q  the trial wavefunction   exp12r   do not respect the cusp condition  the derivative of the wavefunction need to cancel the  1r coulomb term   without a correct cusp condition  the local energy diverge at the origin  which be likely cause convergence problem with the value of the integral   in general  the leading term should be  expzr  where  z be the nuclear charge    to create a trial wavefunction for hydrogen with a correct cusp  try add high power of  r   expr  a r2  where  a be the variational parameter   the monte carlo integration also have an error  both accept and reject move should be include in the sum  this code only include accept move    otherwise individual energy value have the wrong weight in the final integral  and the final value of energy  e1  should be divide by the total number in the sum  10000   not just the number of acceptance  
__label__machine-learning __label__neural-network __label__deep-learning __label__research PRON be a second year pure math  apply math and computer science student  PRON have take up a research course and be give the topic to focus on facial recognition use deep learning   PRON have do a fair deal of read on the topic  look at old method such as eigenface and fisherface all the way to new method such as deepface   PRON also feel comfortable with the math and algorithm underlie ann   PRON be wonder whether someone in the community could maybe point PRON in the direction in what PRON  as an undergraduate  could take a deep look at in the category of facial recognition use deep learning   PRON think PRON may be interesting to pursue recognise face give a few image of a face ie  lt  5  maybe someone have a good suggestion   first  tensorflow would be a great resource as PRON be gain a userbase with exponential growth  tensorflow have gpu support for single node calculation  and have recently add a distribute version for large calculation   there be some tutorial on convolution ann  which will probably be the direction PRON will go with a computer vision problem  also  some book be come soon and some be in alpha   but enough of plug a particular library   a perspective base facial recognition problem would be interesting if datum be available eg with two perspective on face  do recognition on a third perspective   a relative base facial recognition problem could be interesting  again  if datum be available eg give two parent  try to find the child   one could probably scrape facebook to get a reasonable datum set for this one   blending and age face and control for sex  hair length  facial hair  etc  for problem where a kid have be abduct and PRON have parent picture and sibl picture and the child have grow up   a simple age recognition problem could be interesting  give baby picture  can PRON recognize the adult or vice versa   just some thought  the possibility be endless  PRON suggest start with a very simple stub of a problem and then slowly add sophistication   hope this help  
__label__classification __label__semi-supervised-learning there be a medical instrument that produce various measurement   these measurement be be use as input to a model to classify the health condition of a patient  the health condition be binary  have the patient either be sick1  or healthy0    give that the consent from the patient have be give  PRON now additionally have some extra datum like  age  weight  health history  how to treat those to improve the model   so far PRON have consider these two approach   first way be to process appropriately the new input and include PRON as extra feature in PRON model  then PRON be use this new input space to build again PRON model  second way be to use unsupervised learning and cluster the patient in a few separate group  then PRON be build a different model for each one of these case  with the input datum PRON originally have  this come mainly from the notion that patient of different age or different weight  or even a health history that reveal a weak  strong immune system would have a different response to the disease   be there any other approach with perhaps material PRON be not familiar with like for instance bayesian inference    
__label__python __label__r __label__data __label__beginner PRON be just start out with learn about data science and programming through various online course   be think a good first project would be a to create a log of what PRON be learn and how long for and then work out way that PRON can visualise this for use in a cv further down the road   be want to get a rough outline for how PRON would set a project like this up   be think of something like a simple site where PRON click on the learn resource PRON use and PRON ask for the amount of time PRON study for and then record the datum and time  maybe more input for tag and percentage complete etc   currently learn r and python   so grateful for any idea or advise PRON have   cheer  PRON think the first point to address be how employer will know PRON be be honest about PRON logging  maybe a good use of PRON time be to not put in number of hour  but project and code reference PRON have make so that the output be more concrete rather than just a number of hour   the problem be PRON know plenty of people who put in 1 hour and learn what a normal person would learn in 5 hour  think about the wow factor in this  what would be convincing to employer   also  if PRON only PRON that be log in hour and nobody else be go to see or use that program  PRON may as well use excel while focus on whatev project PRON be passionate about  
__label__r __label__sql PRON be try to replicate the below sql query in r  select a   bkey from table1 a  left outer join table2 b  on akey  bkey where bkey be null  PRON have read through this post however PRON be still struggle to code PRON specific case  httpsstackoverflowcomquestions1299871howtojoinmergedataframesinnerouterleftright  PRON have try the below but the result doesnot allow PRON to filter for bkey be null  loanstoinsertstg1  lt mergex  priorstg2  y  bankloansstg2   by   accountid   allx  true   any insight   example   key1  lt ca1a2a3a4a5    key2  lt ca1a2a3b4b5    bv1  lt c100  200  300  400  500   bv2  lt c150  250  350  450  550   df1  lt asdataframecbindkey1  bv1    df2  lt asdataframecbindkey2  bv2    expected output as a new df   key1 bv1 key2 bv2  a1 100 a1 150  a2 200 a2 250  a3 300 a3 350  a4 400 na na  a5 500 na na  if PRON understand correctly   table1  lt dataframekey  seq1100adata  rnorm100    table2  lt dataframekey  cseq130repna30    bdata  seq160      assume this be what PRON want  librarysqldf   sqlans  lt sqldfselect a   bkey from table1 a left outer join table2 b on akey  bkey where bkey be null      dplyr version  librarydplyr   dplyran  lt table1   gt filterkey  in table2key     regular r version  rans  lt table1whichtable1key  in table2key     edit after dummy datum and expect output  dplyrans2  lt leftjoindf1df2  by  ckey1    key2     key1 bv1  bv2  1  a1 100  150  2  a2 200  250  3  a3 300  350  4  a4 400  ltnagt   5  a5 500  ltnagt  
__label__numerical-analysis __label__quadrature be there a know error estimate for gaussian quadrature when apply to a discontinuous function   for simple one  dimensional experiment  the error appear to be bound by  c h  where  c be some constant and  h  max xi1xi be the maximum spacing between two node  but PRON have not find a clean way to show this   
__label__predictive-modeling __label__regression __label__svm assume no particular knowledge  let PRON consider a system where a mapping  fmathbbrntime mathbbrk to mathbbrn  time 11 describe the transformation of  mathbbrn entity  additionally  PRON be provide with a white box model for find  mathbfyin mathbbrn such that  fmathbfxmathbfu     mathbfyc for any input   mathbfxmathbfu  give  m training sample    mathbfxmmathbfum  cm with  mathbfxm in mathbbrn   mathbfum in mathbbrk   cm in pm 1 such that  fmathbfxmmathbfum    mathbfym  cm for  m in overline1ldot m  what method be there for perform the follow task   find  cast such that  fmathbfxastmathbfuast    mathbfyast  cast for know  mathbfxastmathbfuast  find  mathbfuast such that  fmathbfxast  mathbfuast    mathbfyast  1  for the first task  PRON suppose that svm be one possible option  while for the second one regression could be try   be there other class of solution that seem more appropriate for this type of problem   
__label__machine-learning __label__neural-network __label__data-cleaning PRON have get PRON hand on a dataset that contain quite a bit of ordinal variable that go from 0 to 3 when convert  however in term of apply PRON in an neural network  how do PRON go about PRON   if PRON use one  hot encoding  then PRON will lose the ordinal relationship in the variable  but be that the way to handle PRON  should PRON just scale PRON to  01  such that PRON get 0  033  066  1   PRON havnt be able to find any material that adress this   thank in advance   
__label__classification __label__time-series __label__regression PRON need help in the analyse of a categorization problem   give a set of date  small set  20 element maximum   PRON would like to group date which be equally distribute  with a tolerance    PRON can be  for instance  monthly or weekly separate date   here be an example  give this repartition   PRON would like to categorize into these two group   the problem be that PRON be a developer  not a data scientist  PRON have an intuition that PRON should be possible to do a kind of regression   PRON have no clue how to analyse this problem  can PRON help PRON with that  please   cheers  ps  PRON have already see this thread  recur event  find in a time series  but PRON have not help PRON   if PRON be a categorization problem then PRON should look for a classification algorithm  not a regression technique  the simple classification algorithm be logistic regression   but by the look of PRON  seem like PRON do not have a label data  set and if that be the case PRON should look for clustering technique  clustering be a part of unsupervised learning technique in ml which create cluster or group of similar data point   PRON can use clustering algorithm to cluster close date together  but since PRON have mention the number of date to be cluster will not be more than 20  seem like PRON can just create a simple logic to group PRON together   pick a base date which can be anything and find the num of day  week  month from the base date to each date in PRON dataset  PRON will get a bunch of number now  PRON can now bucket PRON together accord to a threshold PRON like   although cluster algorithm too would do the same  just the thresholding would be be take care of automatically base on optimal cutoff  try the simple  read  easy to understand  cluster algorithm  k  means  
__label__linear-algebra __label__matrix __label__numerical  x1    a  b  where  x1 be a  ntime p matrix   a be a  ntime  p1 and  b be  ntimes1 update  b with  cis there any update method to compute more efficiently   consider the matrix    beginpmatrixata  amp  x1p1   w1p1   amp  z endpmatrix    in term of minor  mij of the matrix  ata   mij be the determinant of  ata with row  i and column  j remove   PRON determinant be    sumi1p1   1pi  xi sumj1p1   1p1j  wj mij   z mathrmdet   ata     sum1leq i  jleq p1  xi  1ijmij  wj  zmathrmdetata       use the cofactor expansion  apply twice  to the last column and to the last row    in PRON case   x  atb   z  btb and  w  bta be know  so the determinant be    btacatb   btb  mathrmdetata      where  c  big   1ijmij  big be the cofactor matrix of  ata  to evaluate the cofactor matrix  one can use the matrix inversion formula     ata1   frac1mathrmdetata   ct     from which PRON follow that the determinant of  xtx be    bigbtb  btaata1atbbigmathrmdetata      therefore  PRON be sufficient to precompute the matrix   ata1 in time  op3  p2n use cholesky factorization of  ata  which also provide the determinant of  ata  and then on each update evaluate one quadratic form in  atb in time  op2pn this be about  p time good than  op3p2n for compute the determinant directly  PRON think after this the efficiency would probably have to come from exploit special structure of  at and  ata 
__label__machine-learning __label__data-mining __label__dataset __label__data-cleaning __label__data PRON have be work on machine learning and bioinformatic for a while  and today PRON have a conversation with a colleague about the main general issue of datum mining   PRON colleague  who be a machine learn expert  say that  in PRON opinion  the arguably most important practical aspect of machine learning be how to understand whether PRON have collect enough datum to train PRON machine learning model   this statement surprise PRON  because PRON have never give that much importance to this aspect   PRON then look for more information on the internet  and PRON find this post on fastmlcom reporting as rule of thumb that PRON need roughly 10 time as many datum instance as there be feature   two question   1  be this issue really particularly relevant in machine learn   2  be the 10 time rule work  be there any other relevant source for this theme    yes  the issue be certainly relevant  since PRON ability to fit the model will depend on the amount of datum PRON have  but more importantly  PRON depend on the quality of the predictor   a 10times rule may be a rule of thumb  and there be many other   but PRON really depend on the predictive utility of PRON feature   eg  the iris dataset be fairly small but easily solve  because the feature yield good separation of the target   conversely  PRON could have 10 million example and fail to fit if the feature be weak   the ten time rule seem like a rule of thumb to PRON  but PRON be true that the performance of PRON machine learn algorithm may decrease if PRON do not feed PRON with enough training datum   a practical and data  drive way of determine whether PRON have enough training datum be by plot a learning curve  like the one in the example below   the learning curve represent the evolution of the training and test error as PRON increase the size of PRON training set   the training error increase as PRON increase the size of PRON dataset  because PRON become hard to fit a model that account for the increase complexity  variability of PRON training set   the test error decrease as PRON increase the size of PRON dataset  because the model be able to generalise better from a high amount of information   as PRON can see on the rightmost part of the plot  the two line in the plot tend to reach and asymptote  therefore  PRON eventually will reach a point in which increase the size of PRON dataset will not have an impact on PRON train model   the distance between the test error and training error asymptot be a representation of PRON model s overfitting  but more importantly  this plot be say whether PRON need more datum  basically  if PRON represent test and training error for increase large subset of PRON training datum  and the line do not seem to be reach an asymptote  PRON should keep collect more datum  
__label__beginner __label__deep-learning PRON would like to pick up on the topic of deep learning  should PRON begin from the topic of ai before work PRON way into deep learning   start with neural net  PRON be the basic building block of deep learning   neural nets for newbies video be a well  explain introduction  cs449 from willamette university be an free and approachable class in neural network   that should lay the foundation for more advanced deep learning material  deep learning by bengio  goodfellow  and courville be a quality book on the subject  
__label__matrix __label__acceleration __label__fast PRON want to compute a fast matrix  vector product use a matrix  t which have a peculiar quasi  hankel structure  for example   beginequation   t2  left   beginarraycccccccccc   a  amp  b  amp  c  amp  d  amp  e  amp  f  amp  g  amp  h  amp  i  amp  jhline  b  amp  e  amp  f  amp  h  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  c  amp  f  amp  g  amp  i  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  d  amp  h  amp  i  amp  j  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0hline  e  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  f  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  g  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  h  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  i  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  j  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  amp  0  endarray   right    endequation   beginequation    t3left   beginarraycccccccccccccccccccc   a  amp  b  amp  c  amp  d  amp  e  amp  f  amp  g  amp  h  amp  i  amp  j  amp  k  amp  l  amp  m  amp  n  amp  o  amp  p  amp  q  amp  r  amp  s  amp  thline  b  amp  e  amp  f  amp  h  amp  k  amp  l  amp  m  amp  o  amp  p  amp  r  c  amp  f  amp  g  amp  i  amp  l  amp  m  amp  n  amp  p  amp  q  amp  s  d  amp  h  amp  i  amp  j  amp  o  amp  p  amp  q  amp  r  amp  s  amp  thline  e  amp  k  amp  l  amp  o  f  amp  l  amp  m  amp  p  g  amp  m  amp  n  amp  q   h  amp  o  amp  p  amp  r  i  amp  p  amp  q  amp  s  j  amp  r  amp  s  amp  thline  k  amp    l  amp    m  amp    n  amp    o  amp    p  amp    q  amp    r  amp    s  amp    t  endarray   right   endequation    larger matrix   t4t5   have the same block structure  in fact  each successive  ti be contain as a partial block of all  tj  jgti  see above example   all of the unique element be contain in the first column or row  but the matrix do not possess classical hankel  toeplitz  etc  structure typical of fast structured matrix vector multiplication  this matrix arise from a convolution of a certain sort  so PRON be convince that the matrix  vector product can be compute in  mathcalonlog n time or something close rather than  mathcalon2 PRON would appreciate the input of other  or potentially helpful reference   edit  the block structure be control by the parameter  p so that the matrix  tp have a   p1timesp1 upper leave triangular block structure  the  pth row or column block be of dimension   p1p22  
__label__similarity PRON have a dataset of 256 row with 61 column  variable  each row should be consider a vector of dimension 61  if PRON randomly split PRON  by row  in 2 group  how could PRON prove that the 2 group be similar  the origin of the datum be biomedical and nonlinear approach should be preferable   PRON can not actually prove that the two group be similar but PRON can establish a confidence level  threshold  furthermore  PRON be possible that the two group will not be similar  depend on PRON threshold for similarity  if  for example  only one of the two group contain strong outlier   that say  PRON can make comparison base on assumption regard the underlie datum distribution  for example  if the datum can be assume to be distribute as a multivariate normal distribution  PRON can use hotelling s two  sample t  squared statistic  a multivariate generalization of the student s t  test  to test PRON confidence interval   there be other recent method like principal difference analyses specifically design to address these sort of problem  be not sure if the methd be available as an r package  PRON can get the concept  algorithm from the manuscript   see httparxivorgabs151008956 
__label__algorithms __label__testing __label__programming-paradigms develop scientific algorithms be a highly iterative process often involve change lot of parameter that PRON will want to vary either as part of PRON experimental design or as part of tweak algorithm performance  what strategy can PRON take for structure these parameter so that PRON can easily change PRON between iteration and so that PRON can easily add new one   PRON be cumbersome for the user to specify every aspect of an algorithm  if the algorithm allow nest component  then no finite number of option would be sufficient  therefore  PRON be critical that option do not necessarily  bubble up  to the top level  as in the case of explicit argument or template parameter  this be sometimes call the  configuration problem  in software engineering  PRON believe petsc have a uniquely powerful system for configuration management  PRON be similar to the service locator pattern in martin fowler s essay on inversion of control   petsc s configuration system work through a combination of user  specify configuration manage by the solver object  with get and set query  and the options database  any component of the simulation can declare a configuration option  a default value  and a place to put the result  nested object have prefix which can be compose  such that every object that need configuration can be address independently  the option PRON can be read from the command line  environment  configuration file  or from code  when an option be declare  a help string and man page be specify  so that the help option be understandable and so that a properly link gui can be write   the user call a setfromoptions method to make an object configure PRON base on command line option  call this function be optional  and may not be call if the user  person write code that call petsc  be expose the option through some other interface  PRON highly recommend that the user expose the option database because PRON give the end user  person run the application  a great deal of power  but PRON be not require   a typical configuration  call via  petscobjectoptionsbeginobject     object have prefix and descriptive string    petscoptionsrealtsatol      option database key     absolute tolerance for local truncation error     long description     tssettolerance      function and man page on topic    tsgtatol     current  default value     amptsgtatol     place to store value     ampoptionset      true if the option be set    petscoptionslisttstypetime step methodtssettypetslist   defaulttype  typename  sizeof typenameampoptionset    tsadaptsetfromoptionstsgtadapt      configure adaptive controller method       many other       the follow be only call from implicit implementation    snessetfromoptionstsgtsne      configure nonlinear solver     petscoptionsend     note   petscoptionslist   present the user with a choice from a dynamic list  there be a plugin architecture which new implementation can use to expose PRON as first  class to caller   these implementation can be place in share library and use as first  class without recompil program    snessetfromoption   recursively configure the linear solver  preconditioner  and any other component that need configuration   PRON have face this problem several time when develop PRON own simulation code from scratch  which parameter should go in an input file  which should be take from the command line  etc  after some experimenting  the follow turn out to be efficient   PRON be not as advanced as petsc    instead of write an experimental simulation  program   PRON be more inclined to write a python package that contain all the function  amp  class need to run the simulation  the traditional input file be then replace by small python script with 5 to 10 line of code  some line be typically relate to load data file and specify output  other be instruction for the actual computation  good default value for optional argument in the python package make PRON doable for beginner to use the library for simple simulation  while the advanced user still have access to all the bell and whistle   a few example   atomistic simulation environment  also check out gpaw   httpswikifysikdtudkase  tamkin  httpmolmodugentbecodewikitamkin  as a first point  PRON would do the algorithm and software as general as possible  PRON have learn this the hard way   let PRON say PRON start with a simple test case  PRON can do this faster  but then  if PRON make the software too specific  too few parameter  for this initial case  PRON will loose more and more time adapt PRON every time PRON add a new degree of freedom  what PRON do now PRON be spend more time at the beginning  make the thing pretty general  and increase the variation of the parameter as PRON move forward   this involve more testing from the beginning since PRON will have more parameter from the starting point  but will mean that PRON can latter play a lot with the algorithm at zero or a very low cost   example  the algorithm involve calculate the surface integral the dot product of two vector function  do not assume from the beginning the size  geometry and discretization of the surface if in the future PRON may want to change that  make a dot  product function  make the surface as general as possibly  calculate the integral in a nice formal way  PRON can test each function PRON make separately   at the beginning  PRON can and start integrate over simple geometry and declare may parameter at the start as constant  as time go by  if PRON want to change the geometry  PRON can do PRON easily  have PRON make assumption at the beginning  PRON would have to change the whole code every time  
__label__predictive-modeling __label__probability PRON have a dataset where a set of people donate for charity along with the date of the donation  PRON have to find the probability of each donor donate in the next three month   data be available from august 2014  february 2016  PRON have to predict the probability of each person donate for march  june 2016   any help would be appreciate   below be a snapshot of the datum  PRON would  date  amount  1  13  08  14  2485  1  21  11  14  2105  1  17  09  15  1359  2  13  08  14  2542  2  20  04  15  1276  2  12  10  15  2694  3  20  11  14  3556  4  28  07  15  3383  5  13  08  14  1698  5  11  12  14  1725  5  09  06  15  1376  5  17  09  15  3230  regards  first PRON need to find in what kind of distribution be PRON data  linear  exponential  normal  etc   after that PRON need to find the area size under the equation in which PRON event will occur   PRON be all come down to what kind of distribution PRON be on   please double  check if there be the only datum PRON have get  because all PRON have be a single predictor date   if this be indeed PRON only datum source  then PRON only have a single predictor  and PRON independent variable be continuous  now  PRON should plot date vs amount and fit a single linear regression  do the fitting look good  only PRON can tell because PRON do not have the full data  set   if PRON be not a good fit  look at the plot and ask PRON do this look like a curve  if so  PRON may want to fit a spline curve or something like that   PRON should also check the autocorrelation  this make sense because PRON datum look like a time series  PRON will need to check PRON PRON   if this be the case  PRON may want to consider ma and arch model   PRON be not possible for PRON to give PRON accurate advice because PRON do not know PRON datum   PRON think PRON could use time series modeling algorithm as studentt say  also PRON can make window time to find relation between new donate and previous donate and PRON can use amount  may be people with high payment and low payment have different behavior  first of all PRON should change PRON datum in a way that fill gap  PRON mean PRON should add datum about month that a person have not any payment   after that PRON should make a table like this   personid  monthor day or week or 3month   count of payment  count of payment last month sum of amount that pay last month be pay last month  then PRON should find whether PRON file be useful and independent or not  and try to add other file  and then build PRON model   good luck   PRON can use binary logistic regression for this analysis   prior to use binary logit  PRON would have to spend some time prepare the datum for this analysis   PRON can create several rfm type of feature from this datum set  example  number of donation  time between donation  time since the most recent donation  time since the first donation  average donation amount  the amount of first donation  the most recent donation amount  etc   PRON can provide more example  if need    since PRON task be to predict the probability of donation during a four  month timeframe  mar  jun 2016   PRON can create those feature  lead indicator  for each donor as of the end of october 2015  all lead indicator would be create base off of timeframe prior to that cut  off point  PRON observation window be from nov2015 to feb2016  this be where PRON event flag  dependent variable  should come from  1 if a donor donate  again  during the observation window  and 0 otherwise   in order to make this model generalizable  PRON would recommend pull several such cross  section of PRON datum  in addition to the october 2015 slice explain above    PRON could try use the markov model    an illustration of which can be find here   also  PRON could detect pattern in the dataset by plot PRON and then figure out which algorithm to use base on what be the degree of correlation and the nature of the plot   also  what be the number of user PRON have   PRON could group together datum of each user and run the algorithm for the user that be be ask for   regression be the way to go if PRON want to know if a user will donate or not  to find the probability  try markov  
__label__machine-learning __label__social-network-analysis be there any research indicatng that PRON be possible to predict future  some event or objective economic parameter like currency exchange rate  from post on social network  comment  twit  forum  etc     thanx  predict  future  especially in the context of economic be a well  research domain  for example  PRON can check out the research involve  predict stock market use social networking   PRON be sure PRON would find such result for other domain as well  
__label__orange PRON would like to use the feature constructor to extract number from a column of string   input  output  sc 3224  gt  3224  south  gt  0  322523  gt  322523  s1c 4  gt  14  137  gt  137  internal orange 370 documentation say for continuous variable PRON only have to construct an expression in python   great   PRON find information about allow function in the feature constructor code    only expression with no list  set  dict  generator comprehension   be accept    lt  gt     allowed     ellipsis    false    none    true    ab    all    any    acsii     bin    bool    bytearray    byte    chr    complex    dict     divmod    enumerate    filter    float    format    frozenset     getattr    hasattr    hash    hex    PRON would    int    iter    len     list    map    memoryview    next    object     oct    ord    pow    range    repr    reverse    round     set    slice    sort    str    tuple    type     zip     base on the list  PRON be hop to use an expression like this   intfilterstrisdigit  inputstringcolumn  or 0    but that be an  invalid expression   no further feedback   how can PRON extract a number from a string   
__label__python __label__dataset __label__tensorflow __label__training PRON be build a function to pass training information for PRON tensorflow model   PRON be similar to keras   modelfitfeature  label  epochs150  batchsize10   the dataset in use be pima indian which contain exactly 768 record   in keras when PRON pass epochs150  batchsize10 PRON run fine  in PRON model  since at epoch 78 there be no more datum  78  10   PRON start feed empty information   what would be the correct behavior when train a model and number of epoch already consume all datum for model  the obvious answer would be stop training or continue feed datum to model and restart counter and start pass information from begin  any feedback be appreciate   def importdatafilename  batchsize  stride    if filename   dataset  nploadtxtfilename  delimiter       expand dimension of label  if stride  gt  1   start  batchsize  else   raise valueerrorinvalid batch size    feature  dataset    08   label  npexpanddimsdataset    8   axis1   return featuresstart  stride   labelsstart  stride    batchsize  10  for step in rangenumepochs     datum  print step  batchx  batchy  importdatafilename   batchsize  step   batchsize   step  1    print batchx  batchy  
__label__machine-learning __label__neural-network __label__deep-learning how do neural network account for outlier and overfitt   here be a math answer for PRON   neural network be an approximation function  ftheta of the joint distribution  px  y of input datum  x and label  y the learning process be the process of tweak parameter  theta to make  f as close as possible to  p    ftheta  approx px  y  side note  usually  f be consider to approximate the conditional  pyx  but PRON can be view more generally   so  in this term  outlier be the value   x  y clearly outside of the distribution  px  y hence  a neural network can not account for PRON  this have to be do separately  one possibility be to gather more datum to make these outlier look ordinary  ie  from  px  y  as for overfitt  PRON be completely different term  which relate to inability to generalize  this happen because in practice the true distribution  px  y be never fully know  instead  the research have the sample of PRON    hatx   haty  aka  the training datum  if the dimensionality of  theta be large   f can approximate  phatx   haty so well that PRON actually learn the noise and fail to capture  px  y there be way to deal with overfitt  most importantly regularization  which be equivalent to add noise to   hatx   haty so the answer to this question be  PRON can be extend to account for PRON  by use special technique  but if the researcher do nothing  the neural network will not learn to deal with overfitt by PRON  
__label__sph PRON have a 2d implementation of smooth particle hydrodynamic up and run  however when PRON try to move PRON to 3d  use the appropriate 3d kernel  particle always tend to go apart from each other  since PRON 2d implementation work fine and the only thing PRON have change be the kernel  the only parameter that affect PRON be the length of the support   be there a good way to calculate a good value for  h rather than just randomly check   PRON be not an expert in sph and so other should chime in as well  but here be one consideration   h need to be choose so that within a neighborhood of radius  h around a point  there be other point  so  h be relate to the average distance between point  the thing that be different between 2d and 3d be that with the same number of point  PRON average distance be much large in 3d  for example  if PRON scatter  10  6  point in a 2d domain that be the unit square  then on average PRON be 0001 apart   10  6  particle in 2d form  if place purposely  a  1000times 1000  lattice  where in 3d PRON be 001  ie  10 time farther apart  because in 3d  PRON form a  100times 100times 100  lattice   PRON will have to adjust PRON  h correspondingly   two thing  ensure the summation of kernel value which be  sum  mrho  wr  h be amount to 10 to a desire accuracy  check if PRON be use correct value for mass of each particle  if compute from reference density  if PRON use the same value of h as in 2d  PRON be suppose to give a stable simulation   the most important be that the kernel have to be normalise within the influence radius  delta    intdeltadelta wr  rhdr1  the initial interparticle distance in the grid should be calculate as   dxsqrtfracdelta2pin  in 2d or   dxsqrt3frac4delta3pi3n  in 3d  n be the estimate number of neighbour   napprox20  in 2d and  napprox 50  in 3d  if these be satisfied and PRON assume that the parameter be physically correct  both 2d and 3d simulation should be stable use the same parameter  
__label__linear-algebra PRON have the follow set of linear equation    am1nam1nmam  n1am  n1m2n2am  nfm  n  here  m and  n run from 1 to  n  so there be  n2  equation for the unknown  am  n fix boundary condition be use such that  an1n0  etc   PRON would like to solve this system numerically  and for that PRON need to first write PRON in matrix form  m a  f how can PRON programatically generate the coefficient matrix m   edit  try to follow wolfgang s suggestion  ok  so let take the simple case of n2  write the equation by hand and extract the coefficient result in the matrix equation    leftbeginarraycccc   2  amp  1  amp  1  amp  0  1  amp  5  amp  0  amp  1  1  amp  0  amp  5  amp  2  0  amp  1  amp  2  amp  8  endarrayrightleftbeginarrayc   a11  a12  a21  a22   endarrayrightleftbeginarrayc   f11  f12  f21  f22   endarrayright  now if PRON follow wolfgang s suggestion and replace   m  n by  i PRON get a different matrix    leftbeginarraycccc   5  amp  1  amp  2  amp  0  1  amp  5  amp  1  amp  1  2  amp  1  amp  8  amp  1  0  amp  1  amp  1  amp  10  endarrayrightleftbeginarrayc   a1  a2  a3  a4   endarrayrightleftbeginarrayc   f1  f2  f3  f4   endarrayright  PRON need to re  enumerate PRON degree of freedom  right now  PRON have two index  1le m  nle n these span a rectangular array  enumerate PRON from  i0  to  n2  1   PRON turn out that zero  base indexing be so much simple   for example by define  im  nm1nn1 in PRON case  this would correspond to row  wise numbering  PRON can recover the original index by observe that  min1  and  ni  n1  where the two operation be do in integer arithmetic  then PRON can substitute these definition in PRON formula everywhere   specifically  if PRON write PRON linear system in term of  ai  not  am  n  then PRON will get this set of equation      ai1ai1in1ainai  nin12i  n12ai  fi     where PRON have again assume that PRON do the operation  in and  i  n in integer arithmetic   this be the equation PRON have for  i0ldot  n2 now write each of these  n2  equation down one by one and PRON get the correspond matrix entry  
__label__finite-element __label__convergence __label__numerical-analysis consider a method  eg  fem  with variable approximation order  p now  PRON know that the optimal order of convergence be give by    e  c hp1  where  h denote the mesh size and a constant  c that do not depend on  h as a result  PRON can study the experimental order convergence  or  hconvergence  on a series of grid  h  frach2   ldots because PRON can directly compare two error  e1  and  e2  on different grid   on the other hand  PRON have always be puzzle by the way people perform and evaluate  pconvergence study in this context  here  PRON have a fix grid with mesh size  h  but different approximation order  p123ldots PRON problem now be that the constant  c do not depend on  h  but PRON do depend on  p  otherwise  PRON would see that the line in a log  log  plot of the  hconvergence result for different value of  p would intersect in a single point  which PRON  in PRON experience  do not do   now  what be the significance of a  pconvergence study have realize that PRON can not really compare two error   e1  and  e2   use the above formula for the error  give that PRON have be obtain use different polynomial degree   this may be answer a different question that what PRON ask  but PRON think a chart like that be more about call out to a reader whether p  adaption or h  adaption be more effective for converge the underlying model  although the x  axis be often take to be mesh granularity  h  or number of unknown  if PRON normalize PRON to something that quantifie  work   eg ram consumption  flop require  elapse wallclock   then PRON could gather at a glance whether p  refinement or h  refinement be the good strategy to hit a give error bind with minimum  work   certainly this be problem dependent  in particular  whether or not the underlying field be smooth or singular   but a couple of graph like that   work  vs error  could immediately demonstrate eg how coarse  h high  p be a good strategy on smooth problem  but fine  h low  p be good on say a singularity  dominate re  entrant corner   grant  on real application that mix those two regime  the answer PRON be mix  hence the motivation towards adaptive refinement   PRON can conduct PRON  p convergence experiment for different  h since the  constant   c depend on  p  but not on  h  this allow PRON to separate the effect of the  hp1 and the  cp  the estimate of the form  cp  hp1 be justify by the fact that the  cp be typically polynomial in  p  and thus for small  h and large  p  the estimate be dominate by the exponential term   PRON be not so much that PRON want to compare the  prefinement and  hrefinement error directly  instead PRON want to compare the convergence property  eg speed  of each refinement strategy  this require more knowledge of the constant in the apriori error estimate  PRON will illustrate by look at the apriori error estimate of a discontinuous galerkin spectral method  which can be interpret as a high  order fem    PRON be true that the constant  c in the inequality   e leq chp1     depend on the approximation order  p thus  PRON be not immediately clear what to expect when PRON increase the order  p however  the error inequality above hide a lot of information about the convergence behaviour in that constant  in  prefinement PRON become important how the  c depend on  p  in a 1d spectral element method the error estimate on a give element  k have a form  derive in a similar fashion as in canuto et  al e leq mathcalchminm  p1pfrac32mqkhmomega       where  mathcalc be independent of  h and  p   qk be the true solution on element  k  and  hm be the  m order sobolev space  in PRON apply the numerical method to a problem with a  smooth  solution PRON will generally be true that  p1leq m now PRON can more readily see the difference in  p versus  h refinement   if PRON fix  p then the term   pfrac32mqkhmomega     be a constant that PRON absorb into  mathcalc to obtain the  c in the first error estimate  now PRON can perform a standard  hrefinement argument and see that the convergence will be similar to that of a finite difference scheme  ie  some polynomial order on  h  however  if PRON fix the grid  h then the term  hminm  p1 be constant and the error estimate look like   e leq hatmathcalcpfrac32mqkhmomega  where  hatmathcalc    mathcalchminm  p1 now PRON immediately see that the rate of convergence change in the  prefinement study  provide  mgtfrac32 the error decay as a power of  p for  very smooth  solution  eg infinitely differentiable  the decay in the error be fast than any polynomial order  sometimes this be refer to as exponential convergence because if PRON look at a semilog plot of the error against  p PRON see a straight line   as a quick aside PRON note that in spectral method the convergence rate depend on the smoothness of the solution  heuristically  PRON can interpret this as  the large  m be in the error estimate  the smooth the solution   because the true solution be in a high order sobolev space  ie  the true solution be square integrable up to the  mtextrmth order derivative  the convergence of a sobolev norm be often view in term of the decay of fourier coefficient of the solution  qk  particularly for fractional derivative   this be one reason the family of method be refer to as spectral method  
__label__clustering __label__k-means PRON be try to test how well PRON unsupervised k  means cluster properly cluster PRON datum   PRON have an unsupervised k  means clustering model output  as show in the first photo below  and then PRON cluster PRON datum use the actual classification   the photo below be the actual classification   PRON be try to test  in python  how well PRON k  means classification  above  do against the actual classification   for PRON k  means code  PRON be use a simple model  as follow   kmean  kmeansnclusters4  randomstate0fitmydata   label  kmeanslabels  what would be the good way for PRON to compare how well PRON unsupervised kmeans clustering model do against the actual classification   since PRON have the actual label  PRON can compare PRON with the obtain label and evaluate performance  typically purity and nmi  normalize mutual information  be use  read this  evaluation of clustering  document for detailed explanation   if PRON do not have actual label then PRON can use modularity or  silhouette  for measure cluster performance   another possibility to check the performance be to evaluate the confusion matrix  therein the predict and actual label be compare and the classification result for each class be measure 
__label__optimization __label__algorithms __label__reference-request __label__complexity PRON want to know the good optimal algoritm of gcd with PRON complexity if PRON have a any useful source PRON will be glad to have a look at PRON   current state of the art use variant of euclid s algorithm  yet  PRON be prove to be non  optimal   httpdspaceucalgarycabitstream1880466012198937638pdf  even though PRON variant have significant speed improvement  PRON have not hear of a sequential optimal one  PRON guess one of the good modification be lehmer s algorithm  extended euclidean algorithm be also worth mention   furthermore  PRON guess optimality  or at least guarantee bound be obtain on parallel machine  one of such profound work prove a sequential complexity of on2logn   while achieve a paralel complexity  which be worth take a look at   httpwwwcsienukedutwcychengcdtwo20fast20gcd20algorithmspdf 
__label__education PRON have now see two datum science certification program  the john hopkins one available at coursera and the cloudera one   PRON be sure there be other out there   the john hopkins set of class be focus on r as a toolset  but cover a range of topic   r programming  cleaning and obtain datum  data analysis  reproducible research  statistical inference  regression models  machine learning  developing data products  and what look to be a project base completion task similar to cloudera s data science challenge  the cloudera program look thin on the surface  but look to answer the two important question   do PRON know the tool    can PRON apply the tool in the real world    PRON program consist of   introduction to data science  data science essentials exam  data science challenge  a real world data science project scenario   PRON be not look for a recommendation on a program or a quality comparison   PRON be curious about other certification out there  the topic PRON cover  and how seriously ds certification be view at this point by the community   edit  these be all great answer   PRON be choose the correct answer by vote   the certification program PRON mention be really entry level course  personally  PRON think these certificate show only person s persistence and PRON can be only useful to those who be apply for internship  not the real data science job   PRON do the first 2 course and PRON be plan to do all the other too   if PRON do not know r  PRON be a really good program  there be assignment and quiz every week  many people find some course very difficult  PRON be go to have hard time if PRON do not have any programming experience  even if PRON say PRON be not require    just remember  PRON be not because PRON can drive a car that PRON be a f1 pilot   not sure about the cloud era one  but one of PRON friend join the john hopkins one and in PRON word PRON be  brilliant to get PRON start   PRON have also be recommend by a lot of people  PRON be plan to join PRON in few week  as far as seriousness be concern  PRON do not think these certification be go to help PRON land a job  but PRON sure will help PRON learn   as a former analytic manager and a current lead data scientist  PRON be very leery of the need for datum science certificate   the term datum scientist be pretty vague and the field of data science be in PRON be infancy   a certificate imply some sort of uniform standard which be just lack in datum science  PRON be still very much the wild west   while a certificate be probably not go to hurt PRON  PRON think PRON time would be better spend develop the experience to know when to use a certain approach  and depth of understanding to be able to explain that approach to a non  technical audience   there be multiple certification go on  but PRON have different focus area and style of teaching   PRON prefer the analytics edge on edx lot more over john hopkins specialization  as PRON be more intensive and hand on  the expectation in john hopkins specialization be to put in 3  4 hour a week vs 11  12 hour a week on analytics edge   from an industry perspective  PRON take these certification as a sign of interest and not level of knowledge a person possess  there be too many dropout in these mooc  PRON value other experience  like participate in kaggle competition  lot more than undergo xyz certification on mooc   PRON lead data science team for a major internet company and PRON have screen hundred of profile and interview dozen for PRON team around the world  many candidate have pass the aforementioned course and program or bring similar credential  personally  PRON have also take the course  some be good  other be disappointing but none of PRON make PRON a  data scientist    in general  PRON agree with the other here  a certificate from coursera or cloudera just signalize an interest but PRON do not move the needle   there be a lot more to consider and PRON can have a big impact by provide a comprehensive repository of PRON work  github profile for example  and by network with other datum scientist  anyone hire for a data science profile will always prefer to see PRON previous work and coding style  ability   op  choose answer by vote be the bad idea   PRON question become a popularity contest   PRON should seek the right answer  PRON doubt PRON know what PRON be ask  know what PRON be look for   to answer PRON question   q  how seriously ds certification be view at this point by the community   a  what be PRON goal from take these course   for work  for school  for self  improvement  etc   coursera class be very apply  PRON will not learn much theory  PRON be intentionally reserve for classroom setting   nonetheless  coursera class be very useful   PRON would say PRON be equivalent to one year of stat grad class  out of a two year master program   PRON be not sure of PRON industry recognition yet  because the problem of how do PRON actually take the course   how much time do PRON spend   PRON be a lot easy to get a s in these course than a classroom paper  pencil exam   so  there be be a huge quality variation from person to person   PRON think the effect of the certification from coursera be dependent on the individual as well as the class  the requirement say min 3  5 hour a week  if PRON put more  and the material do open up for a lot more than the 3  5 hour  then these class and certification can be equivalent to strong knowledge base and experience in the field  science come to those who request PRON   PRON be almost do with johns hopkins data science specialization on coursera  a course and a capstone leave to graduate   PRON will just give PRON the pro and con of PRON  try to keep PRON as objective as possible   pro   structure around the learning process  PRON will build a portfolio over time  con   different background need for different course  the first few course do not assume previous knowledge  PRON suddenly get not easy to understand in the conceptual course   statistical inference  regression analysis   teach by 3 professor  PRON think PRON be not on the same page about PRON potential audience and PRON ability  need  interest   the good way to be successful at get the job that PRON want PRON to show that PRON can do PRON   the mooc that PRON mention will give PRON a good grounding in the basic and should be enough to get PRON start solve PRON own machine learning  data science problem  try a kaggle competition or two  that be a great way to improve PRON skill  and a decent grade there will be of interest to a potential employer  publish PRON result on github use something like an ipython notebook  which will allow PRON work to be easily see and judge   try an analysis on other public data set  like the uci bike sharing dataset  or the uci diabetes treatment dataset those be lot of fun to try  and show that PRON be keen and willing to develop PRON skill   PRON really depend on the credibility of the institution grant the certificate  for example  data science certification from a harvard  base company be recognize by many industry partner and may make a good choice  PRON do not say what kind of certificate PRON be look for  
__label__interpolation __label__image-processing __label__computer-vision PRON be try to find an approach for integral image resizing   PRON find out that PRON can do PRON with a bilinear interpolation method  but with this approach PRON can only resize by the factor which be a power of two   PRON find a paper  speed up object detection  fast resizing in the integral image domain  which describe a formula which allow to resize arbitrarily     iirx  y  approx frac14a2space cdot bilinearii   2ax  b  2ayb    2a space  resizingspace factor    b space  resizingspace filterspace offset  PRON would like to ask about explanation of this formula  PRON assume that PRON have to find a correct value of resize filter offset but PRON do not know how to do this   
__label__algorithms __label__statistics __label__r PRON distribute PRON datum into 90  training and 10  testing  then PRON build a boost regression model use gbm boost package in r which be very similar to the famous adaboost regression model   there be several parameter that need very careful setting while training  so PRON try several combination between these parameter  ntree  shrinkage  depth  number of observation in leaf node   etc    PRON select the combination that achieve the high accuracy on PRON 10  testing case  PRON use the same 90  training datum for select the optimal parameter setting  PRON do not redistribute the datum each time PRON train to select a parameter    after that PRON use PRON train model  use the optimal parameter setting  to do feature selection and then PRON use 5 fold cross validation to retrain the model with feature select   finally  PRON test the model on the 10  separate datum earlier  PRON main concern now be that PRON think PRON have over  fitting issue  so PRON need PRON suggestion about such datum distribution and whether PRON think that there be over fitting or not   any suggestion on this  PRON would be highly appreciate   
__label__quadrature PRON be deal with a tricky integral that exhibit nans at certain value near zero and at the moment PRON be deal with PRON quite crudely use an isnan statement which set the integrand to zero when this occur  PRON have try this with the nms library in fortran  the q1da routine  q1dax be no different  and with the gsl library in c  use the qags routine    PRON look into cquad  part of the gsl library for c  which be specifically design to handle nans and inf in the integrand  but there be very little useful info in the reference and no example program online that PRON could find  do anyone know any other numerical integration routine for either c or fortran which could do the job   PRON be the author of cquad in the gsl  the interface be almost identical to that of qags  so if PRON have use the latter  PRON should not be difficult at all to try the former  just remember not to convert PRON nans and infs to zero in the integrand  the code will deal with these PRON   the routine be also available in octave as quadcc  and in matlab here   could PRON provide an example of the integrand PRON be deal with   update  here be an example of use cquad to integrate a function with a singularity at one of the endpoint    include  ltstdiohgt    include  ltgsl  gslintegrationhgt     PRON test integrand     double thefunction  double x  void  param    return sinx   x       driver function     int main  int argc  char  argv      gslfunction f   gslintegrationcquadworkspace  ws  null   double re  abserr   sizet neval     prepare the function     ffunction   ampthefunction   fparam  null     initialize the workspace     if   ws  gslintegrationcquadworkspacealloc  200     null    printf   main  call to gslintegrationcquadworkspacealloc failedn     abort         call the integrator     if  gslintegrationcquad   ampf  00  10  10e10  10e10  ws   ampres   ampabserr   ampneval    0    printf   main  call to gslintegrationcquad failedn     abort         print the result     printf   main  int of sinxx in  01  be  16e    e   i evalsn    re  abserr  neval      free the workspace     gslintegrationcquadworkspacefree  ws      bye     return 0     which PRON compile with gcc g wall cquadtestc lgsl lcblas  the output be  main  int of sinxx in  01  be 94608307036718275e01   4263988e13  63 eval    which  give the result compute in maple to 20 digit   094608307036718301494   be correct to 14 digit   note that there be nothing special here  neither to tell cquad where the singularity be  or any special treatment within the integrand PRON  PRON just let PRON return nans  and the integrator take care of PRON automatically   note also that there be a bug in the late gsl version 115 which can affect the treatment of singularity  PRON have be fix  but have not make PRON to the official distribution  PRON use the most recent source  download with bzr branch httpbzrsavannahgnuorg  r  gsl  trunk  PRON could also check out the double  exponential quadrature formula  PRON do an  implicit  change of variable  make sure that PRON  ease  away  boundary singularity  a very nice  fortran77 and c  implementation can be find on ooura s website  
__label__petsc PRON be look at the documentation for petscbagcreate    and PRON say that  the size of the a struct must be small enough to fit in a petscint  by default petscint be  4 byte  the warning about cast to a short length can be ignore below unless  PRON a struct be too large   PRON assume that this mean that a pointer to the struct must be small than a petscint  the example use struct that contain petscints as well as other thing   that say  PRON be under the impression that pointer on 64 bit machine can be large than the 32 bit in a petscint   what do this restriction on the size of the struct mean for user   can petscbags be use on 64 bit system   petscbag be mean for hold parameter or other redundant datum  so PRON be not a place to put huge datum like distribute model state  PRON can be use with 64bit integer  but unfortunately  the format be not binary  compatible with the 32bit integer format  PRON could be fix to be backward  compatible at the expense of make the format slightly more complicated  but PRON doubt anyone be volunteer to do that just now  as for PRON question   what do this restriction on the size of the struct mean for user   PRON should put less than 2 gb in a petscbag  since the data be load strictly  instead of lazily   PRON want this anyway to be able to load PRON with a reasonable amount of memory   can petscbags be use on 64 bit system   absolutely  and the format be compatible between 32 and 64bit system and big versus little endian system  
__label__numerical-analysis __label__computational-physics PRON be wonder if this be the right place to ask this question  PRON find a related question on stack overflow httpsstackoverflowcomquestions814312physicsincomputerscience but PRON be close  so PRON be ask PRON question on se physics   PRON be plan to study graduate level computational physics study  for example  PRON involve programming to solve problem in linear algebra  pde  ode  more specific example be finite volume  finite element  iterative method to solve nonlinear equation   PRON be mechanical engineering major student with basic programming knowledge  during PRON me coursework  PRON do not program heavily  PRON just take a look at the undergraduate cs degree curriculum  and find that PRON do not know most of the cs stuff  algorithm  theory of computer science  parallel computing  etc  this knowledge may be vital in program efficiently  in term of human time spend on programming  PRON be feel that a cs major student can far easily program and debug  something that can take hour for a beginner physics student without cs or computer engineering knowledge   PRON be confuse how a me major student can undertake computational physics study without cs or computer engineering background  be knowledge of a programming language enough   can anyone comment here  thank in advance   PRON be correct that with respect to the software side of thing  PRON be possible a typical cs student who have program a lot in PRON undergrad could perform much quicker than someone with basic programming experience  PRON may not have the solid physics background PRON could  though  so obviously both side have PRON weakness with respect to computational physics   the thing about get involve in computational science be not only do PRON need to have a strong footing in the actual science  but PRON ideally would have sufficiently strong software and algorithmic ability  as wolfgang mention in the comment  a big part of this be just practice  thing like datum structure be a useful topic to study as well provide PRON have the time  but PRON could get away with a basic understanding of what each datum structure be best use for and just use the data structure implementation that come along with language library   at the end of the day  the most beneficial thing PRON could probably start do be try to get a lot of practice programming with whatev languages  PRON expect PRON may use  PRON would probably recommend look into fortran or c  maybe c after PRON understand the basic of c  PRON would also recommend get a book or two to not just help with learn the language  but learn to build good software  some book that come to mind be api design in c and clean code  note that api design in c actually teach a lot of language independent key to build good software  so this be useful whether PRON use c or not  PRON stress develop this background because without PRON  PRON may end up building code that be hard to maintain  that be buggy  and in turn result in lot of lose time in the long run  take the effort to build something clean and modular can really save time in the long run even if PRON put up a little more effort up front   after PRON have these fundamental down  then PRON should investigate more advanced topic like parallel computing   a typical computational physics class will have a modest expectation on the student  ability to program  the physics curriculum often cover only basic programming  from the  coding  point of view  PRON should not worry that much   if PRON be worried anyway  consider follow a tutorial online where the language be the one use in PRON plan class   PRON have basically no programming experience  except for a one semester class PRON take in first year  octave  when PRON start solve problem in computational physics  PRON be basic quantum mechanic   as part of PRON honours year coursework   in the first week of learn how to code in fortran  PRON do trivial thing  like format text for printing  and compile source code contain in one file   in the second week  PRON solve the 1d schrodinger equation use a shooting method   by the fourth week  PRON be cod the numerov  cooley method  and PRON be a few week later  this be a half  semester course   after a few more assignment  that PRON be propagate a gaussian wave packet   the math involve be a little tricky at first  but PRON be the programming which make life very difficult for the uninitiated   PRON be about the fifth week when PRON learn about option to see helpful error message when compile PRON program   PRON mean PRON know which array reference be out of bound  and PRON have not be tell PRON   PRON be livid that this have not be consider important enough for the lecture slide   anyhow  enough about PRON  and PRON hopelessly inefficient 80 hour coding week which be require to ace that course  the point be that PRON do not want to go through what PRON go through  and PRON do not have to   here be the lesson  in order of importance   1   do not bother with fortran or c as PRON first language  if PRON be try to solve mathematical problem   these language  especially c  will only slow PRON down in reach PRON short term goal of complete a scientific computing course   instead  learn a language which be design to help PRON write useful code faster  python be ideal  but octave and matlab have PRON advantage too    with python and the appropriate module instal  mainly just numpy   PRON will have no problem perform  simple  task like read datum file into array  sort the datum  and subject PRON to standard linear algebra operation   in fortran  on the contrary  PRON be likely to give up try to write PRON own parser for input file  or smash the keyboard after get every one of the ten thousand argument to a lapack routine incorrect   c be good to learn than fortran  but in PRON case  PRON will only make write numerical routine more frustrating  as well as look much uglier in PRON text editor  c be a horrendously ugly language  at least fortran code look a bit like math on the page    2   ask for help early  regularly  and online   do not allow simple  idiosyncratic trick of the trade to waste all PRON free time   PRON be ok that PRON do not know that stuff  PRON be unintuitive   3   if PRON can not debug the code in time  do not destroy PRON with hack and arbitrary  bug fix    submit PRON concise pseudo code  along with the source code  and provide a clear writeup of the problem   sometimes  only time and a clear head will help  while at other time  equally often when PRON be first learn  PRON just need the help of someone more experienced   4   get PRON math right before PRON start plan PRON code   algorithms be understand on paper by human  and then teach to computer via programming language   if PRON get the algorithm wrong  PRON will never be able to find the bug  because PRON do not actually exist  PRON just cod the wrong algorithm  correctly    5  forget about parallel computing  until PRON have  master  write program for one processor   most computational scientist just lean on the expertise of colleague here   in summary  learn python as quickly as possible  and try use numpy to solve linear algebra problem   use of a language like python will make PRON life a million time easy  
__label__game-ai __label__philisophy __label__alphago a human player play limited game compare to a system that undergo million of iteration  be PRON really fair to compare alphago with the world  1 player when PRON know experience increase with the increase in number of game play   yes PRON be  if PRON ever compare computer to human PRON should take into acount the fact that computer can work 24 hour a day every day and fast than human  that be big advantage of computer over human    be PRON fair to compare alphago with a human player   depend on the purpose of the comparison   if PRON be compare ability to win a game of go  then yes   if PRON be compare learn ability  then maybe  PRON depend on the task  alphago and system like PRON be capable of learn only in well  describe limited domain  there may be an analogy with sensory learning  PRON may even be possible in theory to take a small piece of brain tissue and run an algorithm similar to alphago s learning process on PRON    in general  the approach use by alphago and other reinforcement learning success be  trial  and  error plus function approximation   PRON seem analogous to perception and motor skill  such as object recognition or rid a bike  as oppose to reasoning skill and game as human play PRON  which go through many more cognitive and conscious layer that have no real analog in a rl system like alphago   a human player play limited game compare to a system that undergo million of iteration  this be an advantage of a machine to learn this kind of task  PRON would equally apply in other simulated environment with simple rule  if PRON goal be to have the most skilled and optimal navigation of such a domain  the implication now be that PRON would not train a human expert through year of study  but to write the simulator and train an alphago  like machine   this be no different a comparison than decide car and road be good solution to long distance travel for the general population than walk or horse and cart  PRON do not matter what underlie the advantage of one over the other  the assessment be cost  benefit  which resolve to a single comparable number   PRON would  however  be wrong to assess alphago as a good general  purpose learning engine than a human  the fact that human do not have to work fully through million of simulation in full detail be important  PRON mean that something about how human learn be still not cover by learn machine  some of these thing be understand and be discuss  such as the ability to focus intuitively on important aspect of what to learn  the ability to reason about the environment  learn analogously or transfer learn from other domain   if PRON read through the abstract of chess ai paper  PRON be often point out that human  search  the chess game tree much more efficiently than computer  which be why PRON be so hard to beat the top human in chess for so many year    the human efficiency may have to do with intuition and judgement  which be difficult to replicate   confidence level  for ai evaluation be one method of address these issue  as be  monte carlo    but PRON be also important to note that human be far more limited in the depth and breadth of PRON  search   which be why  now that PRON have the right algorithm  human can no longer win    be PRON fair   perhaps the more salient question be   be PRON useful to compare alphago to a human player   PRON most certainly be  because PRON tell PRON that PRON have be sometimes term a  strong  narrow ai  that can outperform a human in a single task   why alphago beat lee sedol be a big deal be the complexity of go  the intractability of the go game tree  and the fact that computer be previously ineffective against high  level human go player   this human vs ai evaluation do not strictly fall under the  turing test   imitation game   PRON do fall squarely under the maxim of protagoras that  man be the measure of all thing    this be critical because intelligence be a spectrum  and gauge strength of intelligence  in the context of intractable problem  problem that can not be fully solve due to PRON size  be a function of relative strength of two agent  whether human or ai   this relative assessment be all PRON have  and all PRON may ever have for certain set of problem   the problem with human be not that PRON be not clever  but that PRON mind have cognitive limitation   so to tackle certain problem  intelligent machine be useful   there be no such thing as fairness when compare  PRON define a measure for performance and then compare the value of the measure   one sensible measure for play the game of go be the  number of game win   regardless of any investment in the development of the system  computational or sample efficiency  alphago be currently at the top by this measure   another sensible measure could be  number of game win under a restriction on sample efficiency during training   as other point out  such a measure could be much more favorable for human  
__label__deep-learning __label__convolutional-neural-networks PRON be work on software which deblur the motion blur create by camera movement   PRON have survey some research paper and determine this process require deep learning and cnn  now PRON be look for some book that would be useful in get a more complete picture of the process   if PRON be new to neural network  PRON recommend study the free online book neural networks and deep learning  PRON teach PRON the basic concept and the underlying math  a great starting point to dig deep   once PRON understand the basic concept  PRON recommend watch the stanford lecture cs231n about convolutional neural networks for visual recognition  PRON consist of 16 lecture and teach PRON most of the thing PRON will need to know for PRON project  the also provide all the slide  check out the link below each video   analyze existing project and PRON documentation be a good next step for deep understanding of what be state of the art in cnn  PRON could start with this article about q  learning base on a cnn architecture  but there be many more out there   more specific question will come during PRON project  PRON will find more information in scientific paper and blog from researcher  feel free to ask here as well  if PRON do not find what PRON be look for elsewhere  
__label__machine-learning __label__algorithms __label__data PRON be beginner to datum science  PRON find that some machine learn algorithm perform better  when give particular kind of dataie  numerical  categorical  text  graphical    PRON search about this topic on the web  but no luck   PRON would like to know what kind of datum perform good accord to give machine learn algorithm   PRON be good to explain briefly why certain kind of datum suitable for certain machine learn algorithm   hope answer to this question will help beginner in data science   update  PRON be good if PRON can explain what be the type of datum most suitable for follow algorithm   naive bayes  svm  regression  k  means  deep neural network   give the list of algorithm PRON provide  these fall under 3 major classification of ml algorithm   1  classification algorithms  naive bayes classification  decision tree  random forest  knn  support vector machine  svm   neural networks  etc   2  regression algorithms  linear regression  logistic regression  lasso regression  etc    note  although logistic regression have regression in PRON name  PRON be essentially a classification algorithm   3  cluster algorithms  k  means clustering  fuzzy c mean  mixture of gaussian  etc   PRON may also be know there be 4 type of ml algorithm   1  supervised learning  2  unsupervised learning  3  reinforcement learning  4  semi  supervised learning  among these 4  first and second be the most important one   supervised learning be apply when PRON have a label data set ie  PRON already PRON output variable  dependent variable  for example  a data set which contain the size of the house  independent variable  and correspond house price  dependent variable   PRON can predict the house price of new datum point with respect to the size of the house  another example be  determine if a tumor be harmful or not harmful when PRON already have a list of tumor which be harmful or not   in supervised learning PRON know the problem statement and have all the necessary feature to get answer   in unsupervised learning  PRON do not have label datum  PRON do not have any output variable  PRON do not know the problem statement  PRON be apply when PRON need to find a structure in the datum set and extract meaningful insight out of PRON  for example  a data set of walmart contain PRON customer s buying pattern  give this  walmart will ask PRON datum scientist to extract some meaning  the data scientist may choose to apply k  means clustering and find how customer be segment  group a customer  buy x  y  z product  group b customer  buy u  v  x product   classification and regression algorithm be use when deal with a supervised learning problem and cluster algorithm be use when deal with unsupervised learning   now come back to PRON original query   1  naive baye  best apply to a data set contain multiple feature  independent variable  and an output variable which take two discrete value  yes  no   thus  categorical datum   2  svm  best apply to a data set contain infinite number of feature and PRON need to reduce these feature down to a number so that PRON can be compute  since PRON be a classification algorithm so PRON good work upon categorical datum   3  regression  linear regression be apply to a continuous numerical datum set in which the dependent and independent variable exhibit linear relationship  for example  size of the house vs house price   logistic regression be a classification algorithm so PRON be best apply to categorical datum   3  k  means  k  means can apply to many type of data set  what PRON do be segment datum point into cluster  datum point with similar feature be cluster together   4  neural networks  neural networks can be shallow neural network and deep neural network and both of these could be apply to supervised or unsupervised problem as PRON have separate algorithm for both the case  PRON be the most powerful and popular class of ml algorithm  PRON can be use in every problem statement  main intuition behind PRON learn from PRON own error  PRON do not have much knowledge about neural network so PRON will not write further more   if PRON want to learn more about ml and neural network PRON can apply for machine learning by andrew ng class  PRON be the good course out there for beginner like PRON  
__label__machine-learning __label__audio-recognition PRON be build a binary sound classifier use the esc50 dataset  PRON have take one class   dog bark   to be positive and the rest of the 49 class to be negative  as the dataset be imbalanc PRON be run into lot of training issue  PRON try build a model but could not get a f1score great than 03   PRON be use mfcc and fft as feature  PRON have try use lr and svm to train without much success  can not use deep learning model as PRON be a real  time system and can not have much delay   how can PRON approach this problem   on the github page of the esc50 dataset  there be a list of try classifier as well as link to the relevant paper  the good one be all use some kind of deep learning  mostly cnn  and currently the good score be 855   a baseline random forest  notebook available here  achiv 443  and svm  available here  achieve 396    PRON would recommend PRON take a look at the approach link above and see if PRON can improve upon PRON  however  for high accuracy PRON probably need some kind of deep neural network  
__label__ethics accord to wikipedia artificial general intelligenceagi   artificial general intelligence  agi  be the intelligence of a   hypothetical  machine that could successfully perform any  intellectual task that a human being can   accord to below image today artifical intellgence be same as that of a lizard   lets assumeor not  that within 10  20 year PRON human be successful in create a agi or agi  as agi have the same intelligence and emotion as that of human because accord to wikipedia definition PRON can perform same intellectual task of a human  then can PRON destroy an agi without PRON consent  do this be consider as murder   firstly  an agi could conceivably exhibit all of the observable property of intelligence without be conscious  although that may seem counter  intuitive  at present PRON have no physical theory that allow PRON to detect consciousness  philosophically speak  a  zombie  be indistinguishable from a non  zombie  see the writing of daniel dennett and david chalmers for more on this   destroy a non  conscious entity have the same moral cost as destroy a chair   also  note that  destroy  do not necessarily mean the same for entity with persistent substrate  ie PRON  brain state  can be reversibly serialize to some other storage medium andor multiple copy of PRON can co  exist  so if by  destroy  PRON simply mean  switch off   then an agi may conceivably be reassure of a subsequent re  awakening  douglas hofstadter give an interesting description of such an  episodic consciousness  in  a conversation with einstein s brain   if by  destroy   PRON mean  irrevocably erase with no chance of re  awakening   then  unless PRON have a physical test which prove PRON be not conscious  destroy an entity with a seemingly human  level awareness be clearly morally tantamount to murder  to believe otherwise would be substrate  ist  a moral stance which may one day be see as antiquated as racism   even if machine with true artificial general intelligence be create  PRON apparent intelligence would still be by definition artificial  the word simulation be a synonym and could be use to redefine agi as simulated general intelligence   keep that in mind  a machine that appear to be express emotion would only be the result of a series of complicated algorithm allow a computer to assess the situation and respond in an intellectually appropriate manner base on external stimulus and condition  every possible action this machine could possibly make would be derive from a list of possible action the machine be capable of  no matter how large the number of possible action grow  the machine be still a series sensor  program instruction  and cycle of execution   destroy such a machine could potentially be the destruction of property if PRON be not own by the person who destroy PRON  but would PRON be murder  no   a broken machine can potentially be rebuild and reactivate if PRON be break  PRON never really die  PRON be destroy  a live be that be kill be really dead and can not be rebuild and make alive once again  these key difference lead PRON to agree with the previous answer and conclude that no  destroy an artificial intelligence without PRON consent would not be murder  
__label__neural-network __label__feature-construction __label__representation PRON be actually in the process of conceive a board game ai use neural networks  PRON need to encode a situation of a game  let PRON say chess  in such a way that PRON be a unique representation  so  PRON can not just input  let PRON say  1st pawn x position on the board  because then the datum 2nd pawn x position will be treat as a whole different value  which PRON be  but there be no meaning to  be the first pawn  or  be the second pawn   how can PRON cope with that  sorry if PRON question be stupid  PRON be new to this field  
__label__nlp __label__predictive-modeling __label__word-embeddings give a sentence    when PRON open the   door PRON start heat automatically   PRON would like to get the list of possible word in   with a probability   the basic concept use in word2vec model be to  predict  a word give surround context   once the model be build  what be the right context vector operation to perform PRON prediction task on new sentence   be PRON simply a linear sum   modelmostsimilarpositivewheniopenthedooritstart    heatingautomatically     word2vec work in two model cbow and skip  gram  let PRON take cbow model  as PRON question go in the same way that predict the target word  give the surround word   fundamentally  the model develop input and output weight matrix  which depend upon the input context word and output target word with the help of a hidden layer  thus back  propagation be use to update the weight when the error difference between predict output vector and the current output matrix   basically speak  predict the target word from give context word be use as an equation to obtain the optimal weight matrix for the give data   to answer the second part  PRON seem a bit complex than just a linear sum   obtain all the word vector of context word  average PRON to find out the hidden layer vector h of size nx1  obtain the output matrix syn1word2vecc or gensim  which be of size vxn  multiply syn1 by h  the result vector will be z with size vx1  compute the probability vector y  softmaxz  with size vx1  where the high probability denote the one  hot representation of the target word in vocabulary   v denote size of vocabulary and n denote size of embed vector   source  httpcs224dstanfordedulecturenoteslecturenotes1pdf  update  long short term memory model be currently do a great work in predict the next word  seq2seq model be explain in tensorflow tutorial  there be also a blog post about text generation   miss word prediction have be add as a functionality in the late version of word2vec  of course PRON sentence need to match the word2vec model input syntax use for train the model  low case letter  stop word  etc   usage for predict the top 3 word for  when PRON open  door    printmodelpredictoutputwordwheniopendoor     topn  3  
__label__classification __label__sampling PRON be work on a rare event classification problem  PRON have 95  of the datum as a majority class and 5  of the datum as the minority class  PRON use classification tree algorithm  PRON be measure the goodness of the model use confusion matrix   as the i have the minority class just 5  of the total datum  even though PRON prediction performance of minority class be close to 70   the total number of error be high   for example  here be PRON confusion matrix   0  1  0  213812  7008  1  29083  16877  though the minority classclass 1  have predict 16877 time correctly70  and the misclassifcation be just 30   but the absolute value of the misclassifcation be very high29083  compare to the correctly predict minotriy class  16877   which make the solution less usable for the business   be there any idea on handle these kind of  issue in such rare event modelling   kind note  PRON have  balance the target variable use the smote algorithm before apply classification tree   if PRON be willing to use the caret package in r and use random forest  PRON can use the method in the following blog post for downsampl with unbalanced dataset  httpappliedpredictivemodelingcomblog201312828rmc2lv96h8fw8700zm4nl50busep  basically  PRON just add a single line to PRON train call  here be the relevant part    gt  rfdownsampl  lt trainclass    datum  training     method   rf      ntree  1500     tunelength  5     metric   roc      trcontrol  ctrl       tell randomfor to sample by stratum  here       that mean within each class    stratum  trainingclass       now specify that the number of sample select      within each class should be the same    sampsize  repnmin  2    PRON have have some success with this approach in PRON type of situation   for some more context  here be an in  depth post about experiment with unbalanced dataset  httpwwwwinvectorcomblog201502doesbalancingclassesimproveclassifierperformance 
__label__machine-learning __label__distributed __label__map-reduce __label__dimensionality-reduction PRON have a big sparse matrix of user and item PRON like  in the order of 1 m user and 100 k item  with a very low level of sparsity   PRON be explore way in which PRON could perform knn search on PRON  give the size of PRON dataset and some initial test PRON perform  PRON assumption be that the method PRON will use will need to be either parallel or distribute  so PRON be consider two class of possible solution  one that be either available  or implementable in a reasonably easy way  on a single multicore machine  the other on a spark cluster  ie as a mapreduce program  here be three broad idea that PRON consider   assume a cosine similarity metric  perform the full multiplication of the normalize matrix by PRON transpose  implement as a sum of outer product   use locality  sensitive hashing  lsh   reduce first the dimensionality of the problem with a pca  PRON would appreciate any thought or advice about possible other way in which PRON could tackle this problem   PRON hope that the follow resource may get PRON additional idea toward solve the problem   1  research paper  efficient k  nearest neighbor join algorithms for high dimensional sparse data   httparxivorgabs10112807  2  class project paper  recommendation system base on collaborative filtering   stanford university   httpcs229stanfordeduproj2008wenrecommendationsystembasedoncollaborativefilteringpdf  3  project for the netflix prize competition  k  nn  base   httpcscarletoneducscomps0910netflixprizefinalresultsknnindexhtml  4  research paper  hubs in space  popular nearest neighbors in high  dimensional data  on the curse of dimensionality phenomenon and PRON relation to machine learning  in general  and k  nn algorithm  in particular  httpjmlrorgpapersvolume11radovanovic10aradovanovic10apdf  5  software for sparse k  nn classification  free  but appear not to be open source  may clarify with author   httpwwwautonlaborgautonweb10408html  6  several discussion thread on stackoverflow   httpsstackoverflowcomquestions20333092knnwithbigsparsematricesinpython  httpsstackoverflowcomquestions18164348efficientnearestneighboursearchforsparsematrices  httpsstackoverflowcomquestions21085990scipysparsedistancematrixscikitorscipy  httpsstackoverflowcomquestions10472681handlingincompletedatadatasparsityinknn  httpsstackoverflowcomquestions5560218computingsparsepairwisedistancematrixinr  unlike all previous discussion  which refer to python  this one refer to r ecosystem   7  pay attention to graphlab  an open source parallel framework for machine learning  httpselectcscmueducodegraphlab   which support parallel clustering via mapreduce model  httpselectcscmueducodegraphlabclusteringhtml  PRON may also check PRON answer here on data science stackexchange on sparse regression for link to relevant r package and cran task view page  httpsdatasciencestackexchangecoma9182452   if PRON be work on collaborative filtering PRON should pose the problem as a low  rank matrix approximation  wherein both the user be item be co  embed into the same low  dimensionality space  similarity search will be much simple then  PRON recommend use lsh  as PRON suggest  another fruitful avenue for dimensionality reduction not yet mention be random projection   PRON should use  pysparnn  a recent implementation by facebook in python which be bloody fast  PRON be also easy to use  
__label__machine-learning __label__data-mining __label__clustering __label__algorithms __label__data-cleaning dataset  PRON be give the number of minute individual customer use a product each day and be try to cluster this datum in order to find common usage pattern   PRON question  how can PRON format the datum so that  for example  a power user with high level of use for a year look the same as a different power user who have only be able to use the device for a month before PRON end datum collection   so far PRON have turn each customer into an array where each cell be the number of minute use that day  this array start when the user first use the product and end after the user s first year of use  all entry in the cell must be double value  ex  2000 minute use  for the clustering model  PRON have consider either set all cell  day after the last day of datum collection to either 10 or null  be either of these a valid approach  if not what would PRON suggest   PRON believe PRON problem boil down to cluster time  series of different length  accord to PRON question  PRON want the long time  series of a power user to be consider similar to time  series of similar pattern but much short   therefore PRON should look into clustering technique and distance metric which allow for these property  PRON do not know PRON language of choice but here be some of the many package in r that PRON may find interesting    fréchet distance  one of the package offer this be kmlshape   dynamic time warping include in base r   permutation distribution clustering  package pdc  this would also solve PRON data format problem as to set value to 1 or null would not be need anymore  hth  
__label__machine-learning __label__classification __label__time-series __label__feature-selection __label__feature-extraction PRON be work with time  series data that have to be classify into two class  blue and red  or at least classify the datum as one class  red   PRON be unable to come up with feature that distinctly separate the datum   please advise as to how do PRON need to approach this problem   PRON could calculate the distance between time series use dynamic time wrapping dtw and then PRON could cluster PRON use k  mean or so  here be a python implementation of the dtw or use the dtw package in r 
__label__machine-learning __label__dataset __label__dimensionality-reduction __label__encoding hi have dataframe with large categorical value over 1600 category be there any way PRON can find alternative so that PRON do not have over 1600 column   PRON find this below interesting link httpamunateguigithubiofeaturehashingsourcecode  but PRON be convert to class  object which PRON do not want  PRON want PRON final output as a dataframe so that PRON can test with different machine learning model   or be there any way PRON can use the genetrated matrix to train the other machine learning model other than logistic regression or xgboost   be there anyway PRON can implement   one option be to map rare value to  other    this be commonly do in eg natural language processing  the intuition be that very rare label do not carry much statistical power   PRON have also see people map 1hot categorical value to low  dimensional vector  where each 1hot vector be re  represent as a draw from a multivariate gaussian   see eg the paper deep knowledge tracing  which say this approach be motivate by the idea of compressed sensing   baraniuk  r compressive sensing  ieee signal process magazine 24  4  2007    specifically  PRON map each vector of length n to a short vector of length log2n    PRON have not do this PRON but PRON think PRON would be worth try   PRON can read the datum and first get a list of all the unique value of PRON categorical variable  then PRON can fit a one hot encoder object  like the sklearnpreprocessingcategoricalencoder  on PRON list of unique value   this method can also help in a train test framework or when PRON be read PRON datum in chunk  PRON have create a python module that do all this on PRON own  PRON can find PRON in this github repository  dummypy  a short a tutorial on this  how to one hot encode categorical variables in python  
__label__databases __label__methodology PRON be work on a scientific article  where PRON be go to work with a large sql base database  this work consist of everything from wash datum  create sub  database etc  how can PRON document this work  in a scientific matter  so PRON know what have be do with the datum   PRON could add all sql statement to github and then use the commit descriptions  depend on how extensive this be PRON can also write a manual change log  or PRON simply excessively comment inside PRON sql statement  then PRON would recommend to also add some kind of timestamp   if PRON be familiar with jupyter and use python and sql together PRON can also use this  
__label__discontinuous-galerkin PRON be look into the book of riviere  discontinuous galerkin methods for solve elliptic and parabolic equations   in the comparaison of section 212  copy below   the example of rectangular mesh indicate that the dg be more economic  have les dof  then the cg when use a certain space of element  can any one explain in more detail please  what be the difference between the two space qk and pk   thank PRON for PRON help   example    size of problem  for dg  the total number of degree of freedom be proportional to the number of element in the mesh  the constant of proportionality be a function of the polynomial degree  for cg  the degree of freedom depend on the number of vertex and possibly the number of vertex and element in the mesh  for instance  consider a structured mesh of 5 × 5 rectangular element  the degree of freedom for a dg approximation of degree 1  2  3  4 be 75  150  250  375  respectively  whereas the degree of freedom for a cg approximation of degree 1  2  3  4 be 36  121  256  441  respectively  thus  on such small mesh  if k ≥ 3  the cg method be more costly than dg  the reason be that PRON have to use the space qk on rectangular element for cg  but PRON can still use the space pk on rectangular element for dg    the argument be mislead  the different space that be use be  p1textspan1x  y and  q1textspan1x  y  xy for high order  PRON be   p2textspan1x  y  x2y2 and  q2textspan1x  y  x2y2xy  x2y  xy2x2y2 if PRON continue this  PRON be true that the number of basis function for the  qk space grow faster than the number of basis function for  pk  but this be not the important quantity  what matter be not how many basis function PRON have on a give mesh for a give polynomial degree  k  but this   how many  qk shape function do PRON need to reach a certain level of accuracy vs how many do PRON need for  pk  in this metric   qk space typically win because the additional shape function help PRON cancel term in the taylor expansion of the solution  and consequently the error PRON get on one quadrilateral cell with a  qk element be typically substantially small than the error PRON get by splitting this one cell into two triangle and use a  pk element on PRON   in other word  while PRON be true that for the same polynomial degree   qk continuous element have more unknown than  pk discontinuous element if  k be sufficiently large  this do not matter  PRON get something for these additional shape function in term of error reduction  
__label__optimization __label__admm PRON have be read a lot of paper on admm lately  and also try to solve several problem use PRON  in all of which PRON be very effective  in contrast to other optimization method  PRON can not get a good intuition as to how and why this method be so effective  of course  PRON have see convergence analysis for a few case  but nothing that give PRON too much insight   be there some intuition behind admm  how do the first scientist to use PRON come up with this idea  some geometrical intuition would be good  but any insight anyone have will help   if PRON remember correctly  the admm ist often state as an algorithm to solve    minx  y fx   gyquadtextstquad axby  c  for two convex  lower  semicontinuous functional  f and  g and linear  bound operator  a and  b  PRON find the follow special case of  a  i   b  i and  c0  illustrative  in this case the constraint say  x  y  0   ie PRON can substitute to get the problem    minx fx   gx  now solve this can be hard  while solve problem of the form    minx rho fx   tfrac12x  z2  can be easy   PRON can make up example for this PRON  a popular one be  fx   lambdax1  and  gx   tfrac12ax  b2     in admm PRON start from the  splitt form     minx  y fx   gyquadtextstquad x  y0  and build the  augment lagragian     lrhox  y  z   fx   gy   ztx  y   tfracrho2x  y2  with the lagrange multipli  z now PRON alternatingly minimize the augement lagragian in the different direction  x and  y  ie iterate    xk1   mathrmargminx lrhox  yk  zk    yk1   mathrmargminy lrhoxk1y  z  and update the multipli accord to    zk1   zk  rhoxk1   yk1  this should explain the name alternate direction method of multiplier   analyze these minimization problem for  x and  y closer  PRON observe that for each update only need to solve a problem of the  simple form   eg for the  x update    xk1   mathrmargminx fx   tfracrho2x  yk  rho zk2   neglect term that do not depend on  x    admm for the problem    minx  y fx   gyquadtextstquad axby  c  be derive similar but then the intermediate problem for the update be still a bit difficult but may be comparably simple in comparison to the original one  especially in the case of  fx   lambdax1  and  gx   tfrac12ax  b2   or equivalently  fx   lambdax1    gy   tfrac12y2  and the constraint  ax  y  b  the update be more or less straightforward to implement  
__label__visualization PRON have find a number of resource that mention tufte s  data density index  and  datum  to  ink ratio  when consider the analysis of particular visualisation and visualisation technique  but PRON be yet to come across any paper that actually critique the measure PRON   be there critique of these principle available   there be critcism s of tuft s principle   some example   sometimes PRON must raise PRON voices  stephen few  perceptual edge  even after many year of work in the field of datum visualization  which have involve a great deal of experience and study that have expand PRON expertise into many area that tufte have not specifically address  PRON have only on rare occasion discover reason to disagree with any of PRON principle  the topic that PRON be address in this article  however  deal with one of those rare disagreement   minimalism in information visualization  attitude towards maximize the data  ink ratio  from the abstract   people do not like tufte s minimalist design of bar  graph  PRON seem to prefer  chartjunk  instead  
__label__text-mining __label__data-cleaning __label__unsupervised-learning __label__anomaly-detection PRON have be look for method that can help figure out anomaly in textual datum store in database  major goal be to use a unsupervised learning method to detect the anomaly  further how can PRON find a context in the datum set and figure out contextual anomaly   this be the first hit on google for  anomaly detection text    PRON be a little old but PRON be likely a good starting point  PRON seem the author use heuristic algorithm  several measure of distance  to define an anomaly   another option may be to look at the move average of a give token  word  in PRON corpora over time and see if PRON t  stat be great than some threshold  there be probably many  many good way to do PRON but that may work depend upon PRON goal   this assume PRON be happy with neural network  if PRON be not  this answer probably be not of much use to PRON   firstly  a little bit about anomaly detection via autoencoder  apology if PRON be already familiar with this   an autoencoder be a neural network which learn to reproduce PRON own input when compress through a  bottleneck  layer  for example  PRON may want to find a low  dimensional feature representation of a set of 100 x 100 image  PRON neural network architecture have an input layer of 10000 element  an output layer of 10000 element  and one of PRON hidden layer will be relatively narrow compare to the input space  say 100 node   the objective be to train the network to produce an output as close to the input as possible  while throw away all but 100 nodes’ worth of activation  PRON be try to produce as lossless a compression of the input as possible  so those 100 node should be a very information  rich representation of the kind of datum PRON train PRON on    how do this have any bearing on anomaly detection   PRON hear PRON ask  well  if PRON train PRON autoencoder on non  anomalous datum  PRON will learn a non  anomalous low  dimensional feature representation  this will mean the reconstruction error from push something through the autoencoder will be low for datum similar to what PRON be train on than PRON will for other arbitrary datum  if PRON receive input that be substantively different from what PRON be train on  the reconstruction error will be high  so  give a set of novel input  those input with the high reconstruction error be the most anomalous  as PRON be poorly reconstruct from the non  anomalous feature representation   if PRON datum have a temporal structure  and PRON have plenty of training sample  PRON may want to consider construct an autoencod lstm  an lstm be a neural network architecture for encode and decode sequentially dependent datum  and a full description of how this work be beyond both the scope of this post and PRON own ability  there be many magnificent resource available online for get to grip with this   here be a relevant paper on use lstms for anomaly detection in time series in general  PRON may be that lstm be unnecessary for PRON purpose if the data be not strongly sequentially dependent  
__label__optimization __label__nonlinear-programming disclaimer  PRON have edit the question repeatedly for clarity and to target the most relevant answer   PRON have the follow general problem    min h1cdot h22    such that    g1wedge g2h1wedge h22  0  where  hi  giin mathbbrnwedge mathbbrn be skew  symmetric matrix   acdot b denote matrix multiplication of matrix  a and  b  and  lambdain mathbbr  awedge b be the geometric product of  a and  b  in order to make the problem solve nice  ie  no infeasibility trap at  h10   for instance  PRON have reformulate the problem with the following constraint     g1wedge g2lambda h1wedge h22  0  lambdage 0  h121  h221  PRON test case be  n6  and  g1g2e12e34e56  a bad case scenario   obviously  the problem be degenerate due to symmetry as well as unitarily invariant on  mathbbr6 PRON have introduce  lambda and constraint 2 and 3 in order to exclude some slack in the solution  PRON expect outcome be  h1  frac1sqrt12left2e12e34e56right and  h2frac1sqrt2lefte34e56right with a minimum of 112 and  lambdasqrt24  PRON have use ipopt to implement the minimzation  but PRON fail to find this solution  instead a solution be find that be slightly bad  PRON be a little perplexed as all function be convex in each variable  but PRON be no expert in nlp   edit  as per geoff oxberry  the problem be not convex  as a matter of fact  the initial guess of  h1g1  and  h2g2  be a maximum of the objective function under the constraint   so how do PRON improve PRON initial guess or change constraint to improve solution   PRON problem be not convex   first   h12   1  and  h22   1  be not convex constraint  even though the function on the left  hand side be convex  the easy way to picture this in a simple case be to note that the unit circle  or sphere  be not a convex set because the line between any two point on the circle  or sphere  be not contain in the set   second  the function  h1  cdot h22 be probably not convex because PRON contain bilinear term  bilinear term be know not to be convex  and PRON doubt that compose PRON with a 2norm change the non  convexity   since ipopt be a local optimization solver only guarantee to find the global solution under convexity assumption  PRON probably converge to the local minimum nearest PRON guess  though without know more about PRON guess and the problem iteration  PRON can not really say for sure   
__label__regression PRON have a model with 13 independent variable  all of PRON be significant and 2 of PRON be categorical variable  and 678 observation  PRON run the ridge regression and lasso on the data set use glmnet package  here be the output   PRON be wonder how PRON can relate the variable to PRON number and what l1 norm mean   thank   
__label__machine-learning __label__nlp __label__stanford-nlp PRON want to understand the intent of the customer use PRON search query  let PRON say if a customer be interested in yoga pant  PRON can either search for yoga pant or exercise pant or workout tight etc  be there a model that PRON can use to find out all the search keyword that can be relate to yoga pant   what PRON be look for be name entity recommendation  PRON must tell PRON this be an extremely tough problem  stanford have open source a ner tagger but PRON need to train PRON on a large amount of datum  and PRON will have to create a tag dataset  look into this medium blog for a good tutorial  PRON would suggest that PRON look into PRON requirement  if PRON be too limited then PRON do not need something like this  PRON can work through a simple vocabulary   PRON could look into rocchio s algorithm  word2vec and other method that use co  occurence   a simple starting point would be to query a large docment collection  PRON collection  the internet or a combination of both  with the query at hand  take the most prominent word in the result  td  idf result  collection  and add that to the original query  the general idea here be that the significant word in the result be word that be closely related to the query  PRON cn experiment in how far PRON need the general context  the internet  or the domain specific context  PRON collection  and how PRON weigh the contexthow much of the significant word PRON will add and what PRON weight will be   but PRON can discuss that later   PRON think these be the method that PRON can try out  please feel free to add more to this list    highly precise with a little low recall be to use a dictionary with almost all possibility  manual effort  but must be worth PRON     use word2vec  mikolov have already train text datum and create word vector  use this vector space  PRON can figure out which word be similar  PRON can try out and find a threshold above which PRON can say which word be similar  for example  yoga and exercise would have decent similarity    train custom w2v  if PRON have enough datathis be an unsupervised model  so PRON do not need to worry about tag the datum but find huge amount of datum relevant to the working domain    PRON can use an rnn to find the most similar word in a corpus and use PRON for query  this give a bit more flexibility than w2v 
__label__pde __label__stability PRON be interested in understand how to perform stability analysis for couple  to keep thing do  able  let say linear  pde s  in the case of a single pde  i understand the logic behind the vn analysis  mode expansion and look at the amplification factor of any give mode   however for couple pde s PRON seem like PRON will not be able to cancel thing as nicely   for example  base off the note here  httpsenwikipediaorgwikivonneumannstabilityanalysis  specifically start at eqn  2   if this represent the error for one function PRON will depend on the other via the coupling  PRON seem that the  eat term will not cancel nicely to get eqn  6   presumably i can still restrict the analysis to one mode at a time assume both pde be linear   that is  rather than the  eat term cancel as PRON do in that simple example  in the couple case i would be leave term contain the relative size ie  ea  bt where  a and  b be the exponent of the mode expansion amplitude of the error for the two pde function respectively  whoa  a mouthful    PRON seem like the solution be to cast as a linear matrix problem but PRON get a bit stuck here too  certainly one can write the time discretization in term of an amplification factor  matrix  a act  n time on some initial vector to give the new state  but how do one then look at the spatial discretization  contain inside the amp matrix  a in this context  or do PRON just look for eigenvalue of that amplification matrix and then do finite  difference afterwards   any and all help be appreciate   
__label__machine-learning __label__regression __label__apache-spark PRON be work on spark mllib and have a project where PRON have to make prediction for numeric datum base on non  numeric feature  PRON be a bit confused about which regression algorithm to use from spark mllib library primarily due to be new at this  the algorithm present in spark mllib library be   linear model  svm  logistic regression  linear regression   naive bayes  decision tree  ensemble of tree  random forests and gradient  boosted trees   isotonic regression  can anyone provide PRON some guidance as to which algorithm will be suitable for prediction for numeric datum base on non  numeric feature   PRON would suggest ensemble of tree  random forests and gradient  boosted trees    here be a nice reference for handle such datum with decision tree   a linear regression would work  but the real issue here be feature extraction  PRON have to encode PRON categorical feature somehow  likely by vectoriz PRON  PRON can one  hot encode PRON feature  treat PRON as text and countvectorize PRON  etc  
__label__svm __label__graphs __label__kernel PRON have a collection of graph object of variable size  input  which be each pair to another graph of variable size  output   the task be  give an input graph  produce the most likely output graph   PRON have be look at  structured output  technique such as ssvm but as far as PRON can tell  PRON all act on output of fix size  or at least a size that match the input size  ie sequence tag   be there any tool that can map an input to a structured object of arbitrary size   PRON better try graph embedding and for that PRON propose go through horst bunke from university of bern who have be do PRON for year  just search PRON in google scholar and go through PRON publication network  co  author  cite paper  cite paper  etc  for example this and this as more classic paper or this as an exact answer to PRON question  PRON be pretty confident PRON will find PRON solution as PRON be exactly PRON research direction  PRON be on mobile so reference be difficult now but PRON can update PRON answer later   the other point that PRON would like to mention be that graph embedding have two kind   embed the vertex  node  of one graph into a n  dimensional space ie each vector will be a node of the graph   see this   embed a dataset of graph into a n  dimensional space in which each vector will be an entire graph   PRON need the second one   the other thing worth to mention be look at the problem from network science point of view in which the statistical and structural of a graph be widely study  there be plenty of graph  network measure which be less or more tolerante to the number of node eg clustering coefficient  use PRON as feature can be feed to a classifier to see how PRON work   the last but not the least be what PRON actually ask  what PRON be look for be call graph kernel  PRON be nothing but the definition of the ml  know concept kernel but on graph  so kernel method can get benefit of that by skip the feature extraction step  like what PRON suggest above  and directly work with structural input  for this purpose PRON would suggest that PRON equip PRON with the graph kernel eg this paper   good luck  
__label__fluid-dynamics __label__efficiency PRON be make a model of a square box where water come in and the water level rise  PRON want PRON to be a transient  turbulent  vof  model  the velocity of water enter change in time   02  to  02  m  s    for now PRON use a steady  laminar inflow of  02  m  s  a grid of  10000  cell  the geometry be roughly  100times 100times 6  m  and solution method piso   to run the model PRON have use time step size  01  s and  3600  time step   10  minute   big time step lead to divergence  PRON take  25  hour to calculate this and PRON be not even turbulent yet  in the end PRON have to model a period of  12  hour  with this speed of calculation that will take more than  5  day   what be thing PRON can adapt in fluent which will  extremely  reduce this calculation time   thank in advance   the reduction of calculation time in cfd be a general problem  although PRON have never work with fluent  PRON do not think that there be feature that magically speed up PRON simulation  that be why PRON can only give some very general hint  for further insight  PRON suggest PRON browse the internet  eg cfd  online for the highlighted keyword   use mesh adaptation to reduce the scale of PRON problem  in the region away from the wall  PRON maybe can reduce the number of cell  for the near  wall region PRON have to take care of the boundary layer  monitor the y value help PRON estimate how coarse PRON can go  also the use of wall function for PRON turbulence model can be consider   do some try and error with the relaxation parameter  this be eventually balance stability and convergence speed   and  as paul suggest  PRON can simply use more computation power  
__label__machine-learning __label__nlp what ml technique can be use to determine relevance or context between sentence   for example    PRON be go to the event at 9 pm  can PRON meet PRON there    here  there be context between the two sentence  the person be indicate PRON will be at the event at 9 pm  and be ask the recipient to meet PRON there   conversely    PRON be go to the event at 9 pm  can PRON meet PRON at school tomorrow    base on this text alone  PRON appear as if the sentence represent two disjoint thought   additionally  although the example above be of adjacent sentence  the context may exist across multiple sentence  or sentence that be not adjacent    PRON be go to the event at 9 pm  PRON have hear PRON be fabulous  can PRON meet PRON there    what PRON want to do be to extract the relation within the sentence and in this case beyond  so basically a cross  sentence relation extraction   ambiguity and the context of word be a big deal when do this kind of task  there may be multiple way to get there  but PRON also depend on PRON goal  production environment  research or just do PRON for fun to get into nlp    PRON would recommend PRON look into graph lstms  eg this paper  
__label__finite-difference __label__mathematica PRON be try to iterate the following equation     xkn1xk  nepsilon  xk1n2xkn   xk1nsqrtepsilon  etakn      where  n denote which time step PRON be on and  k be the location on the string with periodic boundary condition   epsilon be the size of the time step and  eta be a random variate from a gaussian distribution with mean 0 and variance 1    eta be re  pick after each time step and be varie from location to location  PRON generate PRON initial  x randomly with  x0   randomreal0  π   10   which give PRON 10 real number between 0 and  pi  which be PRON 1d lattice site variable   PRON then define  eta by  η0   randomvariatenormaldistribution0  1   10   which give PRON 10 random number from PRON distribution  PRON be take the time step to be  epsilon  1100  then to define PRON iteration equation PRON write  xjn   1    xjn   ϵ  xj  1n   2 xjn   xj  1n    sqrtϵ  ηjn   but this fail to iterate when PRON try when just look at a sigle site and PRON neighbor   if someone could be so kind as to help PRON put in periodic boundary condition  as well as to get the iteration to work  PRON would be grateful   PRON be use mathematica 8   thank PRON   PRON be be a while since PRON cod something in mathematica so PRON will write in fortranish pseudocode instead  ntimestep  20  nx  10  x  array0nx1  0ntimesteps   x1nx0   randomreal0pi    boundary condition   x00   xnx0   xnx10   x10   do i1ntimestep  eta1nx   randomnormaldistribution01   do j1nx  xji   xj  i1   epsxj1i12xj  i1xj1i1    sqrtepsetaj   enddo  x0i   xnx  i   xnx1i   x1i   enddo  note that in the formula of the iteration PRON give there be two mistake   first use n  instead of n1  secondly PRON have two j1 instead of one j1 and one j1  in mathematica PRON recommend do this with sparse matrix  vector product as follow  first PRON will define a circulant matrix  c  with the periodic bcs  which appear on the rhs of PRON equation   in matrix notation PRON equation read  beginequation   xn1   xn   epsilon c xn   sqrtepsilon  etan   endequation   or  beginequation   xn1    i  epsilon c  xn   sqrtepsilon  etan   endequation   so  here be the matrix definition  the classical  1 2 1  stiffness matrix be   stiffnessmatrixn     sparsearrayband1  1   gt  2   band1  2   gt  1    band2  1   gt  1     n  n   0     from this definition the circulant matrix be construct with a simple adjustment   circulantmatrixn     blockk  stiffnessmatrixn     replacepartk    1  n  gt  1    n  1  gt  1       for a give  n  size of PRON system  PRON the define  c  circulantmatrixn    the identity matrix can be compute with identitymatrixn   conversely  a sparse implementation read  eyen     sparsearrayi   i   gt  1     n  n   0     so  to implement  a   i  epsilon c PRON write  a  eyen   ep c   PRON be now ready to iterate   x  x  ax  sqe eta   where  sqe  sqrteps  the follow implementation be for a noise  eta which be independent of position  PRON can adjust PRON for PRON need   iterationnmax   ep   size     with    a  eyesize   ep circulantmatrixsize    x0  randomreal0  pi    size    sqe  nsqrtep      nestlista   sqe randomvariatenormaldistribution0  1   size   amp   x0  nmax     input be the number of iteration nmax  the  epsilon guy ep and the size of PRON system  which in PRON example be 10   output be a matrix where each entry  eg sol1    be the state of PRON system at a give time   if PRON want only the last output  either use last in PRON output or replace nestlist with nest  
__label__finite-volume be there a conservative fv second  order  or first  order  accurate immerse boundary method for compressible flow include move boundary  in the literature    by compressible flow PRON mean the navier  stokes eqts  for an ideal gas   most of the result PRON be able to find deal exclusively with incompressible flow   PRON would be nice if the publication describe the method in enough detail that one can reproduce PRON result   there be several research group that be work on ibms for compressible flow  a quick google search lead to the follow paper   httpwwwmejhuedufsagpublicationspapersashapecompressiblejcppdf  httpmultiscalemodelingcoloradoeduliupublicationsliujcp2007pdf  httpctrstanfordeduresbriefs05detulliopdf  httpwwwsciencedirectcomsciencearticlepiis0045793006000065  httponlinelibrarywileycomdoi101002fld3872abstract  httpwwwtandfonlinecomdoiabs101080106185622013791391uvtnhnn0jro  httpdutw1479wbmttudelftnlwimacademypresentationsc1pressandersepdf  most of PRON seem to give enough detail so PRON can get start with PRON own implementation  few people will be ready to give so much detail that PRON can write PRON own solver from scratch  when write such code  the devil be in the detail  
__label__pde __label__software __label__finite-element __label__finite-volume the heat equation be discretiz in space with fv  or fem   and a semi  discrete equation be obtain  system of ode   this approach  know as the method of line  allow to easily switch from one temporal discretization to another  without code duplication  in particular  PRON can reuse any time integrator for ode without much effort  this be very convenient because if PRON decide to change PRON spatial discretization from fv to say fe  PRON still get a semi  discrete equation and PRON time integrator still work   now PRON be try to implement the method of rothe for the same problem  however  discretiz in time first force PRON to rewrite the spatial discretization for every temporal discretization scheme PRON may want to use  this eliminate the reuse of time integrator that PRON previously have  and make PRON very complicated to write modular software that can discretize a pde use both the method of line or the method of rothe   be there a way of implement both approach  without code duplication   edit   in convection dominate problem  the fe discretization need stabilization both in time and space  make the method of rothe the  good  choice  however  this be not the case for fv  dg method   in the method of line  the pde be discretiz first in space  and then in time  in the method of rothe  the pde be discretiz first in time  and then in space  the third possibility be to discretize both in space and time simultaneously  also know as space  time  discretization   a discussion about the method of line and the method of rothe can be find here  for more information the book  finite element methods for flow problems  from donea and huerta be a good resource   PRON do not really have much more to say than PRON already do on the page PRON link to  but for PRON the primary argument go like this   in many problem  one need to adapt the mesh between time step  the conceptual framework for do this be the rothe method where one can choose the spatial discretization independently at each time step whereas the method of line a priori assume that the pde be convert into a system of ode  which be incompatible with mesh adaptation   on the other hand  if PRON do not want to adapt PRON mesh  then PRON do not matter  in most case  if the spatial discretization be go to be the same between time step  then PRON do not matter whether PRON want to first discretize in space and then in time use PRON favorite time integrator  or the other way around  PRON will come out with the same discrete problem to solve in every time step  in such case  the rothe method and the method of line be then the same   this extend to the case where PRON only want to adapt the mesh every once in a while  PRON can consider this as the method of line apply to a number of time step  then adapt the mesh  then one more set of time step where PRON apply the method of line  or PRON can think of this as the rothe method where PRON just happen to only adapt the mesh every once in a while  PRON will essentially come out to the same numerical scheme  just a different philosophical viewpoint   PRON may be worthwhile add one more point  in the ode world  one often use high order scheme with several stage or multiple step  thus  there be significant benefit to bottle up these algorithm into package that PRON only have to hand an ode system in one way or another  on the other hand  for time dependent pde  most of the time one use rather simple time step method  with the notable exception of some hyperbolic solver   for example  crank  nicolson  bdf2 or just the forward or backward euler scheme  for these simple time integrator  PRON be not particularly difficult to hand  code the time integration since PRON be so much simple than the spatial discretization  what this mean be that the price to pay for thinking in term of the rothe method  not be able to use an ode solver package  be a small one  whereas the price to pay for use the method of line  not be able to adapt the mesh between time step  be a large one  that may explain why most people in the adaptive finite element world prefer to think along the rothe method   as a corollary to the last point  and come back to the original question  PRON be indeed true that PRON be difficult in the rothe method to package everything up nicely in an object orient way  however   i  as long as PRON stick with a single class of ode integrator  PRON can of course still tabulate the coefficient of the various stage of the integrator in a class and have the code that compute PRON be entirely encapsulate  and  ii  the fact that one commonly use relatively simple time integrator for time dependent pde mean that the amount of code necessary to implement the time integration be usually vastly small than the amount of code that deal with the spatial discretization  in other word  PRON know of no way how to nicely separate the spatial from the temporal discretization use the rothe method  but PRON have not usually find this to be too much of an obstacle since so little code be necessary to write  say  a bdf2 or rk4 scheme  
__label__ode __label__computational-physics __label__runge-kutta PRON have two ball  with mass  m1  and  m2 PRON be connect by a massless rope  one be lie on a table and be spin in circle at a constant speed and PRON be connect by a rope through a hole to a second ball  which be hang   PRON have to create a code in python to solve this problem use runge  kutta method  but PRON really have hard time approach that problem   PRON figure out PRON have to describe this system as couple oscillator  but PRON do not really know how to transform that equation to make PRON usable by rk algorithm   any help will be appreciate  help with create code  and explanation how to transform classic newtonian equation will be appreciate  edit   here be code i prepare   from pylab import   from math import sqrt  from numpy import array  zero  arange  def ukladstate  time    g981  r  4   v0state1    a  kstate0    g1state1    g2l0state0state3state3k  mstate0gcosstate2     g3state3    g41state0l0gsinstate22state1state3   rdot  state0   vr  phidot  state1   vphi  vrdot    state2   g    state3   r  sqrtphidotstate3   state2    vphidot   2rdot  phidotr  return arrayrdot  phidot  vrdot  vphidot    def eulery  t  dt  derivative    ynext  yderivativey  tdt  return ynext  def rungekuttay  t  dt  derivative    k1dtderivativey  t   k2dtderivativeyk12t05dt   k3dtderivativeyk22t05dt   k4dtderivativeyk3tdt   ynext  y16k1  2k2  2k3k4   return ynext  vr1  vphi1  m21  m11  dt01  to0  te10  t  arangeto  te  dt   n  lent   y  zerosn4    y00vr  y01vphi  y02m2  y03m1  for i in rangen1    yi1rungekuttayitidt  uklad   print yi1   for clarification vr and vphi be just part of speed in polar set of coordinate  and dot indicate first derivative   
__label__neural-network __label__backpropagation PRON be a well know fact that a 1layer network can not predict the xor function  since PRON be not linearly separable  PRON attempt to create a 2layer network  use the logistic sigmoid function and backprop  to predict xor  PRON network have 2 neuron  and one bias  on the input layer  2 neuron and 1 bias in the hidden layer  and 1 output neuron   to PRON surprise  this will not converge  if PRON add a new layer  so PRON have a 3layer network with input  2  1   hidden1  2  1   hidden2  2  1   and output  PRON work  also  if PRON keep a 2layer network  but PRON increase the hidden layer size to 4 neuron  1 bias  PRON also converge  be there a reason why a 2layer network with 3 or less hide neuron will not be able to model the xor function   a network with one hidden layer contain two neuron should be enough to seperate the xor problem  the first neuron act as an or gate and the second one as a not and gate  add both the neuron and if PRON pass the treshold PRON be positive  PRON can just use linear decision neuron for this with adjust the bias for the treshold  the input of the not and gate should be negative for the 01 input  this picture should make PRON more clear  the value on the connection be the weight  the value in the neuron be the bias  the decision function act as 01 decision  or just the sign function work in this case too    picture thank to  abhranil blog   if PRON be use basic gradient descent  with no other optimisation  such as momentum   and a minimal network 2 input  2 hide neuron  1 output neuron  then PRON be definitely possible to train PRON to learn xor  but PRON can be quite tricky and unreliable   PRON may need to adjust learn rate  most usual mistake be to set PRON too high  so the network will oscillate or diverge instead of learn   PRON can take a surprisingly large number of epoch to train the minimal network use batch or online gradient descent  maybe several thousand epoch will be require   with such a low number of weight  only 6   sometimes random initialisation can create a combination that get stick easily  so PRON may need to try  check result and then re  start  PRON suggest PRON use a seed random number generator for initialisation  and adjust the seed value if error value get stuck and do not improve   yes  there be a reason  PRON have to do with how PRON initialize PRON weight   there be 16 local minimum that have the high probability of converge between 05  1   here be a paper that analyse the xor problem  
__label__linear-algebra __label__condition-number PRON be show  yousef saad  iterative method for sparse linear system  p 260  that  condaa  approx conda2   be this true for  aa as well   in case  a be  ntimes m with  n ll m  PRON observe that  condaa  gg condaa  do that mean formulation in term of  aa be preferable in this case   well  let PRON look at why  ata have approximately the squared condition number of  a  use the svd decomposition of  a  usvt  with  u in mathbbrn time n   s in mathbbrn time m   v in mathbbrm time m  PRON can express  at a as   at ausvtt usvt  vst ut u s vt  v st s vt  which PRON arrive at by note that  u be orthonormal  such that  ut u  i further PRON note that  s be a diagonal matrix  such that the final decomposition of  ata can be express as  v s2 vt  with  s2  mean  st s  yield a diagonal matrix with the first n singular value from  s square in the diagonal  this mean that since the condition number be the ratio of the first and the last singular value   condafracs1sn for  a in mathbbrn time m    condat afracs1  2sm2fracs1sm2conda2   now  PRON can perform the same exercise with  aat    aat  usvt  usvtt  usvt v st ut  u s2 ut  which mean that PRON get the result  condaatfracs1  2sn2  since  s2  here mean  sst  a subtle difference from the notation above   but note that subtle difference  for  ata  the condition number have the mth singular value in the denominator  while  aat have the nth singular value  this explain why PRON be see significant difference in the condition number   aat will indeed be  better condition  than  ata  still  david ketcheson be correct  PRON be compare condition number between two vastly different matrix  in particular  what PRON can accomplish with  ata will not be the same as what PRON can accomplish with  aat  in exact arithmetic conda2condaacondaa    see eg  golub and van loan  3rd ed  p70  this be not true in float point arithmetic if a be nearly rank deficient  the good advise be to follow the above book recipe when solve least square problem  the safe be svd approach  p257  use varepsilon  rank instead when compute svd  where varepsilon be the resolution of PRON matrix datum   if  ainmathbbrntim m with  nltm  then     mathopmathrmrankata   mathopmathrmrankaat   mathopmathrmranka  leq n  lt  m     so that  ata in mathbbrmtimes m can not be full rank  ie PRON be singular   accordingly the condition number be  kappa2atainfty due to finite precision arithmetic  if PRON compute condaa  in matlab PRON obtain a large number  not inf   the claim that  declaremathoperatorcondcond  cond a2 approx cond at a  for square matrix  in the question and  edit  PRON misread  in artan s answer be nonsense  counter  example    newcommandbigomathcaloa  beginpmatrix  epsilon  amp  1  0  amp  epsilon endpmatrix   quad epsilon ll 1    for which PRON can easily check that  cond at a  bigoepsilon4 while  cond a2  bigoepsilon2 
__label__machine-learning take  h  to be a class of binary classifier ever a domain  x  d be a unknown distrbution over  x  and  f be the target hypothesis in  h if PRON fix some  h  in h PRON have the expect value of the training error equal to generalization error  that is    mathbbelshld  fh  could PRON please come up with the proof of this statement   would PRON recommend another page inside stackexchange for these theoretical question about machine learn   thank PRON   
__label__python __label__visualization __label__pca __label__jupyter the matplotlib library be very capable but lack interactiveness  especially inside jupyter notebook   PRON would like a good offline plot tool like plotly   there be an awesome library call mpld3 that generate interactive d3 plot   this code produce an html interactive plot of the popular iris dataset that be compatible with jupyter notebook  when the paintbrush be select  PRON allow PRON to select a subset of datum to be highlight among all of the plot  when the cross  arrow be select  PRON allow PRON to to mouseover the datum point and see information about the original datum  this functionality be very useful when do exploratory datum analysis   import matplotlibpyplot as plt  import numpy as np  import panda as pd  import seaborn as sb  import mpld3  from mpld3 import plugin   matplotlib inline  iris  sbloaddatasetiris    from sklearnpreprocess import standardscaler  x  pdgetdummiesiris   xscal  standardscalerfittransformx   dim  3  from sklearndecomposition import pca  pca  pcancomponent  dim   ysklearn  pcafittransformxscal    define some css to control PRON custom label  css      table    border  collapse  collapse     th    color   ffffff   background  color   000000     td    background  color   cccccc     table  th  td    font  family  arial  helvetica  san  serif   border  1px solid black   text  align  right         fig  ax  pltsubplotsdimdim  figsize66    figsubplotsadjusthspace4  wspace4   tooltip   nonedim  n  200  index  nprandomchoicerangeysklearnshape0sizen   for m in rangedim    for n in rangem1    axm  ngridtrue  alpha03   scatter  axm  nscatterysklearnindex  mysklearnindex  nalpha05   label     for i in index   label  xixi     tastypeint   labelcolumn    row  0formatxindexi     labelsappendstrlabeltohtml      axm  nsetxlabelcomponent   strm    axm  nsetylabelcomponent   strn     axm  nsettitlehtml tooltip   size20   tooltipm   plugin  pointhtmltooltipscatter  label   voffset20  hoffset20  css  css   pluginsconnectfig  tooltipm    pluginsconnectfig  plugin  linkedbrushscatter    test  mpld3figtohtmlfigfig   with openoutputhtml    w   as textfile   textfilewritet   see PRON in action on PRON blog   update  july 9  2016    PRON just find out that plotly have an offline mode and be now open source  PRON have a lot of the bell and whistle prepackag  but mpld3 may still be appropriate in some case   PRON would prefer this to be a comment instead of an answer  as PRON intention be not to plug  advertise  but PRON be currently work on PRON thesis which may be of interest to PRON as PRON kind of do what PRON want  in reality PRON be a cluster visualization tool  but if PRON use k  mean with k1 PRON have an interactive plot where PRON can search for term  select an area and see the content of each node  and other stuff  take a look and see if PRON work for PRON   httpsgithubcomlilykosclusterix 
__label__geometry __label__spatial-data PRON be currently try to calculate the geographic distance between two region as PRON want to correlate PRON with PRON similarity of another aspect  eg  similarity in word usage    currently  PRON have use a simple approach of determine the geographic center point of each region  a region correspond to a province  state of a country  and then calculate the haversine distance  1  between these two point   this approach seem rather ad  hoc to PRON and as PRON be no expert in this area  PRON be curious if PRON guy know a more reasonable approach   thank    1  httpenwikipediaorgwikihaversineformula  the haversine formula be really the good way to calculate distance on the earth unless PRON have outstanding accuracy concern   the next step pass the haversine involve take into account that the earth be not a sphere but ellipsoidal and which involve a much more expensive computation   there be a good page with some embed javascript at  httpwwwmovabletypecoukscriptslatlonghtml  that provide some explanation of issue involve and some alternative such as the law of cosine  less accurate but faster than the haversine formula   
__label__r PRON want to ask a question on how to calculate woe and iv from weighted good bad indicator in r  PRON find the woe package in r  but PRON only calculate woe and iv from non weight good bad indicator and PRON seem the indicator must be on 0 and 1 only   do anyone know how to manipulate the good bad indicator in this woe package or get information of woe and iv calculation package with weighted indicator   
__label__machine-learning __label__neural-network __label__keras PRON have a problem that do not seem to fall into a common machine learn category  and PRON be wonder if this still could potentially be solve with ml   problem  PRON have two signal record from two sensor  and would like to determine whether PRON be correlate  ie record the same physical event  or not   the catch  PRON do not have access to the full signal time series of both sensor  but only one at a time  PRON can only exchange a small descriptor on the order of 32 bit to see if the signal match or not   PRON current approach be to calculate a bunch of numerical signal feature such as mean  derivative  zero  crossing  fft etc  and see which one provide the good correlation  but that seem to be a lot of guesswork and do not work very well in any case   so now PRON have the following idea   start with a neural network which take a fix window out of the signal   possibly the fft of that window  as an input  and produce a 32bit output  pick two random correlate sample out of the pool of example  and run the network twice  once with each sample  and PRON fft   take the difference between the two output value as error measure and perform backpropagation as usual  repeat from 2  until the difference for all example be below a threshold  here be PRON question   do this approach seem feasible at all   as someone relatively new to machine learning  how would PRON implement this   PRON have have a look at keras  would this be a suitable starting point   thank in advance  and good regard  florian  addendum  PRON have find this somewhat related post  be PRON possible use tensorflow to create a neural network that map a certain input to a certain output    but PRON do not think that this be the same problem  as PRON do not actually care what the output look like  just that PRON be as unique as possible for each match pair of sample   if PRON understand correctly PRON question  PRON want a function that take a signal  a fix window  and output a 32bit representation in a way that the correlation between PRON and any other signal be preserve  mathematically speak give signal  s1  and  s2  in  s and a correlation function  corrs1  s2 PRON want some function  f  s rightarrow b  where  b be the space of 32bit binary number  that PRON could use another correlation function for instance  corrfb1  b2  approx corrs1  s2  if that be what PRON want PRON should at hashing technique in particular learning to hash  essentially in hashing want PRON do be PRON represent PRON input by binary number in a way that the ham distance between the binary number preserve some target similarity  distance  function such as the correlation  in particular for cross  correlation  inner product  PRON have some method base on random projection   so once PRON have learn  design  PRON hashing function  f  what PRON would do be   b1  fs1   send b1  receive b1  b2  fs2   return hb2  b1   this value be go to tell PRON if the signal be correlate  for the record  the paper refer to from the comment above   dpsh  feature learning base deep supervised hashing with pairwise label   be available on arxiv  httpsarxivorgabs151103855  the paper apply the same idea to image  but the principle be more or less exactly identical to PRON original proposal  
__label__ode __label__time-integration __label__implicit-methods PRON have an ode for a scalar function  u  ut of the form      fracdudtlu       here the function  l  lu satisfie      l00  quad lule0      then PRON be easy to see that the solution  u  ut have the follow property    i  if the initial value  u0ge0   then  utge0  for any  tgt0     ii  if the initial value  mle u0le m with  mgt0   then  mle utle m  question  PRON need to find a numerical method to solve this ode and under arbitrary step size maintain the two property or only one of two  PRON only know that the euler backward method have these two property  do anyone of PRON know high order  eg third  order  method to solve this problem   any link to literature or for further reading would be greatly appreciate    the two property be usually call positivity  preserving and monotonicity  preserving  make PRON easy to find this question    look at httpwwwamsorgjournalsmcom200675254s0025571805017941s0025571805017941pdf and httphomepagescwinlwillemdocartsiamhrspdf PRON seem that implicit euler be the exception among implicit linear multistep method in that only implicit euler allow for timestep to be arbitrary while preserve monotonicity  section 53   PRON say the restriction on implicit linear multistep method  timestep be not much good than on explicit multistep method   there be a discussion of monotonicity  preserve runge  kutta method in hairer  wanner vol  ii  section iv11  implicit euler be the only one among PRON have a threshold factor of  infty  see also httpwwwcscammumdedutadmorpublinearstabilitygottliebshutadmorsirev01pdf  which discuss high  order strong stability  preserve method and give an overview of available method  not unconditionally ssp  though   
__label__python __label__linear-regression PRON recently move to python for datum analysis and apparently PRON be stick on the basic  PRON be try to regress the parameter of the follow expression  z20x3ynoise  and PRON get the right intercept but the x and y parameter be clearly wrong  what be PRON do miss  code below   import numpy as np  import panda as pd  import statsmodelsformulaapi as smf   generate true value  and noise around PRON  nprandomseed5   x  nparange1  101   y  nparange1  101   z  20  x  3  y  nprandomnormal0  20  100   datum  pd  dataframexx   yy   z   z    lm  smfolsformulaz  x  y   data  datafit     print the coefficient  lmsummary    return  where the x and y parameter be both 15  instead of be 1 and 3  what be wrong   PRON think that PRON be see what PRON be see because the model see the relationship of each set of point in PRON data frame  which be govern by the equation   z  20  x  3y  noise  but all the model see be the result z  not that equation which PRON know create z  so PRON attempt to build a model which consider how z be accomplish without know there be noise  while know that both x and y be in this equation because PRON explicitly tell PRON the be in the model   base on this datum   at least this be what PRON get without a seed  when PRON run PRON datum  so PRON be close with probably different z due to different noise   x  y  z  1  1  32824550  2  2  21382597  3  3  80615424  4  4  30958157  5  5  42192197  6  6  75649622  7  7  29815352  8  8  40167267  9  9  59752065  10  10  53402601  because x and y be the same for each point  and because PRON formula have x  3y  noise   z be also equal  to 4x noise or 4y noise or 2x  2y noise for each row  there be many way to get the same change to z with x  amp  y in exact proportion plus some noise   so the regressor be be assign equal influence plus an equal share of the noise  PRON be the most parsimonious evaluation of x and y  PRON be not what PRON expect know the equation  but PRON be the answer PRON should get  if PRON use a linear model which reduce variable of zero influent  PRON may even get a 0 or na value for y  to test that lm be work  simply reverse the order of y change the relationship between x  amp  y within that equation and PRON have a completely different result  PRON think the one PRON be look to find   y  nparange10001   coef  std err  t  pgtt   0025  0975   intercept  00439  0000  119009 0000  0043  0045  x  12184  0038  32471  0000  1144  1293  y  32130  0038  85629  0000  3139  3288  PRON build PRON model right  but in this case the data set be not create in a way to test for what PRON hop to see  but PRON be not wrong  
__label__eigensystem __label__eigenvalues __label__basis-set PRON have an energy dispersion curve obtain from the eigenvalue of    ek   texteigt eik   th eik   h0  where  h0  and  t be  ntimes n square matrix   th be the hermitian transpose  t  and  k be the wave  vector  so  there be just 2 matrix  t and  h0  that determine the problem   PRON want to reduce the size of matrix  t and  h0  in a way that the approximation result in the same eigenvalue for a limited energy range  ie  e   13   be PRON possible to construct new matrix  t1  and  h1  such that  e1k   texteigt1eik   t1h eik   h1 be close to  ek for a limited energy range  how   
__label__matlab __label__numerical-analysis __label__quadrature PRON come with a the follow code to evaluate a double integral use gauss legendre quadrature in matlab  m100    generate weight and abscissa   wx  xxleg11m     wtheta  xthetaleg02pi  m     define function  psix  theta  hypergeom3412x2exp1itheta      integrate with respect to x  intx  zeros1m    for num1m  intxnumsumwxpsixx  xthetanum      end  sumwthetaintx   PRON define the function legx1x2m  in a different script to generate the weight and abscissa and PRON just call PRON in PRON code  PRON matlab code run significanlty slowly compare to nintegrate of mathematica   PRON would like to make PRON code faster because PRON idea of use matlab be that PRON be fast than mathematica  be there any way PRON can make PRON code run faster   attach be a profile summary when i run the code  the reason may be that PRON method be not as sophisticated  PRON would bet that nintegrate use an adaptive method and split the integral in part which be then refine accord to error estimate  if PRON do not improve PRON algorithm  PRON probably will not beat PRON   as someone point out  mupadex be the costly part  this come from the number of function evaluation  specifically hypergeom   PRON can reduce PRON by some divide  and  conquer algorithm  PRON be not a good idea to use such a high number of gauss  node anyway  a good one be as mention to resort to a low order integrator and refine the grid   if PRON want code  look at the matlab function integral2calcm for the actual algorithm  integral2 t  or at PRON wrapper integral2m  in case anyone want to run the code above  here be a work leg function   function  x  w   lega  b  n    generate the abscissa and weight for a gauss  legendre quadrature   beta   1n1sqrt41n12  1     w  x   eigdiagbeta1diagbeta1     absc  diagx    wght  2w12    linear map from11  to  a  b   xa1abscb1absc2   w  wght   x  x    w  w    end 
__label__keras __label__autoencoder PRON have be look at autoencoder from the kera blog here  httpsblogkerasiobuildingautoencodersinkerashtml  PRON be wonder  what motification would be necessary in order to be able to give PRON different surface ie 2dimensional vector  of which some of PRON have large spike   for example here PRON see a surface that look clean   how could a neural network look like  if PRON want to remove individual spike from this surface   be PRON right in think that a normal fully connected feed forward propagation would be sufficient  if so  be there any way to control threshold when spike be should be eliminate   also  would PRON agree that the training principle would still be the same as show in the kera blog  would PRON work if PRON simply train PRON with many good example of clean surface to recognize PRON   a possible approach would be a denois autoencoder  PRON be like a normal autoencoder but instead of train PRON use the same input and output  PRON inject noise on the input while keep the expect output clean  hence  the autoencoder learn to remove PRON  this kind of autoencoder be also describe in the blog post PRON link to   in PRON case  PRON could just train PRON denois autoencoder inject to the input spike of the height PRON expect to be remove   about what kind of architecture  eg fully connected  convolutional   only actual test can tell PRON what be appropriate and what be not  
__label__numerical-analysis __label__finite-difference __label__python __label__wave-propagation PRON be simulate the propagation of a light pulse use the equation    fracpartialpartial zafrac12cdot k0nabla2ra  with    k0frac2pilambda0  the propagation with a step size of  dz be do use the equation    leftvec1icdotfracdz2nabla2rrightan1leftvec1icdotfracdz2nabla2rrightan  with    a0vecrexpleftfracr22right  now PRON add a lens with a focal length of  f  which be do use the equation    a0la0expleftifracleftvertvecrrightvert22fright  PRON problem be now  PRON discretize  a along a vector  vecr from  0  to  rmax with  rnum element  radial approach   that work for  fgtfcrit if  fltfcrit  the oscillation of the lens factor be not properly resolve anymore  result in artifact in the simulation   for PRON current input value PRON need at least more than  50000  point  but even that be not enough due to a small value for  f  use that amount of point increase the amount of computation time and  when display the move pulse  the amount of storage  be there any way to decrease the amount of point  while still be able to resolve the lens oscillation  PRON be already use sparse matrix for the calculation  but for storage PRON have to use a dense matrix   
__label__matlab __label__linear-solver __label__symbolic-computation PRON have an overdetermined system  too many equation   express as ax  b  in matlab  when PRON try to solve PRON use ab  PRON receive the error   warning  system be inconsistent  solution do not exist   however  if PRON substitute value for the symbolic variable  the system can be solve  and the answer present match what PRON calculate by hand  be matlab unable to handle overdetermined system involve symbolic variable   tldr  use matlab primarily for numerical computation  not symbolic computation   matlab be primarily design as a numerical computation package  and PRON symbolic capability be add later  also  as a general rule  PRON be easy to compute quantity with concrete value than PRON be to calculate the same quantity in the abstract  for instance  no close form solution exist for general polynomial equation of a single variable of degree 5 or great  give both of these trend  the expectation be that  for any give symbolic expression and matlab operation  matlab be unlikely to return a symbolic solution   for the specific case of overdetermined linear system  matlab backslash  as note by davep  be do least  square  so PRON be do a qr factorization  follow by a linear solve  probably by lu decomposition  rref  on the other hand  be put the system into reduce row echelon form   in principle  the rref algorithm be essentially gaussian elimination  probably augment by good choice of pivot for some numerical stability  division be do by pivot  which probably make symbolic computation relatively straightforward  even if the result expression be beyond heinous    qr decomposition can be compute with many algorithm  modify gram  schmidt  givens rotation  householder transformation   modified gram  schmidt be probably the simple  and involve norm and division   heuristically  since rref be likely to yield simple expression  that be probably why rref work  but backslash fail  in either case  try to use these algorithm for symbolic manipulation be generally not a good strategy because these algorithm be design primarily for numerical computation  not to grind through symbolic expression  could PRON use PRON for symbolic manipulation  sure  be PRON a good idea  probably not  
__label__inverse-problem PRON work on an inverse problem for PRON phd research  which for simplicity s sake PRON will say be determine  beta in   lbetau equiv nablacdotk0ebetanabla u   f  from some observation  uo   k0  be a constant and  f be know  this be typically formulate as an optimization problem for extremiz   ju  lambda  beta   frac12intomegaux   uox2dx  intomegalambdalbetau  fdx  where  lambda be a lagrange multipli  the functional derivative of  j with respect to  beta can be compute by solve the adjoint equation   lbetalambda  u  uo  some regularize functional  rbeta be add to the problem for the usual reason   the unspoken assumption here be that the observed datum  uo be define continuously throughout the domain  omega PRON think PRON may be more appropriate for PRON problem to instead use   ju  lambda  beta   sumn  1nfracuxn   uoxn22sigman2   intomegalambdalbetau  fdx  where  xn be the point at which the measurement be take and  sigman be the standard deviation of the  nth measurement  the measurement of this field be often spotty and miss chunk  why interpolate to get a continuous field of dubious fidelity if that can be avoid   this give PRON pause because the adjoint equation become   lbetalambda  sumn  1nfracuxn   uoxnsigman2deltax  xn  where  delta be the dirac delta function  PRON be solve this use finite element  so in principle integrate a shape function against a delta function amount to evaluate the shape function at that point  still  the regularity issue probably should not be dismiss out of hand  PRON good guess be that the objective functional should be define in term of the finite element approximation to all the field  rather than in term of the real field and then discretiz after   PRON can not find any comparison of assume continuous or pointwise measurement in inverse problem in the literature  either in relation to the specific problem PRON be work on or generally  often pointwise measurement be use without any mention of the incipient regularity issue  eg here  be there any published work compare the assumption of continuous vs pointwise measurement  should PRON be concern about the delta function in the pointwise case    the measurement of this field be often spotty and miss chunk  why interpolate to get a continuous field of dubious fidelity if that can be avoid   PRON be perfectly right  most of the time  interpolation to a continuous field cover the entire domain be not an option  think about weather prediction problem  where measurement  point  source  be available only at select domain location  PRON would say point  wise datum be more the norm than the exception when PRON consider  real  life  inverse problem   PRON good guess be that the objective functional should be define in term of the finite element approximation to all the field  discretize  then  optimize   rather than in term of the real field and then discretiz after  optimize  then  discretize    the two approach be not equivalent  except for very simple problem   there be a vast body of literature compare the two approach  each with PRON advantage and drawback   PRON would point PRON towards max gunzburger s monograph  in particular the end of chapter 2    be there any published work compare the assumption of continuous vs pointwise measurement  should PRON be concern about the delta function in the pointwise case   PRON can represent PRON source term exactly  namely  PRON source term will be model as a  discrete approximation to a  dirac distribution  arraya et al   2006   or PRON can approximate the source term by some regularize function  as be do  for example  in the immerse boundary method   have a look  for starter  at this recent paper by hosseini et al   and reference therein    to expand on gohokie s answer  if PRON be interested in regularity question  PRON can also ask what  point measurement  really be  in physical practice  PRON can not measure anything at a  point   rather  PRON be always go to get some kind of average over some kind of space  time chunk  a thermometer be not a point but an extended object  and PRON take time to adjust to the temperature of the medium around PRON  a concentration measurement device need a finite sample size  etc   what this mean mathematically be that the delta function in PRON functional be  really  average over sufficiently small area andor time interval  consequently  the right hand side in the dual equation be also finite  and no regularity issue arise   of course  in practice  PRON will typically not be able to resolve the small space or time interval on which PRON measure with a finite element mesh  that is  on length scale PRON can resolve  the right hand side do look singular  and consequently so do the solution  but  since PRON be already introduce a discretization error  PRON can also regularize the characteristic function of the volume over which PRON measure by a discrete approximation with the same weight  if PRON do PRON right  PRON will introduce an error that be no large than the discretization error  at the benefit of receive a perfectly nice right hand side function for the  discrete  dual equation  
__label__svm suppose PRON use a linear support vector machine with slack variable on a dataset that be linearly separable   could PRON happen that the support vector machine report a solution that do not perfectly separate the class   as an illustration  be the situation in the picture possible for a support vector machine with slack variable  although there be a  good  boundary that allow perfect classification  the support vector machine go for a sub  optimal solution that misclassifi two sample   have linear separable datum mean that an optimal solution exist  the support vector machine be an approach to identify the optimal solution with multiple step through lagrange multiplication  this method be guarantee to give PRON the optimal solution if there be one  
__label__machine-learning this be more of a hypothetical than something PRON be actively try to solve  PRON just strike PRON that a machine learn algorithm that specifically look at two piece of datum and have to label one as great than the other or something of that nature may be inherently different than classify each separately and compare the strength of the classification   not sure if PRON read the question correctly  but PRON have implement multi  variate sub  sample optimisation routine use the kolmogrov  smirnov test   the advantage be that if two distribution exactly track each other  so would be significantly correlate  but be at quite different scale  eg one be half the value of the other   PRON would still rank as very different distribution at similar scale  only when both distribution and scale match be the rank high   spatially per  pixel comparison algorithm be also quite common in raster analysis   there be rank algorithm base on machine learning that be aim to build ranking model  training datum for these model be give in the form of partial ordering between each pair of element in a sample  a brief description  together with a list of useful reference  be give in the correspond wikipedia page  
__label__matlab __label__computational-physics __label__software __label__quantum-mechanics PRON want to look at time evolution of the density matrix of some  very simple  spin system  but PRON be have trouble with PRON approach   PRON want to use a simple for  loop time  step method  not as ode solver like ode45 as this be suppose to be a demonstration code   PRON work in matlab  numerical calculation   so PRON will include code snippet where relevant   what PRON do  PRON construct the spin  only hamiltonian for PRON system in two part  first part describe the zeeman interaction of the spin system with an external magnetic field   sa and  sb be two electron   sc be a nucleus      hmagnetic   omega0esaz   sbz    omega0nscz      where  omega0e  b0gammae be the electron larmor frequency  and  omega0n the nuclear larmor frequency   arguably the nuclear zeeman interaction could be ignore  but PRON have include PRON for the completeness  sake    the second part of the hamiltonian describe the hyperfine interaction of one of the electron with the nucleus via the hyperfine interaction      hhyperfine   amathbfsamathbfsc   asaxscx   sayscy   sazacz       where  a be the hyperfine coupling constant  PRON spin  only hamiltonian be then      h  hmagnetic   hhyperfine      in PRON matlab code this read as    pauli spin matrix  ix  05   0   1   1    0    iy  05   0 1i  1i   0    iz  05   1   0    0   1    ie  eye2     successive kronecker product of three element  tkron  x  y  z  kronkronx  yz     spin matrix  sax  tkronix  ie  ie   say  tkroniy  ie  ie   saz  tkroniz  ie  ie    sbx  tkronie  ix  ie   sby  tkronie  iy  ie   sbz  tkronie  iz  ie    scx  tkronie  ie  ix   scy  tkronie  ie  iy   scz  tkronie  ie  iz     input constant  gamman  42577e3   hz  mt  gammae  176e8   hz  mt  hfc  10   hyperfine couple in mt  b0  001    magnetic field in mt   derived constant  a  gammae  hfc   omega0  gammaeb0   electron larmor frequency  omega0n  gammanb0   nuclear larmor frequency   hamiltonian  hmag  omega0saz  sbz   omega0nscz   hhyperfine  asaxscx  sayscy  sazscz    h  hmag  hhyperfine   now  because there be no election  electron spin interaction PRON then define a similarity transform from xyz basis to electronic singlet  tiplet  st  basis such that      hst   m1hm     which in matlab be   s2  1sqrt2            s   t0   t   t  PRON   0   0    1    0  s2 s2   0    0  s2 s2   0    0  0   0    0    1    mn  eye2    nucleus stay in alpha  beta basis  m  kronme  mn     change to st basis  hst  mhm   m be unitary therefore use transpose  PRON then start PRON spin density in a pure singlet state and use the integrate form one the liouville von neumann equation     hatrhot   eihaththatrho0eihatht      to iterate through time point as follow  rh0  zeros8    rh011   05   rh022   05   t  linspace01e61000    1 PRON of spin evolution  for i  1lengtht    t  ti    e  1iht   PRON  expme    ep  expme     rht  emrh0ep   propagate rh0 to time t  sdi   rht11   rht22    end  plott  sd    problem  well  PRON do not work  more specifically  PRON would expect the singlet density sd to oscillate between 1 and 025 due to sleftrightarrowt interconversion  the result PRON get be indeed oscillatory  but between  1 and 1  furthermore the trace of the density matrix be not always 1   question  can PRON address any of the problem mention above directly   can PRON reference PRON to a piece of literature  or  even better  code  which do this step  by  step propagation of the density matrix with the lvn equation   if this be not a possible way of do density matrix propagation  can PRON explain to PRON why   please remember that PRON realize this be not the optimal way of do spin density simulation  PRON specifically want to do a simulation this very way   ok  PRON figure PRON out  there be a couple of problem   PRON similarity transform be bad  this be the major issue   PRON should not have transpose the ep propagator  PRON nuclear gyromagnetic ratio be wrong  although that end up not really matter due to the nuclear zeeman interaction be small in comparison to other part of the hamiltonian   PRON have fix the problem by do the  correct  similarity transform first  and then work in st basis throughout  what help be to define the singlet and triplet projection operator  question 3d from this assignment  because even if PRON end up transform to some cookie basis  as long as PRON be consistent throughout  ie also transform the projection operator   PRON end up with the correct result   anyway  here be PRON code  and here be why PRON want to understand that subject better  PRON hope this help someone at some point  
__label__machine-learning __label__nlp __label__named-entity-recognition PRON be try to find the good way to extract information from bank statement  a bank transaction be not a natural text but still human readable   PRON would like to extract a bunch of datum if present like payment method  date  amount  vendor  customer name and even information like an order  invoice id or the reason  PRON have set of datum that PRON can use for the training  for instance  100k vendor  payment method word  etc   also the solution must work with multi language  the bank transaction could be in different language  not only english    be name entity recognition the good way to go   here be some sample datum PRON can have in input   dividend  pos purchase non  pin  rciringcentral  inc   impdebley 83652  alicuota genus  directdebit equity commerce merch fee 158713 62000329526385 ccd  advance auto parts jacksonville  thank for PRON help  be currently work on something in this domain   the rough process PRON be currently follow be   extract datum from pdf  ubiquitous version of bank statements nowadays  into more usable format  currently  convert PRON to txt file first as an intermediate step   generically  bank statement  from a specific bank  tend to be structure in the same format  hence  while convert the txt to csv  PRON can structure the algo such that PRON know what to pick up  base on a rough analysis of the txt file   use 2 different datum frame to store the statement content  one df keep track of the transactional datum  store 3 value  date  description and amount  and the other to keep track of the metadata  store 2 value  key and value   PRON can determine the rough structure of the txt file by go through a few txt  eg the transaction generally begin after a specific element s occurrence in every bank statement  ignore the page footer  header that recur in case of multi  page transaction  also  another observation would be that all datum in a specific column of the transaction  the transaction be read in as a table by PRON program  link give below   appear together   do the above enable the transaction and meta datum to a relatively structured and processable format  perform analysis on the description column or the entire transactional df PRON have thus generate   PRON can use the follow code to convert a give pdf to a txt file   from   future   import unicodeliteral  import os  import sys  reloadsy   syssetdefaultencodingutf8    from pdfminerpdfinterp import pdfresourcemanager  pdfpageinterpreter  from pdfminerpdfpage import pdfpage  from pdfminerconverter import xmlconverter  htmlconverter  textconverter  from pdfminerlayout import laparams  from cstringio import stringio  def pdfparserdata    dest  data3txt   print  nnnnnd  fp  filedata   rb    rsrcmgr  pdfresourcemanager    retstr  stringio    codec   utf8   laparam  laparams    device  textconverterrsrcmgr  retstr  codec  codec  laparam  laparam    create a pdf interpreter object   interpreter  pdfpageinterpreterrsrcmgr  device    process each page contain in the document   for page in pdfpagegetpagesfp    interpreterprocesspagepage   datum   retstrgetvalue    datum  unicodedatautf8   errorsignore     print datum   write datum to a file  print datum  with opend   w   as f   fwritedata    set the work directory  path   fbank  payments   oschdirpath   fls  oslistdirpath   for x in fls   if x3pdf    pdfparserpathx  
__label__r __label__classification __label__decision-trees essentially  this be PRON data set  x class  sex  age survived freq  1  1st  male child  no  0  2  2nd  male child  no  0  3  3rd  male child  no  35  4  crew  male child  no  0  5  1st female child  no  0  6  2nd female child  no  0  7  3rd female child  no  17  8  crew female child  no  0  9  1st  male adult  no  118  10  2nd  male adult  no  154  11  3rd  male adult  no  387  12  crew  male adult  no  670  13  1st female adult  no  4  14  2nd female adult  no  13  15  3rd female adult  no  89  16  crew female adult  no  3  17  1st  male child  yes  5  18  2nd  male child  yes  11  19  3rd  male child  yes  13  20  crew  male child  yes  0  21  1st female child  yes  1  22  2nd female child  yes  13  23  3rd female child  yes  14  24  crew female child  yes  0  25  1st  male adult  yes  57  26  2nd  male adult  yes  14  27  3rd  male adult  yes  75  28  crew  male adult  yes  192  29  1st female adult  yes  140  30  2nd female adult  yes  80  31  3rd female adult  yes  76  32  crew female adult  yes  20  if there be no frequency but only single value datum  then PRON know how to invoke rpart to construct a decision tree for PRON  how to do PRON consider the frequency of each class   PRON be a beginner in r thanks  to use the column name freq as PRON case weight  PRON can call rpart with the argument weight  freq  
__label__algorithms __label__complexity PRON have the following algorithm give   input  regular matrix  a in mathbb rn  n  output  lu  decomposition of  a  lu  for k  1      n do  for j  k      n do   rkj   akj  − sumi1k1  lkirij  end for  for i  k  1      n do   lik    aik  − sumj1k1  lijrjkrkk  end for  end for  give every elementary operation      have the cost 1  how can PRON be derive that the complexity of this algorithm be 23n3  12n2  16n  PRON be really interested in understand how such a closed formulae for the complexity be derive   in order to compute  rkj  PRON need to do  k1  multiplication and  k addition  for a total of  2k1  operation  PRON need to do this for  j  k  n  ie  n  k time for a total of   n  k2k1 operation to compute all of the  rkj for a give  k then PRON need to do this for all  k from 1 to  n  so the cost of this first loop be     sumk1n  n  k2k1    nsumk1n  2k1   sumk1n k2k1      2nsumk1n k  sumk1n 1  2 sumk1n k2  sumk1n k     2nfrac 12 nn1   n  textsometh   frac 12 nn1       PRON need to look up the formula for the third sum  PRON forget PRON exact value  but PRON be proportional to  n3  PRON can do the same calculation for the second inner loop of PRON algorithm  
__label__algorithms __label__forecast PRON dataset have the following field   id  represent the customerid  storeid  represent a chain  productid  represent the product purchase  year  the year of the transaction  month  the month of the transaction  day  the day of the transaction  weekday  the weekday of the transaction  transactionvalue  the amount pay by customer  PRON be look for a forecast to predict the follow statement  in next week there be a high probability that the product to be purchase on wednesday   basically return something like this   monday  tuesday wednesday  a  80   2   1   b  10   20   34   c  34   83   10   which algoritm be PRON usually use to predict these value   thank   
__label__emotional-intelligence __label__human-like PRON be talk about relationship between ai  eg 2 of PRON form a couple  3  in family like relationship    what knowledge could come out of such experimentation   one example may be self  play in game  since neural network and deep learning depend on massive amount of datum  one way to generate datum be to have two virtual machine play each other and record the experience  an example discussion can be find at httpwwwcscornelleduboom2001sptsinterisgammonhtm which use reinforcement learning  PRON believe alphago also use this technique of self  play  and use two independent neural network  one reduce the search space and the other decide on the good move in the remain space  that in a sense cooperate to decide on the next move  
__label__linear-algebra __label__algorithms __label__eigenvalues let  m be a matrix which have the follow property   1   m be hermitian  2   m have only rational entry  3   m be know to have rational eigenvalue  what algorithm be there for exactly compute the eigenvalue of  m   mathematica seem to be able to diagonalise such matrix very very quickly  so such method must exist  PRON be hop PRON be not trade secret of wolfram inc    PRON be sceptical that mathematica be use the rational root test on the characteristic equation since the number PRON would need to factorise get large quickly   the characteristic polynomial of a matrix  m can be write as   chimz   zn  textrmtracemzn1ldot  detm  since PRON know ahead of time that all the entry of  m be rational  PRON can be assure that  detm be rational  say  detm   p  q now  the rational root theorem assure PRON that  provide  chim have a rational root  lambda  k  l  then  k divide  p and  l divide  q but  PRON know ahead of time  somehow  that all eigenvalue of  m be rational   PRON can compute the determinant of  m much faster than the full characteristic polynomial by compute the lu  factorization of  m since all the entry be rational  the entry of the triangular factor be also rational  PRON would of course have to ensure that the factorization be do symbolically so that the determinant be also compute exactly   try all rational number  k  l such that  k  p and  l  q be also prohibitively expensive  however  PRON could use a conventional eigensolver to get a set of approximate eigenvalue  hatlambda1ldotshatlambdan  then look for the exact  rational eigenvalue with the desire divisibility property nearby to each approximate one   a cursory google search do not turn up anything for the exact eigendecomposition of rational matrix  so PRON doubt that anyone have write a program to do this before  however  PRON may have luck with the linear algebra feature of sympy   there be a  simple  method  simple in the sense that PRON need only few line of procedure   here PRON use maple  but mathematica can do PRON also with pslq    step 1  maple calculate the root of the characteristic polynomial with  for instance  200 digit   step 2  maple use the lll method of lenstra that give the minimal polynomial of the root  if PRON know the root with sufficient precision   here the minimal polynomial have degree  1  but  for sake of security  PRON ask for a polynomial of degree  2  if PRON obtain a polynomial of degree  2   increase accuracy   the time of calculation  in the follow random instance  be 012   roll   rand10  10  10  10    digits 200   f   1   for i to 10 do f   fx  roll1roll     end do   f   f   numerexpandf     t   time     r   fsolvef    withpolynomialtool    for i to 10 do  printminimalpolynomialresi   2   end do   timet  
__label__machine-learning __label__neural-network __label__deep-learning __label__keras __label__decay PRON be currently train a cnn with keras and PRON be use the adam optimizer  PRON plan be to gradually reduce the learning rate after each epoch  that be what PRON think the decay parameter be for   for PRON  the documentation do not clearly explain how PRON work   decay  float   0  learn rate decay over each update   however  when look at the use learning rate in tensorboard  PRON stay the same as the initial learning rate  so  how do this decay parameter actually work   adam use mini batch to optimize  during optimization  PRON may need go down hill  the cost function  so quickly use a high learning rate  when PRON reach to point which be near to relatively optimal point PRON have to reduce the learning rate in order not miss the optimal point  in other word PRON have to decay learning rate to have more accurate step by reduce the learning rate  mini  batch optimizer have multiple step during one epoch  which all of PRON may not be true but because PRON try to minimize the cost for each batch of input datum  PRON finally reach to the relative optimal point   for each epoch  tensorflow use same learning rate and after finish epoch  the next epoch will be start use the current learning rate divide by the decay parameter  PRON should not be negative because PRON be use gradient descent which imply move toward low  level place   recently PRON be look the code of optimizer in keras and PRON find that as the follow code   if selfinitialdecay  gt  0   lr    1    1   selfdecay  kcastselfiteration   kdtypeselfdecay      learning change each iteration and not each epoch  
__label__finite-element __label__matlab __label__iterative-method __label__preconditioning PRON implement a fem solver in matlab for poisson s equation in 3d  use hexahedron and sparse matrix for the laplacian  PRON be use the backslash but now PRON have to use a few iterative method  gmres with ilu for example   with  nx   ny and  nz  in general   n  be the number of element  50  100 and 200 as test case  PRON have some problem  for  n50  PRON take more than  600  second use cgs with incomplete lu preconditioner  for  n100  PRON take more than 60 minute and PRON be still run  PRON do not set the tolerance or the maximum number of iteration since PRON do not seem to affect the timing problem  be this possible or PRON should not take that long  be there a way to speed PRON up   here be the code    l  u   ilukb     uh  flag  relre  iter  resvec   cgskb  fbl  u    where kb be the laplacian and fb the right  hand side   thank PRON   PRON stiffness matrix be symmetric  positive definite so the krylov  base solver of choice for this problem be conjugate gradient  PRON want code similar to this  l  icholk    u  pcgk  b  tol  maxiter  l  l     assume PRON want to use an incomplete factorization as a preconditioner  the stiffness matrix for poisson s equation on a cube be relatively well  condition so PRON may want to experiment with no preconditioner or a very simple preconditioner like diagonal scaling  but  since PRON say the matlab implementation of ichol be quite fast  that be likely to be good choice   other krylov  base iterative algorithm  like cgs and gmres that PRON mention  be primarily for unsymmetric or non  positive definite system  
__label__computational-physics __label__software can PRON recommend a good book that discuss several method for the numerical solution of time  dependent and time  independent schrödinger equation  PRON have search the internet several time but could not find good reference   PRON would be very helpful if the book also contain matlab  python example   PRON be try to solve the tdse in strong laser field  and try to implement the split operator method or crank  nicholson method   
__label__pde PRON want to simulate on PRON computer the solution of the heat equation in 3 space dimension with cauchy initial datum  that be    partialt u  traxcdot delta u   u0xu0x     where  u0in cmathbbr3mathbbr  even if  a be constant PRON be not sure what be numerically the good way to do this but PRON be sure this must be in standard toolbox of matlab  scilab etc  however PRON have quite some trouble with the documentation   could someone give PRON pointer in the right direction   more general  what be a good book or lecture note that deal with numeric for parabolic pde  with multidimension in space    
__label__statistics __label__probability __label__distribution PRON face a dataset which describe out of each quarterly total loan amount of a bank  how much  dollar amount  be charge off  for example  say 2015 qrt 1  the total loan amount be 10000 usd  by some time point  the charge off principal turn to be 500 usd  so the estimate default rate for this quarter be 5    now PRON have a series of such datum point and the datum in each quarter can be assume independent   be there any know distribution  continuous distribution  can be apply for each quarterly default rate or default amount  give a certain time point   when PRON come to the count  namely out of n loan from 2015 qrt 1  there be m loan charge off  the the default rate can be estimate as m  n and the underlie distribution be binomial  n  p   however  in dollar amount version  any distribution can be available to model the datum   cheer  
__label__ode __label__numerical-modelling __label__implicit-methods PRON have a computer simulation system of body connect by spring  so PRON movement be govern by    xn1   xndelta tkxn  r  where  r be the idea distance between every two body  and  delta t be the timestep   use the backward euler method  PRON end up with the equation    xn1   xndelta tkxn1r  which simplify to    xn1   fracxndelta tkr1delta tk  PRON then find that  delta tkxn1r xn  xn1   so PRON know the magnitude of PRON movement  and PRON can easily calculate a direction vector to apply to PRON   PRON calculate such interaction between each two set of body that be join by a spring  PRON have about 28 body   and then add up the movement vector for each body and apply the movement   this be where PRON run into a problem  the number PRON get just keep grow and grow through the iteration until the reach infinity and PRON simulation explode  unless PRON set a small timestep  delta t PRON think backward euler should be able to avoid this problem and be unconditionally stable   be the problem on PRON implementation  or be there a limit to backward euler s stability too that be govern by the time step  delta t  the implicit euler method be unconditionally stable alright  but what PRON be do be not the implicit euler method  rather  what PRON do be compute where the particle would be at the end of the time step use only 2particle interaction  compute the location update  and then sum these update up for all particle interaction  but the implicit euler method would compute all of these update where PRON use as  xn1 the final position use all interaction  not just the one interaction PRON be currently consider   the method PRON describe be call  operator splitting   and PRON be not unconditionally stable even if each building block  each interaction separately  be discretiz in a way that be unconditionally stable  
__label__python __label__neural-network __label__classification __label__computer-vision PRON have two group of image  each one with 1000 sample   the speckle pattern  in this context  be the same as a random pattern or  white noise  image  so these image be fundamentally different   in group one  each figure be generate by consider a random function that return something similar to a speckle pattern  see fig  1    in group two PRON follow the same procedure as group 1  but PRON plot a small point above that can be position anywhere and with any color  see fig  2    PRON want to classify both group and PRON already try to do PRON with simple neural network  but PRON have be unsuccessful   what be the good technique for this kind of problem   fig  1   fig  2   PRON be go to have to do some experimenting to figure out what be  good   but PRON would recommend start with a convolutional neural network   since PRON be only detect a very small difference  though  the pixel value PRON should give PRON a good indication of where the colour be  and whether there be colour at all  PRON be a bit surprised that a simple architecture have not give PRON any luck   base on the give example  if PRON be literally the same everywhere except in a small region  just subtract image 1 from image 2 to find the difference   and then PRON can check the positive and negative value where PRON be differ by a considerable margin to classify PRON  this will  probably  work   have PRON try rule  base approach   base on PRON example  PRON can think of two way   for each picture  get a list of all rgb value PRON contain   the first contain only gray  which have rgb value  x  x  x    the second one contain color  so some pixel with rgb value  x  y  z   where  x neq y or  y neq z   or  if that be faster  convert all picture to gray scale and check if the convert picture be identical to the original   scan the image for area  large than one pixel  with identical color  if PRON dot be large enough and the background noise be uncorrelated for neighboring pixel  PRON be very likely that these area be the dot PRON be look for   PRON find the answer in the paper link above   the author use a cnn to solve the problem  PRON will post the code   httpslinkspringercomarticle101007s0017001708820 
__label__convolutional-neural-networks PRON wish to write a bot that can use screen footage to play a game  specifically for the game  nidhogg    to that end PRON have determine that a cnn should do the feature detection and a feedforward neural network should determine the action to take   for this question PRON wish to focus on the cnn   PRON idea be to first train the cnn as an autoencoder to aid in unsupervised pattern recognition and to add hidden layer after every epoch  but PRON be unsure whether this would actually be fast or lead to high accuracy  as oppose to immediately train all hide layer at once    the reason PRON be unsure be because PRON imagine the following scenario   input  gt  hidden1  gt  output  if the output layer need to approximate the input layer be PRON not wasteful to let PRON have PRON own weight backpropagated when PRON actually try to determine the  inverse  function of the hidden layer  be PRON possible to  directly  determine an inverse function for a layer even if consist of multiple filter   if so  say PRON have train PRON first hidden layer so that the error be minimal and use PRON as an input layer whilst PRON add another hidden layer  can PRON repeat PRON strategy   input  gt  hidden1  gt  output  inputhidden1   gt  hidden2  gt  output  PRON seem like what PRON need be reinforcement learning method rather than ae  unsupervised  method have to be like  input   cnn feature extractor  encode side of ae  ie vgg    classifier  ie dnn    choose one of the action   observe change in the game and check if PRON be a wrong  like character be die  or good  character be move forward  and give a rating to action  feedback of decision    after a while  ai will learn how to jump over obstacle  avoid stuff etc  
__label__algorithms __label__evaluation PRON devise a classification algorithm which be useful for specific complicated distribution of class of datum  the method work good on the artificial datum which can not be solve by typical algorithm in the literature  but yet i could not find any real datum to evaluate the algorithm on real problem   also there be mathematical support for the basis of the approach and the way PRON work   so i be wonder if an algorithm with good mathematical support but no evaluation on real datum have any chance to get accept in computer science conference like sdm or cvpr   
__label__dataset __label__training __label__naive-bayes-classifier PRON be new in sentence classification  naive bayes classifier    q1  if any training datum be label in such a way    class  compsci love python programming    class  compscguido van rossum be know as author of python programming  language    class  zoologyburmese python may be best know as the large snake of choice among reptile owner     here  prior probabilityclass  zoology13    can PRON label the same training set as   class  compsci love python programming  guido van rossum be know as author of python programming language    class  zoologyburmese python may be best know as the large snake of choice among reptile owner     here  prior probabilityclass  zoology12    q2  in sentence classification  can PRON break the sentence   PRON love python programming  but PRON be scared of python   into this way    class  compsci love python programming    class  zoologybut PRON be scared of python      
__label__python __label__regression __label__xgboost __label__evaluation PRON want to keep objective as  reg  linear  and evalmetric as customised rmse as follow   def customisedrmsepred  dtrain    n  lenpred   pred  nparraypred   actual  nparraydtraingetlabel     crmse  npsqrtnpsumnppowerpreds110actual1   11  2n   return  custom  rmse   crmse  when PRON run  train function as follow    model  xgbtrainparamlist  xgbtrain  numround  watchlist  none  customisedrmse  earlystoppingrounds30   output PRON be get be this     0  train  rmse151904  val  rmse152102  train  custom  rmse0607681  val  custom  rmse0610993  multiple eval metric have be pass   val  custom  rmse  will be use for early stopping   will train until val  custom  rmse have not improve in 30 round    1  train  rmse144936  val  rmse145103  train  custom  rmse0588831  val  custom  rmse0589902  and so on   PRON question be  PRON be get optimise use  rmse  or  custom  rase  or both   what PRON have to do to remove  rmse  as PRON come by default with  reg  linear     
__label__text-mining there be a text summarization project call summarist  apparently PRON be able to perform abstractive text summarization  PRON want to give PRON a try but unfortunately the demo link on the website do not work  do anybody have any information regard this  how can PRON test this tool   httpwwwisiedunaturallanguageprojectssummaristhtml  regards   pasmod  PRON date back to 1998  so most likely have be abandon  or  acquire  by microsoft as the creator currently work there and have do since publish that research   see httpresearchmicrosoftcomenuspeoplecylists97pdf  and httpresearchmicrosoftcomenuspeoplecyl for the author  maybe PRON could try to contact PRON  
__label__bigdata __label__apache-hadoop __label__efficiency __label__scalability __label__performance hadoop be a buzzword now  a lot of start  up use PRON  or just say  that PRON use PRON   a lot of widely know company use PRON  but when and what be the border  when person can say   good to solve PRON without hadoop    PRON be an economic calculation  really   when PRON have a computing  problem   in the most general possible sense  that PRON can not solve with one computer  PRON make sense to use a cluster of commodity machine when do so a allow PRON to solve the problem  and b be cheap than a forklift upgrade to a big computer  or upgrade to specialize hardware   when those thing be true  and PRON be go the  commodity cluster  route  hadoop make a lot of sense  especially if the nature of the problem map  no pun intend  well to mapreduce   if PRON do not  one should not be scared to consider  old  cluster approach like a beowulf cluster use mpi or openmp   that say  the new yarn base hadoop do support a form of mpi  so those world be start to move closer together  
__label__machine-learning __label__r __label__data-mining PRON be work with boruta package and PRON get result give PRON what be the confirm important and wihch be confirm unimportant  but PRON do not understand how deal with the other result to get prediction   PRON also get other result use attstat function    gt  borutadf  lt attstatsfinalboruta    gt  printborutadf   meanimp  medianimp  minimp  maximp  normhit  decision  typepeau  0052161381  005319181 106177593  13761527 000000000  rejected  peaucorps  0337867005  069941457 143046058  13157235 000000000  reject  sensibilite  0298730583  003562052 232789187  12437708 001010101  rejected  imperfection  1085442700  113736889 108261162  39439827 025252525  reject  brillance  0155435794  004064857 205771685  16201306 000000000  rejected  grainpeau  0308457441 037074790 125886417  10066092 000000000  rejected  ridesvisage  2219190852  209472451 049672532  46268287 058585859  reject  allergies  1228700001 153519783 234908815  10861057 000000000  rejected  mains  0418998135  060501091 195125346  24157828 002020202  reject  interetalimnaturelle  0219513644  013509538 191244722  24638977 004040404  reject  interetoriginegeo  0044901047  020265705 175743061  20440470 000000000  reject  interetvacance  0544245351  052582992 201453419  25365610 008080808  reject  interetcomposition  1834644587  179862852 078017284  40194479 056565657  rejected  croisprofilprio  0430628660 046854338 159416930  08246945 000000000  rejected  croisprior1milieu  0831128967  089799604 141082202  21581193 002020202  reject  dataquest1  0005916672  007367412 147822460  18950212 001010101  reject  priorite2  0375410897 018306898 217961692  12224462 000000000  reject  priorite1  0884636401  067775007 006793765  21748436 008080808  rejected  croisallage  3253301471  334038398 070187807  57070283 083838384 confirm  dataquest4  0141692419  012823091 151949778  24514657 000000000  reject  age  3650800042  378393702  050330531  61230279 090909091 confirmed  nbregift  2207524545  215076093 042833863  56948709 061616162 confirmed  w  1572900079  163250280 210383051  52070615 042424242 confirmed  nbreachat  66762088258 7004268284 4246182158 838079672 100000000 confirm  but PRON do not know what mean of meanimp  medianimp  minimp  maximp and normhit and how deal with PRON to get prediction    thank  
__label__python __label__pandas PRON have a pandas dataframe with binary value column  PRON would like to replace value in each cell with PRON frequency in rspective column in place  PRON question be how to keep track of the current column while use apply on the subset of column like here   to be apply from 8th column to the end    traindataix8    traindataix8applyx  what should come here    PRON know that traindataixcolnumbervaluecounts0  will return number of zero in colnumber but how can PRON use PRON inside apply function   import panda as pd  import numpy as np  df  pd  dataframenprandomrandint2size10  4    column  listabcd     df  a  b  c  d  0  0  1  1  1  1  1  0  1  0  2  1  0  0  1  3  0  1  1  1  4  0  0  1  1  5  0  1  1  1  6  0  0  0  0  7  1  1  1  0  8  1  1  1  0  9  1  0  1  0  value  dfapplypdvaluecount   value  a  b  c  d  0  5  5  2  5  1  5  5  8  5  newdf  pd  dataframe    for x in dfcolumn   newdfx   dfxapplylambda row  valuesxrow    newdf  a  b  c  d  0  5  5  8  5  1  5  5  8  5  2  5  5  2  5  3  5  5  8  5  4  5  5  8  5  5  5  5  8  5  6  5  5  2  5  7  5  5  8  5  8  5  5  8  5  9  5  5  8  5  PRON create a df with random integer 0 and 1   then count PRON by column in value df   then loop through each column and replace each cell with PRON respective count  be random PRON be close to 55 split  but PRON can see the c column have 82 split  
__label__c++ __label__nonlinear-programming PRON be currently try to solve nonlinear constrained minimization problem as implement in matlab  fmincon  function  PRON expectation be  minimizefun1x0ub  lb  fun2  where x0 be initial state  fun1 be function that need to be minimize  ub be upper bound  lb be low bound and fun2 be function that provide vector of nonlinear equality  inequality as describe in httpwwwmathworkscomhelpoptimugfminconhtml as nonlcon function  these vector be change through iteration as well  PRON be non  linearly dependent on xn  n  th iteration of solution vector   in matlab implementation PRON be in a form cxlt0  this be the last piece of code that need to be port from matlab to c and PRON ve be struggle a lot while try to find appropriate c library contain this algorithm  this be why PRON be seek help here and PRON would much appreciate if PRON could provide PRON expertise   good example of what PRON want to do be first one on this page httpwwwmathworkscomhelpoptimugconstrainednonlinearoptimizationexampleshtmlf10960stiddoc12b only difference be that PRON need boundary as well   thank in advance   peter  if PRON function be not differentiable  PRON should be careful about how PRON use finite difference  if PRON want to use derivative information  PRON good bet be probably some sort of semismooth newton  type method  a set of note describe such method can be find here   twelve to thirty variable be probably on the upper end of what be doable with pattern search  also call direct search  method  a recent review paper by rios and sahinidis in journal of global optimization on derivative  free optimization method  such as pattern search method  can be find here  along with a companion web page  a less recent review paper on these method by kolda  lewis  and torczon in siam review can be find here  these method work fairly well with expensive function evaluation  and do not necessarily require differentiability or derivative information   many of these method require some sort of convexity to guarantee convergence to the global optimum  so if PRON be to solve PRON problem rigorously  PRON may need to couple these method above with a branch  and  bind strategy  however  if PRON do not care about rigor  an approach like matlab s fmincon may work well enough  there be no guarantee anymore   finite difference will most likely give PRON a member of the subdifferential of PRON nondifferentiable function  which could suffice for PRON problem instance and particular input datum to return a sufficiently accurate result for PRON purpose  in that case  PRON should probably look at the library mention in the answer to the question christian link in PRON comment   if all PRON need be a c library to solve nonlinear optimization problem  PRON can use roboptim  even though roboptim be initially develop with robotic optimization problem in mind  PRON be suitable for any nonlinear optimization problem  PRON provide a simple c interface with plugin for multiple nonlinear solver  ipopt  nag  etc    use that kind of wrapper make PRON easy to use another nlp solver  if PRON can not provide gradient  finite  difference computation can be do automatically   PRON be open source so PRON can check out the source code on github  httpsgithubcomroboptim  the analysis do by geoff oxberry be essential for the choice of nonlinear solver that will be call by roboptim  note that when deal with that kind of solver  parameter tweaking can have a huge impact on performance  and PRON may still get stick in local minima  PRON really depend on the kind of problem PRON be deal with    note  PRON be one of the developer of this project  
__label__algorithms __label__education first  an example   give an image  2d array  to write down a mathematical notation for the function of pixelation  for example   fig  0  the pixelated version  right  of the give 2d array  leave   what would be a mathematical notation for the above simple function   the second part   how about many everyday compute algorithm that can apply on any arbitrary dimensional datum if one want to write PRON down in mathematical notation  PRON as a computer scientist how would PRON start  proceed and finalise  consider the case that PRON work will be check by a pure mathematician   to whom may can please add a new tag math  notation  to answer the first part of PRON question  there be probably many function that achieve the same goal  so  what would be a mathematical notation for the above simple function  will have many answer  for instance  the image could be coarsen by decrease the resolution by one  half in each dimension so that each pixel in the coarse image correspond to four pixel in the original image  and the pixel color could be determine by average  in which case PRON could use algebraic formula with indexing  the few signal process question PRON have get on here have use special notation for filter  PRON be not familiar with signal processing  so PRON can not really describe PRON well  or what PRON mean   the second part of PRON question on the correspondence between algorithm and mathematical notation be broad  to give a brief overview  the church  turing thesis state that any function that be  algorithmically calculable  if and only if PRON can be compute by turing machine  so a facile answer would be   any algorithm can be express in mathematical notation as a turing machine   few people describe algorithm in this manner  PRON will see PRON in a theory of computation class  but PRON would not see PRON in an undergraduate introduction to algorithms class  what seem to be more common for algorithmic description be pseudocode  there be no standard pseudocode use everywhere  but one common variant can be find in the algorithms textbook by cormen  leiserson  rivest  and stein  PRON can find pseudocode also in several applied mathematic paper in siam journal  more mathematical algorithm tend to involve equation with standard notation that augment some sort of pseudocode like that in the cormen  et al  book  other simply describe PRON algorithm in long  form sentence  again use mathematical notation as need  if PRON be short enough  some include matlab code  or similar sort of code  in PRON paper  if the program use to implement an algorithm be long  typically some sort of description be give  the program may or may not be accessible to reader   as for PRON comment about pure mathematician  that really depend on audience as well  in more formal setting  computer science class  some research setting like certain mathematic journal   a proof may be require to show that a propose algorithm achieve PRON stated objective  in less formal setting  the algorithm may simply be state without proof  exposition without proof be more common in journal with a less mathematically rigorous readership  such as some application  orient engineering journal   so both of PRON question have many answer  and the latter be certainly open to interpretation and preference  
__label__machine-learning __label__scikit-learn __label__svm PRON be suggest in svm to experiment with different classifier use the various kernel available  tune the parameter be advise too  PRON be experiment with various gamma and regularization parameter  be there a way to print  margin  for each experiment   PRON can check out this example on the sklearn site  
__label__nonlinear-equations __label__newton-method PRON just know how to do newton  raphson iteration in time  independent 1d nonlinear differential equation  then PRON apply to time  dependent 1d nonlinear differential equation  and PRON get confuse   below be just test time  dependent 1d nonlinear differential equation PRON want to solve  PRON just make this up from heat equation  to make PRON nonlinear      fracpartial upartial tfracpartial2 upartial x2u20     for this equation  PRON assume PRON would need initial condition  and boundary condition     ux  t0left   beginalign   1qquadamp1leq xleq 1  0qquadamptextotherwise   endalignedrightqquad  u5t0quad u5t0     PRON set the  x range of  5 leq x leq 5   and plan to create code in matlab   PRON start with space discretization by use second  order difference     fracpartial upartial tfracui12uiui1delta x2ui20     where  i denote space discretization number  then PRON apply backward  euler discretization for time discretization  which be     fracd qx  tdtft  x  qx  tqquadlongrightarrowqquad  fracqj1qjdelta tfj1      where  j denote time discretization number   this be just intermediate step for PRON  PRON be plan to apply trapezoidal rule  second order backward difference formula  tr  bdf2  later   now apply this  the equation look like     fracui  j1ui  jdelta tfracui1j12ui  j1ui1j1delta x2ui  j120     again   i and  j denote space and time discretization respectively   then PRON apply newton  raphsonnr  scheme  which be for give equation     fu0     the solution be determine by iteration     fukdelta ukfuk    qquad qquad uk1ukdelta uk       PRON be run out of space for discretization  where k denote nr iteration number  and if  fu be system of equation   fudelta u should be jacobian   do PRON have to set initial and boundary condition for  delta ui  j as well   now PRON equation look like     fracdelta ui  j1kdelta ui  jkdelta t     fracdelta ui1j1k2delta ui  j1kdelta ui1j1kdelta x2    2ui  j1kdelta ui  j1k     fracui  j1kui  jkdelta t   fracui1j1k2ui  j1kui1j1kdelta x2   ui  j1k2     this be where PRON get stick   how do PRON proceed after this   be PRON miss something  like another boundary  initial condition   because PRON only know  ui10 and  delta ui10  there be like  11 unknown due to backward difference   j1     some literature say that PRON should solve this equation for  each time step   or  be this right way to apply backward  euler scheme and nr scheme to the time  dependent nonlinear differential equation   be there any good example solve time dependent nonlinear differential equation with newton  raphson iteration   PRON seem to be confuse with what be unknown in PRON algebraic equation  as mention also in the comment by  origimbo    let say PRON have  n inner node for space discretization  ie  delta x  10n1 the value  ui1 shall be obtain from initial condition  a more usual notation be  ui0    the notion  PRON have to solve PRON algebraic equation in each time step  mean that in each time step the value  ui  j be know  either from initial condition or from previous time step  and PRON unknown  the value to be find  be only  ui  j1 now this be the task for nr method   when apply nr method  PRON have to choose the value  u0i  j1  typically PRON take  u0i  j1ui  j PRON mean  fu0  represent  n nonlinear algebraic equation have  n unknown  u1j1 up to  un  j1 have the form     fracdelta ui  j1kdelta t     fracdelta ui1j1k2delta ui  j1kdelta ui1j1kdelta x2    2ui  j1kdelta ui  j1k     fracui  j1kui  jdelta t   fracui1j1k2ui  j1kui1j1kdelta x2   ui  j1k2     in above  the value  ui  j   uj1k be know  the unkown in above be only three  but in each equation different one   namely  delta ui1j1k   delta ui  j1k  and  delta ui1j1k the boundary condition be  delta u0j1k0  and  delta un1j1k0  by the way  the sign in PRON pde before the diffusion term be wrong  PRON should be minu  
__label__reference-request PRON be a physics undergrad  look for a good introductory book on computational science  and numerical method  mostly PRON be look for applied book   simply because  in a theoretical book  if PRON can not see why PRON be study PRON  PRON be easy to lose motivation    background  PRON have the follow background   strong background in programming  cjava  javascript  basic  introductory mathematica   parallel computing  enough to compile  fiddle with cuda sample  and to use mutex  lock semi  competently   mid  undergraduate physics  so  mechanic  wave  and special relativity  at a decent level  as well as introductory lagrangian   introductory linear algebra  vector  tensor calculus  PRON  learn  this material  but PRON really have not apply PRON much   proofs  intro real analysis  halfway through baby rudin  for those familiar  so PRON be decent with rigor   goal  PRON be look for an applied book  because the simulation PRON write to figure out a problem be sometimes insufficient  several of these area   mechanic  PRON current method be to treat everything as a rigid sphere  with a spring force  and some damping on the plane of contact whenever a collision occur  PRON do not think PRON have advance from this in  maybe  three year  so  PRON would be very useful to learn a new method  plus  with PRON own method PRON sometimes run into bug  finite element analysis look like a very interesting topic but the only book PRON have find on PRON be very rigorous and not very apply  so PRON be hard to be introduce to the material   wave motion  PRON be pretty straightforward to discretize  nabla2phifracpartial2phipartial t 2 and just to use PRON  but whenever PRON run into some instability  large error in energy  or an unstable simulation  or anything else that PRON do not know how to solve   PRON only solution be to decrease the timestep  increase the resolution  PRON only find new thing every once in a while  such as a decent method for a nonreflecting boundary condition     electromagnetism  PRON know vector calculus  so  why not   fluid mechanic  PRON seem like part of this be not that complicated  but PRON be not familiar with PRON   other  special relativity seem to necessitate electromagnetism  so that be off limit for now  and PRON be not that knowledgeable in quantum mechanic  PRON be not opposed to pure math thing  attractor  etc  so long as there be interesting thing compute  and not just several result prove and unused   so  to clarify  PRON be look for an applied computational  possibly physics  base book  which start at the introductory level  introductory not be synonymous with easy   and ideally provide a broad overview of multiple method  with lot of application  notably in the field above   PRON have look quite a bit for book in these area  but usually PRON turn out to either be way over PRON head  or theoretical to the point where PRON do not know what the purpose of what PRON be prove  deriving be   PRON understand that this be a very soft question  and PRON be hesitant post PRON because of that  because PRON be new to this corner of stackexchange  and because PRON may be see as too localized  PRON do not think PRON be too localized  since PRON have find question like these very useful in get a grasp on a large field that PRON be new in  usually when PRON want to learn more about a topic PRON pick up a book  and to figure out what book to pick up PRON go to stackexchange  and usually the book recommendation on question other have ask be great  PRON do not find any question applicable to PRON level on this site  so PRON think PRON be appropriate to ask one  of course PRON will take any objection into consideration   one book that be recommend to PRON  not here  be  a first course in computational physics  by devries  the practice problem look especially good  interesting   look at the following  simulate the physical world  hierarchical modeling from quantum mechanics to fluid dynamics  computational physics  an introduction  for note base on the 2nd book by the author of the book PRON see here  for more advanced stuff see  computational physics  there be plenty of other book too   PRON suggest PRON check out these book   ab shiflet and gw shiflet  introduction to computational science  princeton university press  2006  rh landau  mj paez  and cc bordeianu  a survey of computational physics  princeton university press  2008  hope this help   another suggestion be   an introduction to scientific computing   twelve computational project solve with matlab  httpwwwljllmathupmcfrai2sc  the twelve project cover many of the area PRON mention  in PRON post  be each in a separate chapter  and be  more or less independent of each other   bill  PRON will give an update base on what PRON have do in the time since ask this question   a first course in computational physics  devries  this cover many fundamental topic  root finding  interpolation  integration  incl  monte carlo method   linear algebra method  ode  pde  and fourier analysis  in a conversational tone so that everything seem motivated  and be very interesting for start out in numerical method   computational physics  an introduction  vesely  PRON have find that the bulk of this book cover the same thing the devries book cover  but PRON do so a bit more formally and use more powerful and general  specialize   tool  there be too few practice problem  in PRON opinion  PRON also have a chapter on hydrodynamic   an introduction to scientific computing  danaila  the project in this be excellent and definitely complement the material in the other two book PRON mention  PRON discuss aspect of each problem more in depth and provide challenge   mean that the tool in devries and vesely do not just trivially solve each problem   try tveito  langtangen  nielsen  cai  element of scientific computing  this be for beginner and pedagogically excellent  
__label__machine-learning __label__feature-scaling PRON be struggle with a conceptual problem relate to feature scaling   let PRON assume PRON be build a classifier  eg  a nn  and let PRON assume PRON rely on future scaling for the input feature of PRON model   in this context PRON will normalise the training set use PRON mean and PRON std and PRON would do the same with the testing set use the testing mean and std   let PRON also assume PRON succeed in build PRON classifier and PRON move to production where PRON try to classify new input   however for such new input the mean and std be unknown  how can PRON scale PRON appropriately before process with PRON model  may be PRON could use the mean and std from trainingtest   PRON really do not know which be the correct practice here  any hint   thank PRON for PRON help   PRON should apply the normalization only on PRON training dataset  PRON test set should be keep completely separate and should be use only when PRON final model have be choose  if PRON use include the testing set in the normalization  PRON can be see as use the testing set in the training procedure  this be call datum snoop   PRON should pre  process training dataset and use the obtain mean and std  when process the testing set afterwards  note that the testing dataset transformation will likely be imperfect  PRON will not have zero mean or unity standard deviation  but this testing dataset can safely be use since PRON have not affect any step of the learning process  
__label__machine-learning PRON would like to use non  atomic datum  as a feature for a prediction   suppose PRON have a table with these feature    column 1  categorical  house   column 2  numerical  2322   column 3  a vector   12  22  32    column 4  a tree    2323  2323    2323  2323    boolean  categorical     column 5  a list  122  boolean   PRON would like to predict  classify  for instance  column 2   PRON be make something to automatically respond to question  any type of question  like  where be foo bear     PRON first make a query to a search engine  then PRON get some text datum as a result  then PRON do all the parse stuff  tag  stem  parse  splitting    PRON first approach be to make a table  each row with a line of text and a lot of feature  like  first word    tag of first word    chunk   etc   but with this approach PRON be miss the relationship between the sentence   PRON would like to know if there be an algorithm that look inside the tree structure  or vector  and make the relation and extract whatev be relevant for predict  classify  PRON would prefer to know about a library that do that than an algorithm that PRON have to implement   when PRON come to deal with many disparate kind of datum  especially when the relationship between PRON be unclear  PRON would strongly recommend a technique base on decision tree  the most popular one today to the good of PRON knowledge be random forest  and extremely randomized tree   both have implementation in sklearn  and PRON be pretty straightforward to use  at a very high level  the reason that a decision tree  base approach be advantageous for multiple disparate kind of datum be because decision tree be largely independent from the specific datum PRON be deal with  just so long as PRON be capable of understand PRON representation   PRON will still have to fit PRON datum into a feature vector of some kind  but base on PRON example that seem like a pretty straightforward task  and if PRON be willing to go a little deeper on PRON implementation PRON could certainly come up with a custom tree  splitting rule without actually have to change anything in the underlying algorithm  the original paper be a pretty decent place to start if PRON want to give that a shot   if PRON want pseudo  structural datum from PRON text datum though  PRON may suggest look into doc2vec  recently develop by google  PRON do not think there be any good open  source implementation now  but PRON be a pretty straightforward improvement on the word2vec algorithm  which have implementation in at least c and python   hope that help  let PRON know if PRON have get any more question  
__label__machine-learning __label__python __label__nlp __label__nltk __label__sequential-pattern-mining PRON be create a sequence labeling program use pycrfsuitebio taging  and nltk  the program should be able to process query with different context   PRON have train different model for each context and save PRON separately  one model to process flight booking query  one model to process query to send sm etc   PRON have an interface where user can enter query from any context  can anyone suggest PRON the good way to find and use respective model for that specific query other than iterate over each model  or be i completely wrong about use different model   
__label__image-recognition PRON have a simple gauge display analog value range from 0 to 4   here be an image of the gauge  unfortunately there be no way to get a analog or digital signal for the value   how do PRON read the value of the gauge   PRON idea be to make an image every 5 minute and get the value by analyze PRON  PRON be think of manually generate reference image with the black needle in different position and then compare PRON to the real image   since all the processing should be do on a raspberry pi without internet connectivity  a good approach would be a preconfigur docker image for image comparison which help do the image comparison locally  maybe support by a python or php script   how to proceed   this be a simple digital image processing task  PRON be propose a quite efficient solution without any error correction mechanism  so this be not a production quality approach  but as a first proof of concept PRON should do the trick   the easy approach would be splitting the image in two channel  one for the color red and one for the color black  PRON can neglect the rest outside a certain threshold  if the lighting be good  this should also deal with the background   step 1  crop away everything outside the gauge to simplify PRON life  the easy way to do this be just put the camera close enough that PRON only capture the gauge  that way PRON do not have to implement PRON in PRON algorithm   step 2  the center of PRON image will be the red dot in the middle of the gauge  PRON can easily identify PRON with a filter look for the large concentration of red in the red channel of PRON image  calculate the center and remember the coordinate   step 3  do the same for the large concentration of black in the black channel  this will be the thick end part of the black needle  calculate the center as well and remember the coordinate   step 4  calculate the vector between those two coordinate  the angle of the vector will tell PRON the angle of the needle  take care of the direction of the vector   once PRON have the angle of the needle  PRON can easily derive the number PRON be point to   there be obviously much more elegant approach to solve this problem  but this quick and dirty algorithm should do the trick without a lot of hassle  PRON need to make sure that the camera be position in the right angle  because PRON have not implement any error correction in PRON algorithm  PRON also need to make sure the lighting be sufficient and that there be not any other significant influence on the image quality  as long as the orientation be stable and the image quality high  this should enable PRON to read the gauge with good precision  
__label__optimization __label__algorithms problem statement   PRON be try to solve a problem statement use c  as programming language  in the problem system for an input  double  decimal  say  hi  the output generate be a form of dataset contain number of parameter   fi   pi and  ti   PRON somehow have to filter out only those entry in the data set which would satisfy the following condition    fi  gt  fmin  where  fmin be some constant    pi  gt  pmin  where  pmin be some constant  and   ti  lt  tmax  where  tmax be some constant   be there an efficient algorithm PRON could use in such case where PRON could zero in on an optimal set of value for  hi for which the output parameter value be well within the constraint  also PRON think use genetic algorithms in this case make sense but somehow PRON be not able to formulate and derive a fitness function for PRON requirement  any suggestion on how to approach to solve such problem be truly appreciate   kindly do not downvote this question as vague   actually what be PRON try to do be to solve a nonlinear optimization problem  as far as PRON understand  PRON have a dataset and accord to the particular input apply  then PRON get an answer   one way to approach this problem would be to consider a grid of point for hi  with respect to evaluate the parameter come from the dataset   then PRON can formulate PRON fitness function such that in case the result parameter do not respect the constraint  then the cost of PRON objective will go to 10  53 for instance   after PRON have evaluate all the grid point  PRON can choose the good solution and refine the grid to have a good result   consider that in this case PRON do not have any guarantee of the optimality of the obtain solution   alternatively PRON can investigate the usage of the simulated annealing approach to solve PRON problem  httpsenwikipediaorgwikisimulatedannealing  
__label__parallel-computing __label__mpi __label__trilinos PRON be use trilinos for a problem and PRON have an epetramultivector object with 1 vector of length mnnframe  PRON need to turn PRON into an epetramultivector object with nframe of length mn  the first mn entry need to go into the first vector  the second into the second vector and so on   PRON have this work fine with just one process  what PRON be do be extract all the datum to a standard c array on process 0  reshape that into a 2d standard c array  an array of pointer actually   and then create a new epetramultivector with PRON 2d array   with more than one process  however  because everything be on process 0  when PRON create the new epetramultivector  all the datum for the other process be nonsense  access undefined memory  PRON guess    so PRON question ultimately be  how do PRON reshape a trilinos epetramultivector correctly with more than one process   PRON have write a function that do PRON  PRON do not do PRON in place  and be not particularly well write  but for posterity PRON will add PRON solution  PRON realize that PRON really do not need to reshape the standard c array  but PRON will leave PRON there until PRON become a problem for PRON   int reshapemultivectorepetramultivector   ampreshapedmultivec  const int length  const int numvec  const epetramultivector  amporiginalmultivec  epetrampicomm  ampcomm      function to reshape a multivector  do PRON by copy the value  so additional memory will be require    note that this function do not delete the original vector    input    reshapedmultivec  this be a reference to a null pointer that will point to the    new  reshape  multivector     length   global length of the new multivector    numvecs   the number of vector in the new multivector     originalmultivec  the original multivector that be be reshape     comm   mpi communicator   output    int   error flag     int err   int numvecsorig  originalmultivec  numvector     int lengthorig  originalmultivec  globallength     extract phasegradx and phasegrady as standard array so that PRON can reshape PRON   double   originalmultivecstd  new doublenumvecsorig    forint i0iltnumvecsorigi    originalmultivecstdi   new doublelengthorig      err  multivectortoarrayoriginalmultivecstd  originalmultivec  false    iferr  return err    copy the value to all the processor after extract PRON   forint i0iltnumvecsorigi    comm  broadcastoriginalmultivecstdi   lengthorig  0      double   reshapedstd  new doublenumvec    forint i0iltnumvecsi    reshapedstdi   new doublelength      err  reshape2darrayoriginalmultivecstd  lengthorig  numvecsorig  reshapedstd  length  numvecs    iferr  return err   epetramap reshapedmaplength0comm    initialize the vector  int nummyelement  reshapedmap  nummyelement     int  myglobalelement  reshapedmap  myglobalelement     reshapedmultivec  new epetramultivectorreshapedmap  numvecs    double   ap  reshapedmultivecgtpointer     for  int j0  jltnumvec    j     double  v  apj     fill PRON  for  int i0  iltnummyelement    i     vi   reshapedstdjmyglobalelementsi          free memory  forint i0iltnumvecsorigi    delete   originalmultivecstdi      delete   originalmultivecstd   forint i0iltnumvecsi    delete   reshapedstdi      delete   reshapedstd   returnexitsuccess      reshape array be something that be a bit matlab  specific  most other environment  include trilinos  petsc    allow PRON to create a vector or matrix but not let PRON re  interpret PRON as something else  PRON need to copy the element by hand into a matrix of different size  
__label__mpi __label__conjugate-gradient what be the difference between these two method  can a problem be solve by one method will be able to solve by the other  can both  or one of PRON be parallelize with openmp andor mpi   the conjugate gradient method only work to solve the system   a x  b   if  a be symmetric and positive  definite  also work for negative definite    the reason PRON must be symmetric be that conjugate gradient work by minimize  or maximize  the function   fx   frac12  xt a x  bt x   note that the derivative be   fx   frac12  at x  frac12  a x  b  and if  a be symmetric  at  a so the above reduce to   fx   a x  b   at the minimum   fx   0  and  x be the solution to PRON system   this last step should make PRON obvious why  a must be symmetric   the positive  negative definite property be more subtle  but PRON be require so that the extrema exist   the biconjugate gradient method will work for any system   PRON do so by solve both   a x  b  along with   at x  b  simultaneously   the detail of which PRON be not familiar with  so PRON will not pretend to know   PRON be sufficient to know that biconjugate gradient be the more general of the two   PRON do have stability issue  and if  a be symmetric  then conjugate gradient will perform less work to get to the solution   as far a parallelism  if PRON system be large  PRON can gain a lot of parallelism in the linear algebra involve in the solver iteration  so there should be no reason a parallel linear algebra library would not lead to performance gain   the conjugate gradient method be the provably fast iterative solver  but only for symmetric  positive  definite system  what would be awfully convenient be if there be an iterative method with similar property for indefinite or non  symmetric matrix   the cg method seek approximate solution at each step  k within the krylov subspace   kka  b   b  ab  a2bldot  akb  the essential idea of the biconjugate gradient method be to maintain a second krylov subspace   kka  b   b  ab   a2bldotsakb  and seek a recurrence with similar orthogonality property to that of cg  but without the stability issue of solve  aax  ab  unfortunately  that fail if PRON apply PRON naively  however  by perform one step of the generalize minimum residual  gmres  algorithm after each bicg step  the result iteration be stable  this be usually refer to as bicg  stab   so  bicg  stab be  in principle  a more general solver than cg but suffer bad efficiency when apply to the problem for which cg be intend  bicg or bicg  stab require more matrix  vector multiplication and more dot product  so if PRON parallelize PRON via distribute  memory multiprocessing PRON will incur more communication overhead  but nonetheless PRON can be scale up as much as PRON like   there be two thing worth note here which be more important than all that other junk PRON just say   for every iterative method  bicg  gmres  qmr    there be a matrix that will make PRON fail to converge in finite  precision arithmetic   therefore  come up with a good preconditioner for PRON specific matrix be probably more important than use the optimal outer  level iterative solver   edit  for open  source library  the two most popular be petsc and trilinos  PRON highly recommend PRON also get the python binding  respectively petsc4py and pytrilinos  PRON can also try eigen  on the one hand  PRON do not have many feature  but on the other hand  PRON have just what PRON need and no more  if PRON intend to read the code rather than just use PRON  eigen may be the easy        see also  yousef saad  iterative methods for sparse linear systems  nachtigal et al  how fast be nonsymmetrix matrix iterations  
__label__floating-point PRON know that PRON should use a tolerance for compare float point number   but for compare vector  PRON can think of 3 possible solution correspond to different distance metric   compare the component of each vector individually  the vector be equal if all 3 be within tolerance   this option would behave like the uniform norm  give a cube of tolerance   compare the sum of all the absolute difference to some tolerance   this would behave like the taxicab norm  give a simplex of tolerance   calculate the euclidean length of  veca  vecb  and see if PRON be within tolerance   this would give the standard euclidean norm with a sphere of tolerance   but PRON main concern be numerical stability   euclidean norm  feel like  the good option  but PRON be worried that all the calculation would induce more rounding error   to a less extent option 2 could also introduce error    for example  if the x component of the vector be much large than y and z  add together all the difference could swamp any contribution from y and z   so PRON be currently lean towards option 1   can anyone weigh in with an authoritative take on this problem   the answer to this question depend largely on PRON application   instead of sweat the exact numerical implementation think more about what each potential embodiment PRON suggest mean   for example  do one or more of the compute distance have a physical interpretation   be the unit and scale of the vector component the same   in the scenario where all parameter have the same unit  and different scale of value  PRON should think more about the good way to dimension PRON system   nondimensionalization help to chase away the evil numerical precision gnome   as PRON use x  y  and z as value name  PRON suspect PRON be look at some sort of position in space   the two norm have the distinct advantage of have a continuous derivative  so lack any real problem information PRON would probably start there  
__label__finite-element __label__pde __label__boundary-conditions consider a linear fem model of a 2d domain  with some time  dependent pde valid inside PRON  say  the linear wave equation  there be a dirichlet boundary condition prescribe on some boundary of the domain  and some zero initial boundary condition throughout the domain   PRON want to prescribe the dirichlet condition as a step function in time  now  PRON know that in this case  when the dirichlet condition be not constant in time this be not trivial  and some special term need to be add to the weak form of the problem   but if PRON be correct  boundary condition be apply from  t0 and initial condition be at  t0   PRON want to prescribe the step function so that the bc be constant from  t0  PRON be worried though that the bc will remain undefined at  0    since the initial datum be not prescribe at the closed node   be there a correct way to apply the step function in the way PRON describe  or be PRON just define a domain with a constant bc   
__label__data-mining __label__svm __label__state-of-the-art this question be in response to a comment PRON see on another question   the comment be regard the machine learning course syllabu on coursera  and along the line of  svm be not use so much nowadays    PRON have only just finish the relevant lecture PRON  and PRON understanding of svm be that PRON be a robust and efficient learn algorithm for classification  and that when use a kernel  PRON have a  niche  cover number of feature perhaps 10 to 1000 and number of training sample perhaps 100 to 10000  the limit on training sample be because the core algorithm revolve around optimise result generate from a square matrix with dimension base on number of training sample  not number of original feature   so do the comment PRON see refer some real change since the course be make  and if so  what be that change  a new algorithm that cover svm s  sweet spot  just as well  good cpu mean svm s computational advantage be not worth as much  or be PRON perhaps opinion or personal experience of the commenter   PRON try a search for eg  be support vector machine out of fashion  and find nothing to imply PRON be be drop in favour of anything else   and wikipedia have this  httpenwikipediaorgwikisupportvectormachineissue    the main sticking point appear to be difficulty of interpret the model  which make svm fine for a black  box predict engine  but not so good for generate insight  PRON do not see that as a major issue  just another minor thing to take into account when pick the right tool for the job  along with nature of the training datum and learn task etc    svm be a powerful classifier  PRON have some nice advantage  which PRON guess be responsible for PRON popularity   these be   efficiency  only the support vector play a role in determine the classification boundary  all other point from the training set ne not be store in memory   the so  call power of kernel  with appropriate kernel PRON can transform feature space into a high dimension so that PRON become linearly separable  the notion of kernel work with arbitrary object on which PRON can define some notion of similarity with the help of inner product  and hence svm can classify arbitrary object such as tree  graph etc   there be some significant disadvantage as well   parameter sensitivity  the performance be highly sensitive to the choice of the regularization parameter c  which allow some variance in the model   extra parameter for the gaussian kernel  the radius of the gaussian kernel can have a significant impact on classifi accuracy  typically a grid search have to be conduct to find optimal parameter  libsvm have a support for grid search   svm generally belong to the class of  sparse kernel machines   the sparse vector in the case of svm be the support vector which be choose from the maximum margin criterion  other sparse vector machine such as the relevance vector machine  rvm  perform good than svm  the following figure show a comparative performance of the two  in the figure  the x  axis show one dimensional datum from two class y01 the mixture model be define as pxy0unif01  and pxy1unif515   unif denote uniform distribution   1000 point be sample from this mixture and an svm and an rvm be use to estimate the posterior  the problem of svm be that the predict value be far off from the true log odd   a very effective classifier  which be very popular nowadays  be the random forest  the main advantage be   only one parameter to tune  ie the number of tree in the forest   not utterly parameter sensitive  can easily be extend to multiple class  be base on probabilistic principle  maximize mutual information gain with the help of decision tree   in answer this question one significant distinction to make be whether PRON be talk about linear support vector machines or non  linear  that is  kerneliz support vector machines   linear svm  linear svm be both in theory and practice very good model when PRON datum can be explain by linear relation of PRON feature  PRON be superior over classic method such as linear  aka  least  square  regression because PRON be robust  in the sense that small perturbation in the input datum do not produce significant change in the model  this be attain by try to find the line  hyperplane  that maximize the margin between PRON data point  this maximum margin hyperplane have be show to give guarantee on the generalization ability of the model over unseen datum point  a theoretical property other machine learn method lack of   linear svm be also interpretable as any other linear model  since each input feature have a weight that directly influence the model output   also linear svm be very fast to train  show sublineal training time for very large dataset  this be achieve by make use of stochastic gradient descent technique  much in the fashion of current deep learning method   non  linear svm  non  linear svm be still linear model  and boast the same theoretical benefit  but PRON employ the so call kernel trick to build this linear model over an enlarged space  the visible result be that the resultant model can make non  linear decision on PRON datum  since PRON can provide a custom kernel encoding similarity between data point  PRON can make use of problem knowledge to make such kernel focus in the relevant part of PRON problem  do this effectively  however  can be difficult  so in general almost everybody use the plug  and  play gaussian kernel   non  linear svm be partially interpretable  as PRON tell PRON which training datum be relevant for prediction  and which be not  this be not possible for other method such as random forests or deep networks   unfortunately non  linear svm be slow  the state  of  the  art algorithm be sequential minimal optimization  which show quadratic performance  and be widely implement through the libsvm library in a number of machine learning library  scikit  learn include   popularity of these method  PRON be true that svm be not so popular as PRON use to be  this can be check by google for research paper or implementation for svm vs random forests or deep learning method  still  PRON be useful in some practical setting  specially in the linear case   also  bear in mind that due to the no  free lunch theorem no machine learning method can be show to be superior to any other over all problem  while some method do work better in general  PRON will always find dataset where a not so common method will achieve good result  
__label__optimization __label__python __label__convergence __label__scipy the model PRON be work on be a multinomial logit choice model  PRON be a very specific dataset so other exist mnlogit library do not fit with PRON datum   so basically  PRON be a very complex function which take 11 parameter and return a loglikelihood value  then PRON need to find the optimal parameter value that can minimize the loglikelihood use scipyoptimizeminimize   here be the problem that PRON encounter with different method    nelder  mead’  PRON work well  and always give PRON the correct answer  however  PRON be extremely slow  for another function with a more complicated setup  PRON take 15 hour to get to the optimal point  at the same time  the same function take only 1 hour on matlab use fminunc  which use bfgs by default   ‘ bfgs’  this be the method use by matlab  PRON work well for any simply function  however  for the function that PRON have  PRON always fail to converge and return  desire error not necessarily achieve due to precision loss’  PRON have spend lot of time play around with the option but still fail to work    powell   PRON quickly converge successfully but return a wrong answer  the code be print below  x0 be the correct answer  nelder  mead work for whatev initial value   and PRON can get the datum here  httpswwwdropboxcomsaap2dhor5jyxy94datacsv  thank   import panda as pd  import numpy as np  from scipyoptimize import minimize   httpswwwdropboxcomsaap2dhor5jyxy94datacsv  df  pdreadcsvdatacsv   indexcol0   dfhh  dfhh  b  dfixb0b4value  nt5  p  dfixp1p4value  nt4  f  dfixf1f4value  nt4  sdv  dfixlagb1lagb4value  def lix    b1  x0   coeff on price  b2  x1   coeff on feature  a  x27   take first 4 value as alpha  e  npexpa  b1p  b2f    1  4    nt4    nt4  build matrix  ntj  for each exp    e  npinserte  0  1  axis1    nt5   denom  esum1   return nplogb  esum1   denomsum    x0  nparray3231028223  023965953  084739154  025418215338757007038036966    nprandomseed0   x0  x0  nprandomrand6   minl  minimizeli  x0  methodnelder  meadoptionsxtol   1e8   disp   true     minl  minimizeli  x0  methodbfgs     minl  minimizeli  x0  methodpowell   optionsxtol   1e12   ftol   1e12    print minl  update  030714 simpler version of the code  now powell work well with very small tolerance  however the speed of powell be slow than nelder  mead in this case  bfgs still fail to work   
__label__r PRON have 2 vector  1000 time 1   let call PRON  y1 and  y2 each vector represent a normal distribution with certain mean and variance   PRON plot the contour plot use the follow r code   x  lt y1  y  lt y2  s  lt subplot   plotlyx  x  type   histogram   showlegend  false    plotlyempty     plotlyx  x  y  y  type   histogram2dcontour   showlegend  false    plotlyy  y  type   histogram   showlegend  false    nrow  2  height  c02  08   width  c08  02    sharex  true  sharey  true  titlex  false  titley  false    layouts   and i get the follow plot  what PRON would like to do now  be to plot a 3d contour plot  so that PRON can actually see the  mountain  that be create after plot the histogram of  y1 against the histogram of  y2   so in the z  axis PRON would like to have the frequency of the value  any suggestion   this plot not use frequency but kernel density   freqz  lt withdataframex  y   masskde2dx  y  n  50    withfreqz  plotlyx  x  y  y  z  z  type   surface    
__label__python __label__scikit-learn __label__pandas __label__pca PRON post PRON question on stack overflow  but there someone suggest that PRON should try PRON here  what PRON be do now   ok  first to PRON datum  PRON have a word  bi  gram frequency matrix  1100 x 100658  dtype  int   where the first 5 column contain information about the document  so every row be a document and every column a word  bi  gram like  of  the  on  the  and  that     PRON want to visualize the datum  but before PRON do that  PRON want to reduce the dimension  so PRON think PRON do that with pca from sklearn  first PRON set the column label with  mypandadataframecolumn  word  bi  gram  then PRON delete some doc  column  because PRON want to see what kind of information PRON can get if PRON only look at the proficiency   del existingdfsubset    del existingdfprompt    del existingdfl1    del existingdfessayid    then PRON set the proficiency column to be the index with  mypandadataframecolumnssetindexproficiency    inplace  true  drop  true   and then PRON do this  from sklearndecomposition import pca  x  500  pcax  pcancomponent  x   pcaxfitmypandadataframe   pcacopy  true  ncomponent  x  whiten  false   existing2dx  pcaxtransformmypandadataframe   existingdf2dx  panda  dataframeexisting2dx   existingdf2dxindex  mypandadataframeindex  existingdf2dxcolumn    pc0formati  for i in rangex    but with this implementation PRON can only set 1100 ncomponent as a maximum  this be the number of document  row   this make PRON suspicious  PRON try a couple of example  tutorial  but PRON can not get PRON right  so PRON hope someone can help PRON find out what PRON be do wrong  if would also be very happy about a good example  tutorial that can help PRON with PRON problem  thank PRON   with good regard   give m row of n column  PRON think PRON be natural to think of the datum as n  dimensional  however the inherent dimension d of the datum may be low  d  lt n d be the rank of the m x n matrix PRON could form from the datum  the dimensionality of the datum can be reduce to d with no loss of information  even  the same actually go for row  which be less intuitive but true  d  lt m so  PRON always make sense to reduce dimensionality to something  lt d since there be no loss  PRON typically reduce much further  this be why PRON will not let PRON reduce to more than the number of row  
__label__machine-learning __label__decision-theory PRON be create a decision tree and at the very root level PRON  PRON be get negative information gain   as per PRON knowledge  information gain be always  gt  0   any explanation  please   please look at the node below    6942            5633    1111   ig  h69   42    h56331111       69111  lg69111  42111  lg42111     89111   5689lg5689  3389lg3389     22111   1122lg1122  1122lg1122      0004  PRON turn out that for every feature  the ig  lt  0   what should PRON do to decide the feature at the root node   thank  
__label__parallel-computing __label__performance __label__gpu __label__image-processing PRON be try to do the speed up analysis of the rotate mask filter  section 423    let  n2  be the pixel in the image and let  m2  be the neighborhood of a give pixel  what PRON have for PRON sequential code be basically the follow step  for every pixel in the image  for every rotate mask  compute dispersion  end  compute average brightness with the mask of low dispersion  update pixel value  end  there be one formula to compute the idea speedup of an algorithm  displaystyle sp  fractstp where  sp be the speed up give  ts time to run the sequential algorithm and  tp time to run the parallel algorithm   the first thing PRON do not understand be that if  ts and  tp be real measure time value or if PRON can take the complexity base on the input size   assume the latter be possible  the complexity of this algorithm would be  onm where  n be the total number of pixel and  m be the total number of pixel in the neighborhood  have this  ts  nm and  tp  fracnmp where  p be the number of processor  PRON get the ideal speedup  sp to be linear  be this true   PRON feel like PRON be miss something  could PRON enlighten PRON to understand how to compute the speedup   PRON do not know much about the rotate mask filter  so PRON be answer in general   for the speed up  PRON can do both  if  ts and  tp be real time measure  PRON will obtain the actual  real speed up  if  ts and  tp represent theoretical time measure  then PRON will obtain the theoretical speed up  of course  PRON would like that both be the same but this depend on what PRON take into account for the theoretical time measure   yes  PRON be normal that PRON obtain a linear speed up  indeed   tp  fracnmp say that PRON can perfectly split PRON work among PRON work unit  without any extra cost  in this case  the speed up be  optimal   ie linear   so  in theory  PRON should obtain a linear speed up  if PRON measure PRON but experimentally PRON be not linear  this mean that  tp neq fracnmp and that PRON be miss something in PRON theoretical model  this can be a small amount of redundancy in the computation  for some reason  some computation be do twice   a suboptimal load balancing  not every work unit receive the same amount of work   communication  some datum must be communicate between work unit which take time   synchronization  at some point  some work unit must wait for other unit before go on     let PRON make a small and very simple example   PRON have a list of  n number  xi and PRON want to compute for each  i the number  yi  xi12  xi2  xi12 PRON could say  PRON have  n number  yi to compute  each of the  p work unit compute  n  p number  so the speed up be  p  PRON may be not that easy    PRON suppose that PRON do not share the memory between the work unit and for the extremity  PRON just restart on the other side of the list    example  PRON have  20  number   with one work unit  PRON can square all the  xi and then all the addition      20  square operation and  40  addition operation   first attempt  with two work unit  PRON split the work from  1  to  10  and  11  to  20 here be the problem  to compute  y10 on the first work unit  PRON need  x11 the same for the second work unit  which require  x10 the same for  y1  which require  x20 and  y20  which require  x1    each work unit make  12  square operation and  20  addition operation   the thing be that now there be redundant computation  PRON have  24  square operation instead of  20     if PRON generalize this to  n number and  p process  PRON obtain that each process need  n  p2  square operation and  2n  p addition  so   tp will be something like  3n  p  2  and so the speed up will not be linear any more  even if PRON will look like linear if  ngtgtp    second attempt  to avoid the redundant computation  PRON first compute the square of all the number in parallel and then compute the sum in parallel   with two work unit  the first work unit compute the square from  x1  to  x10  with the second the square from  x11 to  x20 then  PRON compute the  y1  to  y10 with the first work unit and  y11 to  y20 with the second work unit  now  PRON have exactly the same number of operation as in serial  miracle  nop   PRON will have the square of  x11 compute on the second unit  but PRON be require in the first work unit for the computation of  y10 so PRON have to communicate the result between the work unit  which cost time   again  the speed up be not optimal  look like the one of the first attempt    now  there could be many other thing that could go wrong   how do PRON get the list of number  if read from a file with one single work unit  this be a big bottleneck   what if instead of the square operation  PRON have a more complicated function that be sometimes easy to compute  sometimes hard  PRON will be very difficult to balance the work between the work unit     the typical problem be that there be some portion of PRON work that do not actually parallelize  this be usually capture in the formula for amdahl s law  that being say  there must be at least one step in the above pseudocode that actually happen serially  if this be a share  memory program  PRON suspect that there be serialization in  update  pixel   
__label__eigenvalues PRON would like to use the  eigtool of professor trefethen for pseudospectra but PRON have a generalize eigenvalue problem to solve     lambda m x  k x   PRON seem that eigtool take only one matrix as input  be PRON possible to use the eigtool PRON case too  ie have two matrix as input   in addition  PRON matrix be really big  but sparse  so PRON do not think that PRON be possible to invert PRON   in PRON case  m be singular and not invertible   maybe  PRON set  lambda x  m1kx   then if PRON need to implement the matrix  vector product in eigtool    y  m1k v   v be any give vector   then PRON can do PRON by two step    1  compute the matrix  vector product   u  kv   2  solve linear system  PRON  u  PRON think that PRON can modified some line in eigtool   professor mark embree kindly answer to PRON email  PRON report what PRON suggest to do    for large scale problem  this be what PRON generally recommend  note that this do not make special provision for the energy norm    use an iterative method  like  eig  in matlab  to compute a d  dimensional dominant invariant subspace for invkm  suppose the column of v span this subspace  generally  the column of v will be eigenvector associate with the large eigenvalue of invkm  which hopefully correspond to the rightmost eigenvalue of the pencil  k  m     let the column of u form an orthonormal basis for v  eg  in matlab  u  orthv    define ginv  invuinvkmu    this will be a d  by  d matrix   compute the pseudospectra of ginv   these pseudospectra will be contain within the pseudospectra of the full  size problem that would come if PRON take v to span the invariant subspace associate with all the finite eigenvalue of the pencil   experiment with enough value of the dimension  d  to make sure that the pseudospectra have converge in the rightmost part of the spectrum      generally PRON have k and m store as sparse matrix  then use km to compute invkm   in PRON case PRON seem to work  PRON hope PRON will be useful for someone else too  
__label__machine-learning let PRON suppose that PRON have a legacy system in which PRON do not have the source code and this system be on a mainframe write in cobol  be there any way use machine learn in which PRON can learn from the input and output the way the executable work  do this analysis could lead to develop some rest  soap webservice that can substitute the legacy system in PRON opinion   let PRON assume from the outset that the space of input be too large to allow exhaustive tabulation   the essential issue when apply ml be that the program be model be likely in general to   be discrete  ie operate  at least in part  on integer  boolean or categorical variable   contain various conditional  looping construct  if  while  for etc    have side  effect that affect other part of the program  eg non  local variable  or world state  eg write to a file    these pose obstacle for ml method such as ann  the ml approach which be most immediately compatible with these issue be genetic programming  gp    a recent specialisation of gp that be specifically concerned with the transformation of exist software system be genetic improvement  gi    however neither gp  gi  nor any other current ml technique  be a  silver bullet  here   despite decade of research  gp still work best at synthesize relatively small function  certainly not entire legacy program   because PRON be only possible to train on a very small subset of a program s input space  there be little guarantee that the program will generalize to input PRON have not be train on   how will the success of side effect be formally determine for training purpose   some of these issue could be address to some degree if the program have a comprehensive test suite  but replace an entire nontrivial program be not likely anytime soon  replace small part of the program that have good unit test be more realistic goal   here be a case study show how gi be successfully use to fix error in the implementation of apache hadoop  by operate only on the program binary   the high value of the question  how to replicate legacy system with machine learn   excellent question   the optential of replace legacy system use learning be obvously interesting to many organization for many reason   some legacy system be develop in language for which resource be dwindle   the configuration management and source control be sometimes lack such that the version of the system s component currently in production can not easily be determine   the last knowledgable maintainer of the code sometimes leave the organization and the new hire find the code to be a rat s nest and PRON design poorly document   understand the scope of such a project  let PRON assume an adequate budget for development and provisioning   this be no small task  but PRON be within the scope of current technology and  once do  PRON can be do again and again with increase return on investment   a cursory evaluation may reveal a budgetary need of about 40 million usd  but the return could be in the hundred of million if the intellectual property could be protect adequately   this rampd cost will decrease over time    let PRON also assume asynchronous read only access to at least these legacy system data set during the learning and validation project phase   production database transaction detail  production database content  production file system use by the legacy system and the file use  incoming and outgoing user interface daadditional considerations  produce a soap or restful wrapper around the result of learning can be accomplish use industry standard it methodology and process   PRON be important to note that the method for alter system behavior to handle change request and managing configuration must be in place prior to deployment of the new system   ta stream with time stamp   incoming and outgoing system interface datum stream with timestamp  these contain the training datum   note that the input and output of the legacy system and the input and output of the learning process be two distinct pair of set   machine learning approach  there be a few reasonable approach   here be just a few of PRON   create a smart system that learn source code in an appropriately choose language that represent the algorithm within each legacy system component and PRON interconnection   create a smart system that characterize the processing behavior of each legacy system component and PRON interconnection as a set of module contain rule  for a rule base system such as drools   create a smart system that characterize the processing behavior of each legacy system component and PRON interconnection as a set of module contain declaration  for a declarative system such as apache s hadoop or lexis  nexis s ecl   create a smart system that learn an circuit equivalent to each legacy system component and PRON interconnection use deep learning and neural net  testing and validation  once the learning be do  the replica must be test   the resource for a test system must be provision with sufficient resource to run in parallel with production and another system must be provision to validate the replica use pattern base comparision  pattern because the time stamp will rarely be identical and some deviation may be allowable    any discrepancy must be fix by hand  by incremental learning  if the learning component be reentrant   or by adjust the learning and rerun the learning phase   additional considerations  produce a soap or restful wrapper around the result of learning can be accomplish use industry standard it methodology and process   PRON be important to note that the method for alter system behavior to handle change request and managing configuration must be in place prior to deployment of the new system  
__label__optimization __label__python __label__linear-programming PRON be try to implement dinur  nissim algorithm and be stick at how to solve the set of linear inequality with multiple unknown and a large number of equation along with constraint   example   beginalign   02 leq c4 leq 066  066 leq c3 leq 156  096 leq c3  c4 leq 226  endalign   constraint     0 leq ci leq 1  and many other equation with the number of unknown go up to  cn where  n be the size of the database  so PRON need a solution which work for a large number of equation   PRON have try to look for some library but most of PRON solve maximization or minimization problem so be not sure if PRON possible to convert these equation to one of those problem   if PRON have a set of linear inequality and linear equality  PRON can use linear programming software to find a solution  or determine there be no solution   PRON can make up any  linear  objective function to be minimize or maximize  for instance  the function which equal 0  
__label__ode __label__biology __label__epidemiology PRON work fairly heavily in mathematical biology  epidemiology  where most of the modeling  computational science work be still dominate by set of ode  admittedly sometimes fairly elaborate set of PRON  one of the pluss of these model be that PRON be rather easy to describe and replicate  a table of parameter value  and the equation PRON and PRON have give someone everything PRON need to replicate PRON research in whatev way PRON feel like implement PRON   but somewhat more complex model have start to become more popular  agent  base model  in particular  seem to be both hard to describe in a publication  and hard to replicate  because PRON be not necessarily perfectly describe by a set of ode  be there any guideline  or just practical experience  behind describe these model in a way that reader understand what happen  and make PRON relatively straightforward to replicate   PRON do not work in that business but naively PRON think there be three part to a complete description  a description of the datum landscape PRON live in  describe this in term of the data structure  graph  direct or undirected  weighted or unweighted   tree  array    and the datum associate with each node  make note of special case handle such as periodic boundary condition or assume state for neighbor outside the test region  presumably this have a fairly clear connection with PRON problem domain   a description of the internal state of the agent and how PRON make decision  again  hopefully this have a reasonably clear interpretation   a description of the relative timing andor synchronization of action and update between the agent and the landscape  and between pair or group of agent   pseudo  code  or even real code if PRON be not too polluted with implementation detail  will help   the good way by far be to include all of PRON code as supplementary material  if possible  also include file with the relevant random seed need to recreate PRON result  not only do this allow people to recreate PRON result  which PRON may not care about   PRON also allow PRON to more easily continue where PRON leave off  this allow for new collaboration and citation to PRON work  unfortunately  this come with the difficulty of force PRON to clean up PRON code  and make sure PRON bug free  hence  PRON be more an ideal than what be usual in practice  but at the very least  PRON should archive a version of PRON code use to produce PRON result  that way if another researcher ask for code  PRON can produce PRON   in term of the description in PRON paper  then PRON would concentrate on a high  level  implementation independent description of the key novel feature of the model  this be the practical part most good paper achieve   concentrate on the feature that will change the result qualitatively if PRON be tweak  most model PRON work with produce quantitative result  but the specific quantity be usually not of interest  only the qualitative behavior  since the parameter be usually far from one observable in nature   thus  PRON focus on describe the part of the model  that if change will change the qualitative behavior of the system  if this mindset force PRON to describe every last detail of PRON model down to the implementation  then PRON know that PRON model be not very robust  and thus should be scrap   a good way to test if PRON in  paper description be sufficient  be to ask a friend  or student  who do not work on this project with PRON to describe how PRON may implement PRON model be pseudo  code  if PRON do not get stuck while try this  as in PRON arrive at a sketch of a model which should produce the same qualitative result   then PRON know PRON have do a good job of description   there be something call the odd  overview  design  and details  protocol  propose by volker grimm and other for describe an agent base model  PRON consist of a list of element that be need for understand the functioning of an abm and aim at make description of such model more standardised   the checklist of what have to be describe consist of   overview  purpose  entities  state variable  and scale  process overview and schedule  design  basic principle  emergence  adaptation  objectives  learning  prediction  sensing  interaction  stochasticity  collectives  observation  details  initialisation  input datum  submodels  more detail can be find in  grimm  v  berger  u  deangelis  d l  polhill  j g  giske  j   amp  railsback  s r  2010   the odd protocol  a review and first update  ecological modelling  221  2760–2768  
__label__matlab __label__finite-difference __label__data-analysis PRON have experimental datum contain horizontal and vertical component of speed and PRON need to evaluate this      fracpartial2partial xi partial xjleftvivjright      denote  u as horizontal component and  v as vertical  PRON should obtain      fracpartial2partial xi partial xjleftvivjright  rightarrow fracpartial2partial x2u2fracpartial2partial y2v2   2fracpartial2partial x partial yuv      PRON have use central finite difference of the second order for  partial2partial x2  and finite difference of the first order for mixed derivative  in matlab    h and ww be height and width respectively  for ii3h2  for jj3ww2   d2dx2  aii  jj112uii  jj22  43uii  jj12  52uii  jj2  43uii  jj12  112uii  jj22h2    d2dy2  bii  jj112vii2jj2  43vii1jj2  52vii  jj2  43vii1jj2  112vii2jj2h2    mixed derivative  cii  jjvii1jj1uii1jj1vii1jj1uii1jj1vii1jj1uii1jj1vii1jj1uii1jj14h2     sum of derivative  lghtii  jjaii  jjbii  jj2cii  jj     end  end  result be not very satisfactory so far   wild unconnectivity  all around the screen   do PRON have a mistake somewhere   would PRON use different kind of scheme   what kind of data treatment  spline   would PRON recommend for obtain smooth result   example of input datum   output   PRON reformulation be wrong  the product rule for derivative be   uv    uv  uv apply this in PRON situation  with  partialx instead of  fracpartialpartial x and  partialxy   fracpartial2partial xpartial y for brevity would yield    partialxyuv   partialypartialx u v  upartialx v    partialxyu v  partialx u   partialy v  partialy upartialx v  upartialxyv      note that PRON have four different term  not three    if PRON want to have a second  order accurate scheme  mix scheme of different accuracy be in general a waste of effort   the standard stencil be    partialx uij  approx fracui1jui1j2hqquadpartialy uij  approx fracui  j1ui  j12h  and    partialxy  uij  approx fracui1j1ui1j1ui1j1ui1j14h2  PRON can either apply the second directly to   uvij  or plug in both stencil on the right  hand side   or better yet  both  and compare the result   
__label__deep-learning __label__tensorflow __label__word-embeddings __label__sampling __label__loss-function PRON read about nce  a form of candidate sample  from these two source   tensorflow writeup  original paper  can someone help PRON with the follow   a simple explanation of how nce work  PRON find the above difficult to parse and get an understanding of  so something intuitive that lead to the math present there would be great   after point 1 above  a naturally intuitive description of how this be different from negative sampling  PRON can see that there be a slight change in the formula but could not understand the math  PRON do have an intuitive understanding of negative sampling in the context of word2vec  PRON randomly choose some sample from the vocabulary v and update only those because v be large and this offer a speedup  please correct if wrong   when to use which one and how be that decide  PRON would be great if PRON could include examplespossibly easy to understand application   be nce good than negative sampling  good in what manner   thank PRON   take from this post  httpsstatsstackexchangecom  a245452154812  the issue  there be some issue with learn the word vector use an  standard  neural network  in this way  the word vector be learn while the network learn to predict the next word give a window of word  the input of the network    predict the next word be like predict the class  that is  such a network be just a  standard  multinomial  multi  class  classifier  and this network must have as many output neuron as class there be  when class be actual word  the number of neuron be  well  huge   a  standard  neural network be usually train with a cross  entropy cost function which require the value of the output neuron to represent probability  which mean that the output  score  compute by the network for each class have to be normalize  convert into actual probability for each class  this normalization step be achieve by mean of the softmax function  softmax be very costly when apply to a huge output layer   the  a  solution  in order to deal with this issue  that is  the expensive computation of the softmax  word2vec use a technique call noise  contrastive estimation  this technique be introduce by  a   reformulate by  b   then use in  c    d    e  to learn word embedding from unlabelled natural language text   the basic idea be to convert a multinomial classification problem  as PRON be the problem of predict the next word  to a binary classification problem  that is  instead of use softmax to estimate a true probability distribution of the output word  a binary logistic regression  binary classification  be use instead   for each training sample  the enhance  optimize  classifier be feed a true pair  a center word and another word that appear in PRON context  and a number of kk randomly corrupt pair  consist of the center word and a randomly choose word from the vocabulary   by learn to distinguish the true pair from corrupt one  the classifier will ultimately learn the word vector   this be important  instead of predict the next word  the  standard  training technique   the optimize classifier simply predict whether a pair of word be good or bad   word2vec slightly customize the process and call PRON negative sampling  in word2vec  the word for the negative sample  use for the corrupt pair  be draw from a specially design distribution  which favour less frequent word to be draw more often   reference   a   2005   contrastive estimation  training log  linear model on unlabeled datum   b   2010   noise  contrastive estimation  a new estimation principle for unnormalized statistical model   c   2008   a unified architecture for natural language processing  deep neural network with multitask learn   d   2012   a fast and simple algorithm for train neural probabilistic language model    e   2013   learn word embedding efficiently with noise  contrastive estimation   basically this be select a sample from the true distribution which consist of the true class and some other nosy class label  then take the softmax over PRON   this be base on sample word from true distribution and noise distribution   here the basic idea be to train logistic regression classifier which can separate the sample obtain from true distribution and sample obtain from noise distribution  remember when PRON be talk about the sample obtain from the true distribution PRON be talk about only one sample which be the true class obtain from the model distribution   here PRON have explain about nce loss and how PRON differ from the nce loss   noise contrastive estimation  solution for expensive softmax  
__label__machine-learning __label__python __label__scikit-learn PRON be confused about how i choose the number of fold  in k fold  when i apply cross validation to check the model  be PRON depend on datum size or other parameter   the number of fold be usually determine by the number of instance contain in PRON dataset  for example  if PRON have 10 instance in PRON datum  10fold cross  validation would not make sense   kfold cross validation be use for two main purpose  to tune hyper parameter and to better evaluate the performance of a model   in both of these case select  k depend on the same thing  PRON must ensure that the training set and testing set be draw from the same distribution  and that both set contain sufficient variation such that the underlining distribution be represent  in a 10fold cross validation with only 10 instance  there would only be 1 instance in the testing set  this instance do not properly represent the variation of the underlie distribution   that be say  select  k be not an exact science because PRON be hard to estimate how well PRON fold represent PRON overall dataset  PRON usually use 5fold cross validation  this mean that 20  of the data be use for testing  this be usually pretty accurate  however  if PRON dataset size increase dramatically  like if PRON have over 100000 instance  PRON can be see that a 10fold cross validation would lead in fold of 10000 instance  this should be sufficient to reliably test PRON model   in short  yes the number of fold depend on the data size  PRON usually stick with 4 or 5fold  make sure to shuffle PRON datum  such that PRON fold do not contain inherent bias   depend on how much cpu juice PRON be willing to afford for the same  have a  low k mean less variance and thus  more bias  while have a high k mean more variance and thus  and low bias   also  one should keep in mind the computational cost for the different value  high k mean more fold  thus high computational time and vice versa  so  one need to find a sweet spot between those by do a hyper tuning analysis   also  PRON need to keep the size of PRON datum in mind  if PRON data be very less  then even use a k  fold crossval would not make sense  so  PRON may want to use a leave  one  out cv  loocv   
__label__computational-geometry __label__curve-fitting PRON have a discrete contour represent by a set of point  the contour look like a polygon but if PRON zoom PRON see that the edge be rugged  that be because PRON be obtain while work on a finite difference grid   PRON would like to fit a polygon to this contour  which be close in the least  square sense   be there any simple know algorithm to do this   the hough transform be an image processing algorithm for extract feature for an image  the classical version of the algorithm be design to extract line from a binary image  such as this    give the ability to do this  PRON can make a script fairly easily  here be one use the hough transform  and associate utility  in matlab  PRON probably require the image processing toolbox  but hopefully PRON can track that down   the only shortcut present in this code be that PRON tell the  houghpeak  function to search for 4 line  PRON can be smart than this by do other peak detection method in the hough space  read the wiki if PRON be not familiar with the output of the hough transform  httpsenwikipediaorgwikihoughtransform    one caveat be that the hough transform will only give PRON whole line  PRON will have to post  process PRON to get line segment  this example work particularly well without any of that since the input be a convex polygon whose vertex be at  near the edge of the image   clear all  close all  clc   load image  PRON suspect PRON input may be different  but the same idea hold   img  imreadpolygonpng      make a black  white image use the  blue  as the new white value  bw  img1   255    use hough transform to find line   h  theta  rho   houghbw    p  houghpeaksh4    line  houghlinesbw  theta  rho  p     visualization  imagescbw   colormap gray   hold on   for l  1lengthlines   plotlineslpoint11  lineslpoint21       lineslpoint12  lineslpoint22r     end  there be a approach that use notion from discrete geometry  discrete geometry be a discipline that work with object define as set of pixel that try to mimic PRON standard counterpart  PRON define discrete segment  discrete circle  discrete plane etc  in PRON case  there be an algorithm  1  that have a definition of what a discrete segment be  and that reconstruct in the input image the set of maximal segment  ie segment that can not be further extend by add new pixel to PRON  see also the extension to fuzzy datum  23   the approach be successfully apply to 3d reconstruction from multiview image  4    1  i debled  rennesson  j  p reveilles  a linear algorithm for segmentation of digital curve  international journal of pattern recognition and artificial intelligence  volume 9  n 6  december 1995    2  i debled  rennesson  j  l remy and j rouyer  degli  linear segmentation of discrete curve into fuzzy segment  discrete apply mathematics  151122  137  october 2005    3  i debled  rennesson  f feschet and j rouyer  degli  optimal blur segment decomposition of noisy shape in linear time  computer  amp  graphic  301   2006    4  httpshalinriafrinria00349084  PRON be quite surprised why no  one mention the famous douglas peucker algorithm for polyline simplification  since PRON have contour point in hand  PRON could benefit from PRON directly  contour approximation in opencv use this method  see this for usage  and PRON could also find a matlab implementation here or here  
__label__research __label__ethics __label__social __label__reasoning white house publish the information about ai which request mention about  the most important research gap in ai that must be address to advance this field and benefit the public    what be these exactly   accord to ibm research organization in the response to white house as part of prepare for the future of artificial intelligence  ai depend upon many long  term advance  not only from ai researcher  but from many interdisciplinary team of expert from many discipline  include the follow challenge   machine learning and reasoning   currently ai system use supervise learning use huge amount of dataset of label datum for training  this be very different to how human learn by create concept  relationship  common sense reasoning which give ability to learn much without too much datum  therefore machine learn with common  sense reasoning capability should be research further more   decision technique   current ai  base system have very limit ability for make decision  therefore new technique must be develop  eg model systemic risk  analyze tradeoff  detect anomaly in context  analyze datum while preserve privacy    domain  specific ai system   the current ai  base system be lack of ability to understand the variety of domain of human expertise  such as medicine  engineering  law and many more   the system should be able to perform professional  level task such as designing problem  experiment  manage contradiction  negotiating  etc   data assurance and trust   the current ai  base system require huge amount of datum and PRON behaviour directly depend on the quality of this datum which can be bias  incomplete or compromise  this can be expensive and time consume especially where PRON be use for safety critical system which potentially can be very dangerous   radically efficient compute infrastructure   the current ai  base system require unprecedented workload and computing power which require development of new computing architecture  such as neuromorphic    interpretability and explanation   for people to follow ai suggestion  PRON need to trust system  and this be only when PRON be capable of know user  intent  priority  reasoning and PRON can learn from PRON mistake  these capability be require in many business domain and professional  value alignment and ethic   human can share the common knowledge of how the world function  the machine can not  PRON can fail by have unintended and unexpected behaviour only because human do not specify the right goal for PRON or PRON omit essential training detail  the system should be able to correct specification of the goal and avoid unintended and undesired consequence in the behaviour   social ai   the ai  base system should be able to work closely to human in PRON professional and personal life  therefore PRON should have significant social capability  because PRON can impact on PRON emotion and PRON decision make capability  also sophisticated natural language capability will need to be develop to allow a natural interaction and dialog between human and machine   source  fundamental question in ai research  and the most important research gap  rfi question 5 and 6   one way of illustrate the deficiency of many of PRON current approach at once be to consider how well PRON be possible to represent  equivalently  learn  commonsense knowledge   in this area  the winograd schema challenge have be propose by levesque  in which each problem be give as input natural language text contain an ambiguous pronoun   babar wonder how PRON can get new clothing  luckily  a very rich old man who have always be fond of little elephant understand right away that PRON be long for a fine suit   here  the program be ask to decide if  PRON  in  PRON be long for a fine suit  refer to babar or the old man  several thousand such question have be collate and propose as a more quantifiable alternative to the turing test   despite the fact that the input domain be natural language  success here be undeniably a pre  requisite for agi and  as imply in PRON answer here  for be able to interact ethically with the human world  
__label__regression __label__linear-regression the bayes error rate be a theoretical bound that determine the low possible error rate for a classification problem  give some datum  PRON be wonder whether an equivalent concept exist for the case of regression algorithm  PRON aim be to determine how far PRON regression algorithm s error be from that theoretical bind  as a way to assess how far be PRON from the good possible solution  be there any way to obtain a bound of the low regression error for a give dataset   PRON realize this question be ask more than a year ago  but PRON think one possibility be to use the bias  variance decomposition to calculate a low bind on the error rate   essentially  the error rate be write as the sum of three term  the bias  the variance  and the irreducible error  one good source for learn about these term be an introduction to statistical learning   assume that the true function   fx  lie within the family of function that PRON machine learning model be capable of fitting  and take the limit as the amount of training datum PRON have go to infinity  then  if PRON machine learning model have a finite number of parameter  both the bias and the variance will be zero  so  the actual error will simply be equal to the irreducible error   as an example  suppose PRON true datum be linear with gaussian noise   y sim na  bx  sigma2 one of the the optimal estimator be obviously linear regression   haty   hata   hatbx  and  as PRON add more training example  the estimate coefficient  hata and  hatb will approach  a and  b  respectively  so  the good error  assume squared loss  PRON could hope to achieve would be equal  sigma2   the inherent error  irreducible noise associate with the data generation PRON  in practice  compute the irreducible error be difficult  impossible    since PRON require knowledge of the true process for generate the datum  but  this critique be also applicable to the bayes error  since that require knowledge of the true class probability   yes  that would  be the sum of the  square of distance of the response variable from the true or the actual regression lineprovid PRON know PRON   
__label__performance __label__krylov-method __label__viennacl __label__opencl __label__parallel-computing PRON be use viennacl s interface to eigen as a way to leverage opencl   specifically  PRON be use the    viennacllinalgbicgstabtag with an eigen sparse matrix   however  the performance be not what PRON hop that PRON would be   what tool on windows 7mac os x  linux should PRON use to understand the performance bottleneck   a very simple mean be to compute by hand the amount of useful work that be to be do  ie either amount of datum to be transfer or the amount of useful float point operation  flop  or both    then do some simple timing and average PRON to get an absolute prediction of performance for a give problem size  time vs problem size  and from this PRON can assess overall performance characteristic  this be a simple and useful mean for make comparison with hardware specification andor performance of other software   furthermore  to do a complete performance breakdown PRON will need tool such as totalview for profile and identify performance bottleneck   general statement   for small system size  there be little benefit of use opencl at all   alright  now for the justification  there be a certain amount of overhead associate with each opencl kernel launch  exact timing depend on the underlying hardware  but as a rule of thumb one can use a pessimistic estimate of 10 microsecond for cpu and 100 microsecond for gpu as PRON have once report in this thread at the intel opencl forum  this be a lot consider that modern hardware provide many gflop of processing power  for example  add two vector with 100000 entry each on a gpu with 100 gb  sec memory bandwidth require 3  8  100000 bytes  approx  2 mb  of datum to be transfer  take 20 PRON  thus  even add up two vector of size 100000 can show significant kernel launch overhead  in practice  kernel launch overhead can be reduce if kernel be enqueu while another kernel be still active  this  however  require again that kernel execution time be sufficiently large   since most operation inside krylov solver be comparable in complexity to vector addition  this often apply to sparse matrix  vector multiplication as well   benchmark for system size below 10000 by 10000 essentially measure opencl kernel launch overhead only  PRON thus recommend to run benchmark again with PRON upper limit 50k by 50k  for small system size  just use bicgstab with the eigen matrix directly  this be one of the reason why viennacl offer generic implementation   or give sparse direct solver a try   for dense system  matrix   the notion of a  small system  be certainly shift to small value  still  below system size of about 1000x1000 overhead become significant again  
__label__floating-point __label__blas give two different blas implementation  can PRON expect that PRON make the exact same float point computation and return the same result  or can PRON happen  for instance  that one compute a scalar product as       x1y1  x2y2   x3y3   x4y4     and one as      x1y1  x2y2    x3y3  x4y4       so possibly give different result in ieee float point arithmetic   no  that be not guarantee  if PRON be use a netlib blas without any optimization  PRON PRON mostly true that the result be the same  but for any practical usage of blas and lapack one use a highly optimize an parallel blas  the parallelization cause  even if PRON only work in parallel inside the vector register of a cpu  that the order how the single term be evaluate change and the order of the summation change too  now PRON follow form the miss associative property in the ieee standard that the result be not the same  so exactly the thing PRON mention can happen   in the netlib bla the scalar product be only a for loop unrolled by a factor 5   do PRON  mp1n5  dtemp  dtemp  dxidyi   dxi1dyi1      dxi2dyi2   dxi3dyi3   dxi4dyi4   end do  and PRON be up to the compiler if each multiplication be add to dtemp immediately or if all 5 component be sum up first and than add to dtemp   in openblas PRON be depend on the architecture a more complicated kernel     asm      volatile       vxorpd    ymm4    ymm4    ymm4  nt    vxorpd    ymm5    ymm5    ymm5  nt    vxorpd    ymm6    ymm6    ymm6  nt    vxorpd    ymm7    ymm7    ymm7  nt    align 16  nt    1   nt    vmovup    208     ymm12  nt    2  x   vmovup  32208     ymm13  nt    2  x   vmovup  64208     ymm14  nt    2  x   vmovup  96208     ymm15  nt    2  x   vmulpd    308     ymm12    ymm12 nt    2  y   vmulpd  32308     ymm13    ymm13 nt    2  y   vmulpd  64308     ymm14    ymm14 nt    2  y   vmulpd  96308     ymm15    ymm15 nt    2  y   vaddpd    ymm4    ymm12    ymm4 nt    2  y   vaddpd    ymm5    ymm13    ymm5 nt    2  y   vaddpd    ymm6    ymm14    ymm6 nt    2  y   vaddpd    ymm7    ymm15    ymm7 nt    2  y   addq   16   0  nt    subq   16   1  nt    jnz  1b  nt     which split the scalar product in small scalar product of length 4 and sum PRON up   use the other typical blas implementation like atlas  mkl  essl   this problem stay the same because each blas implementation use different optimization to get fast code   but as far as PRON know one need an artificial example to because really faulty result   if PRON be necessary that the blas library return for the same result  bit  wise the same  one have to use a reproducible blas library such as   reproblas httpbebopcsberkeleyedureproblas  exblas httpsexblaslip6fr  in general  no  leave associativity aside  the choice of compiler flag  for example  simd instruction be enable  usage of fuse multiply add  etc   or the hardware  eg  whether extended precision be be use  may produce different result   there be some effort to get reproducible blas implementation  see reproblas and exbla for more information   the short answer  if the two blas implementation be write to carry out the operation in the exact same order  and the library be compile use the same compiler flag and with the same compiler  then PRON will give PRON the same result  float point arithmetic be not random  so two identical implementation will give identical result   however  there be a variety of thing that can break this behavior for the sake of performance   the long answer  ieee also specify the order in which these operation be carry out  in addition to how each operation should behave  however  if PRON compile PRON blas implementation with option like  ffast  math   the compiler can perform transformation that would be true in exact arithmetic but not  correct  in ieee float point  the canonical example be the non  associativity of float point addition  as PRON point out  with the more aggressive optimization setting  associativity will be assume  and the processor will do as much of that in parallel as possible by re  order the operation   the other standard  break behavior come via the use of fma  fuse multiply  add  instruction  these be prominent in operation like matrix multiplication  and PRON have the potential to double the throughput of PRON routine  however  PRON perform the operation  abc in a single operation  and PRON only incur a single float point rounding step  this deviate from the ieee standard  which require that this operation have two round step  this make the fma result actually more accurate than the ieee one  but PRON be technically standard  break behavior  
__label__neural-network __label__tensorflow __label__loss-function PRON desire output be not 1hot encoding  but  like a 10 d vector   1  0  1  0  1  0  0  1  1  1   and the input be like the normal mnist datum set   PRON want to use tensorflow to build a model to learn this   then which loss function should PRON choose   if PRON class arre not mutually exlcusive  then PRON just have multiple sigmoid output  instead of softmax function as see in example mnist classifier   each output will be a separate probability that the network assign to membership in that class   for a matching loss function  in tensorflow  PRON could use the build  in tfnnsigmoidcrossentropywithlogit  note that PRON work on the logit  the input to the sigmoid function  for efficiency  the link explain the math involve   PRON will still want a sigmoid function on the output layer too  for when PRON read off the prediction  but PRON apply the loss function above to the input of the sigmoid function  note this be not a requirement of PRON problem  PRON can easily write a loss function that work from the sigmoid output  just the tensorflow build  in have be write differently to get a small speed boost  
__label__game-ai __label__combinatorial-games __label__combinatorics PRON read about minimax  then alpha  beta pruning and then about iterative deepening  iterative deepen couple with alpha  beta pruning prove to quite efficient as compare alpha  beta alone   PRON have implement a game agent that use iterative deepen with alpha  beta pruning  now PRON want to beat PRON  what can PRON do to go deep  like alpha  beta pruning cut the move  what other small change could be implement that can beat PRON old ai   PRON aim to go deep than PRON current ai  if PRON want to know about the game  here be a brief summary   there be two player  four game piece and a 7by7 grid of square  at the beginning of the game  the first player place both the piece on any two different square  from that point on  the player alternate turn move both the piece like a queen in chess  any number of open square vertically  horizontally  or diagonally   when the piece be move  the square that be previously occupy be block  that square can not be use for the remainder of the game  the piece can not move through block square  the first player who be unable to move any one of the queen lose   so PRON aim be to cut the unwanted node and search deeper   to make boost iterative deepen with alpha  beta pruning PRON can use the  sss  search algorithm  PRON a good first strategy algorithm  the sss  algorithm can improve the time efficiency of the overall algorithm but PRON increase the space complexity   PRON be link the wiki to PRON httpsenwikipediaorgwikisss   PRON will update the answer as soon as i get a good solution   try cache or transposition table  without PRON  PRON search tree may explode   first thing PRON be go to want to add be probably a transposition table  as also suggest by smallchess   afterwards  PRON would look into aspiration search andor principal variation search  also see this page    then PRON would look into thing like the killer move heuristic  and maybe also see if PRON can simply implement exist part of PRON engine more efficiently  eg use bitboard for PRON state representation    other than all of that  the chess programming wiki probably have lot of other interesting page as well  
__label__wavelet PRON understand the idea of construct low  pass and high  pass filter as a projection on the numerical range and numerical kernel of dyadic power of a diffusion operator  in the work diffusion wavelet packets   this can be represent as a tree and be implement here   httpsgithubcomaweinsteindwblobmasterwaveletsdwptreem  PRON do not understand how to split each  wj to PRON understanding PRON could make another qr decomposition  as be do for the  vj subspace at level  j  the author say  in the end of section 4 of the above mention article  to choose each  wj s child as have the same dimension   the problem be  PRON do not explain why and how to do this splitting  so PRON start to read the code on github   in line 404  use a handle to function defaultsplitfcn  PRON find a method nowhere explain in article or lecture about diffusion wavelet   use the research  word  line 503  defaultsplitfcn function    average the endpoint of the approximate frequency range of the node  PRON question be  why can PRON do this   thank in advance   
__label__data-mining __label__dataset __label__databases __label__sequence PRON be study sequence mining algorithm and for that  PRON read that PRON should use a transaction  sequence database just like this   transaction PRON would  item  t1   1  2  7   t2   4  5  9   t3   2  7  8  9   t4   6  5  8   t5   5  8  9  10   however  PRON search for some data set and database and PRON find some like this    monthmonthly milk production  pound per cow  jan 62  dec 75    1962  01589   1962  02561   1962  03640   1962  04656   1962  05727  this one be from this site   quarter  male live birth  female live births  male deaths  female death  2000q17639713933463070  2000q27365686633723178  2000q37174684336753511  2000q46979660033573151  2001q17496723232313070  this one be from this site   these 2 last example be data set   question   what be the good approach to transform PRON into sequence database  or do there exist a way to use PRON like this   
__label__regression __label__prediction __label__classifier when PRON fit any model into a data set for prediction  what exactly happen behind the scene   PRON be learn regression and PRON be a little confused about how exactly PRON fit the classifier or regression into a dataset   PRON need to decide which algorithm PRON use base on PRON target variableoutput  dependent variable    if PRON target variable be categorical PRON need to go for classifierbinary  multi   if PRON target variable be continuous PRON need to go for prediction algorithms  ie  regression  decision tree  etc    if the data be time dependent PRON need to use forecasting techniques  time series  croston  etc    for example if PRON be look at linear regression   PRON basic idea be to fit the datum in the form of a line  as PRON remember in PRON schooling PRON teach PRON about linear algebra  where PRON need to find a solution for a line by substitute point in that  if PRON satisfy  then PRON be solvable and many other case  same way  here if PRON target variable be continuous then PRON use PRON feature as variable  PRON attempt to model the relationship between two variable by fit a linear equation to observe datum   equation of linear regression be     y  a  bx  where  x be the explanatoryfeature  variable and  y be the dependenttarget  variable  do not get confuse with logistic regressionlr  and other regression techniques  lr be not a regression technique PRON be a classifier  people generally get confuse with that   regard classifiers  if the data be linearly separableyes  no  10  then PRON can use lr  if PRON be not linearly separable PRON need to use technique like svm   there be a hole a the wall PRON want to cover  let s think about a series question PRON would like to ask   1  what be the shape of the hole   be PRON a circle  a square  a ellipse or maybe a triangle  notice some of those shape be more general than the other  like a square be just a special rectangular   2  how do PRON describe the exact size of that hole   what be the diameter of the circle  how long be the size of the square   what do this have to do with fit the model   the first question about the shape of model be an analogy to the problem of model selection  there be lot of different model PRON can use to fit a model  there be ordinary linear regression where the model can be represent as a straight line  there be also more complex model like polynomial  and like how square be a special rectangular  linear model be just a very special polynomial   once PRON have a particular type of model PRON have in mind  PRON need to find a set a number that completely describe that model  this the process of fit PRON model  in the case of linear regression  the model be a straight line  and the fit the model be to find which straight line   PRON asking for a layman s explanation  and the phrasing of PRON question  suggest to PRON that PRON be get tangle up in the semantic  and PRON totally get that   so let PRON let go of the usual term as much as PRON can  and speak in concept   PRON have a bunch of observation  or maybe even only one observation  this be PRON datum  maybe PRON be one of the particularly inquisitive ancient  and PRON notice some sort of relationship between lunar pattern and tide  while track this datum  PRON start get a sense for what sort of force may be at work that would explain the apparent cause  and  effect relationship  the set of rule that PRON eventually develop become PRON model  the more correct PRON rule  the good PRON will be able to predict the tide tomorrow  next month  and next year   this concept PRON have develop  probably something to do with gravity  may be even more generally useful  if the general concept be re  use with different number  PRON can also help explain earth s annual cycle  and why PRON see the morning star  venus  with certain pattern   in the case where PRON be re  use the concept  PRON be  fit the model to the datum   PRON be test whether the set of rule PRON have develop  the model  can explain the apparent force at work in the observation  the datum  that PRON have   in the usual application of regression  or any statistical method   PRON be not try to formulate the law of physics  PRON be try to find rule that sufficiently govern complex system  often human behaviour  and so PRON be unlikely to explain observation perfectly  but a model be useful to the extent that PRON successfully predict future observation  rather than wait for the future  though  PRON typically perform analysis on part of a data set  and test for predictive power with the remainder of the datum set   a specific instance of regression analysis may be in try to explain housing price  there be many many possible factor  square meterage  footage   number of floor  swimming pool or not  size of property  shape of property  age  likelihood of flooding  distance to neighbour  energy rating  economic condition  quality of the local school  etc  etc  etc  which factor be most important  do one  two  three of the factor explain 95  of the variation in housing price  can PRON come up with an equation that will predict a house s selling price reasonably well  rather than rely on a real estate agent s recommendation  and have to second  guess what PRON  PRON agenda be  how PRON may be play PRON   and if PRON will humour a little editorializing   there be countless example of reliance on expert prove to be a mistake  nate silver s  the signal and the noise   daniel kahneman s  think fast  think slow   philip tetlock s  superforecasting    data science be about find way to derive insight from the datum  rather than expert  opinion  but many datum scientist get catch up in the mechanic  and lose sight of the purpose  PRON be not just about razzle  dazzle infographic  as hamming put PRON   the purpose of computing be insight  not number   so PRON applaud PRON effort to seek a layman s explanation  rather than technical speak  to understand regression  
__label__scikit-learn __label__random-forest PRON have several random forest model that work well  now  PRON would like to do feature selection base on this model  how do PRON find out which feature be use frequently by a model eg randomforestclassifier in scikit  learn   feature importance PRON be a property of the random forrest classifier   see an example here  just to add to the feature importance of the rf s  do not forget to use the hierarchical clustering from scipy and plot the dendogram   PRON will help PRON find the similar column  
__label__convolutional-neural-networks __label__keras __label__python PRON be currently try to implement a convolutional neural network with keras  PRON main aim be to generate 3d model from 2d image  for this PRON have implement a neural network which include a customized layer write by PRON  in this layer PRON rotate and take projection of these rotate model  but to increase diversity PRON want to take different angle value for each image in a batch   PRON make some research and all but have not be able to come up with a starting point  any answer or guidance be greatly appreciate  thank   
__label__r __label__regression __label__linear-regression __label__parameter-estimation PRON have the following model    yitalpha  xitbetai   epsilonit   text    i12  n  text   t12  t  1    betai zigammaetai  2   with  epsilonit  sim n0sigmaepsiloni2 and  etai  sim n0sigmaeta2  also i consider the follow prior specification    pbetagamma  propto 1   psigmaepsiloni2   propto sigmaepsiloni2    psigmaeta2   propto sigmaeta2  which have the following likelihood function    pythetaprodi1nintbetai   left  prodt1tfrac1sigmaepsilonisqrt2piexp left  frac12sigmaepsiloni2yitalpha  xitbetai2right  right  time frac1sigmaetasqrt2piexp left  frac12sigmaeta2betaizigamma2  right  dbetai  PRON would like to estimate the parameter  thetaalphagammasigmaepsiloni2sigmaeta2 so  theta be the parameter vector  only  betai be random  so PRON consider  betai as the latent variable   PRON would like to estimate these by use the follow gibbs sample schema   sample  alpha give  betaii1ngammasigmaepsilon2sigmaeta2y  sample  gamma give  betaii1nalphasigmaepsilon2sigmaeta2y  sample  sigmaepsilon2 give  betaii1ngammaalphasigmaeta2y  sample  sigmaeta2 give  betaii1ngammaalphasigmaepsilon2y  sample  betaii1n give  gammaalphasigmaepsilon2sigmaeta2y  PRON know that   to sample  alpha PRON re  write  1  as  yit   xitbetaialpha  epsilonit  so the full conditional distribution of  alpha be normal with mean  hatalphafrac1ttime n  sumi1nsumt1t   yit   xitbetai and variance  fracsigmaepsilon2t time n  to sample  sigmaepsilon2 PRON consider the regression model in  1   hence PRON can be sample from an invert gamma2 distribution with parameter  sumi1nsumt1t   yit   alpha  xitbetai2 with  n time t degree of freedom   to sample  gamma PRON consider the regression model in  2   so  gamma can be sample from multivariate normal distribution  with mean equal to the standard ols estimator  hatgammaleftsumi1nziziright1zibetai and covariance matrix equal to the standard ols covariance matrix estimator  sigmaeta2leftsumi1nziziright1  to sample  sigmaeta2 again PRON consider the linear regression in   2   hence  sigmaeta2 can be sample from an invert gamma2 distribution with parameter  sumi1nbetaizigamma2 and  n degree of freedom   lastly  PRON can sample  betaii1n from a normal distribution with mean   leftsumt1txit2leftfracsigmaepsilonsigmaetaright2right1leftsumt1txityitleftfracsigmaepsilonsigmaetaright2zigammaright and covariance matrix  sigmaepsilon2leftsumt1txit2leftfracsigmaepsilonsigmaetaright2right1  how can PRON do this in r  if PRON could please give PRON an example for instance how to sample  alpha PRON would be  very grateful   thank    in term of write this in r  here be an example PRON find   httpwwwstatpurdueeduzhanghaomashandoutgibbsbayesianpdf  PRON could then iterate through different level of α and select the level yield the good result   here be the code PRON write to answer PRON question  PRON may not be the most efficient one but PRON work  sharing be care   niterlt10000  nlt  nrowy   tlt10  PRON will take into consideration until t10 to estimate PRON parametre and then PRON will forecast the rest value t1112   etc  result  lt matrix0nrow  niter  ncol  3   here PRON store 3 parameter  alpha  sigmaepsilon  sigmaeta  PRON do not store gamma here  resultbetaslt  matrix0nrow  niter  ncol  n   the beta  resultgammaslt  matrix0nrow  niter  ncol  nrowz    here PRON store the gamma  alphalt0  gammalt0  sigmaepsilonsquaredlt1  sigmaetasquaredlt1  betalt  matrix0  nrow  n  ncol1   beta2lt0  for  i in 1niter     meanalphalt0  for  obs in 1n     for  time in 1t     meanalphalt  meanalphayob  time   betaobsxobs  time       alphalt  rnorm1  mean   1tn    meanalpha  sd  sqrtsigmaepsilonsquaredtn     paramsigmaepsilonsquaredlt0  for  obs in 1n     for  time in 1t     paramsigmaepsilonsquaredlt  paramsigmaepsilonsquar   yobs  time   alpha  betaobsxobs  time2      sigmaepsilonsquar  lt rinvgamma1   nt2  paramsigmaepsilonsquared2   meangammalt0  this be a vector  meangamma1lt0  this be a matrix  meangamma2lt0  sigmagammalt0  forob in 1n     meangamma1lt  meangamma1  zobstzob    this be  k x k   meangamma2lt  meangamma2  zobsbetaob    meangammalt  meangamma  1zobstzob       zobsbetaobs1     sigmagammalt  sigmagamma1zobstzob        meangammalt  solvemeangamma1meangamma2  sigmagammalt  solvemeangamma1   gammalt  mvrnormn  1  mu  meangamma  sigma  sqrtsigmaetasquaredsigmagamma   resultgammasilt  gamma  meansigmaetasquaredlt0  for  obs in 1n     meansigmaetasquaredlt meansigmaetasquar   betaobs   tzobsgamma2    sigmaetasquaredlt  rinvgamma1  n2  meansigmaetasquared2   resultilt  calpha  sigmaepsilonsquar  sigmaetasquar   meanbetalt0  meanbeta1lt0  this be scalar  meanbeta2lt0  this be scalar  sigmabetalt0  for  obs in 1n     for  time in 1t     meanbeta1lt  meanbeta1  xob  time2   sigmaepsilonsquared  sigmaetasquar   meanbeta2lt  meanbeta2  xob  timeyob  time    sigmaepsilonsquared  sigmaetasquaredtzobsgamma     meanbetalt1meanbeta1meanbeta2  sigmabetalt1meanbeta1  beta2lt  rnorm1  mean  meanbeta  sd  sqrtsigmaepsilonsquaredsigmabeta    betaobslt  beta2  resultbetasi  obslt  beta2    ifi100     printi      
__label__machine-learning __label__text-mining __label__multilabel-classification PRON be try to use binary relevance for multi  label text classification   here be the datum PRON have   a training set with 6000 short text  around 500  800 word each  and  some label attach to PRON  around 4  6 for each text   there be  almost 500 different label in the entire set   a test set with 6000 short text  around 100  200 word each    the difference of size between PRON two set exist because of the source be different   so  PRON want to use binary relevance to find the label of the text in the test set  to do PRON  PRON create a dictionary with all the different word in the entire training set and remove stop word  word who appear only once and word who appear in more that 10  of the text  PRON get 14714 different word in PRON dictionary   PRON idea be to create a matrix where each row represent a document and each column a word and each value be the number of occurrence of a word in a document  but with 14714 word and 6000 document  PRON will get a matrix of 88 million of integer  PRON try  just to see  to create PRON and PRON laptop do not support PRON    PRON even do not have the time to create PRON y matrix and generate a model  PRON want to use a logistic regression  for only one label   so  PRON question be   be PRON a good way to make multi  label classification or be there a good method   be PRON a problem to have a training from one source and to use PRON to make a model to predict datum from another source  be the different size of the document a problem   do PRON use logistic regression for this kind of problem   thank PRON   edit  PRON also want to add the most frequent word in PRON dictionary  after the clean part  be common word and totally useless in the field of PRON research  biology   use  much  two  use  possible  example   how can PRON pass through PRON    just some view  suggestion  after remove the stop word do PRON stem  lemmatize the text PRON get   that would probably reduce the number of unique word in PRON corpus and bring some form of word to the same level  but caution while use stem as PRON sometimes create noise   try postagging and see what can be the important tag PRON want to keep and eliminate the one that PRON feel be give less relevance to the text   do PRON try to find the some important term from each document by use tf  idf or chi  square method  PRON may be helpful to see the relevance of term for each class  document   see how far can PRON reduce the dimension of the matrix after use the above  if not PRON have already use PRON and then apply logistic regression or whatev classifier PRON want to   on PRON second question   PRON do not think have datum from different source should be a problem since PRON be try to create model base on keyword  hopefully PRON should be able to see  PRON be not confident on this part   PRON have work on text classification before with big document and large corpus with not so fruitful result with various model   hope this help  
__label__python __label__pandas __label__plotting PRON have an aggregate dataframe m as in the picture   3 column  one column for year one column for month and one column for monthly ratio   PRON want to plot a line in bokeh with month progression on the x axis and monthratio on y axis  month be aggregate by year  so PRON do not want data point from the same month but different year plot in the same spot  more like a continuous progression  any idea how to do that   
__label__libraries __label__c++ __label__programming-paradigms many computational scientist that PRON know of  include PRON for example  be not computer scientist  as such PRON be often not very well aware more advanced technique in oop  on the other hand  most of advanced programmer be not computational scientist  PRON may not be well aware of what oop feature be essential or relevant when PRON come to write a scientific computing software   consider mainly c  but maybe also in general   what oop feature do PRON think be absolutely necessary in a number crunch software and why  what feature do PRON think be generally consider good  but may because unnecessary overhead  what be some of the   myth  when PRON come to overhead of use more advanced feature   PRON generally like to hear thing regard   1  template  2  virtual function  3  smart pointer and garbage collector   edit   4  operator overload  5  boost library  technically not an oop feature but very popular c library    excessive  use of template in c can lead to humongous compilation error and may lead to frustration with non  computer scientist  in form of expression template PRON can significantly speed up PRON code though  and that be something PRON should be aware of   PRON be not a big user of oop in general  and this have PRON reason  PRON thought on some of the feature PRON mention   templates  convenient when PRON work  a massive nightmare to debug when PRON do not  furthermore  PRON do not really know what be go on  behind the scene   which can be a source of error or inefficiency   virtual function  can be very convenient and elegant  but keep in mind that call that function be more expensive than call a static function  do not use these thing in the innermost loop of a big calculation   smart pointer  as with virtual function  watch out for hidden overhead when assign and dereferenc  do not use in performance  critical part of PRON code   garbage collector  not necessarily an oop feature  eg the boehm gc   but an awesome thing nevertheless  again  this can because PRON program to pause and wait while clean up the memory  but PRON should avoid excessive creation  allocation and destruction  freeing of object in performance  critical part anyway  right   one thing PRON leave out be operator overloading  which be an extremely convenient way of express thing in a readable manner  again  though  watch out for overhead in tight loop  and remember that in general  the compiler will not be able to optimize anything  think common sub  expression elimination  register variable  etc    a good alternative be expression template  but write PRON own can increase code complexity and give PRON weird bug  think the same kind of mess as regular template can be   have say that  use other people s template  assume PRON be try and test and mostly bug  free  can make life so much sweet  remember though  can  as in not always   in summary  PRON be a performance  elegance trade  off  PRON would definitely recommend oop  or anything similar  for the user interface andor  big picture  part of the code  PRON would  however  stay away from PRON in kernel or anything performance  critical due to the hidden overhead and the fact that the compiler usually can not optimize much   PRON think that the most important lesson for scientist that start programming be  there be a very high probability that the programming problem that PRON be struggle with have already be solve by a computer scientist    PRON want to refer here to  design patterns   a lot of typical interaction between object  PRON behaviour and such  be well capture in a limited number of pattern that have a reference implementation in many oo programming language like c  java  python    the reference for design pattern be design patterns by  the gang  of  four   commonly refer to as the gof   PRON be complete  but not easy to read in the evening with a glass of wine  a more tutorial approach be head first design patterns  which PRON can recommend   so when start a programming job  first do the oop analysis  identify object  behaviour  relation   then identify pattern and implement these use PRON reference implementation  
__label__parallel-computing __label__graph-theory __label__unstructured-mesh __label__partitioning PRON be learn to use opencl to optimize some of PRON simulation  PRON realize that PRON need some sort of graph  clustering or graph  partitioning to exploit efficiently local memory for un  order mesh   example  PRON implement elastic cloth simulation with regular mesh of 4bonded vertex  which PRON can separate manually to localized batch  now PRON would like to move to general irregular mesh where each node can have any number of edge   from quick search PRON find some resource but PRON be not sure if PRON adres very well PRON need  PRON have no experience in area of discrete math and graph theory    what PRON want   split mesh to batch such that  all batch have approximately same size  number of node and edge  eg 16 node each correspoding to localsize of PRON opencl kernell  the number of edge between different batch be minimal  to minimize overlap between node load by each workgroup  algorithm which be concise and easy to implement by PRON  for PRON this be just side issue not main topic  PRON do not what to create dependence on some 3rd party software or library even if PRON be open  source   PRON do not have to lead to optimal solution  PRON can be stochastic and rough heuristic  PRON should be fast   on with small prefactor  for a simple  yet not optimal  see below  mesh partition algorithm  PRON can do   1  sort all the cell of the mesh use hilbert sort  2  partition the sorted list of cell into chunk of the desire size  spatial hilbert sort be implement in PRON geogram library  12  and in cgal  3   PRON be reasonably easy to implement  use the stdnthelement   function of the stl  see also  3  for a nice explanation of spatial sorting   the solution be not optimal in the sense that PRON completely ignore the connection between the chunk and thus do not minimize communication between process  but PRON think PRON be an interesting alternative because PRON be extremely fast  but PRON be on logn   rather than on  as request   PRON can be implement in a handful of line  whereas metis use a certain amount of memory and have a processing time that sometimes cancel the gain PRON have on the communication   to complete this answer  PRON mention also scotch  4   PRON have interesting alternative graph partition algorithm  but PRON be not fully satisfactory for PRON if PRON do not want to have an external dependency as PRON say in the question     1  httpaliceloriafrsoftwaregeogramdochtmlindexhtml   2  httpaliceloriafrsoftwaregraphitedochtmlmeshreorder8hhtml   3  httpdoccgalorglatestspatialsortingindexhtml   4  httpwwwlabrifrpersopelegrinscotch 
__label__algorithms __label__optimization hill climbing seem to be a very powerful tool for optimization  however  how to generate the  neighbor  of a solution always puzzle PRON   for example  PRON be optimize a solution   x1  x2  x3 here  x1  be in range   0  01   x2  be in range   0  100   x3  be in range   0  1000000 what be the good way to generate  neighbor   PRON can not really pick a  step size  here  because a step size of 1 be huge to  x1   but very minor to  x3  what be the good generic way to generate  neighbor  in hill climb algorithm   the easy  and safe   be to reparametrize PRON problem  so that all parameter have a comparable range  say  01    alternatively  PRON can use a different step size for each parameter   vincent s solution will get PRON over the first hurdle  PRON may still be wonder how to select the step size even if all the dimension have the same range  in that case PRON help to understand the behavior of PRON function so PRON can select an appropriate value  another way be to adaptively set the step size base on gradient  tread lightly if the function be change a lot    this paper may have some interesting idea too  on a hill  climbing algorithm with adaptive step size  towards a control parameter  less black  box optimisation algorithm 
__label__classification __label__clustering __label__k-means PRON be new to data science but really enthusiastic about PRON  really appreciate community like this one  so thank in anticipation   PRON enter a competition to solve a data mining challenge for an e  commerce website  the challenge be as follow   build a model base on clickstream datum  to detect the phase of the  customer in the customer journey  or relate intent of customer    PRON be not give specific definition of the customer phase  instead PRON be suppose to come up with PRON PRON base on observable behaviour of user  and maybe academic literature   PRON believe there be three main shopping state   directed buyer  goal orient  specific product in mind   deliberate searcher  know the basic category  but not a specific product in mind   hedonic browser  similar to window shopper in real life   after clean up the datum  PRON have a table with session  level summary variable such as   category breadth  how many unique category a user visit in a give session  product to category ratio  time spend in category page vs product page  session source   direct  search engine  social medium  referral   add item to cart  yes  no  order item  yes  no  etc  PRON have the raw analytic export datum  PRON can see all variable in the ibm source bellow   so can basically make any other variable PRON want  but think these be good enough to explain PRON   PRON question be   what method would PRON use in order to come up with latent shopping states for these user  preferably PRON would like to obtain the probability that each individual user be in a specific shopping state  as well as some sort of weight for the factor that compose each state  eg directed shopper have a very high category breadth   initially PRON think some sort of cluster algorithm such as k  means  but that do not give PRON probability  PRON be now lean towards latent class analysis  but PRON still a bit abstract to PRON   if PRON can provide PRON with any additional information let PRON know   thank   source   ramachandran  vandana  viswanathan  siva  and lucas  hank   state of  shopping and the value of information  insights from the clickstream    2010   icis 2010  httpaiselaisnetorgicis2010submissions254  ibm digital analytics export info  httpwwwibmcomsupportknowledgecentersspg9mexportexportdigitaldatafeedhtml  
__label__computational-physics __label__simulation __label__molecular-dynamics __label__computational-chemistry PRON would like to perform a decane  watertip4p  interface simulation use gromacs to analyse the fluctuation of the interface over a long period of time  50ps  1ns    though PRON have a rough idea on how to perform PRON  PRON still be unsure about how to create a initial configuration which look like the figure give below  ar buuren  phd thesis  university of groningen  1995    also once PRON start PRON production run PRON would like to know  how to obtain the height fluctuation of the interface as a function of time   a simple solution to begin with would be to create two system  one with decane in a box of width 1295 nm and another one with water in a box of width 2 nm  depth and height should be as in the picture PRON include with PRON question    once PRON have the two system  run PRON independently for a sufficiently long time so that PRON equilibrate  then take two snapshot  set of position and velocity  of the decane box that be uncorrelated and one snapshot of the water box  translate the position of the water and one of the decane box so that PRON form the configuration PRON want  PRON can do this  for example  with vmd or with mdtraj   finally  create the target system have all the molecule but with the conformation  and velocity  take from the translate box that PRON obtain in the previous step  equilibrate again  and run  
__label__finite-element PRON be try to solve the lamm eqn numerically  use 1d fem  what PRON observe be spurious oscillation at one boundary   rb   which quickly lead to unreasonable value over the entire range  PRON be look for advice on how to modify the fe formulation to eliminate these oscillation   the lamm eqn be     partial c over partial t     1over rpartial  r j  over partial rq    raleq rleq rb  and    j  s omega2 r c  d  partial c over partial r  where  s be the sedimentation coefficient   d be the diffusion coefficient  and  omega the angular velocity  the boundary  condition be that the flux  jra  t   jrb  t 0 for simple case   s and  d be constant  but PRON may also be function of  c   q be a  source function   but PRON be 0 for PRON case  the initial condition be typically  c  constant   the current fem formulation use piecewise linear   hat   function for discretization in space  and an implicit method to compute  ctdelta t from  ct  edit  typical value be  ra59   rb72   c01   s4time 1013   omega227time 10  7    d36tim 107  at long time  the solution be proportional to  expr2  the lamm equation be a specific example of a convection  diffusion pde  PRON can find many reference on the difficulty  of numerical solution of this equation  eg leveque    oscillation in the solution near the boundary  due to characteristic of the numerical method be a common problem  a key parameter in  understand the likelyhood of oscillation be the mesh peclet number give by     p  fracsomega2rdelta x2d      where  delta x be the mesh spacing  oscillation almost always occur when this number be   gt1 for the lamm equation and the  constant PRON provide  for a reasonable mesh  this number be   lt1 however  due to the boundary condition  particularly at the right  end  there be a very steep gradient in the solution  this  along with even the moderate peclet number be responsible for the oscillation there   the obvious solution be to refine the mesh to reduce the peclet number  this can be do non  uniformly so that  for example  the mesh  be much more refined at the right end  often  this produce a model that too computationally  costly to solve  but that do not appear to be the case here   another alternative be to artificially increase the value of  d one of the comment suggest to instead use a forward  difference  upwind   finite difference method  this have the unfortunate side  effect of significantly increase  d so that the solution be very  smear  compare  with exact  in the finite element method  PRON can achieve the same effect by slightly increase  d directly   by do a simple web search  PRON be able to find a number of paper relate to numerical solution of the lamm equation for the particular  application PRON be interested in  many of PRON describe the difficulty of use basic fe method for this problem and propose more  sophisticated alternative  such a move  grid method  to overcome these problem  PRON think this one  by liu and chen  do a particularly nice job in this regard   finally  use the parameter PRON provide  PRON compute the follow solution to PRON equation use  500  two  node finite element and  an implicit time integration algorithm  as PRON can see  for these parameter and this level of mesh refinement  there be no noticeable  oscillation at the right end  
__label__algorithms __label__outlier suppose PRON have a data set  amount of money  100  50  150  200  35  60  50  20  500   PRON have google the web look for technique that can be use to find a possible outlier in this datum set but PRON end up confused   PRON question be  which algorithm  technique or method can be use to detect possible outlier in this data set   ps  consider that the data do not follow a normal distribution  thank   a simple approach would be use the same thing as box plot do  away than 15  median  q1  or 15  q3median   outlier   PRON find PRON useful in lot of case even PRON not perfect and maybe too simple   PRON have the advantage to not suppose normality   PRON can use boxplot for outlier analysis  PRON would show PRON how to do that in python   consider PRON datum as an array   a   100  50  150  200  35  60  50  20  500   now  use seaborn to plot the boxplot   import seaborn as sn  snboxplota   so  PRON would get a plot which look somewhat like this   seem like 500 be the only outlier to PRON   but  PRON all depend on the analysis and the tolerance level of the analyst or the statistician and also the problem statement   PRON can have a look at one of PRON answer on the crossvalidated se for more test   and there be several nice question on outlier and the algorithm and technique for detect PRON   PRON personal favourite be the mahalanobis distance technique   one way of thinking of outlier detection be that PRON be create a predictive model  then PRON be check to see if a point fall within the range of prediction   from an information  theoretic point of view  PRON can see how much each observation increase the entropy of PRON model   if PRON be treat this datum as just a collection of number  and PRON do not have some propose model for how PRON be generate  PRON may as well just look at the average   if PRON be certain the number be not normally distribute  PRON can not make statement as to how far  off  a give number be from the average  but PRON can just look at PRON in absolute term   apply this  PRON can take the average of all the number  then exclude each number and take the average of the other   whichever average be most different from the global average be the big outlier   here be some python   def avga    return sumalena   l   100  50  150  200  35  60  50  20  500   m  avgl   for idx in rangelenl     printoutli score of  0    1formatlidx   absm  avgelem for i  elem in enumeratel  if iidx        gtgt   outlier score of 100  4  outlier score of 50  10  outlier score of 150  3  outlier score of 200  9  outlier score of 35  12  outlier score of 60  9  outlier score of 50  10  outlier score of 20  14  outlier score of 500  46 
__label__c++ PRON write the follow program to solve the three body problem  sun  earth  jupiter   initialize the system such that the total angular momentum be 0  the result of the simulation be really bad  and the sun tend to infinity   PRON look at the code for more than an hour try to find the problem  do anybody see what be wrong with PRON program   thank PRON very much    include  ltiostreamgt   use stdcout   use stdendl    include  ltfstreamgt   use stdofstream    include  ltcmathgt   use stdsqrt   int main      double time0   dt005    double g19838e29    double m16e24   m219e27   ms199e30    double x11   y10   x252   y20   xs0   ys0    double t11   t2118618    double vx10vy12mpix1t1   vx20   vy22mpix2t2    double rx   m1x1m2x2msxsm1m2ms    ry   m1y1m2y2msysm1m2ms     x1  x1  rx   x2  x2  rx   xs  xs  rx   y1  y1  ry   y2  y2  ry   ys  ys  ry   double lx1x1m1vx1   lx2x2m2vx2    double vxs0   vys  lx1lx2msxs     double r120   r1s0   r2s0    ofstream outputtbjupiter  earthdat     output  ltlt  time  ltlt     ltlt  x1  ltlt     ltlt  y1  ltlt     ltlt  x2  ltlt     ltlt  y2  ltlt     ltlt  xs  ltlt     ltlt  ys  ltltendl   whiletime  lt  12     r12  sqrt   x1x2x1x2    y1y2y1y2     r1s  sqrt   x1xsx1xs    y1ysy1ys     r2  sqrt   x2xsx2xs    y2ysy2ys     vx1  vx1  gmsdtx1xsr1sr1sr1s   gm2x1x2dtr12r12r12    vy1  vy1  gmsdty1ysr1sr1sr1s   gm2y1y2dtr12r12r12    vx2  vx2  gmsdtx2r2sr2sr2s   gm1x2x1dtr12r12r12    vy2  vy2  gmsdty2r2sr2sr2s   gm1y2y1dtr12r12r12    vxs  vxs  gm1xs  x1dtr1sr1sr1s   gm2xs  x2dtr2sr2sr2s    vys  vys  gm1ys  y1dtr1sr1sr1s   gm2ys  y2dtr2sr2sr2s    x1  x1  vx1dt   y1  y1  vy1dt   x2  x2  vx2dt   y2  y2  vy2dt   xs  xs  vxsdt   ys  ys  vysdt   time   dt   output  ltlt  time  ltlt     ltlt  x1  ltlt     ltlt  y1  ltlt     ltlt  x2  ltlt     ltlt  y2  ltlt     ltlt  xs  ltlt     ltlt  ys  ltltendl     outputclose     return 0     edit   the correction of the first answer be good but now PRON change the calculation of the angula momentum  PRON be wrong  and the program do not work properly anymore  the code be the following    include  ltiostreamgt   use stdcout   use stdendl    include  ltfstreamgt   use stdofstream    include  ltcmathgt   use stdsqrt   int main      double time0   dt0005    double g19838e29    double m16e24   m2500  19e27   ms199e30    double x11   y10   x252   y20   xs0   ys0    double t11   t2118618    double vx10vy12mpix1t1   vx20   vy22mpix2t2    double rx   m1x1m2x2msxsm1m2ms    ry   m1y1m2y2msysm1m2ms     x1  x1  rx   x2  x2  rx   xs  xs  rx   y1  y1  ry   y2  y2  ry   ys  ys  ry   double ly1x1m1vy1   ly2x2m2vy2    double vxs0   vys  ly1ly2msxs     double r120   r1s0   r2s0    ofstream outputtbjupiter  earthdat     output  ltlt  time  ltlt     ltlt  x1  ltlt     ltlt  y1  ltlt     ltlt  x2  ltlt     ltlt  y2  ltlt     ltlt  xs  ltlt     ltlt  ys  ltltendl   cout  ltlt  x1  ltlt     ltlt  y1  ltlt     ltlt  vx1  ltlt     ltlt  vy1  ltlt  endl   cout  ltlt  x2  ltlt     ltlt  y2  ltlt     ltlt  vx2  ltlt     ltlt  vy2  ltlt  endl   cout  ltlt  xs  ltlt     ltlt  ys  ltlt     ltlt  vxs  ltlt     ltlt  vys  ltlt  endl   whiletime  lt  4     r12  sqrt   x1x2x1x2    y1y2y1y2     r1s  sqrt   x1xsx1xs    y1ysy1ys     r2  sqrt   x2xsx2xs    y2ysy2ys     vx1  vx1  gmsdtx1xsr1sr1sr1s   gm2x1x2dtr12r12r12    vy1  vy1  gmsdty1ysr1sr1sr1s   gm2y1y2dtr12r12r12    vx2  vx2  gmsdtx2xsr2sr2sr2s   gm1x2x1dtr12r12r12    vy2  vy2  gmsdty2ysr2sr2sr2s   gm1y2y1dtr12r12r12    vxs  vxs  gm1xs  x1dtr1sr1sr1s   gm2xs  x2dtr2sr2sr2s    vys  vys  gm1ys  y1dtr1sr1sr1s   gm2ys  y2dtr2sr2sr2s    x1  x1  vx1dt   y1  y1  vy1dt   x2  x2  vx2dt   y2  y2  vy2dt   xs  xs  vxsdt   ys  ys  vysdt   time   dt   output  ltlt  time  ltlt     ltlt  x1  ltlt     ltlt  y1  ltlt     ltlt  x2  ltlt     ltlt  y2  ltlt     ltlt  xs  ltlt     ltlt  ys  ltltendl     outputclose     return 0     PRON problem be that variant euler be the incorrect method for gravitational system   PRON need to look into symplectic integrators   this will solve the stability issue PRON be have since this type of integrator hold certain value constant  in this case energy    also  as a comment on PRON code  PRON should use temporary variable for hold derivate before PRON update state variable   as PRON have PRON now  update variabl one by one  mix information from two time step   right now PRON be update the velocity explicitly and the position implicitly   unless PRON have a good reason to do so  PRON be good to stick to a single method for a problem so PRON have know stability characteristic   edit   as the comment state  PRON be use a symplectic integrator  so that be not the issue   PRON simply have a couple small typo in PRON code   first  in the second term of PRON update for v2  PRON use x2 and y2 rather than  x2xs  and  y2ys    this should not make much difference though since PRON sun should stay near 00   PRON main error be that in the update for the sun velocity  PRON multiply by the product of radii rather than divide by PRON   the four line be fix here   vx2  vx2  gmsdtx2xsr2sr2sr2s   gm1x2x1dtr12r12r12    vy2  vy2  gmsdty2ysr2sr2sr2s   gm1y2y1dtr12r12r12    vxs  vxs  gm1xs  x1dtr1sr1sr1s   gm2xs  x2dtr2sr2sr2s    vys  vys  gm1ys  y1dtr1sr1sr1s   gm2ys  y2dtr2sr2sr2s    double g19838e29      maybe use  g  6673e11  and try something lake this   q  qqq   one  f  1q  g  2q  h  3q   two  three   
__label__machine-learning __label__visualization __label__deep-learning __label__autoencoder suppose PRON have an input layer with n neuron and the first hidden layer have  m neuron  with typically  m  lt  n then PRON compute the actication  aj of the  jth neuron in the hidden layer by   aj   fleftsumlimitsi1  n  wi  j  xibjright  where  f be an activation function like  tanh or  textsigmoid  to train the network  PRON compute the reconstruction of the input  denote  z  and minimize the error between  z and  x now  the  ith element in  z be typically compute as     zi  fleft  sumlimitsj1  m   wj  i   ajbi right     PRON be wonder why be the reconstructed  z be usually compute with the same activation function instead of use the inverse function  and why separate  w and  b be useful instead of use tie weight and bias  PRON seem much more intuitive to PRON to compute the reconstruct with the inverse activation function  f1  eg   textarctanh  as follow      zi   sumlimitsj1  m  fracf1ajbjwj  it      note  that here tie weight be use  ie   w   wt  and the bias  bj of the hidden layer be use  instead of introduce an additional set of bias for the input layer   and a very related question  to visualize feature  instead of compute the reconstruction  one would usually create an identity matrix with the dimension of the hidden layer  then  one would use each column of the matrix as input to a reactivation function  which induce an output in the input neuron  for the reactivation function  would PRON be good to use the same activation function  resp  the  zi  or the inverse function  resp  the  zi    PRON do not think that PRON assumption  w   wt hold  or rather be not necessary  and if PRON be do  PRON be not in order to somehow automatically reverse the calculation to create the hidden layer feature  PRON be not possible to reverse the compression in general  go from n to small m  directly in this way  if that be the goal  then PRON would want a form of matrix inversion  not simple transpose   instead PRON just want  wij for the compressed high  level feature representation  and will discard  wij after auto  encoder be finish   PRON can set  w   wt and tie the weight  this can help with regularisation  help the autoencoder generalise  but PRON be not necessary   for the autoencoder to function PRON do not actually matter what activation function PRON use after the layer that PRON be pre  training  provide the last layer of the autoencoder can express the range of possible input  however  PRON may get vary quality of result depend on what PRON use  as normal for a neural network   PRON be quite reasonable to use the same activation function that PRON be build the pre  train layer for  as PRON be the simple choice   use an inverse function be possible too  but not advisable for sigmoid or tanh  because eg arctanh be not define  lt  1 or  1  so likely would not be numerically stable  
__label__supervised-learning __label__unsupervised-learning __label__labels __label__semi-supervised-learning PRON be aware of the existence of semi  supervised learning approach  such as the ladder network  where only a subset of the datum be label  be there any method or paper which consider correctness probability for the label of that training datum subset  that is  some label may be correct with 100  probability  while other may have only 70  or 45  probability of be correct  any link to paper or work in this direction be highly appreciate   PRON do not know any paper  PRON would be greatly appreciate if someone would link some   in PRON case  PRON always test by un  label PRON already know label data by use a  traditional  67   33   train  test  split and check how the labelling perform in various metric  accuracy  logloss  etc     moreover  there be various category of semi  supervised learning  for instance  if PRON would use active learning  pool base approach  include incrementally sample  s each time   PRON could view how the performance range   finally  PRON may use as well cross  validation in the same sense for hyper parameter tune for PRON semi  supervised learning algorithm  all in all  in PRON perspective semi  supervised learning  have a lot yet to offer and PRON question may be rather more domain  datum specific and visuals the data and clustering may give PRON new insight  
__label__finite-element __label__error-estimation be there finite element method setup that provide error estimate in the  w1infty norm  ie  bound on  uh  uinfty   which family of element can be use for implement PRON    crosspost from mathoverflow  where PRON encounter little interest  but probably here PRON can find more people with a fem background    chapter 8 of brenner and scott s mathematical theory of finite element methods be devote to this subject  in particular  theorem 8111 and the corollary give PRON that   u  uhw1infty  le c hk  1uwkinfty  for linear elliptic problem with sufficiently smooth coefficient  provide that the finite element space satisfie some other inequality  PRON leave PRON as an exercise  how typical  to verify that this apply to the usual lagrange  hermite and argyris element   if PRON want to go beyond the standard laplace equation  alan demlow  at texas aampm university  have derive estimate  
__label__clustering __label__regression __label__decision-trees __label__k-means __label__discriminant-analysis PRON have start work on some modeling task where PRON start with regression as PRON dependent variable be continuous  while work on PRON  PRON find that non of PRON variable be significant  also PRON find that most of the data point fall in the some range  so PRON decide to segment PRON dependent variable into four category  PRON have see many document on binning variablegini method or entropy method  use only binary response but PRON be not sure PRON be possible with multiple class  PRON be interested to know that PRON can do bin for multiple class  PRON want to bin PRON numeric independent variable use PRON four class  PRON would be grateful if PRON could share any literature or link or paper or let PRON know the approach of binning   ultimately PRON objective be to find average of dependent variablesay balance  by use segmentation with PRON categorical independent variable  like industry type  and other binned numerical variable   PRON technique be below to find average number of PRON dependent variable use the below method  1  PRON be plan to do decision regression tree in case  PRON go ahead with numerical dependent variable as PRON be  or classification decision tree with four class  2  k mean cluster to segment independent variable to come up with average of dependent variable   
__label__c++ __label__vectorization just speak from PRON experience in c  PRON really tricky to take advantage of the simd capability of modern cpu s for more complex algorithm  if PRON see an opportunity from a high level oo perspective  PRON would then have to break the whole thing down to a very non  oo kind of code  babysitt every register individually even if PRON have no interaction with each other and not only the part that benefit from vectoriz but often the entire flow of datum along with datum format have to be redesign into a more basic and hard to manage form   the automatic vectorization toolsqvec  openmp  offer no benefit either since PRON can only deal with just the most basic datum type and loop with no oo concept support at all and imo only succeed in obscure the code and limit PRON option over the manual way   meanwhile  when cod for gpgpu s  PRON essentially get to do PRON in a perfectly oo mannermore  less  depend on the language  while focus on the journey of one piece of datum through the pipeline which just get duplicate to every other piece of datum  brilliant   PRON know that gpu  core  be not the same as a simd unit on a cpu but from what PRON have gather PRON be much close to that than actual individual core since PRON have to move in a kind of a lockstep   so basically  why be that   before PRON try to answer PRON question let PRON comment on the word  low level  in PRON question statement   in PRON opinion  PRON prefer not to say one programming modelone of simd and simt  be at a low level   as an example  during PRON undergraduage diploma project PRON have experience many low level issue about gpu programming  which PRON have not encounter in cpu programming  most of these issue involve the memory architecture of gpu  such as memory coalesce access  bank conflict  etc  PRON can search these topic to get a feeling of low  level  ness from gpu programming   and now return to PRON question  by  low level  PRON assume PRON mean that PRON have to refactor PRON code or even redesign PRON algorithm to fit the simd model  while on gpueg  use cuda  PRON may not require too much work like that   in some case PRON source code for cpu can even compile without any error use cuda compiler  but to vectorize a serial program in simd PRON have to translate all branch codeif  while block     PRON be not quite sure of what PRON mean by  oop  in PRON question statement  please tell PRON if PRON make a wrong assumption    the reason be that gpu programmingcuda  use a different model from the one use in cpu programming  the model for gpu programming be call simt  t be for thread   in simd model  all arithmetic operation have to perform in a synchronize way  so any branch execution be not allow   while in cuda  branch be allow in term of cuda syntax  and more importantly  PRON be support by hardware architecture and cuda runtime  when a branch execution happen in cuda  simt  the thread manager will coordinate the execution  those thread with identical execution path will be execute  and the diverse thread will be queue for later execution  during this process  other independent thread may be bring in to keep a high throughput   as PRON can see  the simt model help PRON to handle the branch execution   finally  PRON would like to let PRON know that PRON can programming simd in a  high level  by introduce another syntax layer   PRON can try httpsispcgithubio  PRON think PRON be compare use high  level library build on gpgpu technique  like c amp  to programming simd at the low possible assembly language or intrinsic level  that be not a fair comparison   since PRON specifically mention c amp  let PRON use a basic example to argue that PRON premise be not true  the introductory amp example show how to parallelize the following simple vector addition use amp    include  ltiostreamgt   void standardmethod     int acpp     1  2  3  4  5    int bcpp     6  7  8  9  10    int sumcpp5    for  int idx  0  idx  lt  5  idx     sumcppidx   acppidx   bcppidx      for  int idx  0  idx  lt  5  idx     stdcout  ltlt  sumcppidx   ltlt   n        to do this  PRON have to use an amp library function and write PRON loop in a specific way  what if PRON instead want to parallelize this loop use simd instruction on the cpu  PRON only have to compile PRON with a reasonably modern compiler and suitable flag   compile the above code with gcc 483 on linux and disassemble PRON  PRON obtain   gcc o3 c vectestcpp  ampamp  objdump m intel d vectesto  vectesto   file format elf64x86  64  disassembly of section text   0000000000000000  ltz14standardmethodvgt    0   55  push  rbp  1   53  push  rbx  2   48 83 ec 68  sub  rsp0x68  6   c7 04 24 01 00 00 00  mov  dword ptr  rsp0x1  d   c7 44 24 04 02 00 00  mov  dword ptr  rsp0x40x2  14   00  15   48 8d 5c 24 40  lea  rbxrsp0x40   1a   c7 44 24 08 03 00 00  mov  dword ptr  rsp0x80x3  21   00  22   c7 44 24 0c 04 00 00  mov  dword ptr  rsp0xc0x4  29   00  2a   48 8d 6c 24 54  lea  rbprsp0x54   2f   66 0f 6f 04 24  movdqa xmm0xmmword ptr  rsp   34   c7 44 24 20 06 00 00  mov  dword ptr  rsp0x200x6  3b   00  3c   c7 44 24 24 07 00 00  mov  dword ptr  rsp0x240x7  43   00  44   c7 44 24 28 08 00 00  mov  dword ptr  rsp0x280x8  4b   00  4c   c7 44 24 2c 09 00 00  mov  dword ptr  rsp0x2c0x9  53   00  54   66 0f fe 44 24 20  paddd  xmm0xmmword ptr  rsp0x20   5a   66 0f 7f 44 24 40  movdqa xmmword ptr  rsp0x40xmm0  60   c7 44 24 10 05 00 00  mov  dword ptr  rsp0x100x5  67   00  68   c7 44 24 30 0a 00 00  mov  dword ptr  rsp0x300xa  6f   00  70   c7 44 24 50 0f 00 00  mov  dword ptr  rsp0x500xf  77   00  78   8b 33  mov  esi  dword ptr  rbx   7a   bf 00 00 00 00  mov  edi0x0  7f   48 83 c3 04  add  rbx0x4  83   e8 00 00 00 00  call  88  ltz14standardmethodv0x88gt   88   ba 01 00 00 00  mov  edx0x1  8d   be 00 00 00 00  mov  esi0x0  92   48 89 c7  mov  rdi  rax  95   e8 00 00 00 00  call  9a  ltz14standardmethodv0x9agt   9a   48 39 eb  cmp  rbx  rbp  9d   75 d9  jne  78  ltz14standardmethodv0x78gt   9f   48 83 c4 68  add  rsp0x68  a3   5b  pop  rbx  a4   5d  pop  rbp  a5   c3  ret  disassembly of section textstartup   0000000000000000  ltglobalsubiz14standardmethodvgt    0   48 83 ec 08  sub  rsp0x8  4   bf 00 00 00 00  mov  edi0x0  9   e8 00 00 00 00  call  e  ltglobalsubiz14standardmethodv0xegt   e   ba 00 00 00 00  mov  edx0x0  13   be 00 00 00 00  mov  esi0x0  18   bf 00 00 00 00  mov  edi0x0  1d   48 83 c4 08  add  rsp0x8  21   e9 00 00 00 00  jmp  26  ltglobalsubiz14standardmethodv0x26gt   as PRON can see  the compiler have automatically use mmx instruction to optimize PRON loop  and without have to add any cpu or library  specific annotation  so PRON would say that PRON claim that use simd be more difficult than use gpgpu technique in high  level code be not true  quite the contrary  
__label__finite-element __label__boundary-conditions __label__comsol __label__microfluidics PRON have an experimental fem model prepare for electrophoresis and dep study in comsol 52a   PRON be primarily use the electrostatics physics module and microfluidics of comsol module as of now   so far i have plot the electric field distribution use the following equation   sqrterer  ephi ephi ezez   specifically   sqrtes2eres2eres2ephies2ephies2ezes2ez   yes  PRON design be in r  phi  z coordinate system as evident from the equation   PRON question   a  PRON be not sure how do i plot the electric field gradient next ie ∇e2  the quantity should have unit of v2m3   b  be write the custom equation the only way to plot this   PRON believe something custom like the following should do the job    decnorme2x   decnorme2y   decnorme2z    but comsol do not allow PRON to use comma separate those differential   thank  
__label__algorithms __label__library do anyone know of a java library or algorithm for efficiently implement the second chebyshev function   to be clear  PRON be refer to this expression     varthetax   sump le xlog p    psix   summ1inftyvarthetaleftsqrtmxright  where  varthetax be the first chebyshev function and  psix be the second chebyshev function   thank very much   larry  
__label__pde __label__finite-difference __label__stability __label__fourier-analysis PRON have a question concern the von neumann stability analysis of finite difference approximation of pde  there seem to be a wealth of online source explain the application of this stability analysis to a few example case  most commonly the heat equation     frac  partial upartial t    alpha  frac  partial2 upartial x2  the procedure be also elaborate in the wikipedia article  so please refer to the link source for a detailed stability analysis on this equation  now PRON question  be the von neumann stability analysis applicable if PRON pde involve a constant term  c  ie some sort of source term     frac  partial upartial t    alpha  frac  partial2 upartial x2 c  would not introduce a constant prohibit one from eliminate the variable introduce in the fourier expansion  can the von neumann stability analysis apply here  and if yes  how   from PRON link  consider the definition of the round off error and the statement  since the exact solution must satisfy the discretized equation exactly  the error must also satisfy the discretized equation   this be actually only true if the pde be homogeneous  that is  if PRON can write PRON in the form   mathcalluut  ux  uxxldots0   with all term involve the dependent variable or PRON derivative   in PRON case however  PRON have  mathcalluc keep the notation from PRON link have assume both the numerical solution   nin and the true trajectory   uni satisfy the discretised equation  mathcall  hence use the linearity of  mathcall the round error satisfy  mathcallepsilon  mathcallnmathcallu   c  c0   or in other word the homogeneous version of the operator  this mean that the von neumann analysis on condition for the stability of the round  off error remain the same  
__label__applications PRON have do some research regard the application of machine learning to cyber  security  after these recent attack  PRON think that ai  base cyber defense can prevent PRON  PRON have also read about research regard the same in mit  and that ai can detect more than 80  of malware  be ai actually so promising in this department   there be project out there attempt to apply machine learning  ai to cyber  security in different way   one that PRON be familiar with be apache metron   another relate project be apache spot   PRON think if PRON read over the doc for these two project respectively  PRON will probably give PRON some good insight on this subject    hope this will give PRON glimpse on malware or viruses in detail  even  though i have include in some fictious scenarious but all fall inline  with the question   malware be a software design to invade or damage a computer system without the owner ’s informed consent  some malicious software be virus  worm  wabbit  trojanhors  exploit  backdoor  spyware  scumware  stealware  parasiteware  adware  rootkit  keylogger  dialer  hoax  PRON be important to be aware that however all of PRON have similar purpose but each one behave differently  due to different behavior of different malware  each malware group use different alternative to remain undetected  and antivirus software require improvisation to counter attack such virus for computer protection   so here come wannacry which be call ransomware  what be ransomware   ransomware be a particularly nasty type of malware that block access to a computer or PRON datum and demand ransom money to unblock PRON datum   wannacry ransomware virus attack be definitely one of the bad digital disaster to strike in year  increasingly PRON appear that the hacker behind PRON be amateur  the attack happen on friday  12 may 2017  a large cyber  attack use PRON be launch  infect more than 230000 computer in 150 country  and demand ransom payment  the attack have be do by multiple method  include phish email  doc  pdf  shopping website and on unpatched system as a computer worm  how do PRON work   when a computer be infect from the ransomware  PRON typically contact a central server to activate the information  and then begin encrypt file on the infected computer with that information  once all the file be encrypt  PRON post a message ask for payment to decrypt the file  datum – and threaten to destroy the information if PRON do not get pay  often with a timer attach to ramp up the pressure   the so  call wannacry be base on a security vulnerability in old version of microsoft windows  which be still run on many medical device today  the windows flaw be discover by the national security agency year ago  and publicize recently after hacker get a hold of the nsa file  the worm be a form of  ransomware  that infect computer and computer network  lock down critical file until the victim agree to pay a ransom   back to PRON question   can ai stop attack like wannacry   now let imagine that PRON have a program a smart anti  virus or close to  anti  virus with ai  and be regard to be a virus scanner which use heuristic to detect malware  and this scenario i will call anti  virus with ai as smart anti  virus  what be this smart anti  virus   smart anti  virus be one that can detect malware that have get capability of stop PRON activity when a system scan be go on  even if a virus have take control of the system  here i do not mind much which type of system or old version or current  PRON can be program to detect and avoid typical virus attack   so main task of this smart anti  virus would be  to analyse  scan  detect  prevent  terminate and also protect PRON be self from harm  so any ai PRON possess be likely to be direct at that goal  to use PRON intelligence effectively  here be ai technique which can be apply in antivirus detection   in a virtual environment the antivirus software classifie sequence or feature by PRON behavior by allow PRON  this new detection be more effectual in inspect system information  include system file  and diagnose which kind of malware be infect by differentiate with the traditional method  basically two technique be apply to detect malware use artificial intelligence and PRON be –  heuristic technique  heuristic technique be an artificial intelligence technique  be a method to solve a problem  commonly an informal method  PRON be particularly use to rapidly come to a solution that be reasonably close to the good possible answer or optimal solution   metaheuristic technique  metaheuristic be mainly apply to problem for which there be no adequate problem specific algorithm or heuristic  concrete method for virus detection use neural network can be implement  the main metaheuristic use some technique for the malware detection be pattern match  automatic learning  environment emulation  neural network  baye network  hide markov model  datum mining and other   a whitepaper publish by an major antivirus provider panda  securityis an perfect example of how  PRON antivirus have evolve with time to counterattack the different  malware which be become  basic  intelligent day by day   year 1990  first generation  purely base on signature detection and script heuristic   year 2000  second generation  personal firewall to identify and stop network worm base on packet signature  panda security integrate the smartclean functionality into the anti  malware engine  design to disinfect and restore the operating system from a spyware or trojan backdoor infection   year 2004  third generation  truprevent technology with behavioral analysis and behavioral blocking  adaptable to new malware exploit and technique   strong textyear 2010  fourth generation  real  time sensor network  automated malware collection  automated malware processing  and classification  automated malware remediation   this field be already a very hot research field and security company be spend fortune on research and development for develop product with build in artificial intelligence to counter such malware  symantec one of the big security product provider in the world have develop star  symantec ’s security technology and response  which have an engine  call sonar  a core part which scan and detect the malware   sonar system use artificial intelligence  technique to learn the difference between good and bad application  PRON look for sequence of suspicious behavior in run program that be uncharacteristic of legitimate software  when sonar observe such a suspicious sequence  PRON can terminate and remove the offend program immediately  without any virus fingerprint   advancement in the technique utilize by anti  virus software have make the average lifespan of malware shorter from month and week to day and hour  fight between anti  virus and malware still continue … … but ai be PRON hope to save civilization  however  organization which use legacy system be the main target or victim of malware   therefore  even though ai be actually so promising basing on the above  i would like to also have some hint on ai in fighting against malware which have also gain in intelligence   malware with ai can be use to accomplish these goal   the problem with this scenario be that   virus be usually develop by isolated individual with limited  resource  but scanning software be develop by large group and  corporation with much great human capital and resource   viruses usually have limit goal and do not support broad  development effort like the development of advanced ai   if ai be develop in service of virus  a thousand time more ai  would be develop to keep that ai in check and defeat PRON  so PRON  would not win that race   the virus  once release into the wild  be stick with PRON  programming  this give the scanner time to decompile and study  PRON  and come up with new counterattack that be not anticipate  by the virus writer   play the game  think of the cat  and  mouse operation of scanner and virus like a game  ai can be use to plot complex move and win this game  just like other game   currently  the good game playing software use deep learning with reinforcement technique   the problem with this scenario be that really  good ai require ton of top  end hardware to run effectively  computer virus need to be lightweight and nimble little piece of code that slip into and out of system and avoid detection  PRON can not really command the necessary resource to be truly smart   PRON suppose PRON could imagine an ai take over a whole network of computer  like a botnet do now  PRON could use the intelligence of the whole network to prevent the takeover of any one of PRON host  theoretically that could work  at least for a while  under present  day condition  PRON imagine that if such an attack be ever develop  a new defense would have to be develop for PRON   fictional scenario  PRON read a novel recently which spin out the smart virus scenario very  effectively and convincingly  PRON be  ai apocalypse   by william  hertling  and be the  second book in PRON singularity series   in this book  a russian student create a computer virus that use advanced evolutionary algorithm to evade detection  the trouble start with the smart virus start evolve out of control at superfast speed  soon the virus evolf into a civilization as smart as human but with machine speed and reflex  and of course PRON try to take over the world   this book be interesting in depict the smart virus as evolve into distinct character with different goal and personality  but retain a machine perspective  so PRON would recommend that and the rest of the series too  if PRON be a fan of sci  fi   the problem with this scenario be   there probably be no such thing as out  of  control evolution at the speed depict in the book   PRON be unrealistic to imagine a russian teenager whip up a virus  that display more intelligence and adaptability than the rest of the  world combine  even if PRON do happen to know about biology   even as virus get smart  overall security get smarter too  so a  smart virus of the future may face a mathematically impenetrable wall  of security that no virus could get through with any amount of  smart   in this way  anti  virus effort be not like a game  because PRON do not have to create a playing field that the smart virus could actually win   therefore  PRON would conclude by say ai can stop or prevent malware  but in near future  there be a possibility that even malware can become artificially intelligent  
__label__runge-kutta __label__numerical-modelling __label__adaptive-mesh-refinement __label__oscillations __label__numerical-limitations PRON be implement an ode solver use the cash  karp method on equation with the follow form     frac  d ed z    frac  1mu0c  frac  d 2 ed z2   frac  imu0ce tag1     and the output appear to be work fine  the second spatial derivative being approximate by backward finite difference   although when PRON convert the system to a system of first order ode     u  frac  partial epartial z  tag2    frac  d ud z    mu0c u  ie tag3     when use cash  karp on this new system  here be an image of the amplitude vs z output  the amplitude display unwanted high frequency oscillation  these oscillation should not be present and be result in the adaptive step become very slow since PRON be tackle large domain and vary parameter  boundary condition yet these oscillation remain constant at a spatial frequency of  50   textm1 PRON suspect the oscillation be due in part because the constant  mu0c approx 100  be large  yet a constraint be that PRON can not rescale the variable  domain  see  full explanation of problem    PRON have try adjust the adaptive step method and vary the tolerance  but these fast oscillation appear to remain  any suggestion on how to remove these numerical oscillation while use an explicit scheme would be of interest   
__label__machine-learning __label__pattern-recognition so PRON almost know a little about these subject  and PRON find PRON similar to each other  anybody can explain the difference   machine learning be a form of pattern recognition  machine learning be basically the idea of training machine to recognize pattern and apply PRON to particle problem  data science be the science of apply machine learn to practical problem such as create good search engine result or classify image  patten recognition be pretty much the umbrella term here  however  PRON think that the pattern recognition term be sort of fall out of style with how modern datum scientist be train neural network and other machine learning model  
__label__python __label__visualization __label__numpy __label__paraview how can PRON access a field in paraview s programmable filter as a numpy array   PRON want to   import an exist field as a numpy array  create a similar array for output  register PRON as a new field for visualization  PRON be familiar with the wiki  but PRON do not address the issue   give this example   elev0  inputs0pointdataelevation    elev1  inputs1pointdataelevation    output  pointdataappendelev1  elev0   difference    PRON would like to do  assume elev0 and elev1 be numpy array    diff  elev1  elev0  diff  diff  gt  10   10  output  pointdataappenddiff   difference    but this do not work as those be vtkarray object   read the paraview python api  find the follow solution to convert back and forth between vtkarray and numpy array  this use the numpysupport and vtkdatasetadapter module   from paraviewnumpysupport import vtktonumpy  from paraviewvtkdatasetadapter import numpytovtkdataarray  vtkdataarraytovtkarray  import numpy as np   get paraviewvtkdatasetadaptervtkarray object  vtkarr  inputs0pointdataelevation     convert to numpy array  shape be  nnode   for scalar   nnodes  ncomponents  for vector  nparr  vtktonumpyvtkarr    do math on a copy so that original field be not alter  nparr2  nparrcopy    nparr2   20   convert back to vtkarray  vtkarr2  vtkdataarraytovtkarraynumpytovtkdataarraynparr2     append new field to output  output  pointdataappendvtkarr2   new field    with paraview 42  PRON original code will work as expect  the follow be now acceptable   output  pointdataappendnumpyarray   name   
__label__statistics __label__matrices consider regular matrix approximation inequality    a  qqta    lt  e  where PRON try to approximate matrix  a by a low rank orthonormal matrix  q PRON have read an article on probabilistic algorithm for such an purpose  PRON simply pick random  gaussian etc   vector  omega  then generate vector from the range of  a by  y  aomega finally  construct  q by these vector   y  after gram  schmidt etc  detail can be find in  find structure with randomness  probabilistic algorithms for constructing approximate matrix decompositions    PRON question be the following   let PRON assume PRON have several matrix  ai with  same structure  and each have the same rank  k PRON may give detail on the structure if PRON interest  and PRON find  qi for each correspond matrix  ai again assume PRON may define a distribution over  qi depend on the distribution PRON use during approximate matrix  qi  the distribution of  omega    then  PRON get a new matrix  a with again same structure and rank  k but this time PRON only have a submatrix  aj ie only several column  PRON want to approximate  q for this new matrix use only give column   to summarize PRON have two phase   1  PRON somehow learn a model for  q use give full matrix  ai  2  give a subsample  aj of a novel matrix  a  that be not present in the first phase   PRON derive PRON corresponding  q  PRON hope  PRON may simply recommend PRON some useful reference  PRON do not even know where to look   thank PRON for PRON time   
__label__linear-algebra __label__matrices __label__reference-request give two matrix  a and  b  PRON would like to find vector  x and  y  such that     min sumij   aij   xi yj bij2     in matrix form  PRON be try to minimize the frobenius norm of  a  mboxdiagx  cdot b cdot mboxdiagy   a  b circ  x ytop  in general  PRON would like to find multiple unit vector  x and  y s in the form    min sumij   aij   sumk1n si xik   yjk   bij2     where  si s be positive real coefficient   this be equivalent to singular value decomposition  svd  when   bij   1  do anybody know what this problem be call  be there a well  know algorithm like svd for the solution of such problem    migrate from math  se   this be far from generalize svd   if b be a positive matrix  PRON can use PRON package birsvd  httpwwwmatunivieacatneumsoftwarebirsvd  the paper httpwwwmatunivieacatneumsoftwarebirsvdsvdincompletedatapdf  describe the method there also give reference that PRON may consider to do a literature search  
__label__sparse __label__compressive-sensing PRON be try to get a conclusive numerical value for mse as the performance metric of a few cs sparse recovery algorithm  to do this  PRON vary the number of measurement   m  take from an  n dimensional vector with  m much less than  n use the sense matrix  a   mtimes n   the goal be find the mse at each  m  at the moment  PRON be recover randomly generate sparse signal sense use random sensing matrix   a   for 30 sense matrix and 30 signal for each  a  this total 900 trial for a single  m PRON be curious if there be a low limit to the number of trial PRON should perform for each  m so that PRON get a conclusive value of mse  how do PRON defend against a person who say PRON should do more   kindly point PRON to the appropriate forum if the question be not relevant to computational science   
__label__beginner PRON be new here  PRON hope this kind of question be legit   PRON have a background in mathematic  PRON field be probability theory  where PRON be quite strong  top 2  of university    PRON have little experience in programming but PRON guess PRON get thing do once PRON know what be out there  ie what program language can do for PRON    PRON want to learn more about datum science and dive into the field particular when PRON come to programming  PRON would also like to get go on manageable yet interesting datum science relate task  build some  usefull  statistic skill may be desirable  too    PRON know kagglecom be there other place PRON could look into   any project PRON may take on   thank a lot    if PRON be new to both machine learning and programming  try take a look at this guy s tutorial  httpwwwpyimagesearchcom PRON have be use PRON for a while  and PRON think PRON be great because of how visual  hand  on  and practical PRON be  for example  PRON have facial recognition and handwriting recognition lesson  PRON be use what PRON learn in PRON tutorial to build a self drive remote control car  similar to this  httpszhengludwigwordpresscomprojectsselfdrivingrccar    PRON would check out the follow 2 site as really good starting point   httpwwwbecomingadatascientistcom  interviews  resource  challenge  amp  forum  httpdatasciencemastersorg  web resource for self  edification 
__label__feature-extraction __label__sequential-pattern-mining here be all 34 row of PRON training datum    20   0  0  0  4  0  0  5  0  0  0  0  0  0  0  0  0  0  6  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     15   0  0  0  4  4  0  0  0  0  0  0  0  0  0  0  0  0  3  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     9   0  0  0  4  4  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     20   0  0  0  6  6  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     16   0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  5  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     17   0  0  0  3  4  0  0  0  2  0  0  0  0  0  0  0  0  0  3  0  0  4  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     14   0  0  0  1  0  0  0  3  4  0  2  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     13   0  0  0  5  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     17   0  0  0  3  3  3  0  0  0  0  0  0  0  0  0  0  0  3  3  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     22   0  0  0  4  5  0  0  0  0  0  0  0  0  0  0  0  0  4  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     19   0  0  0  4  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     20   0  0  0  5  0  0  0  0  0  0  0  1  0  0  0  0  0  0  4  6  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     15   0  0  0  5  4  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     20   0  0  0  6  5  0  0  0  0  0  0  0  0  0  0  0  0  5  0  3  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     13   0  0  0  1  2  2  0  0  2  0  0  0  0  0  0  0  1  4  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     20   0  0  0  5  5  0  0  0  0  0  0  0  0  0  0  2  0  2  5  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     11   0  0  0  6  3  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     21   0  0  0  5  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     20   0  0  0  5  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  3  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     21   0  0  0  5  5  0  0  0  0  0  0  0  0  0  0  0  0  5  4  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     22   0  0  0  3  5  0  0  0  0  0  0  0  0  0  0  0  0  4  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  3  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     23   0  0  0  4  6  0  0  0  0  0  0  0  0  0  0  0  0  5  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     20   0  0  0  4  7  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  0  0  0  0  2  1  0  0  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     24   0  0  0  1  4  6  0  1  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  1  0  1  0  0  1  1  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  1  0  0  0  0  0  0  0     22   0  0  0  6  1  0  0  2  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  0  0  1  2  0  0  0  0  0  0  2  0  0  0  0  0  0  0  1     23   0  0  0  5  6  0  0  0  0  0  0  0  0  0  0  0  0  3  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     22   0  0  0  2  7  0  0  2  1  0  0  0  0  0  0  0  0  0  2  0  0  2  0  0  1  1  0  0  0  0  0  0  2  0  0  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     22   0  0  0  6  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  2  0  1  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     24   0  0  0  4  5  2  1  0  0  0  0  0  0  0  0  0  0  0  0  3  3  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  3  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     23   0  0  0  6  5  0  0  0  0  0  0  0  0  1  0  0  0  2  2  1  0  0  0  0  0  0  0  0  0  0  0  2  3  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     22   0  0  0  5  3  0  3  0  0  1  0  0  0  0  0  0  0  2  3  1  0  0  0  0  0  0  0  0  0  0  0  2  1  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     24   0  0  0  5  7  0  0  0  0  0  0  0  0  0  0  0  0  4  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     23   0  0  0  1  0  0  0  0  6  5  0  0  0  0  0  0  0  0  0  0  0  0  2  2  0  0  0  0  1  0  0  0  0  0  0  0  3  0  0  0  0  2  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0     21   0  0  0  4  5  0  0  2  0  0  0  0  0  0  0  0  0  0  2  0  0  3  0  0  0  0  0  0  0  0  0  0  3  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0    PRON be try to create a number sequence generator that  give the first number in the tuple  like 22   PRON produce a list of number that look like the second    0  0  0  6  1  0  0  2  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  0  0  1  2  0  0  0  0  0  0  2  0  0  0  0  0  0  0  1   PRON would like PRON to automatically discover the constraint  the sum of the list equal the first number  some position in the list be always zero  etc    be there something that could produce this backward classifier  PRON would prefer PRON to be some library in python which do not require gpu to run   PRON could reframe the problem as a regression  prediction of a single real  value dependent variable base on several independent variable  if PRON choose to model with regression  then PRON technically a  calibration  problem  a calibration problem take a know observation of the dependent variable be use to predict correspond explanatory variable  
__label__matlab __label__libraries __label__automatic-differentiation be there any open  source auto  differentiation library for matlab  PRON be aware of commerical package such as httptomoptcomtomlabproductsmad and plenty of c library  but PRON can not find much more matlab other than httpwwwmathworkscommatlabcentralfileexchange15235automaticdifferentiationformatlab which be not a major package   add  as a bare minimum of functionality  PRON need to be able PRON to handle vector as variable  in that mathworks package PRON reference  for example  here be a test case   val   1 2 3    some value  x  myadval    create a vector with that value   x    4 5 6    this should give the derivative  4  5  6   xinner  sumx   x to get the inner product   the derivative here be  2 4 6  here   ideally  the above would simply be xinn  x  x   this article in siam review describe how to implement operator  overload automatic differentiation in matlab  and give a good introduction to the technology   if PRON be really interested in open source and like to promote  support PRON  then PRON should probably take a look at the open source matlab clone octave  there be an extension to PRON  call ad that support automatic forward differentiation  unfortunately  PRON seem to be unmaintained currently   the wikipedia page for automatic differentiation have link to many software package or library  include several for matlab   httpenwikipediaorgwikiautomaticdifferentiationsoftware  the sintef matlab reservoir simulation toolbox include a gpl  license ad library  the usage be mostly gear towards numerical application in subsurface flow  but the library PRON be usable for more general purpose   here be a basic runthrough of PRON example as PRON would run PRON from the base directory of mrst   startup    load ad base module  mrstmodule add ad  fi  val   1  2  3    x  initvariablesadival    y  x4  5  6    disp  jacobian    fullyjac1    z  sumxx    disp  jacobian    fullzjac1    jacobian    mrst ad also support multiple vector value function  a  ones5  1    b  ones5  1     a  b   initvariablesadia  b    c  ab  2b   fullcjac1    fullcjac2    there exist a work  in  progress user guide at the website  where chapter 7 have some usage of the ad library  this library be primarily gear towards problem that require all jacobian value and be vectoriz use forward mode only  PRON do include some nice thing though  like 2d table interpolation and so on  PRON have be use for moderate size non  linear problem  order of 500000 unknown    the non  linear solver and component be heavily focused on reservoir application at the moment  this part of the library be presently be rewrite and may in the future have some use outside of this specific domain  however  if PRON just want the ad part  the adi class and the initvariablesadi function should be sufficient   disclaimer  PRON be one of the developer attach to this research group   PRON realize this be an old question  but when look for this PRON today PRON find adigator  which be open source  and seem to handle vector  PRON have not test PRON yet PRON  but PRON seem to be actively develop  
__label__neural-network __label__convnet __label__gradient-descent convolutional neural networks  cnns  use almost always the rectify linear activation function  relu      fx   max0  x  however  the derivative of this function be    fx   begincas  0  amptextif  x leq 0 1amptextotherwiseendcases   ignore that be not differentiable at  0   as PRON think PRON be do in practice   for input  0 this be fine  but why do not PRON matter that the gradient be 0 at every point  lt  0  or do PRON matter   be there publication about this problem    if a neuron output 0 for every sample of the training datum  PRON be basically lose  correct  PRON weight will never be adjust again    ignore that be not differentiable at 00  as PRON think PRON be do in practice  yes see  relus be not differentiable at zero  if a neuron output 0 for every sample of the training datum  PRON be basically lose  correct  PRON weight will never be adjust again   yes see what be the  quotdying reluquot  problem in neural network  
__label__pde __label__finite-element __label__boundary-conditions in general  dirichlet boundary condition will not be satisfy exactly for fem for non  homogeneous boundary condition  the fem code PRON have see set the degree of freedom to interpolate the dirichlet boundary condition but PRON have not find any mathematical justification for this  PRON seem to PRON that set essential boundary condition should probably minimize some functional of the error  eg minimize  u uh  over the portion of the boundary that the dirichlet bc be apply  even though this would be more computationally expensive   be there any justification for set the bc like this and if so  what would the proper norm be   there be mathematical justification for set dirichlet boundary degree of freedom to a value  however  PRON should adjust PRON variational form accordingly  if PRON be look at a general problem  say   find  uinmathcalu such that   au  wlw    forall winmathcalv  where   mathcaluuint nabla u2  lt  infty  u  gtext  on  gammad   mathcalvuint nabla u2  lt  infty  u0text  on  gammad  instead PRON can write  u  v  g where  vinmathcalv and  g be the dirichlet condition  then the variational form become   avg  wlw  or by use the linearity of  a   av  wlwag  w  in a finite element code  PRON can form PRON element stiffness matrix as if there be no boundary condition  then PRON take the column of the local matrix which correspond to the dirichlet boundary condition  scale PRON by the coefficient PRON want to enforce  and subtract PRON from the right  hand  side  this be the discrete form of what PRON write above   ag  w then PRON zero out that column and the correspond dirichlet row  place a 1 in the diagonal and the coefficient PRON wish to enforce  this decouple the equation from the system and yet set the value PRON wish to enforce   PRON recommend the finite element method  linear static and dynamic finite element analysis  by tom hughes  PRON have an expand discussion of this issue start on page 8  
__label__deep-learning __label__nlp __label__word-embeddings __label__reference-request PRON have research about this problem and do not find any specific solution or code or methodology  PRON have a resume database and i parse the resume use regular expression and have field like job title  education etc  PRON want to build a taxonomy  ontology of job title  be there any proven way in nlp or deep learning which achieve this    
__label__machine-learning __label__scikit-learn PRON be use gaussian process regressor to fit datum for a bayesian optimiser  this be a relevant part of PRON python code   from sklearngaussianprocess import gaussianprocessregressor  from sklearngaussianprocesskernel import  rbf  matern  rationalquadratic   expsinesquared  dotproduct   constantkernel   kernel  maternlengthscale  lensc  nu25   gp  gaussianprocessregressorkernel  kernel   l210  xi  npconcatenatenpranduniform100300l26npranduniform0303l242npranduniform00300l21npranduniform300300l21npranduniform0003l26npranduniform50200l26axis1   yi  maplambda x  f1x   xi   gpfitxi  yi   while1     some code to get the new values cc and nv  xi  numpyrowstackxi  cc    yi  npconcatenateyinv     gpfitxi  yi   each iteration of the bayesian optimiser  PRON add a new element to  xi and  yi and fit  gp  the gaussian process  again  as the size of  xi and  yi reach over 200  the time take to fit the datum become noticeable  this decrease the overall efficiency of PRON bayesian optimiser   the bayesian optimiser have to make around 1000 iteration  each iteration need to fit the datum all over again  so  PRON be wonder if there be a way to incrementally fit the datum  if that be not already be do by sklearn s gaussianprocessregressor   thank PRON   PRON think with gpr  the computational cost increase on an order of n3  so the amount of time with more data increase drastically   PRON do not think the current version of sklearn s gpr implement the kind of incremental fit PRON be expect  with large datum and iteration  PRON suggest implement PRON own version on a gpu or something since gpr implementation be not that difficult  
__label__deep-learning __label__notation __label__activation-function in many case an activation function be notate as g  eg andrew ng s course course   especially if PRON do not refer to any specific activation function such as sigmoid   however  where do this convention come from  and for what reason do g start to be use   the addition of the activation layer create a composition of two function    a general function  to be define for a particular context  be usually denote by a single letter  most often the low  case letter f  g  h   so PRON come down to the reason that PRON use the hypothesis representation hxwxb which be a function  and that be wrap by an activation function denote as g the choice of g seem to be purely alphabetical  
__label__machine-learning __label__r __label__xgboost PRON be work on a project and PRON be use xgboost to make prediction  PRON colleague send PRON the model file but when PRON load on PRON computer PRON do not run as expect   when PRON change one variable from the model from 0 to 1 PRON do not change the result  in 200 different line   so PRON start to investigate  PRON compare a lot of different result and PRON be all different   PRON run the xgbtree PRON show the maxdepth be 0  but PRON be suppose to be 4  when PRON run xgbtreeresults PRON say the maxdepth  4   PRON also try a lot of different save method  rda  rds  model  but none of PRON work   any suggestion would be welcome  thank   edit  post the sessioninfo    PRON   r version 325  2016  04  14   platform  x8664w64mingw32x64  64bit   run under  windows  gt 8 x64  build 9200   locale    1  lccollate  englishunit states1252  lcctype  englishunit states1252   3  lcmonetary  englishunit states1252 lcnumeric  c   5  lctime  englishunit states1252  attach base package    1  stat  graphic  grdevice util  dataset  method  base  other attach package    1  plyr184  car21  4  acepack141  ckmeans1ddp346  4   5  hmisc40  0  formula12  1  survival240  1  memisc0997  1   9  mass73  45  information009  minerva145  randomforest46  12   13  proc18  xgboost04  4  caret60  73  lattice020  33   17  ggplot2220  misctools06  22  reshape086  datatable198   21  dplyr050  e107116  7  lubridate160  psych169   25  readr100  stringr110  stringi112  load via a namespace  and not attach     1  rcpp0128  class73  14  assertthat01  digest0610   5  foreach143  r6220  matrixmodels04  1  stats4325   9  lazyeval020  minqa124  sparsem174  nloptr104   13  rpart41  10  matrix12  4  labeling03  splines325   17  lme411  12  foreign08  66  munsell043  compiler325   21  mnormt15  5  mgcv18  12  htmltools035  nnet73  12   25  tibble12  gridextra221  htmltable17  codetools02  14   29  modelmetrics110  grid325  nlme31  125  gtable020   33  dbi05  1  magrittr15  scales041  reshape2142   37  doparallel1010  latticeextra06  28 rcolorbrewer11  2  iterators108   41  tools325  parallel325  pbkrtest04  6  colorspace13  1   45  cluster203  knitr1151  quantreg529  mine   r version 331  2016  06  21   platform  x8664w64mingw32x64  64bit   run under  windows  gt 8 x64  build 9200   locale    1  lccollate  portuguesebrazil1252  lcctype  portuguesebrazil1252  lcmonetary  portuguesebrazil1252   4  lcnumeric  c  lctime  portuguesebrazil1252  attach base package    1  stat  graphic  grdevice util  dataset  method  base  other attach package    1  caret60  73  ggplot2210  lattice020  33  plyr184  xgboost04  4   6  shinydashboard053 shiny0142  load via a namespace  and not attach     1  rcpp0128  nloptr104  iterators108  tools331  digest0610  lme411  12   7  jsonlite11  nlme31  128  gtable020  mgcv18  12  matrix12  6  foreach143   13  parallel331  sparsem172  stringr110  matrixmodels04  1 stats4331  grid331   19  nnet73  12  datatable196  r6213  minqa124  reshape2142  car21  3   25  magrittr15  scales040  codetools02  14  modelmetrics110 htmltools035  mass73  45   31  splines331  rsconnect043  pbkrtest04  6  mime05  xtable18  2  colorspace12  6   37  httpuv133  quantreg529  stringi111  munsell043  chron23  47  the error be due to the r version  PRON colleague be run a 325 and PRON be run the 331  and thank to stereo PRON could notice this and test on the same version  
__label__python __label__numpy PRON wish to implement the follow expression in python      xi  sumj1i1ki  j  jai  jaj      where  x and  y be numpy array of size  n  and  k be a numpy array of size  ntime n the size  n may be up to about 10000  and the function be part of an inner loop that will be evaluate many time  so speed be important   ideally PRON would like to avoid a for loop altogether  though PRON guess PRON be not the end of the world if there be one  the problem be that PRON be have trouble see how to do PRON without have a couple of nest loop  and that be likely to make PRON rather slow   can anybody see how to express the above equation use numpy in a way that be efficient  and preferably also readable  more generally  what be the good way to approach this sort of thing   here be a start  first  PRON apology for any mistake   PRON experiment with a couple of different approach  PRON be a bit confused by the limit on the summation  should the upper limit be  i  rather than  i1    edit  no  the upper limit be correct as provide in the question  PRON have leave PRON as be here because another answer now use the same code  but the fix be simple   first a loop version   def loopedverk  a    x  npemptylikea   for i in rangexsize    sm  0  for j in range0  i1    sm   ki  j  j   ai  j   aj   xi   sm  return x  PRON make PRON a single loop with numpy slice   def vectorizedverk  a    ktr  zeroslikek   ar  zeroslikek   sz  lena   for i in rangesz    ktrii1   k1diagonalszi1   a   ai1   arii1   a1   a   return npsumktr  ar  1   the numpy version with one explicit loop be about 25x faster on PRON computer when  n5000  then PRON write a cython version of the  more  readable  loop code   import numpy as np  import cython  cimport numpy as np  cythonboundscheckfalse   cythonwraparoundfalse   def cythverdouble    1  k not none   double   a not none    cdef double    x  npemptylikea   cdef double sm  cdef int i  j  for i in rangelena     sm  00  for j in rangei1    sm  sm  ki  j  j   ai  j   aj   xi   sm  return x  on PRON laptop  this one be about 200x faster than the loop version  and 8x faster than the 1loop vectorized version   PRON be sure other can do good   PRON play with a julia version  and PRON seem  if PRON time PRON properly  comparable to the cython code   here be the numba solution   on PRON machine the numba version be  1000x faster than the python version without the decorator  for a 200x200 matrix   k  and 200length vector  a     PRON can also use the autojit decorator which add about 10 microsecond per call so that the same code will work with multiple type   from numba import jit  autojit  jitf8f8f8         autojit  def loopedverk  a    x  npemptylikea   for i in rangexsize    sm  00  for j in range0  i1    sm   ki  j  j   ai  j   aj   xi   sm  return x  disclosure  PRON be one of the numba developer   what PRON want seem to be a convolution  PRON think the quick way for achieve PRON would be the numpyconvolve function   PRON may have to fix the index accord to PRON exact need but PRON think PRON would like to try something like   import numpy as np  a   1  2  3  4  5   k   2  4  6  8  10   result  npconvolvea  ka1   
__label__algorithms __label__predictive-modeling __label__regression __label__correlation PRON have correlation value for profit base on three different attribute  attribute1attribute2attribute3  now PRON want to make prediction for the profit base on these 3 different correlation value   PRON want suggestion for some efficient algorithm that can help PRON to make prediction  from these correlation value   note  PRON be new to this so PRON would appreciate some suggestion if there be some lack of understanding in PRON   the correlation coefficient will tell PRON  how much predictive power the individual attribute posess with respect to profit in a linear sense   easier  the correlation coefficient tell PRON how line  like PRON data be    there may still be a very predictive arc like structure in PRON datum which the correlation coefficient can not capture   so PRON can either calculate a regression line for profit with the attribute have the high correlation coefficient  or PRON can calculate a hyperplane for all three attribute to get the desire prediction  regression line and hyperplane be usually calculate by least  square fitting   but beware to overestimate predictive power  because PRON be extrapolation what PRON do  new datum may behave different than old datum which make extrapolation potentially less exact the further PRON be away from know datum   PRON be sorry to disappoint PRON but correlation do not imply causation    PRON need to make sure that the data be statically significant  enough datum  the difference be not negligible  etc   PRON would suggest z  test  or chi  square  depend on the nature of PRON datum  
__label__python __label__data-cleaning today PRON encounter this strange behavior in python do datum manipulation  why change a will affect b below    gtgtgt  a    hello   1  2    gtgtgt  b  a   gtgtgt  a0   5   gtgtgt  b   5  1  2   PRON only ask a to change  why be b change  but the follow be fine    gtgtgt  a  3   gtgtgt  b  4   gtgtgt  a  3   gtgtgt  b  4  PRON guess be that PRON be do pass by reference     but in both case be pass by reference  what be go on here   in PRON first case  list what PRON be do be copy a list  which be reference a list to another list  ex  list b be reference by list a    in order to avoid that  PRON have to consider deepcopy in python in order to safely make edit to the copy list without those change be reflect in the original   for example   import copy  a    hello   1  2   b  copydeepcopya   a0   5  printb    this will give the original list   gtgt    hello   1  2   this be the documentation that will be helpful   another way to get around as sophie mention in the comment be slice   b a     a0   5  printa    gtgt   5  1  2   printb    gtgt    hello   1  2   in PRON second case  PRON be just a reference to a variable as PRON mention in the question  
__label__optimization __label__pde if PRON have a partial differentail equation such as  fracpartial2 upartial t2   c2 fracpartial2 upartial x2  with boundary condition  u0t   0  and  u1t0   PRON can solve this system use separation of variable   u  xxtt this mean PRON have an ode   xx    c2lambdaxx0   with  lambda as separation constant   to look for solution PRON set up the wronskian and search for  lambda value for which the wronskian determinant be zero  mean PRON find an eigenvalue  for  easyboundary condition this can also be do manually  however PRON want to do PRON numerically   now PRON look over a range of  lambda  look for minima  and optimize around those minimum   know how the wronskian be set up  be there an optimization method or numerical trick PRON can perform to speed up the process   
__label__finite-element __label__pde __label__numerical-analysis __label__fluid-dynamics __label__navier-stokes  PRON have ask this question on mathoverflow too   let   tgt0    i0t   dinmathbb n   lambdasubseteqmathbb rd be nonempty and open    mathcal vleftphiin ccinftylambdamathbb rdnablacdotphi0right and   voverlinemathcal vleftcdotrighth1lambdamathbb rdhoverlinemathcal vleftcdotrightl2lambdamathbb rd   operatorname ph denote the orthogonal projection from   l2lambdamathbb rd onto  h   a0udelta u for  uinmathcal da0h0  1lambdamathbb rdcap h2lambdamathbb rd    auoperatorname pha0utextfor  uinmathcal damathcal da0cap v and   bu  vucdotnablavtextfor  uin l2lambdamathbb rdtext  and  vin h1lambdamathbb rd   f  ito h   uin l2imathcal da with  uin l2i  h and   uta0utbututnabla ptfttextfor all  tin itag1 for some  p  ito h1lambda  assume that  lambda be sufficiently regular such that   1 be well  define  PRON can be show that   1 be equivalent to   utautoperatorname phbututfttextfor all  tin itag2 PRON want to solve   2 numerically and PRON be only interested in  u  and not in  p    PRON know that there be many reference for the numerical study of   1 however  PRON seem to PRON that all the consider scheme do not use   2 PRON only use   2 for theoretical result like existence and uniqueness of solution  maybe PRON be wrong and PRON just do not see that these scheme use   2  in any case  PRON question be  be PRON able to provide a numerical scheme which solve   2 directly   or be there something which prevent PRON from do that  PRON idea be to apply  for example  a semi  implicit oseen discretization in time  ie consider   fracutnutn1hautnoperatorname phbutn1utnftntextfor all  ninleft1ldot  nrighttag3 with   tnnhtextfor  ninleft0ldot  nright and  ht  n for some  ninmathbb n after that for each  ninleft1ldot  nright   3 should be solvable by a finite element method  or be there some problem that PRON do not see     the something that prevent PRON from use such a scheme with projection be the numerical realization of the projector  phcolon l2 to h  PRON do not know of a formulation that be better accessible to numerical algorithm than expression of the projection via     v0  phv leftrightarrow  beginbmatrixi  amp  nabla  div  amp  0endbmatrix   beginbmatrixv0   endbmatrix    beginbmatrixv  0endbmatrix       where the    be a dummy variable   which PRON would approximate by mean of mixed finite element   which  probably  would realize   3 as a standard mix fem of the navier  stokes in the   v  pformulation   so  if PRON have a reasonable numerical realization of  ph  there be nothing wrong with the scheme sketch in   3 
__label__machine-learning __label__classification PRON be try to build a classifier that will classify datum point of different class  but which significantly overlap  but PRON be not sure how to good approach the problem  PRON have read that tree model can perform well in this situation  and that support vector machines may as well  but PRON be unclear what other approach may be good for tackle this problem  can anyone give PRON some direction   PRON would say that PRON strongly depend on the nature of PRON label  why PRON overlap  PRON be use to work with fish and PRON be use to get several false  negative datum  PRON be simple  there be not enocuh fish to occupy every suitable place  in that case PRON usually favour presence datum  however  the input variable  feature could be not good for an adequate discrimination and that be why PRON appear overlapped   PRON would need additional info    PRON can favour one or more label penalize other however that be always at the expense of accuracy  in that case svm with case weight may work  likewise other will if PRON allow the inclusion of case weight  good luck   there be many approach PRON could take here  assume datum overlap in such a way that the class be still distinguishable from a human expert — human performance be generally regard as the performance ceiling in supervised classification task — PRON can approach this from the classifi side of the feature side   first  PRON would consider the feature representation PRON be use for PRON task  a classifier be only as good as the information PRON give PRON  and there be many way PRON can adjust how PRON be represent PRON input datum  for example  if PRON be work in text classification  use a unigram representation will typically lead to different performance as compare with a bigram representation  similarly  there may be non  linear transformation PRON can apply to continuous  value datum that a classifier may not pick up on PRON own  PRON should consider the domain in which PRON problem exist and create a set of feature generation and feature selection cross  validation experiment use a hold  out training datum set   next PRON would consider what alteration PRON can make on the classifier end  if PRON datum be highly skewed  ie  one class be much more prevalent that another   PRON may not make sense to use an algorithm like naïve bayes  at least not with out some sample approach  if PRON problem be not high  dimensional  PRON recommend generate a graphic that will let PRON explore the spatial breakdown by class  there be certain overlap pattern that linear classifier like support vector machines will not be able to classify without use a kernel function  such as a radial basis function  
__label__machine-learning __label__python __label__regression __label__optimization __label__online-learning PRON have this dataset  and PRON be use y   a  xn    b  xn  hill function as the model  where a be the limit of the hill curve  b be the point at which a2 be reach  for n  1  and n be the cooperativity or steepness of the curve   currently  PRON be store all x  y value  compute the parameter from scipyoptimizecurvefit  and plot the curve  if new datum point come along  PRON re  calculate the parameter with the oldnew datum   be there a way to update the parameter of the model without store all of the previous old data point  once the initial parameter be obtain from the previous data point   example  PRON fit the curve to the first 1000 data point and have PRON parameter  next  PRON discard some or all of the old datum  then  when PRON see the 1001st point PRON simply update PRON parameter and plot the curve again and so on for every new data point   edit  PRON exist code be as follow  not super elegant    import matplotlibpyplot as plt  from scipyoptimize import curvefit  def filestreamfilename    with openfilename   r   as infile   for line in infile   yield mapfloat  linestripsplitt     def hillmodelx  a  b  n    return  floata  xn     b  xn  for x in x   def getparamsxall  yall  prevpar  none  fn  hillmodel    if prevpar be none   ainit  binit  ninit  yall1   yall0   10  else   ainit  binit  ninit  prevpar  optpar  optcov  curvefitfn  xall  yall  p0ainit  binit  ninit    afinal  bfinal  nfinal  optpar  return afinal  bfinal  nfinal  def main     filename    datatsv   filestreamer  filestreamfilename   xall  yall          get some intial datum from stream  for  in xrange1000    x  y  filestreamernext    xallappendx   yallappendy   pltscatterxall  yall    initialize param of model  a  b  n  getparamsxall  yall   ymodel  hillmodelxall  a  b  n   pltplotxall  ymodel   r    pltshow     rolling update  seenall  false  help stop when all datum be fit  while true   for  in xrange1000    try   x  y  filestreamernext    xallappendx   yallappendy   except   seenall  true  break  a  b  n  getparamsxall  yall  prevpara  b  n   fn  hillmodel   ymodel  hillmodelxall  a  b  n   pltscatterxall  yall   pltplotxall  ymodel   r    pltshow     nothing more to update  return  if seenall   return  if   name        main      main    the code currently read in some x  y value  calculate the a  b  n parameter and when more x  y value be add  the code update a  b  and n param  as PRON can see  PRON need to store previous x  y value  which PRON do not want  PRON want to update the parameter as new x  y value be see and from the previous a  b  and n value only   
__label__fluid-dynamics __label__boundary-conditions __label__computational-physics __label__navier-stokes PRON have some question about the implementation of non  reflect outflow boundary condition for navier stokes equation   follow  poinsot  lele  boundary conditions for direct simulations of compressible viscous flows   pirozzoli  colonius  generalized characteristic relaxation boundary condition for unsteady compressible flow simulation   both author suggest an equation like  beginequation   fracdmathbfubdt   mathbfdub    mathcalmathbftub     mathbfsubquadquad  1   endequation   where  mathbfub   rho  rho u  rho v  rho w  rho e be the conservative variable at the bind b   mathbfdub be a certain characteristic treatment of x  flux   mathcalmathbftub be the transverse therm   mathbfsub be the source therm   now consider a finite difference method and suppose that the bound be locate at  n12   as usual in this problem   consider furthermore  n1  n2dot  ngn ghost point   equation  1  can be apply just in  n12   so PRON question be   how can PRON discretise the derivative in  n12  with some fd scheme in order to solve  1  at this location   how can PRON treat the ghost nod   
__label__python __label__performance __label__machine-learning __label__gpu __label__cuda PRON be previously run PRON neural network use the lasagne library to build and train neural network in theano on an nvidia gtx 750 ti  PRON be use a genetic algorithm to tune the hyperparameter of PRON neural network  so PRON buy a gtx 780 ti to speed up train each neural network  the 780 ti should have over 3 time the single precision float point performance of the 750 ti  but PRON do not see any speed  up at all   PRON check gpu utilization in gpu  z and PRON be the same for both gpu  mid  high 60s   why be the beefy gpu use the same percentage of resource  but not run any faster   PRON uninstall all nvidia software include the driver  restart twice in the process   then re  instal everything use the cuda 75 toolkit installer and then PRON instal the cudnn 75  5   library  header  binary  PRON restart PRON neural network and PRON be still perform as poorly as before   some probably important information   this be on windows 7  for nvcc PRON be use cl  version2013 because that be what PRON have and PRON work fine  cpu be an intel core i5  2500 k  33ghz  8 gb system memory  data be load off of a samsung ssd 850 evo 500 gb drive  3 gb of gpu memory  other spec can be find at the link above   card be plug into a pci  e gen2  1x16  slot  the network vary a lot  but PRON be all multilayer perceptron with between 1  5 hide layer  15  150 neuron per layer  all denselayers  75 input  10 output  currently no drop  out or gaussian noise before any layer  batch size be 1024  data be shuffle before each epoch  hidden layer activation function be all elu  output activation function can be sigmoid  tanh  or elu  loss function be mean squared error  updates use nesterov momentum  training data size be 274 mb with 934918 sample  validation data size be 305 mb with 103880 sample  PRON first thought be that PRON network be too small to make any difference on a small vs big gpu  but the big gpu be stress just as much as the littler one when run  plus these network be run faster on the grid k520 gpus on amazon ec2  on ubuntu   both the k520 and 780 ti use the kepler gpu architecture while the 750 ti be nvidia s first maxwell card    what could be cause PRON new card to be so slow  how can PRON get PRON to run faster than the old crappier card   edit  run the nn on ubuntu PRON see a significant speed  up  not as big as PRON hop  but definitely noticeable  PRON be probably not make full use of the gpu with PRON small network  one possible reason for the slowness in windows be wddm mode vs tcc mode  https devtalknvidiacom default  topic895331cuda  setup  and  installation  tesla  k20vs  titan  x  performance  for  the  same  code  post47248844724884  PRON be go to try instal PRON old card in a pci  e 1x slot and make that the display driver and use PRON new card for computation only  edit the second  here be what one of PRON neural network would look like   inputvar  tmatrixinput    input  lasagnelayersinputlayershape1024  75   inputvar  inputvar   network  lasagnelayersdenselayerinput  40   nonlinearity  lasagnenonlinearitieselu   w  lasagneinit  glorotuniform     network  lasagnelayersdenselayernetwork  113   nonlinearity  lasagnenonlinearitieselu   w  lasagneinit  glorotuniform     network  lasagnelayersdenselayernetwork  76   nonlinearity  lasagnenonlinearitieselu   w  lasagneinit  glorotuniform     output  lasagnelayersdenselayer   network  numunits10   nonlinearity  lasagnenonlinearitiestanh   prediction  lasagnelayersgetoutputoutput   loss  lasagneobjectivessquarederrorprediction  targetvar   loss  lossmean    update  lasagneupdatesnesterovmomentumloss  param  learningrate05  momentum05   testprediction  lasagnelayersgetoutputoutput  deterministic  true   testloss  lasagneobjectivessquarederrortestprediction  targetvar   testloss  testlossmean    trainfn  theanofunctioninputvar  targetvar   loss  update  update   valfn  theanofunctioninputvar  targetvar   testloss   PRON suppose  PRON right and PRON network be not that big to 100utilize the gpu  the bottle  neck here seem to be not the gpu PRON  but the transfer rate between ram and vram and here the difference between 750 ti and 780 ti be not that significant  PRON can try to improve the training speed by hide the latency of memory transfer  PRON have to assure PRON transfer new datum set to the card  while PRON be calculate the previous one  PRON may also need to split PRON task in to small portion   PRON be also important to notice  that 750 ti use a new  maxwell  gpu  architecture  while 780 ti use the predecessor  kepler  gpu  generation  see here for more detail  
__label__machine-learning PRON be try to design an orbital rendezvous program for kerbal space program   PRON be focus on when to launch PRON spacecraft so that PRON can end up in the general vicinity of the target   if PRON can control the ascent profile  the remain dependent variable be the ship s twr and the target s altitude   PRON want to try a computer learn solution   what be the good way to formulate the problem of learn the time to launch base on some twr   how can PRON make an algorithm to compute the general equation of launch time to a altitude base on PRON ability to accelerate   what type of learning problem could this be classify as   what be some approach to solve problem with know dependent variable   
__label__pde __label__finite-element __label__matlab __label__boundary-conditions  PRON write the code to fix this problem  now  PRON should validate PRON use a test function to see the error in PRON code  PRON can not figure out how start from u can derive the function g on the edge  in particular PRON do not manage to compute the outward normal   this be PRON code to calculate g  httpsdpastedehzym  p  t  e be output of initmesh   
__label__predictive-modeling __label__correlation PRON would like to ask if the pearson correlation between field  but not the class field  of a dataset affect somehow the performance of naive bayes when apply PRON to the dataset in order to predict the class field   as PRON probably know  naive here imply that the  field  be independent  so PRON question boil down to do correlation imply dependence  yes  PRON do  see here   httpsstatsstackexchangecomquestions113417doesnonzerocorrelationimplydependence  so  if PRON feature show correlation then this will have an adverse effect on the naive assumption  despite this fact  naive bayes have be show to be robust against this assumption  if PRON model still suffer from this  however  PRON could consider transform the space to be independent with method such as pca   yes  PRON will affect the performance of naive bayes   PRON be call naive because PRON assume an independence between the feature  which in practice be rarely the case  however  PRON be show to be fairly robust to this and to be able to perform well on real  world problem  so have correlation will go against the naive assumption   however  correlation be not necessarily a bad or a good thing for the performance of PRON model  correlation between feature in naive bayes simply mean that if one feature  say  PRON be class a  then the other features  will often say the same  therefore  if PRON correlate feature happen to be good predictor  PRON model will actually benefit from PRON  if PRON happen to be bad predictor  PRON model will be bad off  
__label__algorithms __label__approximation-algorithms PRON be look for a description of algorithm which allow PRON to detect key turning point on the road among a set of all give point   PRON have ilustrat PRON problem on the below image   green spot  those be the start and end point of a route   blue spot  those be the key point which should be detect by the algorithm   red dot  this be the datum about the road that have be drive   could someone point PRON to the algorithm name or publication that deal with this problem   thank in advance   PRON can combine the turn information  derivative of x  y over time  with speed information  derivative of position over time   the rationale be that on critical point PRON need to slow down and then accelerate again   but the good way would be  as aron say  to compare PRON path with a map and detect when PRON have pass a crossing   btw  do PRON key point really need to be turn point  the gps navigator should be able to find the turning point PRON when PRON supply PRON PRON via   wodzu  this be a problem from radar datum processing for track generally unfriendly item  try search for algorithm like imm  interact multiple model   book  adaptive filtering and change detection  by gustafsson  isbn 0471492876  give quite a bit of useful background on the field in general   in general  zero apriori information on likely route  case PRON would use kalman filter  however  for a road network PRON can deduce several hypothesis on where the car be go  and imm  and PRON sister algo  base on continuous pathfinding solution would be of more value here  if PRON do not have to be real  time and have the whole dataset handy  there exist even good statistical technique  see a bunch of q  as at cross  validate se    edit  what PRON have leave out at first  gps do not have a neat error distribution  please be aware that there be some nasty property  both in position and velocity error   
__label__finite-difference __label__quadrature gaussian quadrature improve on newton  cotes formula by allow the abscissa to vary along with the weight in order to integrate high order polynomial   can this idea be extend to numerical differentiation  to wit  can PRON choose a set  hi such that  f be much better approximate by a weighted sum of evaluation at  xhi than PRON could be at equally space datum point  for example  maybe the follow relation could hold for some  j   beginalign    fx   sumi1nfracwihifxhi    mathcaloleft hfnjrightnj    endalign    yes  as PRON may know  numerical differentiation and integration be closely related to  polynomial  interpolation  the idea to approximately differentiate or integrate a give function be to approximate PRON with a function  often an interpolate polynomial  that can be differentiate or integrate exactly  for example  the standard central difference quotient formula  for  fx can be derive from differentiate a quadratic  interpolate polynomial through the point  x  h  x  xh  the benefit ist that the error in approximate the derivative or integral be only determine by the error in approximate the function  which be well understand for polynomial interpolation  in particular  PRON turn out that uniform interpolation point be a poor choice in general  and interpolation point base on root of orthogonal polynomial be much good  in the context of quadrature  this lead to the different variant of gaussian quadrature  legendre  chebyshev  jacobi  laguerre  hermite    in the context of differentiation  this be refer to as spectral collocation  since PRON be the basis of spectral method for solve differential equation  rather than a finite  difference quotient this be usually realize by a differentiation matrix that map the value of  f at a selection of collocation point to the correspond value of  f  in contrast to the standard finite  difference matrix  these be usually dense  spectral method be therefore global method   the deep relation between these concept be explain in  trefethen  lloyd n  approximation theory and approximation practice  other titles in applied mathematics 128  philadelphia  pa  society for industrial and applied mathematics  siam   isbn 978  1  611972  39  9pbk   305p   2013   zbl126441001   in particular  chapter 21 be concern with spectral collocation  and theorem 211 be precisely the kind of error estimate PRON write   the standard choice for gaussian quadrature be the root of the legendre polynomial  and PRON can in fact use the same point for differentiation as well  aptly call legendre collocation   here be a matlab script that set up the correspond differentiation matrix  more commonly use be the root of chebyshev polynomial lead to chebyshev collocation  which be the basis of the remarkable chebfun matlab package  and PRON julia child  approxfunjl   
__label__numerical-analysis __label__nonlinear-equations __label__discretization __label__accuracy do the order of accuracy of a combination of scheme apply to solve a system of non  linear equation  match those of the same scheme apply to the linearised version of the system   in other word  do linearisation alter the order of accuracy of a scheme   
__label__linear-algebra __label__dense-matrix if PRON have a psd  symmetric matrix a and PRON need to do lu docomp on bi  a  di  where di be a diagonal psd matrix and where di change randomly    be there an easy way to go from the lu of a  some function of di to get the lu of bi   any reference in the literature would be greatly appreciate   use a cholesky decomposition or an ldl decomposition instead of lu   judge from answer to   diagonal update of a symmetric positive definite matrix  solve a system with a small rank diagonal update  can diagonal plus fix symmetric linear system be solve in quadratic time after precomputation   there be no good way to update any of those decomposition  PRON would do no bad to recompute the decomposition from scratch each time unless the rank of the diagonal matrix be really low  in which case  maybe PRON could do something similar to sherman  morrisonwoodburytype update  
__label__optimization __label__least-squares PRON need to implement a non  linear fitting algorithm in fortran and choose to use minpack s flavor of the levenberg  marquardt algorithm as a basis for the least  square stuff  however  PRON seem to misunderstand something very basic about how these routine be to be use  PRON understanding be that PRON have to call the routine lmdif1 and define a function fcn to be call by the minpack routine that calculate the square of the difference between the datum to be fit and the model function  lmdif1 would then return the good solution for the free fitting parameter in an array x  PRON have write a small test program  give below  to try this out  the program produce a set of  datum  by calculate value from the model function  expax  in this case  with a give parameter a   05 in this case  and change PRON by a small random  error   this dataset be then fit with the same model function  with a be the free parameter  but PRON do not really work  the fit be usually very bad  unless PRON almost give the  correct  solution as a starting guess   consider that levenberg  marquardt be say to be quite robust and see how well PRON work eg in gnuplot  PRON wonder what PRON be do wrong here  any idea  the code below should work when link to the minpack routine   tom   test lmdif  program testanf  implicit none  integer  parameter   m10n1lwa  mn5nm  integer   info  iwan   double precision   xnfvecmwalwatol1d3  external   fcn  x105d0  call lmdif1fcn  m  n  x  fvec  tol  info  iwa  wa  lwa   call translateinfoinfo  n  tol   write       ax1   end program testanf  subroutine fcnm  n  x  fvec  iflag   implicit none   m  number of datum point   n  number of fitting parameter  integer  intentin    m  n  iflag  integer   i  real  allocatable  save   tfdata     real   tmin  tmax  dt  rand  double precision  intentinout    xn   double precision  intentout    fvecm   characterlen1    c  logical  save   inittrue   if  init  then    create  measure datum   tmin0   tmax10   dttmax  tminm  call randomseedput12345678    allocatefdatamtm    do i1m  tiidt  fdataifmodel05d0ti    end do  tmax01maxvalfdata   do i1m  call randomnumberrand   fdataifdatairand05tmax  add measurement error  write10    tifdatai   end do  initfalse   end if   minimize the residual  do i1m  fvecifdataifmodelx1ti2  end do  if  iflag  lt  0  deallocatet  fdata   return  contain   model function  real function fmodela  t   implicit none  real  intentin    t  double precision  intentin    a  fmodel  expat   return  end function fmodel  end subroutine fcn  
__label__mpi __label__cuda __label__opencl PRON be write some image processing software to detect bubble in oil at work  one of the image filter PRON need  perona malik  seem like PRON will take a long time to compute   PRON involve solve some partial differential equation numerically   one possible solution PRON be think of investigate be use a super computer cluster  PRON have never have any experience with beowulf cluster or the like  can anyone suggest some technology PRON should consider  any pro and con of the different option would be useful to know as well   chris  before PRON invest in either a cluster or a supercomputer  PRON think PRON be different beast  other may disagree  PRON suggest PRON invest in a beefy desktop workstation  maybe 2 x 4 or 6core processor   PRON can use this to try out all PRON parallelisation option  mpi  openmp   cuda  opencl  these two will need a gpu   probably other   PRON experience on the workstation will provide useful input to PRON decision on further hardware purchase   ok  so if PRON want to go down the gpu route PRON have a number of option   shader language  hlsl  use with xna  glsl  use with opentk  cg  use with unity  PRON be think of use opentk because PRON let PRON use the gpu and integrate with monodevelop and visual studio which PRON use at work for other project  PRON also alow PRON to use opencl if PRON should need PRON  
__label__machine-learning __label__python __label__deep-learning __label__data-mining __label__tensorflow PRON be use tensorflow  serve to write a server to consume model in production   PRON have a question about consume the service by client  do tensorflow  serve support a rest api  be there be anyway to modify PRON   PRON have check several github project  here  here  and here   
__label__machine-learning __label__data-mining __label__clustering PRON have sparse vector and find that cosine similarity be very efficient to to measure the similarity  now PRON want to cluster these vector base on similarity  hence  can someone please suggest  recommend cluster algorithm that make use of cosine similarity   ps  PRON do not have a predefined number of cluster beforehand and want the clustering algorithm PRON to decide PRON   PRON can see PRON affinity matrix as a weighted adjacency matrix of a graph and apply modularity  base community detection algorithm on that  just note that modularity base algorithm have resolution problem ie find very small community be difficult in presence of large one  
__label__neural-networks have work with neural network for about half a year  PRON have experience first hand what be often claim as PRON main disadvantage  ie overfitt and get stick in local minima  however  through hyperparameter optimization and some newly invent approach  these have be overcome for PRON scenario  from PRON own experiment   dropout seem to be a very good regularization method  also a pseudo  ensembler     batch normalization ease training and keep signal strength consistent across many layer   adadelta consistently reach very good optima  PRON have experiment with scikit  learn implementation of svm alongside PRON experiment with neural network  but PRON find the performance to be very poor in comparison  even after have do grid  search for hyperparameter  PRON realize that there be countless other method  and that svm s can be consider a sub  class of nn s  but still   so  to PRON question   with all the new method research for neural network  have PRON slowly  or will PRON  become  superior  to other method  neural network have PRON disadvantage  as do other  but with all the new method  have these disadvantage be mitigate to a state of insignificance   PRON realize that oftentimes  less be more  in term of model complexity  but that too can be architect for neural network  the idea of  no free lunch  forbid PRON to assume that one approach always will reign superior  PRON be just that PRON own experiment  along with countless paper on awesome performance from various nn s  indicate that there may be  at the least  a very cheap lunch   neural networks have other short coming as well   PRON take much long and far more resource to train a neural network than something like a random forest  so if PRON need speed of training or be resource constrain in anyway  PRON probably should not look at neural networks first  evaluation of a train deep nn can be much more expensive than compete technique too   the effort involve in learn how to architect and train a nn be still much high than compete method  like an svm  people who be just start out in data science should probably use other technique to learn about the nuance of fitting datum before get involve in neural network  and although simple nn with only one or two hyperparameter be often available in  many datum science library  PRON do not perform any good than other technique so be just another ml black  box technique really   while PRON have make a lot of progress in understand how neural network do PRON magic  PRON be still less accessible and dissectible than most compete method  so while nn may solve the problem  PRON may not give PRON as many insight as easily as other technique do   look forward to what other people have to say here   just to add to what have be say in mikewise s brilliant answer   all thing equal  deep learning model generally rank supreme when compare to other algorithm as the size of the dataset increase   like everything  PRON all boil down to the dataset at hand  neural network be good on other dataset but at the same time  PRON will be bad on other dataset  when PRON come to unstructured problemseg visual  text  sound   at this time neural network appear to be the good algorithm  that say  when PRON come to structured datum  a quick scan at the type of algorithm use to win online datum science competition reveal that  the so call machine learn algorithm such as xgboost rank supreme   when PRON come to other model  feature engineering play a big role in the algorithm s efficiency  feature engineering be generally a tricky thing to do and do right  deep learn algorithm do not require as much feature engineeringif any at all  compare to other algorithm  in fact PRON learn feature on PRON own   if the google guy say PRON do not see deep learning come who be to rule out the possibility of some so call machine learn algorithm come out and take over the world by storm   here be a poll on what data scientist say when ask  if deep learning match the hype in real world application    even some of the popular deep learning application like google s alphago be not 100  deep learning  instead PRON be part deep learning  part good old  machine learning   PRON 2 cent be  maybe PRON should not rule out other machine learn algorithm yet  
__label__ai-design __label__strong-ai __label__nlp __label__hci ai be develop at a rapid pace and be become very sophisticated  one aspect will include the method of interaction between ai and human   currently the interaction be an elementary interaction of voice and visual text or image   be there current research on more elaborate multisensory interaction   probably these day PRON be still under the umbrella of  man  machine interaction  in cs  ie there be a  sub  field for interaction between human and machine in cs  but PRON be not aware that PRON have split again to create a sub  sub  field for ai  man interaction   this be one of the main research area of PRON lab which research intelligent prosthetic which also give sensory feedback such as touch and kinaesthesia  the feeling of a limb move in space  to the user   PRON use reinforcement learning to bridge the gap in control and have preliminary work in communicate to the user prediction make by the artificial agent   at risk of seem out  of  scope  PRON think PRON be worth mention there may be a case that human vs ai play of serious game game  such as chess and go  represent the  deep  level of human  ai interaction to date   game theory be also important because PRON underlie all optimization  include  PRON have no doubt  expand and optimize ai  to  human interaction    PRON bring this up because in term of human  to  computer interaction  PRON doubt there be a more effective mean of engagement than computer game  whether  serious  or otherwise   thus PRON may find PRON useful to look at the concept of gamification  which naturally lend PRON to the type of multisensory input and output PRON be interested in    gamification can improve an individual s ability to comprehend digital content  
__label__molecular-dynamics PRON have be try to simulate a simple problem of take about 100  1000 ar molecule in a nve  fix vol  energy  system with equal speed but randomize velocity  and evolve PRON to obtain a maxwellian speed distribution  PRON be choose a lennard jones pair potential with no cutoff  despite be computationally expensive   periodic boundary condition  and a vary integration time step to keep numerical error on energy to below 2   PRON observe that PRON system reach maxwellian  like speed distribution  visually look like a good gaussian  from the initial rather uniform speed distribution  but very soon after some  about 10 in 100  particle accelerate to very large speed even if PRON total energy be be hold constant  to within 2  of initial energy  skew the distribution to one side   PRON have try to vary the no  of particle  increase temperature to allow re  equilibration of what appear like a grow fluctuation but can not make this effect go away  PRON be puzzle because PRON be manage to keep total energy with a 2  bind  have anyone else have observe such long  time behavior in simple md simulation  andor be PRON well  know behavior  PRON be use the velocity verlet scheme  PRON be use a well  scale system  ie  not try to code actual atomic dimension  and use a reasonable well distribute initial condition   do PRON check the centre  of  mass velocity of PRON particle  ie do PRON set PRON to zero at the beginning of PRON simulation  the centre  of  mass velocity should be preserve throughout the simulation  and should be zero to get correct temperature  see flying ice cube    PRON think PRON should capture these few particle behind a door in a separate compartment  there would doubtlessly be some energy that can be gain from PRON   PRON be sure maxwell would be proud of such a demonic scheme if PRON succeed  
__label__keras __label__time-series __label__lstm PRON have a dataset like the follow one   each column be a different numerical feature  each row represent a timestamp  what PRON want to do be to create an lstm model that can make prediction of the future time  step for all the feature  for example  PRON want to use the first 2000 example to train PRON model and use the next 1000 to test PRON  the problem be that PRON do not know how to proceed   since PRON do not have y s value in this dataset  PRON be think of create PRON by shift the time t1 to t have a new column y PRON explain PRON  for example PRON will have a new column 14  with the value 99652 for timestamp 0  which be the value at time  step 1 for the feature 0  and so on for all the time  step and all the feature   the problem be after that  PRON do not know how to feed PRON lstm use keras to make several step prediction with such a dataset   
__label__finite-element let there be triangular  mesh for a simple domain  domain be not important    if this mesh be not uniform  the mesh be produce by adaptive technique   what be the mesh size   be there a mesh size for each triangle  or this be a number relate to the whole mesh on the domain  how do PRON compute PRON   thank PRON   for each element the so  call local mesh size can be define eg as the maximum edge length  this have application at least in residual a posteriori error analysis where the elementwise error indicator be often multiply by the local mesh size   instead of compute the maximum edge length one can show that the jacobian determinant of the affine mapping from the reference element to a global element be proportional to the square of the local mesh size  this be a property that PRON often use when program adaptive mesh refinement use a posteriori estimator since the code for compute the jacobian determinant be already there  assume that the finite element code be implement use reference element    if the mesh be not uniform in the sense that for each pair of triangl PRON local mesh size can be bound by each other  multiply by some constant   then the so  call  global mesh sizemdashie  the maximum of local mesh sizesmdashmay not be a meaningful quantity  for example  in adaptive refinement one usually be concerned about the error versus number of degree  of  freedommdashas indicate in the comment  
__label__python __label__numerical-analysis how do PRON ensure that PRON function below be well condition as  s approach  infty  the problem PRON get be that for large  s the function return an indeterminate form  frac00 PRON would otherwise have expect the function to increase monotonically from 0 to 1 as  s  approach  infty  beginalign   fs   fracxs2sqrt1xs21sqrtxs2xssqrt1xs212xs2sqrt1xs212    endalign   beginalign   xs  es   endalign   import numpy as np  def funcs    xs  npexp1s   num  xs2npsqrt1xs21  den  npsqrtxs2xsnpsqrt1xs212xs2npsqrt1xs212   return num  den  s  nplinspace020  1000   plots  funcs    the function be really smooth as  s rightarrow infty this allow PRON to do the follow strategy   for  sle s0   do the usual evaluation  ie  return  fs  for  sgts0   define the function  ghf1h  do a taylor expansion around  h0   ie  around  sinfty  and return  bar ghh1s where  bar g contain the first few taylor term of the expansion of  gh in the evaluation of these taylor term  PRON need to compute the  kth derivative  gk0 which will involve factor  frac 00  for which PRON will have to apply lhopital s rule  
__label__matlab __label__ode __label__implicit-methods PRON be try to solve a system of coupled ode      beginalign   fracdnadt   amp   eleftjt   f  θhsinhleftfracgnatrightright    fracdθhdt   amp   aleftbp1  θh  2  cθh2  fθhsinhleftfracgnatrightright    fracdpdt   amp   hi  p   dtp  paleftbp1  θh  2  cθh2right    endalign      here  e  f  g  t  a  b  c  h  i  d be constant   j be a time dependent variable  and  pa  101325  the initial value for the three parameter  na  θh  p be  0  05   and  8106  respectively   PRON be able to solve the equation use the explicit euler method  but PRON be have trouble apply the implicit euler method to do the same  for the first equation PRON can use the explicit euler method to estimate the initial value for  na at time point  tn1  but PRON be not able to figure out what should PRON take as a value for the  θh term that also get involve in the newton  raphson iteration scheme for calculation at  tn1  define     beginalign   an1   amp   eleftjn1   f  θhn1sinhleftfracgnan1trightright    bn1   amp   aleftbpn11  θhn1   2  cθhn12  fθhn1sinhleftfracgnan1trightright    cn1   amp   hi  pn1    dtpn1paleftbpn11  θhn1   2  cθhn12right   endalign      then backward euler give PRON     beginalign   nan1   amp   nan  delta tan1    thetahn1   amp   thetahn  delta tbn1    pn1   amp   pn  delta tcn1      endalign      reorganize into the form     beginalign   fn1   amp nan1   delta tan1    nan  0   gn1   amp thetahn1   delta tbn1   thetahn  0   hn1   amp pn1    delta tcn1   pn  0  endalign      let PRON assume that PRON have an equation for  jn1 that can be solve explicitly   then the above system of equation have the form     mathbfrmathbfun1    0     where  mathbfun1    nan1   thetahn1   pn1  newton s method can then be express as     mathbfun1k1   mathbfun1k  mathbft1mathbfun1kmathbfrmathbfun1k      where     mathbftmathbfun1k   fracpartial mathbfrmathbfun1kpartial mathbfun1k         the component of the matrix  mathbft be find use the usual relation     tij   fracpartial ripartial uj         PRON already have initial value for  mathbfu  PRON be not sure why PRON can not use those for PRON newton iteration   if newton iteration do not converge PRON may have to use an alternative algorithm  such as bisection  line search etc   
__label__python __label__interpolation PRON have create a spline to fit PRON datum in python use   spline  scipyinterpolate  univariatesplineenergy  fpp  k4   the equation PRON want to use involve a summation between n2 and n  infinity  where n be the order of the differential at a point eo  however  use   univariatesplinecallspline  e0  nu  n   to call in the value  PRON be unable to get a value for anything past the 4th order differential  be there any other function that people know of for evaluate this function  above about the 8th order there be a pre  multiplier which should set the value to zero but PRON still need to go high than a 4th order   by default PRON use cubic spline  which be 3rd order piecewise polynomial  if PRON take the fourth derivative of a 3rd order polynomial  PRON will end up with 0  if PRON really need these high order differential  use cubic spline will do PRON no good   examine the documentation for the univariatespline object  PRON seem as though PRON be construct a 4th order spline  which be why PRON can not get value for derivative of great than 4th order   all of those derivative  as PRON be aware  would be zero    scipy limit the polynomial degree of spline to be 5th order or less  ie  k  lt 5   if PRON need an 8th order polynomial spline interpolant  PRON would have to find an alternate library  or possibly code PRON up PRON   if PRON need high  order derivative  PRON will not get good result use equidistant datum point  if PRON can sample PRON function at arbitrary node  PRON would recommend use chebyshev point  ie   xk  cosleft  pi frackn  right   quad k0dot n for a polynomial of degree  n  PRON can evaluate the polynomial stably use barycentric interpolation  note that since PRON be use a polynomial of high degree over the entire interval  PRON will probably need less data point  note also that this assume that PRON datum can be represent by a polynomial  ie PRON be continuous and smooth   get high  order derivative from the interpolant be a bit tricky and be ill  condition for high derivative  PRON can be do  however  use chebyshev polynomial  in matlab  octave  sorry  PRON python be not good at all   though  PRON could do the following    PRON will use the sine function as a test case  f  x  sin  4pix     set the number of point and the interval  n  40   a  0  b  1    create a vandermonde  like matrix for the interpolation use the   three  term recurrence relation for the chebyshev polynomial   x  cos  pi0n1n1      v  one  n   v2   x   for k3n  vk   2xvk1   vk2   end    compute the chebyshev coefficient of the interpolation  note that PRON   map the point x to the interval  a  b   note also that the matrix inverse   can be either compute explicitly or evaluate use a discrete cosine transform   c  v  f   ab2   b  a2x     compute the derivative  this be a bit trickier and rely on the relationship   between chebyshev polynomial of the first and second kind   temp   0  0  2n111cend12     cdiff  zero  n1  1    cdiff12end   cumsum  temp12end     cdiff22end   cumsum  temp22end     cdiffend   05cdiffend    cdiff  cdiffend13     evaluate the derivative fp at the node x this be useful if PRON want   to use barycentric interpolation to evaluate PRON anywhere in the interval   fp  v1n1   cdiff    evaluate the polynomial and PRON derivative at a set of point and plot PRON   xx  linspace11200     vxx  one  lengthxx   n   vxx2   xx   for k3n  vxxk   2xxvxxk1   vxxk2   end   plot   ab2   b  a2xx   vxxc  vxx1n1cdiff     the code to compute the derivative can be re  apply several time to compute high derivative   if PRON use matlab  PRON may be interested in the chebfun project  which do most of this automatically and from which part of the code example above be take  chebfun can create an interpolant from literally any function  eg continuous  discontinuous  with singularity  etc  and then compute PRON integral  derivative  use PRON to solve ode  etc   in scipy  if PRON try to compute the n  th order derivative of a k  th order spline  where n  k  then PRON get a valueerror   valueerror  0ltder5ltk4 must hold  PRON could write something like this   def splinederspline  x  nu0    sd  0  try   sd  splinex  nu  nu   except valueerror   pass  return sd  ps  as PRON can see  instead of use   call    PRON can simply write  splinee0  nu  n  
__label__matrices consider the integral    2pi t  frac12  int02pi  afracpartial bpartial xi   dxi  where   a sumnn i  signn   bn einxi   quad quad b sumnn  bn einxi these expansion imply    2pi t  sum kbkbk  furthermore     bk  fracikleftdotyk  sumlj  kdotylyj j  signlsignjright  with dot indicate temporal derivative   now  the canonical variable of the system end up be the complex value variable  ypm 1ypm 2  ypm n  and so the euler  lagrange equation involve term like     1  quad quad fracpartial tpartial ym   quad quad fracddtleftfracpartial tpartial dotymright  therefore  to derive the govern equation  PRON must explicitly expand term like these and write PRON as some function  fypm 1ypm 2     dotypm 1    now  the way PRON be currently compute this term be to note    t  sumk  l  qkl  dotykdotyl  where   qkl   frac14summ1n pmkpml  and   pmn    signm  nsignnym  n  frac12  ymyn  to set up PRON govern equation via the euler  lagrange equation  PRON need to compute term like the one mention above in  1   PRON both lead to the need to calculate    fracpartial qklpartial yn   summ1n leftfracpartial pmkpartial ynpmlpmkfracpartial pmlpartial ynright  PRON need to compute this for all n  so PRON end up need to perform a series of 2n1     2n1ntimes  n2n1 matrix multiplication  for large  n  this computation be absolutely slaughter PRON run time  and generally take up  80  of the run time for each time step  depend on the size of n   this be PRON problem   the way PRON have be try to overcome this be to focus PRON attention on use the best optimize blas library to compute the matrix multiplication  and exploit symmetry of the matrix to try to reduce computation time  however  PRON think PRON have milk this line of reasoning for all PRON be worth without get too deep into the weed  so PRON would like to atleast think about alternative   PRON be very new to numerical method  and may be completely off base  but PRON seem like there should be a way to manipulate the fft here  to avoid do these matrix multiplication as these cauchy product be similar to convolution  ie the pseudo spectral method    however  PRON do not see how one can explicitly do this  especially as PRON need to differential these product with respect to the dependent variable of the system   if anyone have any insight  PRON be greatly appreciate   
__label__finite-element __label__weak-solution in the the finite element method  fem   PRON attempt to obtain the weak form of the describe equation  PRON understand that this be an attempt to reduce the order regularity of the equation  but what be the actual benefit and disadvantage of use the weak form  what do regularity mean in this sense   PRON have check out other question on this topic  the math present there be high than PRON level  if PRON would be a good mathematician  PRON suppose PRON may have derive PRON answer  basically  an answer understand by engineer would be nice   the weak form of the pde mean that PRON have  in some sense   few constraint on the kind of function that PRON can use  to construct a solution   to put PRON into perspective  recall that in fem  PRON look for a solution of the form     sumjujphijx  where  uj be the unknown scalar coefficient  which will be solve for  and  phij be some know function   the main question be   what kind of function  phij be PRON allow to choose   well  after go through a very long  tedious  and mathematically pedantic proof  PRON arrive at a very nice conclusion which be very convenient for engineer who want to implement fem   all  phijx must have a value of zero on the boundary   if the weak form of the pde have a weak derivative of maximum order  k  then PRON be sufficient that the function  phijx have continuity of order  k1  condition  1 be very easy to understand    phijx0  on all point along the boundary of the domain of PRON problem   condition  2 be not entirely obvious  also not 100  mathematically or pedantically correct either  just  close enough for engineer  sake    but condition  2 be the most useful part because PRON reveal the continuity constraint on  phij  for example  take the poisson equation in weak form  assume homogeneous dirichlet boundary condition     intomega knabla u cdot nabla v dxintomega fv   this equation have a weak derivative of maximum order k1 because the gradient here be  effectively  a first order weak derivative  if the weak form have a laplacian operator  nabla2   then k2  etc     thus  PRON can choose a function space such that  phijx have continuity of order zero   in other word  PRON can use function that be continuous  but not necessarily smooth  across the entire domain   this give PRON a lot of flexibility in be able to construct a set of function statisfy condtion 1   why do this matter at all   because  in fem  PRON also seek a third desireable  albeit not absolutely necessary  condition   PRON want the stiffness matrix to be as sparse as possible   this third condition matter because PRON imply that PRON should use function  phij that be not only zero on the boundary  but zero  almost  everywhere except in small subregion of the domain  each subregion correspond to a give  phij should only overlap with a small number of other subregion correspond to other function   PRON can use this condition while exploit with condition  2 to produce a desirable choice of function  phij  lagrange polynomial be one such set of function which satisfy all three condition   of course  lagrange polynomial be not the only choice  but be probably the simple to understand from a beginner  perspective   these function be zero everywhere except in a small region where PRON be piecewise linear  quadratic  cubic  etc   because  phijx be only non  zero in small region for each give  j  the integral form the stiffness matrix be also mostly zero and thus the stiffness matrix will also be very sparse   PRON just want to caution PRON that PRON have water down a lot of mathematic here and that condition  2 be only  approximately correct enough for an engineer s perspective  and be not 100  mathematically correct   PRON leave PRON to the abundance of mathematically inclined user on this site to point out how gross an overstatement condition  2 be in reality   PRON be precisely the opposite of what PRON have  PRON use the weak form to increase the regularity  or alternatively  allow PRON to solve problem with less regularity   integration always smooth   the intuition be actually quite simple  for  most  function  if PRON take an integral  then PRON derivative exist  PRON derivative be the thing PRON just integrate    many time PRON may want to solve a pde which do not necessarily have derivative  so the idea be that PRON just solve the pde for the integral   a common example deal with step function  a step function be not differentiable  but in a sense PRON  weak derivative  be the dirac delta  as in  the integral of a step function have a derivative which integrate like a dirac delta   so to make everything work PRON just take integral everywhere   now PRON can have a pde for thing which be not strictly differentiable  and PRON just tell people  well if PRON look at the integral PRON work   that be the weak form    note  there be a lot more detail  example  sobolev space be a generalization of lebesgue space  since  l2  function be not function but equivalence class of function which be equal almost everywhere accord to some measure  PRON do not have point  wise value  and so PRON only make sense in term of how PRON integrate  again by how integral  smoothen    etc  but PRON be all the say idea  smoothen use the integral  and define the derivative via integration by part   
__label__classification __label__multiclass-classification PRON be try to classify several website by category  finance  health  care  it  etc    PRON have at PRON disposal the content of the page of the website  and PRON use the word to classify  for now  PRON have manually classify some website to train a naive baye model   as PRON be mostly interested in a high precision  eg a classify website must be in the right category   PRON would like to add an  undefined  category in which a website would end up if PRON be not close enough to the other category  to be clear  PRON be not a problem for PRON if a website be not classify  PRON be a problem if PRON be misclassifi   be there algorithm that would allow this  or a way to train an  undefined  category   PRON good guess for now be to train something like a random forest and define a  minimum score  below which a website be  undefined    absolutely PRON can create an approach that force high  precision class tag algorithm  at the natural cost of recall   what be more PRON can do this with  at least  any method that provide a percentage calue for prediction  which be the vast majority of classifier  the key be  as PRON mention  to find the minimum acceptable value of precision and cut the prediction at that value   if a minimum precision be PRON only constraint and PRON solution be not sensitive to the recall  get all or the high possible proportion of the website correctly classify   this be a very simple matter  some low percentage of PRON observation will be classify  but those that be will be more likely to be correctly classify   for example if PRON precision floor be 70   PRON cut could look something like this   obeservation prediction fall into the green segment could be classify as positive example  and prediction fall into the red segment would be unclassified   this approach would be sufficient for naive bayes  some other approach  svm  gradient boost machine etc  may benefit from a custom loss function definition  in which PRON define a function that disproportionately punish false positive prediction   something like     yi  0  rightarrow  d  swedge  yi  1  rightarrow  d1   lp   frac  sumifracpi  yi2dni   lp  loss function   pi  predict class likelihood   yi  actual class  0 for negative example  1 for positive    ni  number of observation   s  penalty parameter for false positive   would heavily punish false positive relative to false negative  PRON can also be adjust to PRON need  to more or less heavily punish false positive  for  s  5   the function look something like   please note that this function do not necessarily define the approach PRON should take  but only one possible approach  to create a custom loss function perfectly tailor to PRON use case  PRON would want to know the relative cost of a false positive and a false negative  and customize the function accordingly   thomas cleberg s approach sound reasonable  but another very simple approach would be to explicitly code an  undefined  category  this be common when deal with text dataset where a word may be new or too rare to stand on PRON own   with a large enough collection of website  surely there be website that be not cleanly classify in one of PRON category  bad case PRON could simply search for such website manually and augment PRON dataset  this approach would not require any change to how PRON train PRON model  
__label__natural-language __label__language-processing PRON have a group of structure in a program that be very specific on PRON meaning  eg  this be a piece of code  randomitem  objectsconceptrandombuyable    ideaexampleobjectsconceptrandomfamily    frienddoe    action   go    target  objectconcentrandomshop      then    action   buy    target  randomitem   several  true    then    question  true   action   know    property   amount    target  randomitem   several  true     PRON have work with natural language parser before  however PRON question be how do PRON go and transform this to natural language  the other way around   be there any way or method  PRON be try to google but PRON do not find anything that seem to tackle PRON problem  PRON honestly do not want to reinvent the wheel  PRON have logical structure in which PRON know who be the subject  what the verb and target be  which method can PRON use to generate language from this   PRON may be simple to generate the language output PRON since PRON have already get a concrete concept structure in code   maybe PRON will also want to be aware of potential future application such as parse PRON own output back to the input   with that say  PRON could investigate markov chains or google around a bit for natural language processing and natural language generation   httpsstackoverflowcomquestions33068943libraryforgeneratingnaturallanguageverbsinjavascript  look at mumble  which be a kind of transducer work from conceptual dependency structure  which be similar to what PRON have get  and produce english output   a lisp implementation of meehan s talespin program  which include a simplified version of mumble can be find here  httpsgithubcomlispmcommonlispcodeblobmastermicrotalespinlisp  the generator part start at about line 1030  PRON should be easy to follow  even if PRON do not know lisp  
__label__machine-learning __label__r __label__prediction PRON want to start with machine learn with a small prediction problem but PRON be not sure PRON choose the right approach  PRON want to make a program that get datum of mechanical failure on car  manufacture time  failure time  reason  and different characteristic of the car   then PRON would give the datum of new car that will be release to market and PRON would try to predict when would PRON fail   PRON be read that the good approach be use survival analysis with r but since PRON be not really familiar with this algorithm  PRON be wonder if there be any other approach   PRON be also just a beginner in ml  who be however not familiar with survival analysis w r   but have tackle a couple of ml project  base on PRON knowledge  PRON could use supervised learning   store datum  preferably in csv format   one column about the duration between buy the car and the car s mechanical breakdown   and the rest about the car s datum  characteristic   next  PRON can run a neural network through PRON datum  and use PRON nn s library s predict   method to predict the duration before breakdown base on PRON datum   PRON could then theoretically  assume that there be a logical correlation between the datum  see which characteristic be most prone to make a car break down   as for implement PRON program  PRON use python with the keras library  which be simple enough for any programmer to use  but there exist many other great ml library  notably tensorflow   do note that PRON be also just a beginner  and that PRON approach may be erroneous  yet PRON do wish PRON good luck on PRON future ml project   PRON think PRON should first clearly specify what the covariate be  what be the target variable be and what be PRON goal   therefore  if PRON have attribute about the car as covariate and target variable be failure time  car fail in 1y  2y    then the good approach be indeed survival analysis  because PRON try to model time to failure   on the other side  if PRON target variable be just a failure  yes or no  then PRON a classification problem  for that  simple model such as decision tree or logistic regression be very well suited   do not use algorithm just because PRON fancy or do not dislike other approach just because PRON be not  machine learning    PRON say PRON want to start with machine learning  so go ahead  do not blindly fit whatev blackbox model  start with simple one and look inside  how PRON work   that be say  pick something more simple  because survival analysis require knowledge or regression and a bit more of statistic   good of luck  
__label__parallel-computing __label__monte-carlo __label__random __label__random-number-generation recently  PRON come across a comment claim that almost all researcher do monte carlo method be do PRON wrong  PRON go on to elaborate that merely choose different seed for different instance of a prng such as the mersenne twister be not sufficient to ensure unbiased result as bad collision can occur  the wikipedia article on the mersenne twister seem to corroborate   multiple mersenne twister instance that differ only in seed value  but not other parameter  be not generally appropriate for monte  carlo simulation that require independent random number generator  though there exist a method for choose multiple set of parameter value   PRON have to admit  PRON be guilty as charge  but so be all the other implementation of parallel monte carlo librari PRON have see so far  in particular alps   the wikipedia article also reference two paper offer remedy   the dynamic creation  dc  scheme  1998  pick parameter set for the mt base on the hypothesis that PRON be independent if the correspond characteristic polynomial be coprime   the jump ahead for  mathbb f2linear rng  2008   PRON reckon PRON be similar to the leap  frog method for lcg   both method have be coautor by matsumoto and nishimura  the original author of the mersenne twister algorithm   PRON be afraid PRON be not very knowledgeable in number theory or algebra and do not fully grasp the above scheme or the math behind the mersenne twister  PRON question be primarily of practical nature   how much do PRON really need to worry about introduce bias to PRON simulation when not employ such a scheme if next to nobody care about PRON in practice  at least in PRON community    if PRON be to implement one of these counter  measure  be PRON right to assume that the jump  ahead one be better suit as PRON be base on a firm theory and be the more modern method   like PRON say  use the mersenne twister for parallel computation be almost always do incorrectly  as the correct method be tricky to implement   by far the easy and good answer would be to move away from the mersenne twister entirely  and use something like the pcg family  which provide multiple stream out of the box   the mersenne twister be know to fail several statistical test  while also be slow than new rng such as the pcg and xorshift family   the reason the mersenne twister be so widely use today be mainly a result of the rng before PRON be far bad  both in performance and quality  PRON also help that the original author open source a highly performant implementation   if PRON want to use mt  PRON can use sfmt as PRON prng and sfmt jump to generate multiple stream   PRON can simply initialise mt with one seed  and then jump ahead by eg  1 cdot 1060   2 cdot 1060   3 cdot 1060 … step to generate multiple stream  jump be somewhat expensive  but PRON only need to do PRON once when PRON initialise PRON prng   really only PRON can answer the question about simulation bias and if PRON be acceptable in PRON application   the standard procedure PRON use   set a pseudo random sequence as a benchmark  standard monte carlo  use a high  of simulation  in risk management 10000 be often use  in other field 100000 to 1 m may be use    run PRON rng over the same input datum for a subset of datum  PRON use 1 year but that be often overkill    compare the result use statistic which describe feature of the datum PRON actually use for make conclusion  decision   PRON use percentil  152550759599   absolute error  standard deviation of the error   all of this be relative to PRON benchmark   now PRON have the analysis  PRON can use PRON own judgement as to if the rng bias be acceptable  
__label__error-estimation __label__wavelet let the daubechies 4coefficient scaling function  phiin c003 be define by  beginalign   phix   amp frac1sqrt34  phi2x   frac3sqrt34phi2x1   frac3sqrt34  phi2x2   frac1sqrt34phi2x 3    phi1   amp frac1sqrt32  qquad phi2   frac1sqrt32   endalign   this definition allow PRON to compute  phi on the dyadic number  mathbbdj  number of the form  k2j to save a google    but to evaluate any  x in mathbbdj  first the value from all previous level must be evaluate and store  make this algorithm  mathcalo2j  be there a more efficient algorithm to evaluate  phi   in addition  give  xin  03  how can PRON evaluate the error if PRON evaluate  phi on  mathbbdj and approximate  phix by  phik2j where  k2j satifie  mink  k2j  x   
__label__preprocessing __label__sql __label__tableau from sql server PRON import multiple table that each have multiple field  unfortunately the field name be not that descriptive  data be originally from sap  but PRON have a separate excel file that have description for all the field name   be there a way how PRON could rename multiple field once by use the excel file  rename PRON one by one would be too slow way to do PRON   do PRON have  know python panda  an easy why would be too load PRON into panda and do the modification there   df pdreadexcel   after comment  or   PRON can rename column in sql with as  if that be possible with PRON sap import  to create a  semi  automate logic should be easy with a decent texteditor  
__label__pde __label__boundary-conditions __label__solid-mechanics __label__well-posedness for geotechnical engineering problem  PRON be common to fix a single component of displacement along a boundary as a dirichlet boundary condition  roller boundary condition   however  PRON be have trouble see why this lead to a well  pose problem   the number of unknown in an elastic problem be equal to the dimension of the problem  ie one unknown for each displacement component for the navier equation   PRON be under the impression PRON need to specify a boundary condition for each unknown in a boundary value problem  why can PRON get away with a single boundary condition for a single component of displacement  be there an implicit stress boundary condition imply when PRON do this  PRON have include an example sketch with a common configuration  PRON intuition say this should have a unique solution but PRON can not see why mathematically   for this problem PRON have mix boundary condition  then  PRON really have 8 boundary condition for the problem that PRON present in PRON sketch  although  4 of PRON be not explicitly write   these be   top side   non  homogeneous neumann bc  normal traction    sigman  c  homogeneous neumann bc  tangent traction    sigmat  0   leave side   homogeneous dirichlet bc   ux0   homogeneous neumann bc  tangent traction    sigmat  0   right side   homogeneous dirichlet bc   ux0   homogeneous neumann bc  tangent traction    sigmat  0   bottom side   homogeneous dirichlet bc   uy0   homogeneous neumann bc  tangent traction    sigmat  0   PRON think that this may answer PRON question regard the number of boundary condition  
__label__machine-learning __label__neural-network __label__deep-learning __label__svm PRON have build PRON model  now PRON want to draw the network architecture diagram for PRON research paper  example be show below    for automated drawing  see how do PRON visualize neural network architecture   httpssoftwarerecsstackexchangecomq28169903 and  httpssoftwarerecsstackexchangecomq47841903  for manual drawing  see httpsreddit574usi 
__label__machine-learning __label__neural-network __label__deep-learning __label__optimization __label__learning-rate PRON be not an expert and do not have theoretical justification for that  but PRON seem to PRON that the small network error be  the small learning rate should be   be there an algorithm to dynamically update learning rate base on total error of the network without rely on any hyper  parameter   PRON intuition be on point  and shrink the learning rate like this be often refer to as  anneal   but link the learning rate to error magnitude neglect certain problematic error surface topology  an excellent motivate example be the rosenbrock  banana  function  which be often use as a test case for optimization algorithm  the  banana  be a low error valley which hide the global minimum  if an optimization path find PRON way into this valley  the path to the global minimum be along a nearly flat gradient   if PRON use an optimization algorithm that naively shrink the learning rate relative to the error  PRON be go to get stick as soon as PRON hit the valley  on the one hand  congrat  PRON have achieve a low error solution  but PRON be not necessarily anywhere near the global minimum  so how can PRON do good   an approach use by modern gradient  base method like adagrad  rmsprop  and adam be to separately assign learn rate to each parameter  and tie the learning rate to the magnitude of the respective parameter s update  the stanford cs231n lecture note explain   adagrad be an adaptive learning rate method originally propose by duchi et al    assume the gradient dx and parameter vector x  cache   dx2  x    learningrate  dx   npsqrtcache   eps   notice that the variable cache have size equal to the size of the gradient  and keep track of per  parameter sum of squared gradient  this be then use to normalize the parameter update step  element  wise  notice that the weight that receive high gradient will have PRON effective learning rate reduce  while weight that receive small or infrequent update will have PRON effective learning rate increase  
__label__machine-learning __label__neural-network PRON have be go through the standford  coursera machine learning course  and PRON be be go pretty well  PRON be really more interested in the understanding of the topic than get the grade from the course and as such PRON be try to write all the code in a programming language PRON be more fluent in  something PRON can easily dig down into the root of    the way PRON learn good be via work through problem  so PRON have implement a neural network and PRON do not work  PRON seem to get the same probability of each class irrespective of the test example  for example 045 of class 0  055 of class 1  irrespective of the input value   strangely this be not the case if PRON remove all the hide layer   here be a brief run through of what PRON do   set all theta s  weight  to a small random number  for each training example  set activation 0 on layer 0 as 1  bias   set layer 1 activation  input  forward propagate   zj1   thetaj  x activationj    matrix operation   activationj1   sigmoid function  zj1    element wise sigmoid   set hx  final layer activation  set bias of each layer  activation 00   1   back propagate   calculate delta   deltalast layer   activationlast layer   y   y be the expect answer from training set   deltaj   transposethetaj   x deltaj1     activationj  ones  activationj     where one be a matrix of 1 s in every cell  and   be the element wise multiplication    do not calculate delta0  since there inst one for input layer   deltacapj   deltacapj   deltaj1  x transposeactivationj    next  end for   calculate d   dj   1train  deltacapj   for j  0   dj   1train  deltacapj   lambdatraining  thetaj   for j  0    calculate cost function   jtheta   1training  yloghx    1ylog1hx   lambda  2   training   theta2  recalculate theta  theta  theta  alpha  d  that be probably not a great deal to go on  if someone can tell PRON if there be any major flaw in PRON code that would be fantastic  otherwise some general idea of where PRON may be go wrong  how to debug something like that that would also be great   edit   here be a quick image of the network  include a test case of input and response   this be after 1 million iteration of gradient descent    the datum set PRON have use be two exam score as the x s and the success  failure of get into a university as the y clearly two test score of 0 would mean failure in get into university however the network suggest 56  chance of get PRON with 0 s as input   edit  2   PRON have run a gradient checking algorithm with the following sort of result   numerical calculation  00074962585205895493  value from propagation  062021047431540277  numerical calculation  00032635827218463476  value from propagation  039564819922432665  etc  clearly there be something wrong here  PRON be go to work through PRON   be PRON input scale  not do so can because the weight to immediately blow up   PRON be common practice to preprocess PRON input to be between 1 and 1 or at least in that range  otherwise PRON be run the risk of PRON gradient explode or vanishing  cover here   
__label__complexity __label__linear-solver __label__sparse PRON be claim that the time and memory complexity of sparse direct solver be  on2 and  on43 for 3d problem and  on15 and  on log n for 2d  respectively   but how do a general  purpose direct solver know about dimensionality  if PRON call a direct solver with an arbitrary matrix  what be the complexity then  be PRON then hide in constant inside asymptotic estimation   a sparse direct solver know the matrix  and hence PRON dimension and PRON sparsity pattern  of course PRON do not know the dimension of the problem dimension before discretization   however  the sparsity patttern reveal the dimension indirectly  in particular  the complexity estimate PRON report be base on the assumption that PRON have a sufficiently fine discretization of a 2d or 3d problem  and hold only asymptotically as the refinement go uniformly to zero   a 3d model of a long  thin bar be most likely  on though in three dimension  as two of the three dimensionswill hardly be refine in practice    now direct solver be base on a tree decomposition of the sparsity graph  and PRON width determine the complexity  now nest dissection argument provide bound on the treewidth and hence the complexity estimate PRON mention  
__label__data-mining __label__nlp PRON would like to post a paper in international conference on soft computing  PRON want to know whether the journal be a repute journal   PRON can check PRON here  at least that one be in the list   httpwwwscimagojrcomjournalrankphpcategory1712amparea0ampyear2013ampcountryusampordersjrampmin0ampmintypecdamppage0 
__label__machine-learning __label__neural-network __label__deep-learning __label__computer-vision __label__caffe why do PRON need for shortcut connections to build residual networks  and how PRON help to train neural network for classification and detection   the short answer be that when a net be very deep PRON become very difficult for gradient to propagate backwards all the way  skip connection offer  short cut  for gradient to propagate further and allow for efficient training of very deep net    why do PRON need for shortcut connections to build residual networks   because otherwise the network would not be a residual network   how  do residual connection  help to train neural network for classification and detection   PRON add shortcut for the gradient  this way the first few layer get update very quick  the vanish gradient problem be no longer a problem  so PRON can easily make network with a 1000 layer  see residual networks paper   there be nothing specific about classification  detection  PRON about very deep network  
__label__machine-learning PRON be use the weka workbench to train a protein fold classifier  PRON import PRON training datum into weka and perform pca  base feature selection  this seem to have work fine  but now PRON can not evaluate PRON train classifier on the test datum because the test data contain all the original attribute  of course  if PRON try to run the feature selection on the test datum  PRON will come up with a different set of feature   in weka  after PRON have apply feature selection to a training set  how do PRON pull those same feature out of a test set   with any such modelling thing  PRON be go to have to recalculate the model use the new training set  ie  the original minus PRON test set    the usual approach be to randomly extract a subset for testing  then train use all of the remain data point   of course there will be some random variability accord to which be extract  so PRON can repeat the process to get some statistical significance   PRON final model will not be train on all of PRON datum  PRON either have to live with that  what PRON usually see in the natural language processing field   or once PRON have determine PRON good parameter compute the final model use all of PRON data  with the understand PRON will not be able to test PRON   PRON appear that the pca  base feature selection generate pseudo  feature that be linear combination of the original feature  after perform feature selection on the training set  PRON have not find a way to pull out the same pseudo  feature from the test set   however  if PRON be not tie to a particular feature selection method  pca in this case   PRON can alternatively use a feature selection method that be more straightforward  for example  PRON end up use a method that sort the feature accord to information gain  use this method  PRON be easy to identify the top n feature that give the good information gain and extract only those feature from the training and test datum   as suggest in the comment  there may be additional alternative implement in different language  which PRON would be willing to consider  give a more detailed response   but this be the good PRON could find in the weka environment  
__label__matlab __label__ode __label__simulation __label__quantum-mechanics answer  give a software for calculate conditional lyapunov exponent  cle  for coupled oscillator in chaos synchronization  however  PRON be hard to follow and there be no graphical output of the plot  and PRON be in c  more complex   do anyone know how to modify the let toolbox which be very good for uncoupled system but PRON do not understand how to work with synchronize system so as to accommodate the cle   PRON have a confusion regard how to include the driver signal while calculate the jacobian matrix for cle since theory say that cle be find for the response system  so do not PRON have to find the jacobian for the driver as well as the response system for similar oscillatorsdrive and response   or should PRON consider both the drive and response system in the software and proceed as if PRON be a single system   how to accommodate an external forcing like a random process in a state equation if any in cle   be there any other implementation for cle   thank PRON  here be a link to an old paper in j comp phys which compute the le for couple random dynamical system httpmesoscopicmineseduacousticsoldpreprintslyapunovpdf  perhaps PRON will find PRON more readable   PRON be a stable  well  test algorithm  the code be develop by PRON co  author who be now at ku  in case PRON want the code   
__label__machine-learning __label__feature-selection __label__feature-extraction PRON find a script on kaggle s titanic competion in wich the creator convert multi  value single feature  namely pclass   123   to 3 binary feature   what be the pro  con of such a conversion  do PRON really improve something  or be PRON bad  or do not change anything   despite take longer to compute  PRON also worry that PRON could introduce a bias in the model   as k3 suggest  as sean owen tell PRON the name  one hot encoding   PRON can provide at least some element to answer PRON   as find on sklearn documentation      integer representation can not be use directly with scikit  learn  estimator  as these expect continuous input  and would interpret the  category as be order  which be often not desire   PRON be implement in sklearn as onehotencoder and in pandas as getdummie  
__label__random-forest __label__ensemble-modeling PRON have notice that when PRON make a small decision tree model  and then extend the model by create an ensemble of tree around the same tree setting  the variable importance be dilute in the sense that the least important and most important variable become a lot more closer together  in some case  there may be almost do distinction in importance   be there method available to either mitigate this effect or to measure PRON  with a view to define any tradeoff between understand variable importance and overall accuracy   PRON have be use tree  base enesembling method such as random forest and gradient boost for several year now  and PRON have to say that PRON have never see that behavior   some package measure variable importance solely base on tree  final split rather than candidate  surrogate split  so if PRON have two important input that be correlate  but one be consistently a tiny bit better than the other  the less important input may never get select as a final split and thus look as important as some of the less valuable input  however  this phenomenon be independent of the number of tree  ensembling  so PRON do not think PRON fully explain the behavior PRON be see  
__label__python __label__tensorflow __label__word2vec __label__word-embeddings __label__gensim so PRON be learn word2vec for the first time and PRON question be quite basic  how to know what approach to use  ie  word2vec in tensorflow or word2vec train with gensim   in what case would implement PRON through the more manual first approach be useful vs the second one  if there be already an easy way to train a word2vec model use gensim  why be that not use always   furthermore  what be the benefit in use a pre  train model like the google news dataset  what happen when there be word that be not include in the news dataset   sorry if this question be basic  PRON just want to get a clear grasp of the overall picture   tensorflow have implementation for a pool of machine learn algorithm  so PRON should be comfortable if PRON application need to build something on top of word2vec  gensim be mainly intend for topic modelling technique  but pretty robust as PRON PRON main work   if PRON want to get a clear grasp of how the algorithm work  then implement manually make sense  else  just go with one of the implementation   google word2vec model be pretty good and cover most of the english word  use PRON  if PRON do not have computational power or time to train a model  manual training give PRON the freedom of choose domain  specific dataset  window size  cbow or sg  length of vector  if there be out of vocabularyoov  word  PRON will throw an error   the advantage of use pre  train vector be be able to inject knowledge from a large corpus than PRON may have access to  word2vec have a vocabulary of 3 million word and phrase train on the google news dataset comprise 100 billion token  and there be no cost to PRON in training time   in addition  PRON be fast and easy to use  just load the embedding and look PRON up  PRON be straightforward to substitute different set of pre  train vector  fasttext  glove etc  as one may be more suited to a particular use case   however  when PRON vocabulary do not have an entry in word2vec  by default PRON will end up with a null entry in PRON embed layer  depend on how PRON handle PRON   PRON will need to consider the scale  impact and how to address PRON  keep  discard  consider online training   as yazhi say a decision must be make about how to handle out of vocabulary word   the advantage of learn word vector from PRON own corpus be that PRON would be derive from PRON dataset  so if PRON have reason to believe that the composition of PRON data be significantly different from the corpus use for the pre  train vector then that may result in good downstream performance  however  that come at a cost in time take to train PRON own vector representation  
__label__matrix __label__eigenvalues __label__dense-matrix suppose a real  dense and asymmetric square matrix  ainmathbbrntime n   all PRON eigenvalue  lambdai in mathbb r  be PRON possible to construct a symmetric matrix  binmathbbrntime n  the eigenvalue of which be  lambdai2  to compute absolute value of  lambdai without the need of solve eigenvalue of  a   
__label__machine-learning __label__linear-regression __label__python PRON have a sample set of datum about lead that get generate every day  lead be nothing but a user express request to be PRON partner or not  sample datum set be as show below  leadid  createdate  status  leadtype  81002924dec17 120000000000000 am  open  leadtype1  80613630dec17 120000000000000 am  open  leadtype2  81213431dec17 120000000000000 am  open  leadtype2  80614731dec17 120000000000000 am  open  leadtype1  80616601jan18 120000000000000 am  open  leadtype2  2800204mar16 120000000000000 am  open  leadtype2  80815601jan18 120000000000000 am  open  leadtype1  80816201jan18 120000000000000 am  open  leadtype2  80625707jan18 120000000000000 am  open  leadtype1  83209117jan18 120000000000000 am  open  leadtype2  83807917jan18 120000000000000 am  open  leadtype1  6600126mar16 120000000000000 am  open  leadtype1  7000128mar16 120000000000000 am  open  leadtype2  80601923dec17 120000000000000 am  open  leadtype2  82206412jan18 120000000000000 am  open  leadtype1  83404314jan18 120000000000000 am  open  leadtype2  83605316jan18 120000000000000 am  open  leadtype1  83811919jan18 120000000000000 am  open  leadtype2  as PRON can see lead type can be of leadtype1 or leadtype2 and this get generate every day   in order to make sense of datum PRON create the follow plot use python  the support code be as follow  note PRON be just a noob to python and ai but PRON want to check if this prove a valid use case for machine learning and what should be PRON approach  import numpy as np  import panda as pd  import matplotlibpyplot as plt    matplotlib inline  infile   leaddatacsv   mydf  pdreadcsvinfileencodinglatin1    fig  ax  pltsubplotsfigsize157     g  mydfgroupbyr4gstateleadtypecountstatusunstack    g  mydfgroupbyr4gstatestatuscountleadtypeunstack    gplotaxax    axsetxlabelr4gstate    axsetxlabelr4gstate    axsetylabelnumber of lead    axsetxticksrangeleng      axsetxticklabelss   item for item in gindextolist     rotation90    basically PRON just read the csv  curat the datum  PRON have clean the original csv  to keep what be meaningful for PRON  PRON also create grouping of number of lead month  year wise so that PRON can see the historical lead generate every month   PRON want to know if machine learning help PRON to predict number of lead generate in next coming month base on previous month datum   if the answer be yes then be linear regression the right path to explore further  
__label__definitions __label__expert-system from wikipedia  citation omit   in artificial intelligence  an expert system be a computer system that emulate the decision  make ability of a human expert  expert system be design to solve complex problem by reason about knowledge  represent mainly as if – then rule rather than through conventional procedural code  the first expert system be create in the 1970 and then proliferate in the 1980s  expert system be among the first truly successful form of artificial intelligence  ai  software   an expert system be divide into two subsystem  the inference engine and the knowledge base  the knowledge base represent fact and rule  the inference engine apply the rule to the know fact to deduce new fact  inference engine can also include explanation and debug ability   crud webapp  website that allow user to create new entry in a database  read exist entry in a database  update entry within the database  and delete entry from a database  be very common on the internet  PRON be a vast field  encompass both small  scale blog to large website such as stackexchange  the big commonality with all these crud app be that PRON have a knowledge base that user can easily add and edit   crud webapp  however  use the knowledge base in many  myriad and complex way  as PRON be type this question on stackoverflow  PRON see two list of question  question that may already have PRON answer and similar questions  these question be obviously inspire by the content that PRON be type in  title and question   and be pull from previous question that be post on stackexchange  on the site PRON  PRON can filter by question base on tag  while find new question use stackexchange s own full  text search engine  stackexchange be a large company  but even small blog also provide content recommendation  filtration  and full  text search  PRON can imagine even more example of hard  cod logic within a crud webapp that can be use to automate the extraction of valuable information from a knowledge base   if PRON have a knowledge base that user can change  and PRON have an inference engine that be able to use the knowledge base to generate interesting result  be that enough to classify a system as be an  expert system   or be there a fundamental difference between the expert system and the crud webapp    this question could be very useful since if crud webapp be act like  expert system   then study the good practice within  expert system  can help improve user experience    the key feature of an expert system be that the knowledge base be structure to be traverse by the inference engine   web site like stack exchange do not really use an inference engine  PRON do full  text search on minimally  structure datum   a real inference engine would be able to answer novel query by put together answer to exist question  stack exchange site can not even tell if a question be duplicate without human confirmation   no  PRON do not think there be any reason to say that  in general  crud app  be  expert system  a give crud app could incorporate an expert system  but by and large crud app be consider among the  dumb  of application exactly because PRON do not feature much intelligence  PRON can just create  read  update and delete entity   from what PRON have see  the close PRON get to see anything like an expert system in a typical enterprise crud app be some validation  business rule logic build use something like drools 
__label__machine-learning __label__classification __label__vc-theory when use vc  dimension to estimate the capability of a binary classifier  PRON can find 3 point in r2 that can be shatter  eg   but PRON can not shatter any 4 point with a circle   this be state in these lecture notess  can anyone give PRON an intuitive explanation   here be a  mental picture    imagine  any  four point in 2d  except for some obvious boundary case  PRON will form a four sided figure  like below    label the two point that be the furth from each other with a  and the other two as    now  a circle contain both  point  will also enclose the  point   that be because PRON diameter be at least the distance between the  label  which be by construction great than the distance to the other point    since this construction work for any four point  PRON get the result  
__label__dataset __label__time-series __label__energy PRON be use python and have the following method to segment accelerometer datum base on energy of signal  def segmentenergydata  th    mag  nparraynplinalgnormdatax     nplinalgnormdatay     nplinalgnormdataz       mag  npmeanmag   above  npwheremag  gt thnpstdmag    indicator  npzerosmagshape   indicatorabove   1  pltplotmag   pltplotindicator  1000   r    pltshow    the problem be that PRON have to pass in a constant th where instead PRON would like PRON to be adaptive like in an automatic segmentation technique in body sensor networks base on signal energy   
__label__linear-solver __label__condition-number __label__conditioning suppose PRON have a linear system and PRON know nothing about PRON conditioning and have no preliminary information about the solution  PRON blindly apply gaussian elimination and obtain some solution  x be PRON possible to determine whether this solution be trustworthy  ie that the system be well  condition  without thorough preliminary analysis of the matrix  do the magnitude of pivot give reliable information   and generally  what be the main guideline for detect ill  conditioning  on the fly    PRON be nearly impossible to to tell if the PRON system be ill condition from just one result   unless PRON have some foresight into the behavior of PRON system  ie know what the solution should be   there be not much PRON can say from a single solution   having say this  PRON can gain more information if PRON solve more than one system with the same  a   suppose PRON have a system of the form  ax  b for a specific a which PRON have no prior knowledge about PRON conditioning  PRON can perform the following test   solve  ax  b for a specific right hand side vector  b  perturb PRON right hand side vector by  bnewbmathbfvarepsilon where  mathbfepsilon be very small in comparison to  b  solve  axnewbnew  if PRON system be well  condition  PRON new solution should be fairly close to PRON old solution  ie  x  xnew should be small    if PRON observe a dramatic change to PRON new solution  ie  x  xnew be large   then PRON system be probably ill  condition   PRON may need to solve several linear system with different right hand side vector to give PRON a good indication of whether the system be ill  condition   of course  this process be a bit expensive   thetan3operation for the first solution and  thetan2 operation for each successive solution  assume PRON direct solver save PRON factor    if PRON matrix a be fairly small  this be not a problem   if PRON be large  PRON may not want to do this   instead  PRON may be good off calculate the condition number  acdota1 in a convenient norm   when be a matrix ill condition  PRON depend on the accuracy of the solution PRON be look for  as much as  beauty be in the eye of the beholder    may be PRON question should better rephrase as be there cheap and robust condition number estimator base on the  lu factorization   assume PRON be interested in the real general  dense  non symmetric  problem in double precision arithmetic PRON would suggest PRON to use lapack expert solver dgesvx which provide a condition estimate in the form of PRON reciprocal   textrcondapprox 1kappaa as a bonus PRON have also other goody like equation equilibration  balancing  iterative refinement  forward and backward error bound  by the way  pathological ill conditioning   kappaa   gt  1epsilon  be signal as an error by infogt0   go into more detail  lapack estimate the condition number in the 1norm  or  inftynorm if PRON be solve  at x  b  via dgecon  the underlying algorithm be describe in lawn 36   robust triangular solf for use in condition estimation    PRON have to confess that PRON be not an expert in the area  but PRON philosophy be   if PRON be good enough for lapack  PRON be for PRON    the solution of an ill  condition system of equation with a matrix of norm 1 a random right hand side of norm 1 will have with high probability a norm of the order of the condition number  thus compute a few such solution will tell PRON what be go on  
__label__python __label__floating-point __label__approximation-algorithms accord to beatson and greengard s short course on fmm    eq  515  amp  516 set k1  q1   PRON can approximate a potential  phi  1r  r use       1over vecrvecr   sumn0inftyrnover rn1    4piover2n1    summ  nn  ynmtheta  phi  ylmtheta   phi      PRON try this in python  the error be such that     left  phipsumn0psumm  nn   cdot  right le  1 over r  rleft  rover r rightp1     def potentialexpansion  p   r1  theta1  phi1   r2  theta2  phi2       return inverse r potential expansion upto the pth term   input         p  term in the expansion to return  r1  theta1  phi1  position vector component for r  r2  theta2  phi2  position vector component for r      coefficient  npzero   p1  p1   complex   for n in xrangep1    for m in xrangen  n1    coefficient  sphharm  m  n  theta1  phi1   sphharm  m  n  theta2  phi2   coefficientsnm   4pi2n1r2r1n  r1coefficient  return coefficientssumreal  use this method PRON be get wrong result  take a simple example  p  5  r1  100   theta1  0   phi1  0   r2  1   theta2  pi2   phi2  0   cosgamma  costheta1costheta2sintheta1sintheta2cosphi1phi2   potential  1root  r12  r22  2r1r2cosgamma   print  direct calculation   s   potential  approxpotential  potentialexpansionp  r1  theta1  phi1  r2  theta2  phi2   print  approximation   s   approxpotential  print  print  error   s   npabsapproxpotential  potential    print  upper bind on error   s    1r1r2r2r1p1    this output completely wrong result   approximation0010101010101  direct calculation00099995000375  error0000101510063503  upper bind on error10101010101e14  should PRON investigate whether or not this be a float point error  if so  how can PRON go about this   in order to inconvenience as many people as possible  long ago  mathematician and physicist decide to use two different convention on whether  theta or  phi be the latitude angle  greengard s note use the physicist  convention that  theta be latitude and  phi be longitude  whereas scipy use  theta for longitude and  phi for latitude  so switch the order of the argument theta and phi fix PRON code   just a small comment  the sequence of summation may because divergence as well  PRON learn in school always to sum up the coefficient from small to large magnitude  how be the line  return coefficientssumreal  sum up  be not specify  PRON may consider that  
__label__algorithms PRON be search for a good algorithm to solve problem 487 on project  euler  PRON do not want code or something like that  PRON only want the name of the algorithm that s best suit  till now PRON think of either use faulhabers formula or use the stirl number of second kind to compute the sum   httpsprojecteulernetproblem487  if there be a good algorithm please let PRON know   
__label__clustering __label__categorical-data suppose PRON have a movie dataframe in panda  one of the feature be genre   PRON have a list of genre name  for example   movieid  genre  1   action  thriller  drama   2   romance  comedy   3   action  romance   how do PRON use this column for a clustering problem  say k  mean clustering   PRON can easily use getdummy function in pandas to convert PRON to numerical vector   the idea be that categorical variable do not have a numerical intuition eg when PRON come to the definition of distance  but just imagine PRON have one feature genre with 3 value comedy  romance and crime  then PRON can model PRON in a 3dimensional space by say comedy   100   crime   010  and romance   001   PRON replace 1 feature with three but intuitively work well   update  PRON just understand PRON question after edit PRON  PRON be a bit fuzzy previously  but PRON keep PRON initial answer and add an update   in this case use the value of the feature genre  unique value of union of all genre set in that column  as new feature and determine PRON presence with 1 and 0 otherwise  should work   movieid  action  thriller  drama  romance  comedy  1  1  1  1  0  0  2  0  0  0  1  1  3  1  0  0  1  0  k  mean will work really bad on such datum  because the method be design to process continuous value  where squared error need to be optimize   rather than try to find a hammer that match PRON  nail   PRON first need to understand PRON  nail  as PRON may be a screw  so what be PRON objective  what be an answer result  and when be a result good  only then PRON can find an algorithm to optimize this problem  if PRON simply try random algorithm  force PRON datum into some unnatural form that do not preserve the relevant property  this be a waste of time  PRON will literally be solve a different problem  
__label__algorithms __label__computational-chemistry __label__statistics __label__molecular-dynamics PRON have be read a recent paper   in PRON  the author perform molecular dynamic  md  simulation of parallel  plate supercapacitor  in which liquid reside between the parallel  plate electrode   to simplify the situation  let PRON suppose that the liquid between the electrode be argon liquid   the system have a  slab  geometry  so the author be only interested in variation of the liquid structure along the  z direction   thus  the author compute the particle number density average over  x and  y   barnalphaz  where  alpha be a solvent specie   that is  in PRON simplified example   alpha be argon  an argon atom     barn   alphaz have dimension of  fractextnumbertextlength3 or simply  textlength3  PRON think   the  xyplane be give by the inequality  x0  lt  x  lt  x0  and  y0  lt  y  lt  y0  the area  a0  of the  xyplane be thus give by  a0  4x0y0  so  the author define the particle number density average over  x and  y as follow    barnalphaz   a01  intx0x0  inty0y0  dxprime dyprime nalphaxprime  yprime  z where  a0  4x0y0  and  nalphax  y  z be the local number density of  alpha at   x  y  z  thus   barnalphaz be simply proportional to  nalpha integrate over  x and  y  but  PRON question be  what be  nalphax  y  z   how be  nalphax  y  z determine in practice   as far as the computer be concern  the argon atom be point particle  PRON be model as have zero volume  although PRON interact by lennard  jones interaction    so how be PRON possible to define a number density   do PRON simply  cut  the  slab  in  slice  along  z and then assign the particle to these slice   there may be 5 particle in the first  z slice  10 in the second  7 in the third  and so on   if PRON then divide 5  10  and 7 by the volume of the respective slice  then PRON have a sort of number density  with unit of  fractextnumbertextlength3 or simply  textlength3  but how do PRON now integrate this  nalphaxprime  yprime  z over  x and  y   do PRON have to additionally perform bin in the  x and  y direction   as be often the case in simulation paper  the mathematical description of the reported quantity be not literally describe the algorithm use to compute these quantity   this typically happen when the main author and the compute monkey be not the same person    in PRON case  there be no point to first compute  nalphaxyz and then integrate out  x and  y as PRON suggest in the question  one may estimate  barnalphaz directly by only construct bin along the z  axis  just compute  the time  average of  the number of particle in a bin and divide PRON by the volume of the bin  that will give PRON an estimate of  barnalphaz in the bin  
__label__algorithms __label__finite-difference __label__python __label__performance __label__iterative-method consider the function    fmathbfx    sumn0n  an left   mathbfbmathbfxcdot nabla rightn frac1r  where  r  mathbfx  sqrtx  x02   y  y02 and  an and  mathbfb be well  know   be there any good way to evaluate this function numerically  for fairly large n    PRON problem be that PRON do not have a general expression for    left   mathbfbmathbfxcdot nabla rightn frac1r  be there a smart way than just recursively do numerical derivative of  1r  PRON be quite scared of this method since multiple derivative could lead to numerical error accumulate   thank in advance   PRON can convert  mathbfbmathbfx into polar coordinate  and do the dot product in this system  this change    mathbfbmathbfxcdotnablanfrac1r to    leftmathbfbmathbfxrfracpartialpartial r    mathbfbmathbfxthetafrac1rfracpartialpartial thetarightnfrac1r  here  PRON be use the subscript  r and  theta to indicate the  r and  theta component  respectively  of the difference  PRON can ignore the second term of this entirely  as PRON involve derivative with respect to  theta and PRON only have a dependence on  r  which further simplify the term to     mathbfbmathbfxrnfracpartialnpartial rnfrac1r  now use the fact that    fracpartialnpartial rn  frac1r    1nfracnrn1  PRON now have a nice analytical derivative that PRON can use in PRON summation  PRON final equation will look like    fmathbfxsumn0n1n anmathbfbmathbfxrnfracnrn1 
__label__nlp PRON be curious about natural language querying   stanford have what look to be a strong set of software for process natural language   PRON have also see the apache opennlp library  and the general architecture for text engineering   there be an incredible amount of us for natural language processing and that make the documentation of these project difficult to quickly absorb   can PRON simplify thing for PRON a bit and at a high level outline the task necessary for perform a basic translation of simple question into sql   the first rectangle on PRON flow chart be a bit of a mystery   for example  PRON may want to know   how many book be sell last month   and PRON would want that translate into  select count     from sale  where  itemtypebook  and  salesdate  gt  512014  and  salesdate  lt  5312014   natural language query pose very many intricacy which can be very difficult to generalize   from a high level  PRON would start with try to think of thing in term of noun and verb   so for the sentence  how many book be sell last month   PRON would start by break the sentence down with a parser which will return a tree format similar to this   PRON can see that there be a subject book  a compound verbal phrase indicate the past action of sell  and then a noun phrase where PRON have the time focus of a month   PRON can further break down the subject for modifier   how many  for book  and  last  for month   once PRON have break the sentence down PRON need to map those element to sql language eg  how many   count  book   book  sell   sale  month   salesdate  interval   and so on   finally  once PRON have the element of the language PRON just need to come up with a set of rule for how different entity interact with each other  which leave PRON with   select count     from sale  where  itemtypebook  and  salesdate    512014  and  salesdate  lt  5312014   this be at a high level how PRON would begin  while almost every step PRON have mention be non  trivial and really the rabbit hole can be endless  this should give PRON many of the dot to connect   turn simple question into answer be not simple whatsoever   the first technology to do this as broadly across technology and accurately will be a big winner   however  there be some out there  fill in the gap with  answering question  with artificial intelligence  eg ibm watson  and amazon alexa   this require solve the language complexity relate to the datum in question  what be in the data store  and what be noun  verb and pronoun   microsoft venture here with english query but  then stop  kuerime be a python base platform do about the same thing   structured query languages  sql  and the like  soql  mdx  hive  impala and the new take on old fashioned sql  have not yet replace much of anything  all these piece be small fix to the grander  end goal  and that lie in artificial intelligence  ai   specifically  machine learning   the question be    can the computer  figure out what PRON want    not yet  PRON take linguists  mathematicians  engineers and more to all contribute PRON piece of the pie so PRON can enjoy some of that sweet artificial intelligent and machine learned cake   there be several approach to create a parser that would parse plain text message into sql  for example  PRON can create a grammar  base parser and use an nlp algorithm to build the structured query  if PRON already have plenty of parse message from one domain  like e  commerce   PRON can try a machine learning approach and use PRON for PRON further parsing   however  PRON think the good approach be to combine a grammar  base parser for text  to  sql translation  and ml to complement the rule  base grammar by fix the syntax  eradicate typo  etc   learn more about the different approach here  
__label__neural-network __label__multiclass-classification __label__object-recognition need for date portal  PRON currently use  openface to extract feature with dnn   PRON do not know whether PRON have a usable meaning  then compare PRON to some model face  feature use euclidian distance and rate accord to these value  result be below satisfactory  PRON be think about svm  but PRON need to distinguish between a few level  rank 1  5   onevsall  not sure if PRON need 5 classifier just to get the measure of similarity  any idea   what have be try   extract feature for a set of model face  photo choose by PRON from google   extract feature from a test set and sort photo accord to average  or minimum  euclidian distance to these model feature  visual estimation  many from those which be in top 10  be noticeably not similar  and not attractive  compare to a model set  labeled test set  accord to PRON taste  as 1 and 0 class  70 and 1640 sample  and cv set  14 and 194   extract feature  train rbf svm  perform parameter search   
__label__iterative-method __label__condition-number __label__solver PRON have to solve an ill  condition sparse matrix  once PRON read that iterative solver be the good tool for such problem  be that true  if yes  why   PRON question really do not admit a simple answer PRON need to know more specific about PRON problem to provide a useful answer   in general  iterative method can be fast than direct factorization for large sparse system of equation if the system be reasonably well condition or if PRON be badly condition but PRON have a good preconditioner or if PRON will apply regularization to help improve the conditioning of the problem   another important issue be whether PRON need an extremely precise solution or whether PRON be willing to live with a less accurate solution   in decide between the approach PRON be important to know   how big be PRON system of equation   do PRON have any special structure  eg symmetric and positive definite    how badly condition be the system of equation   be there an available pre  conditioner if the system be not well condition   how accurate an answer do PRON need   be PRON willing to use some kind of regularization to improve any ill  conditioning  
__label__finite-element __label__numerical-analysis __label__quadrature __label__polynomials __label__integration let  deltasubseteqmathbb r2  denote the triangle span by   00    10 and   01 and   mathbb prdeltaleftpdeltatomathbb rmid pxsumalphale rlambdaalpha xalphatext  for all  xindeltatext  for some   lambdaalphaalphale rsubseteqmathbb rright for  rinmathbb n0  PRON be numerically solve a pde on a rectangle  lambda0atimes0b triangulate use quadratic lagrange finite element as depict in the following figure   in the assembly of the linear system  PRON need to compute integral of the form   intdeltau0circ ftildedeltapsitag1 where  u0  be the solution of the previous time step   ftildedelta be the transformation from a finite element  tildedelta to  delta and  psiinmathbb p2delta  how should PRON compute these integral   by definition of the finte element space   leftu0righttildedeltainmathbb p2tildedelta so  the integrand in   1 belong to  mathbb p4delta  that be why PRON guess PRON should use a quadrature scheme which exactly integration  mathbb p4deltafunction  however  since PRON only now the value of  u0circ ftildedelta at  a000   a110   a201   a31212   a4012 and  a5120  the gaussian quadrature ansatz   intdelta fapproxfrac12sumi05wifaitag2 be somehow limit  in fact  with these  ai PRON be only possible to exactly integrate  mathbb p2deltafunction  and PRON have  w0w1w20    w3w4w513     so  be there no possibility to do good   please suppose that PRON have a fix mesh  ie no adaptive refinement  PRON need to deal with   
__label__matlab __label__anomaly-detection PRON have datum from a smartphone accelerometer  here be the plot of the time series datum   this data have be capture as a stroller go over a half inch platform  the phone have be place in the stroller  the orientation of the phone be not the concern at the moment  PRON be try to segment the  event  of the stroller go onto the platform and back down from the platform which can be see as the sudden spike in z acceleration in the plot  any cue as to how PRON can find the window of this event to generalize for this scenario   
__label__scalability __label__scala PRON know that spark be fully integrate with scala  PRON be use case be specifically for large datum set  which other tool have good scala support  be scala best suit for large data set  or be PRON also suit for small datum set   from listen to presentation by martin odersky  the creator of scala  PRON be especially well suit for build highly scalable system by leverage functional programming construct in conjuction with object orientation and flelxible syntax  PRON be also useful for development of small system and rapid prototyping because PRON take less line of code than some other language and PRON have an interactive mode for fast feedback  one notable scala framework be akka which use the actor model of concurrent computation  many of odersky s presentation be on youtube and there be a list of tool implement with scala on wikiscalalangorg   an implicit point be that tool and framework write in scala inherently have scala integration and usually a scala api  then other api may be add to support other language begin with java since scala be already integrate and in fact critically depend on java   if a tool or framework be not write in scala  PRON be unlikely that PRON offer any support for scala   that be why in answer to PRON question PRON have point towards tool and framework write in scala and spark be one example   however  scala currently have a minor share of the market but PRON adoption rate be grow and the high growth rate of spark will enhance that  the reason PRON use scala be because spark s api for scala be rich than the java and python api   the main reason PRON prefer scala generally be because PRON be much more expressive than java because PRON allow and facilitate the use of function as object and value while retain object orient modularity  which enable development of complex and correct program with far less code than java which PRON have prefer because of widespread use  clarity and excellent documentation   re  size of datum  the short answer  scala work for both small and large datum  but PRON creation and development be motivate by need something scalable   scala be an acronym for  scalable language    the long answer  scala be a functional programming language that run on the jvm   the  functional  part of this be a fundamental difference in the language that make PRON think differently about programming   if PRON like that way of thinking  PRON let PRON quickly work with small datum   whether PRON like PRON or not  functional language be fundamentally easy to massively scale   the jvm piece be also important because the jvm be basically everywhere and  thus  scala code can run basically everywhere    note there be plenty of other language write on the jvm and plenty of other functional programming language  and language beyond scala do appear in both list    this talk give a good overview of the motivation behind scala   re  other tool that have good scala support   as PRON mention  spark  distributable batch processing better at iteratative algorithm than PRON counterpart  be a big one   with spark come PRON library mllib for machine learning and graphx for graph   as mention by  erik allik and tris nefzger  akka and factorie exist   there be also play   generally   PRON can not tell if there be a specific use case PRON be dig for  if so  make that a part of PRON question   or just want a survey of big datum tool and happen to know scala a bit and want to start there   scalanlp be a suite of machine learning and numerical computing library with support for common natural language processing task  httpwwwscalanlporg  scala be suit for both large and small datum science application  consider dynaml if PRON be interested to try a machine learn library which integrate well with apache spark  PRON be still in PRON infancy so to speak in term of number of model offer  but PRON make up for PRON by a broad and flexible machine learn api   to take a look at some sample use case consider  more where that come from   system identification  abott power plant  disclaimer  PRON be the author of dynaml 
__label__inverse __label__collocation __label__chebyshev assume a function  mt strictly increase  essentially grow exponentially  and asymptoptically  grow at a know rate  barg  ie  limttoinftymtmt   barg  in a set of awful integro  differential equation  PRON need to use the inverse of  mt  denote by  qcdot for simplicity  ie  t  m1m  equiv qm  the function  mt will be solve numerically with some sort of spectral collocation method with a polynomial basis  see below    PRON general problem be  with a guess on  mt  how can PRON find  qm   since this equation be grow exponentially  PRON do not think PRON can use a polynomial basis for  mt directly   instead  PRON suspect that PRON should guess a chebyshev basis for  gt constraining for some large  t that  gt   barg  if so  then    mt  equiv expleftint0t gtau  d tau right  which work fine for PRON computation   in particular  PRON be easy with a chebyshev basis to find the partial integral  int0t gtau  d tau for  t node of the chebyshev basis  through calculating integral for a function approximate by chebyshev polynomial   but if so  how can PRON find the  qm function  or a basis of PRON in  m space   obviously  PRON could just evaluate  mt at the chebyshev node of  t to find the appropriate  m  and then use some sort of interpolation to find the approximate  qm  but PRON would prefer an approach that have a chance of use auto  differentiation by throw PRON into PRON collocation method   for example  be there a way to translate this into a nonlinear ode use the inverse  function theorem  and if so do this simplify substantially give that this be a chebyshev basis   
__label__numerical-analysis __label__matrix __label__precision __label__accuracy __label__operator-splitting as far as PRON know  precision error become large as the condition number of a matrix increase   consider a matrix  base operator     a  nabla bullet k nabla    and a matrix  free operator     tildea   b1   b2   b3   qquad bi   partialxi  k partialxi  one difference between the matrix  free operator and matrix  base operator be that the diagonal of  a combine derivative along all 3 direction  whereas the matrix  free operator must add the diagonal for all 3 direction each time   PRON try compare these two approach numerically  and PRON see a difference between PRON on the order of machine accuracy  PRON seem that the difference between the operator also increase where  k be large   here be PRON question   1  be PRON just visualize the local condition number where  k be large   2  be one of these method superior be that the difference become large  or be this just lack of precision become more apparent from the increase condition number   3  be PRON just completely wrong   this should not happen   when solve a linear system  ax  b  PRON suffer from two source of error   round  off error  the ill  conditionedness of the matrix   the condition number of a computational representation of a matrix be  for all practical purpose equal to the condition number of the exact matrix  which PRON can not represent computationally   so PRON do not matter whether PRON represent PRON as a split  matrix  free form  or not   likewise  whether PRON add the three matrix together or not do not make any difference for the difference between PRON computational representation of the matrix and the  exact  matrix  that is  because for all practical purpose  the addition of 3 term be as accurate as the computation of each of these term  3 time round off be  in practice  equal to round off  namely very small   in other word  there will be no practical difference between the two representation of PRON matrix  unless PRON condition number be on the order of one over the level of round  off  that be hopefully not the case   PRON  two approach  be actually the same thing  PRON know already that PRON be mathematically the same  but PRON be also computationally the same thing  except for a possible change in the order of operation  a change in the order of operation will give an error of order of machine epsilon   PRON be right of the condition number  in the following sense  as  k increase  the effect of roundoff error be amplify proportionately  so if  k be much large than the quantity PRON operator be apply to  PRON will notice more roundoff error  
__label__open-source __label__dataset __label__crawling as an extension to PRON great list of publicly available dataset  PRON would like to know if there be any list of publicly available social network dataset  crawl api  PRON would be very nice if alongside with a link to the dataset  api  characteristic of the datum available be add  such information should be  and be not limit to   the name of the social network   what kind of user information PRON provide  post  profile  friendship network      whether PRON allow for crawl PRON content via an api  and rate  10min  1k  month      whether PRON simply provide a snapshot of the whole dataset   any suggestion and further characteristic to be add be very welcome   an example from germany  xing a site similar to linkedin but limit to german speaking country   link to PRON s developer central  httpsdevxingcomoverview  provides access to  user profile  conversation between user  limit to the user PRON   job advertising  contacts and contacts of contacts  news from the network and some geolocation api   yes PRON have an api  but PRON do not find information about the rate  but PRON seem to PRON  that some information be limit to the consent of the user   PRON be not a social network per se  but stackexchange publish PRON entire database dump periodically   stackexchange datum dump host on the archiveorg  post describe the database dump schema  PRON can extract some social information by analyze which user ask and answer to each other  one nice thing be that since post be tag  PRON can analyze sub  community easily   a small collection of such link can be find at here  many of PRON be social graph   a good list of publicly available social network dataset can be find on the stanford network analysis project website   snap dataset  the site contain internet social network datum  facebook  twitter  google plus   citation network for academic journal  co  purchase network from amazon and several other kind of network  PRON have direct  undirected  and bipartite graph and all dataset be snapshot that can be download in compress form   a couple of word about social network api  about a year ago PRON write a review of popular social networks’ api for researcher  unfortunately  PRON be in russian  here be a summary   twitter  httpsdevtwittercomdocsapi11   almost all datum about tweet  text and user be available   lack of sociodemographic datum   great streaming api  useful for real time text processing   a lot of wrapper for program language   get network structure  connection  be possible  but time  expensive  1 request per 1 minute    facebook  httpsdevelopersfacebookcomdocsreferenceapi   rate limit  about 1 request per second   well document  sandbox present   fql  sql  like  and « regular rest » graph api   friendship datum and sociodemographic feature present   a lot of datum be beyond event horizon  only friend  and friend  of friend data be more or less complete  almost nothing could be investigate about random user   some strange api bug  and look like nobody care about PRON  eg  some feature available through fql  but not through graph api synonym    instagram  httpinstagramcomdeveloper   rate limit  5000 request per hour   real  time api  like streaming api for twitter  but with photo   connection to PRON be a little bit tricky  callback be use   lack of sociodemographic datum   photo  filter datum available   unexpected imperfection  eg  PRON ’ possible to collect only 150 comment to post  photo    foursquare  httpsdeveloperfoursquarecomoverview   rate limit  5000 request per hour   kingdom of geosocial datum   quite closed from research because of privacy issue  to collect checkin datum one need to build composite parser work with 4sq  bitly  and twitter api at once   again  lack of sociodemographic datum   google  httpsdevelopersgooglecomapilatest   about 5 request per second  try to verify    main method  activity and people   like on facebook  a lot of personal datum for random user be hide   lack of user connection datum   and out  of  competition  PRON review social network for russian reader  and  1 network here be vkcom  PRON ’ translate to many language  but popular only in russia and other cis country  api doc link  httpvkcom  dev and from PRON point of view  PRON ’ the good choice for homebrew social medium research  at least  in russia  that ’ why   rate limit  3 request per second   public text and medium datum available   sociodemographic datum available  for random user availability level be about 60  70    connection between user be also available  almost all friendship datum for random user be available   some special method  eg  there be a method to get online  offline status for exact user in realtime  and one could build schedule for PRON audience   network repository  httpnetworkrepositorycom  have ton of social network  web graph  bio and brain network  etc  good of all  PRON also have interactive visual analytic tool to compare  explore the various social network  
__label__finite-element let say PRON have decompose a continuous function  yx over some domain  lx use know finite element method with local basis  qix suppose  l be divide into  m  element   if PRON want to know the function  yx at a point  x  p  where  p be not at a nodal value  then PRON will need to first know which element  p be in  PRON can then interpolate between the element node and find the value  yp PRON can do this for all  p in  l and get back PRON continuous function   to do this  one could search through every element to find the where  p lie  this be fine in simple case  but what about more complex case when there be 1000 s of element and PRON be work in 3d  be there an efficient algorithm that PRON be miss that could compute this   PRON could use a domain decomposition approach  divide the domain into coarse subdomain recursively and search through each one  like a binary tree search in 1d    if PRON know more about the point  p  PRON can probably make PRON more efficient   for example  if the point  p do not change  lookup information can be precomput and store during the setup stage  if PRON change in time but depend on property of the solution  PRON can often be determine by transform to the reference element and do the search in parallel   do not do PRON   on general mesh  find an arbitrary point  p require  on operation where  n be the number of cell  PRON can get away with significantly less  say  olog n  if PRON have a mesh that be construct hierachically from a coarse mesh by mesh refinement  but unless PRON use a completely structure grid  find an arbitrary point inside a mesh be simply a very expensive operation   secondly  once PRON have the cell  k in which  p lie  PRON need to find the location  hat p on the reference cell  hat k that correspond to  p for this  PRON have to invert the mapping  phik  hat k mapsto k  and this  in general  require a nonlinear iteration  which be also expensive   as a consequence  most algorithm try to avoid this at any cost  for example  when PRON do integration of function  PRON loop over all cell  and on each cell over the quadrature point  which be define on the reference cell  and so only need to be map forward with  phik   this be a far fast approach that loop over a bunch of quadrature point define in real space  as oppose to reference space relative to each cell    ultimately  the point be that PRON need to re  think the overall approach to do thing so that PRON always start with a loop over all cell  if at all possible   similar to comment from jesse chan  PRON could consider break up the domain into rectangular  prism in 3d  shape  then  one option could be to pre  compute a spatial hash table with the finite element by use a 2d  or 3d  coordinate to generate the key  PRON could then create the table by hash the vertex of each element   to efficiently find what element to do computation in  PRON could hash the coordinate of PRON arbitrary point  p and then loop through the element in the result hash table bucket until PRON find the element PRON need to do local computation in  if PRON build the hash function well  PRON could end up with very quick look up  
__label__dataset __label__performance __label__mnist the mnist handwritten digit dataset use a file format idx  what be the advantage of this file format over alternative such as csv  tsx and ods   generally  PRON will find dataset be distribute in csv format for PRON simplicity and human readable format that PRON could ingest in any programming language with just the package that the language be ship with  usually  tabular datum be export in csv format and that be one of the reason why mnist dataset be not provide in csv format   here be the quote from lecun s website for store the dataset in idx format   the data be store in a very simple file format design for store  vector and multidimensional matrix   in term of performance  binary file format fair better compare to text file format like csv or rich text format like ods   follow be some of the binary file format that be widely use   avro format  parquet format  optimized row columnar  orc   protocol buffers  protobuf   these file format support datum compression  store data type metadata to serialize and de  serialize datum effeciently  
__label__k-means PRON have some datum that PRON would like to cluster with k  mean   one of the feature be the hour of the day   the problem be that the hour  23  be consider far from the hour  0    how can PRON map the datum so that the boundary will be cyclic   modular arithmetic  generally PRON would do end  start mod 24  juliagt  mod2324   1  PRON seem to recall some programming language treat mod of negative number in a different way  so check PRON implementation first   k  mean use the mean   k  means be design for least  square  PRON only work reliably with  variant of  square euclidean distance  sum of squared deviation    counterexample   assume PRON have the two hour 0 and 23   if PRON get assign to the same cluster  k  means will compute the mean   the mean of the two value be 115  PRON be not 235   abuse k  mean with a cyclic  distance  may no longer converge  and will return nonsense result   but there be more case where the concept of a cluster center be not viable on cyclic datum  for example  give an event on every full hour  what be the center  the arithmetic mean be 12  but if PRON take cyclic space into account every hour be an equally good choice in cyclic space  therefore  the concept of a  center  in cyclic space be fragile   alternate cluster algorithm  PRON can try eg pam or dbscan instead  with an appropriate similarity measure   projection technique  as point out by other answer  PRON can project the time to the unit circle via sin  costime24  2pi   by compute the angle of the centroid  PRON can map this back to a point in time  but once PRON want additional attribute PRON get hard to meaningfully normalize the datum  to combine attribute   and PRON can get undefined time  eg if there be two point in a cluster  one at 6 and one at 18    PRON do not discuss this because PRON want to point out that modify the distance function be not a good idea for k  mean   since PRON accept another answer  which say this can not be do  PRON be edit this to include an example of PRON be do   hope this help   original answer   the most logical way to transform hour be into two variable that swing back and forth out of sink  imagine the position of the end of the hour hand of a 24hour clock  the x position swing back and forth out of sink with the y position  for a 24hour clock PRON can accomplish this with x  sin2pihour24y  cos2pihour24    PRON need both variable or the proper movement through time be lose   this be due to the fact that the derivative of either sin or cos change in time where as the  x  y  position vary smoothly as PRON travel around the unit circle   this method work really well for clustering and for keep the distance between 15 minute after midnight and 5 minute before midnight  close  in euclidean space   all of the modulo suggestion do not accomplish this and the cyclic representation that PRON do accomplish be pretty clumsy   finally  consider whether PRON be worthwhile to add a third feature to trace linear time  which can be construct by hour  or minute or second  from the start of the first record or a unix time stamp or something similar   these three feature then provide proxy for both the cyclic and linear progression of time eg PRON can pull out cyclic phenomenon like sleep cycle in people s movement and also linear growth like population vs time   hope this help   example of if be accomplish    enable inline plot   matplotlib inline   import everything PRON need   import numpy as np  import matplotlib as mp  import matplotlibpyplot as plt  import panda as pd   grab some random time from here  httpswwwrandomorgclocktimes   put PRON into a csv   from pandas import dataframe  readcsv  df  readcsvuser  angus  machinelearning  ipythonnotebook  timescsvdelimiter      dfhourfloatdfhourdfminute600  dfxnpsin2nppidfhourfloat24    dfynpcos2nppidfhourfloat24    df  def kmeansshowk  x    from sklearn import cluster  from matplotlib import pyplot  import numpy as np  kmean  cluster  kmeansncluster  k   kmeansfitx   label  kmeanslabels  centroid  kmeansclustercenters   print centroid  for i in rangek     select only datum observation with cluster label   i  ds  xnpwherelabelsi     plot the data observation  pyplotplotds0ds1o     plot the centroid  line  pyplotplotcentroidsi0centroidsi1kx     make the centroid x s big  pyplotsetplinesms150   pyplotsetplinesmew20   pyplotshow    return centroid  now let try PRON out   kmeansshow6dfx    yvalue   PRON can just barely see that there be some after midnight time include with the before midnight green cluster   now let reduce the number of cluster and show that before and after midnight can be connect in a single cluster in more detail   kmeansshow3dfx    yvalue   see how the blue cluster contain time that be from before and after midnight that be cluster together in the same cluster   qed  
__label__r __label__discriminant-analysis PRON have be use the lda package for r  but PRON be miss quite a few feature especially those that can assess the output   be the any prefer package that have some of the follow   univariate test statistics  canonical analysis  multivariate statistics like wilk s lambda  
__label__machine-learning __label__clustering __label__apache-spark __label__unsupervised-learning __label__lda PRON have a dataset with 2 million document  PRON have to separate every document into 4 partal  two part make up around 80  of a document  the third part make up around 15  and the last part 5   the data be not labl and PRON would be too time consume to lable the document by hand  PRON try lda in apache spark to cluster the document  to improve the result PRON now want to give lda an initial word distribution for every cluster   be this even possible with spark  PRON be use spark 22   in the api documentation PRON could not find a method to give lda an initial word distribution   
__label__numerical-analysis __label__numerical __label__conjugate-gradient in the  iterative  conjugate gradient  cg  algorithm  httpsenwikipediaorgwikiconjugategradientmethod  the initial search direction  p0 be set to the initial residual  r0 but PRON can not see why this choice  can PRON get this choice of  p0 from any of the equation present in the article   thank   consider the equation   mathbfpkmathbfrksumiltk  fracmathbfpitmathbfamathbfrkmathbfpitmathbfamathbfpimathbfpi for  k0 equivalently  and in term of what be actually go on  PRON be correct the search direction of steep descent to be  mathbfa orthogonal to the previous search direction  for the first step PRON have no other search direction and just look along the residual directly  
__label__human-like PRON talk with a graduate computer science who say one challenge of make artificial human  like be make random decision  and that computer can not be random  that PRON always need a  seed   but  if a computer s outcome be determine by the chaotic movement of electron  PRON do not seem like PRON should be difficult to program inherent uncertainty into a computer  so  what exactly be stop people from harness this basic component of reality to allow artificial intelligence to make randomized decision  PRON mean  all PRON would need be different neural pathway that rely on the superposition of electron  and that be PRON   computer  the processor in combination with the memory  be design to be deterministic   otherwise no software would ever work  because the computer would be execute PRON randomly   the computer s outcome be not determine by the chaotic movement of electron  but by the much strong deterministic current and voltage between the transistor   a digital machine be  simply speak  not affect by noise   the noise be always present   but 013 volt or whatev get round to 0 v  logic 0  and 302 volt or whatev get round to 33 v  logic 1    PRON be however possible to take advantage of the chaotic movement of electron as well   this be do in special hardware call  hardware random number generator    some use different form of true randomness such as nuclear decay   
__label__machine-learning __label__data-mining __label__classification __label__predictive-modeling __label__svm suppose PRON collect datum for 100000 toss of a fair coin and record  head  or  tail  as the value for the attribute outcome and also record the time  temprature and other irrelevant attribute   PRON know that the outcome of each toss be random so there should be no way of predict future unlabeled data instance   PRON question be how do learn algorithm  support vector machine  for example  behave when PRON apply PRON on random datum such as this   PRON will of course still learn some good decision boundary  PRON know PRON will be meaningless  but there will still be good and good coefficient for the algorithm to learn when fitting to this particular instance of datum from this random process  PRON may produce good than 50  accuracy on the data set  but of course this be purely due to overfitt whatev the data happen to be  PRON will not predict future outcome with more than 50  accuracy  
__label__sparse __label__matrix PRON be look for sparse spd matrix with right hand side   there be uf collection of sparse matrix  however  PRON be not sure how do PRON search of the matrix of these kind efficiently  PRON be do a naive search which have not give PRON any result so far and PRON take arbitrary long for some of the matrix     the comment thread be long enough to convert to an answer    PRON acknowledge that most matrix collection do not include physically reasonable right hand side   if PRON want to test solver performance  PRON have to generate problem to solve  either the right  hand side  b or solution  x from which PRON compute  b get a x and then solve  a y  b to some precision   the latter be convenient because PRON let PRON check accuracy   solvers like unpreconditioned gmres be optimal  over a subspace  in the 2norm of the residual  thus the  a  a norm of the error   this be a significantly weak norm than the 2norm of the error  for example  thus PRON really be useful to compare solver in the error norm   danger of choose a random vector  a common method of generate  x or  b be to draw from an independent random distribution   this be flaw because these random vector will be nearly orthogonal to the eigenvector associate with the small eigenvalue  exactly the mode that slow solver convergence   for example  if PRON use a gaussian distribution with mean zero for a neumann laplacian  have a constant null space   then the average over any significant portion of the domain will be nearly zero  mean that no long  range interaction take place   in this case  a coarse grid become unnecessary since PRON only have to move information far enough for the random process to homogenize   if PRON distribution be orthogonal to that low eigenvector  every random vector   x or  b do not matter when compare to an eigenvector  z  though if  zt x be close to zero   zt b  zt a x will be even small  that PRON draw will have this property of be nearly orthogonal to the  problematic mode   thus converge artificially fast   an independent random distribution will be orthogonal to most or all these  problematic mode   eg  translation and rotation for elasticity    this mistake get publish from time to time  so PRON be worth increase awareness   a fix  if PRON do not know anything about the problem  PRON be inconvenient to tune the random distribution so that PRON expect value have nonzero component in the direction of all the low  energy eigenvector  PRON would have to solve an expensive eigenproblem to find those  bad  vector   instead  PRON would recommend the follow ad  hoc procedure   recursively bisect the graph   at each level of the bisection tree  choose a random value from a random distribution with nonzero mean   define the vector  x to be the sum of the random value assign to all the part that contain PRON  ie  the sum of the path from the root of the tree    this be much like random sampling in a wavelet basis and provide correlation at all scale   PRON do not require any special knowledge about the spd matrix other than PRON sparsity pattern   extension and alternative   collect comment from offline discussion    michael grant ask  what about dense system    PRON reply  for an unstructured dense matrix  PRON would threshold since that should be some measure of locality   for a structured dense matrix  eg  an  hmatrix   PRON already have a hierarchy   the detail of the bisection  partitioning be not that important  all PRON need to do be provide correlation at different scale so that the distribution be not orthogonal to the problematic eigenvector   jack poulson suggest use a combination of  point  source  plane wave  wave packet  and gaussian random vector    PRON reply  plane wave and wave packet require some additional knowledge beyond the matrix PRON   point source be not great for heterogeneous medium problem because the green s function may be local almost everywhere  but global in very select place  eg  fault  wire   the chance of a random point source activate that long  range coupling be low  but PRON be important for the real system  the bulk of the answer above be explain why vector draw from an independent random distribution be inadequate  
__label__machine-learning __label__deep-learning __label__nlp __label__dataset what be the standard  or just good  open datum set to train a model for question answering  ideally  not only in english   thank   
__label__neural-networks __label__machine-learning __label__deep-learning __label__ai-design __label__nlp PRON have be look into viv an artificial intelligent agent in development  base on what PRON understand  this ai can generate new code and execute PRON base on a query from the user  what PRON be curious to know be how this ai be able to learn to generate code base on some query  what kind of machine learn algorithm be involve in this process  one thing PRON consider be break down a dataset of program by step  for example   code to take the average of 5 term  1  add all 5 term together  2  divide by 5  then PRON would train an algorithm to convert text to code  that be as far as PRON have figure out  have not try anything however because PRON be not sure where to start  anybody have any idea on how to implement viv  here be a demonstration of viv   PRON be look into genetic algorithm and thing of that sort when PRON come across this video  the guy in the video seem to know what PRON s do i watch like 20 minute of PRON   interesting stuff and PRON go step by step with everything   httpsvimeocom52539994 
__label__machine-learning __label__unsupervised-learning when train a vae  typically one sample from the latent distribution use the reparametrization trick use a fairly large minibatch size   100  in the decoder  generator half of the vae  PRON be assume this minibatch size allow the network to  smooth  out the error and allow PRON to avoid have to repeatedly sample from the latent space   however  PRON be interested in online scenario where PRON be train the vae on stream datum as PRON arrive  so the batch size would be 1  in this case  PRON can take the vae a long time to converge because the error be highly volatile   be there any way to avoid this issue in practice  PRON be unsure what will happen if PRON have to repeatedly sample from the latent distribution and then take the mean of those sample  or something   aside from obvious performance concern  the other alternative be to wait for enough sample to arrive that PRON can train PRON in a large batch  but even in this case PRON would not be able to wait for 100  sample to arrive   the fact that piece of datum arrive one by one do not define the minibatch size  PRON can have a buffer of size n  with either fifo or with any other eviction policy that suit the statistical property need  and sample minibatch of size m out PRON every time PRON want to update the autoencoder   depend on the buffer eviction policy and the sampling strategy  this may also help to avoid local autocorrelation   note  this answer be originally a comment to the op s question  
__label__machine-learning __label__books __label__reinforcement-learning PRON have be try to understand reinforcement learning for quite sometime  but somehow PRON be not able to visualize how to write a program for reinforcement learning to solve a grid world problem  can PRON suggest PRON some text book which would help PRON build a clear conception of reinforcement learning   there be a free online course on reinforcement learning by udacity  check  machine learning  reinforcement learning  PRON really enjoy reinforcement leraning  an introduction by richard sutton  PRON provide a very nice unifying view on rl  although PRON do not mention the new approach  PRON be from 1998    here PRON have some good reference on reinforcement learning   classic  sutton rs  barto ag  reinforcement learning  an introduction  cambridge  mass  a bradford book  1998  322 p  the draft for the second edition be available for free  httpincompleteideasnetbookthebook2ndhtml  russell  norvig chapter 21   russell sj  norvig p  davis e artificial intelligence  a modern approach  upper saddle river  nj  prentice hall  2010   more technical  szepesvári c algorithms for reinforcement learning  synthesis lectures on artificial intelligence and machine learning  2010411–103   httpwwwualbertacaszepesvarlbookhtml  bertsekas dp  dynamic programming and optimal control  4th edition  belmont  Massachusetts  athena scientific  2007  1270 p  chapter 6  vol 2 be available for free  httpwebmitedudimitribwwwdpchapterpdf  for more recent development  wiering m  van otterlo m  editor  reinforcement learning  berlin  heidelberg  springer berlin heidelberg  2012 available from  httplinkspringercom1010079783642276453  kochenderfer mj  amato c  chowdhary g  how jp  reynolds hjd  thornton jr  et al  decision make under uncertainty  theory and application  1 edition  cambridge  massachusetts  the mit press  2015  352 p  multi  agent reinforcement learning  buşoniu l  babuška r  schutter bd  multi  agent reinforcement learning  an overview  in  srinivasan d  jain lc  editor  innovation in multi  agent systems and applications  1  springer berlin heidelberg  2010 p 183–221  available from  httplinkspringercomchapter10100797836421443567  schwartz hm  multi  agent machine learn    a reinforcement approach  hoboken  new jersey  wiley  2014   videos  courses  PRON would also suggest david silver course in youtube  httpswwwyoutubecomplaylistlistpl5x3mdkkajrl42ijhe4np6e2ol62ofa 
__label__statistics __label__data-analysis __label__testing __label__data-sets suppose PRON have an array of independent value  012345  associate with an array of dependent value  01491625   suppose PRON independent array shift to  0111326334151   the independent value PRON have shift in an arbitrary way  furthermore the number of independent value have increase  but can also decrease   PRON need a mathematically robust and programmatically feasible way of represent the dependent array again but as a  function  of the new value  mean PRON would like to stay true to the original construction  ie in this case fx   x2  but follow the new value   note that this be arbitrary numerical datum that do not necessarily correspond to a clean function like PRON show above  nevertheless PRON be reference datum that PRON would like to use to benchmark a computational program   how can PRON  achieve a new array  base on the old one but follow the new independent value  thank   
__label__machine-learning __label__python __label__algorithms PRON be try to program incremental stochastic gradient descent  isgd  algorithm in logistic regression  initially  PRON cod respective logistic regression  loss function and PRON gradient  also get some idea to proceed rest of workflow  but  PRON have no idea how to apply sequential operation in incremental stochastic gradient descent algorithm which can be use in the respective logistic regression  how can PRON implement the sequential operation in incremental sgd  any way to make this happen in python  how can PRON do that  any idea   initial implementation  import numpy as np  import scipy as sp  import sklearn as sl  from scipy import special as ss  from  sklearn import dataset    load input dataset  xtrain  ytrain  datasetsloadsvmlightfilepath  to  traindataset    xt  yt  datasetsloadsvmlightfilepath  to  traindatasettxt    nfeature  xtrainshape1     logistic loss function  def lossfuncxi  yi  w    wresizewshape01    yiresizeyishape01    lossfnc  sslog1p1npnantonumssexpm1yi  npdotxiw       rslt  npfloatlossfnc   return rslt   gradient function  def gradfncx  y  w    wresizewshape01    yresizeyshape01    gradf1ynpnantonumssexpm1y    gradf2gradf11npnantonumssexpm1ynpdotx  w      gradf3gradf2resizegradf2shape0     return gradf3  class isgd   def  initself  learnrate00001  numiter100  verbose  false    selfw  none  selflearnratelearnrate  selfverboseverbose  selfnumiternumiter  def fittself  x  y    n  d  xshape  selfw  npzerosshaped     for i in rangeselfnumiter    print   n     iteration    i   grd  gradfncselfw  x  y   grdresizegrdshape01    selfw  selfw  grd  print  loss    lossfuncselfw  x  y   return self  def predictxi  w    yhat  w0   for idx in rangelenxi1    yhatwi1xiidx   return 1010npnantonumssexpm1yhat     def updateweightsxi  yi  w    lr08  yhat  predictxi  w   error  yi  yhat  return wlryi  yhatxi  how to proceed rest of workflow   here be blog about hogwild  for parallel machine learning  the particular interpretation of incremental sgd can be find here  hogwild  algorithm for logistic regression   now PRON have no idea how to apply sequential operation in incremental sgd which can be use in the respective logistic regression  how can PRON make this happen  be there any efficient workaround to implement sequential incremental sgd algorithm for logistic regression  what be the efficient programming pipeline to accomplish the task that PRON state above  any more thought   
__label__convergence PRON be a bit confused about the concept of convergence rate and convergence order  let PRON first give PRON the definition PRON use   sorry for the english  PRON be all self translate   let  x be PRON solution   definition 1  the sequence  xk be call linearly converge towards  x  if    exist llt1 text  so that  xk1xleq l xkxquad forall kgeq k0  PRON call the constant  l rate of convergence   definition 2  the order of convergence of a numerical method be  p  if     exist c  gt  0 text  so that  xk1xleq c xkxpquad forall kin mathbb n quadtextwith  clt1  for  p1  note  PRON assume  that choose the start value so that PRON get an converge sequence   question   question 1  can someone explain PRON the difference between  c and  l here   question 2  can someone explain PRON the concept  idea behind the rate  order of convergence   just so PRON hear PRON from another perspective   question 3  also  PRON often see that PRON use linglog and loglog plot  but PRON do not really get why PRON do that  eg  if PRON have linear convergence  PRON can see a linear function if PRON lin  log plot PRON error   why do PRON need the lin  log plot here    xkx be the error in the  kth term  call PRON  ek for a  good  numerical method  PRON want the approximation to get close and close to the desire result so  ek have to decrease to zero  if the error be guarantee to reduce to at least a certain fraction  l of the previous step  PRON have linear convergence    ek1  le l ek this essentially guarantee that the error drop at least as fast as a geometric series  an example would be the sequence  09  099  0999 ldots which converge to 1  the error at the  kth step be  10k so that  l01  but the error could fall faster than linear  consider the sequence   09  099  09999  099999999ldots this converge to 1 too  but at each step the error reduce as the square of the previous step so that  p2 when PRON have a faster than linear convergence order   pgt1    PRON be not really worried about  c  PRON could even be great than 1  but when  p1    c become the same as  l and PRON want PRON to be less than 1   now why the log  linear plot  assume linear convergence with geometrically reduce error  PRON would have   ek  lk  e0  replace  le with equality  take the log of this    logek   loge0   k logl  which be a linear curve  
__label__matlab __label__simulation __label__c++ first some background  PRON be a mathematician by train with very little formal training in programming  PRON be most comfortable in matlab and have be work with PRON for a long time  PRON second choice would be c  but in either language the longest most complicated code PRON have write be maybe between a hundred and two hundred line   PRON will soon start write some code for track charge particle in various electromagnetic field configuration  at first in 2d and then in 3d this spark a debate with PRON friend if PRON should write this in matlab or c   PRON question to this community PRON  what exactly  if any  be the benefit of do computation like this in c over matlab  consider all of the function  toolbox available in matlab  1i can do in matlab in one line what may take dozen in c  old classic argument PRON know   2i do not have to implement any ode solver for example  like rk45   all such thing PRON will need be already efficiently test and implement so one less thing for PRON to code  debug  worry about   3same thing for interpolation routine  interpolation in 2d3d get very messy  super super easy in matlab  4same thing for visualization  once PRON have the datum  super easy to just look at PRON   5a big chunk of this can be vectoriz so matlab code would be very efficient PRON think  not too many loop or anything   6i only need small scale share memory parallelization  in c PRON would use openmp  and matlab already have a nice easy to use parallel toolbox   7matlab script can be run from command line and PRON local super computer have PRON instal  with more than enough license  so PRON can actually run PRON matlab script on PRON super computer  just invoke multiple instance  each with 6  8 thread if necessary   so be there any real reason why PRON should use c instead of matlab  PRON will be the only one develop and then the only one use PRON  PRON will be use PRON for the next year or two for sure and after that PRON do not know yet  all  real  programmer tell PRON that all  real  programming should be do in  real  programming language  ie fortran or c   but today PRON just seem very archaic and outdated opinion at least for mathematical  scientific computation problem  be there a real reason which PRON just do not consider or be PRON just historical irrational bias   PRON question teeter on the edge of topicality for this site but here be PRON hapennyworth  personally  see  this be an opinion and that be where PRON question be close to off  topic   write as a real programmer  fortran  matlab  mathematica  c when PRON have to  c when a gun be hold to PRON head  PRON can not fault the reason PRON advance for stick with matlab   and if any real programmer scoff at PRON choice of matlab  why  just respond well  of course  PRON can code the performance  critical section in c and compile PRON into mex  file  if that become desirable   another opinion of mine be that PRON be actually very difficult to write performance  critical computational kernel in c which be  a  fast and  b  more reliable  robust  accurate than the matlab intrinsic function   the mathworks have put a lot of effort into optimise the execution of much of PRON basic functionality  oft  time matlab function call a tailor version of blas  or similar  which be the product of a compilation of c  or some other language    finally  add another item to PRON list of point in matlab s favour   8 since r2013a matlab s core toolbox now include a unit  testing framework  
__label__optimization __label__sampling __label__parallel PRON be try to optimize an expensive function for which PRON can choose sample point  the difficulty be that many function evaluation may be compute in parallel  take vary amount of time  PRON do not know which keyword to search for to find exist literatureimplementation    so at a time  PRON may have already compute function value at 18 point  with 15 still be compute  and PRON want to start evaluate the function another point  without the run job  PRON could make a model and find which next point may provide the most information  but now  PRON need to somehow tell the model that there be also 15 point for which PRON do not have value  but near which PRON do not want to evaluate the function   specifically  PRON be look for something with minimal assumption and no gradient information  but PRON will be happy if PRON just know the keywords  to search for   PRON can come up with several hack to that kind of work  but PRON wonder if there be any  real  solution    for illustration  say every column be a function evaluation point and every row be a timestep in the below ascii  figure  an x mean the evaluation be still be compute and a  in the bottom row mean PRON be do  a new evaluation start as soon as an old one terminate  with other still busy   xxxxxxxxxxxxxxxx  xxxxxxxxxxxxxxxx  xxxxxxxxxxx xxxxx  xxxxxxxxxxx xxxxx  xxxxxxxxxxx xxxxx  xxxxxxxxxxx x  xxx x  xxxx xxxxxx x  xxx xx  xxxx xxxxxx x  xxx  xx  xxxx xxxxxx x  xxx  xx  xxxx xxxxxx x  xxx  xx  xxxx xxxxxx x  xxx  xx  xxxx x xxxx x  xxx  xxx  xxxx x xxxx x  x x  xxxx  xxxx x xxxx x  x x  xxxx  xxx x xxxx x  x x  xxxxx  xxx x xxxx x  x x  xxxxx  xxx x xxxx x  x x  xxxxx  xxx x xxxx x  x x  xxxxx  xxx x xx x x  x x  xxxxxx  xxx  xx x x  x x  xxxxxxx  xxx  xx x x  x x  xxxxxxx  xxx  xx x x  x x  xxxxxxx  xxx  xx  x  x  x xxxxxxxx  xxx  xx  x  x  x xxxxxxxx  xxx  xx  x  x  x xxxxxx xx  xxx  xx  x  x  x xxxxxx xx  xxx  xx  x  x xxxxxx xxx  xx  xx  x  x xxxxxx xxxx                        each time an evaluation complete  how to choose the next point to evaluate  give several complete and several incomplete evaluation   PRON would use a parallelized gaussian process regression model  the next point to query in this framework be give by the posterior extrema or those of maximum expected improvement  here be an implementation  
__label__fluid-dynamics __label__comsol __label__solid-mechanics PRON be new to comsol and PRON want to model fluid flow in stress field  as start point PRON want a simple 2d box have a fracture from right to leave while pressure gradient vertically  PRON want to know the threshold value of that pressure from top which allow flow of fluid in fracture  can anyone please suggest PRON which module of comsol should PRON be use  PRON see simple fluid flow in fracture example but not where impact of pressure control the flow   
__label__optimization __label__linear-programming __label__convex-optimization consider the follow optimization problem    min qquad  ctx  st    qquad  ax0   xixj  xkxtquad  for some  ineq jneq kneq t   xx1x2  xn  and  quad xjgeq 0   j12  n  here  cc1c2  cn   cigeq 0  and  a be the adjacency matrix   is this problem convex   can PRON be solve in polynomial time   nonlinear equality constraint involve continuous variable be not convex   here be a counterexample for PRON problem   suppose without loss of generality  x1x2   x3x4   0  be PRON nonlinear equality constraint  and let  s  x1  x2  x3  x4  ldot  xn    x1 x2  x3 x4  0 be PRON associate feasible set   then   6  1  3  2  0  ldot  0  in s  and   1  6  3  2  0  ldot  0  in s  but    6  1  3  2  0  ldot  0    1  6  3  2  0  ldot  0    2   72  72  3  2  0  ldot  0  notin s  because   722  6  494  6  254 neq 0  generally speak  nonconvex problem can not be solve in polynomial time   this problem be convert to lp  therefore  PRON can be solve in polynomial time  
__label__fourier-transform __label__computer-arithmetic let  a  b  c  dinmathbbrn be PRON possible to compute    langle ab  cdrangle  faster than 6 fft   PRON can do PRON with 6 fft by do normal convolution  3 fft each   in PRON application PRON know  b  d upfront and can do preprocess  so the cost be actually 4 fft  can PRON do good than this   the reason PRON believe that this may be possible be because fft be unitary  so maybe there be a way to omit the inverse fft in the convolution    edit  where convolution be define by      abisumj1i ajbnj  i  parseval s theorem tell PRON that the dot product between two vector equal the dot product of the fourier transform  possibly up to a constant  consequently  PRON do not need to transform back the result of  aast b and  c ast d and should be able to do the overall operation with just 4 fft  
__label__neural-networks be PRON possible to give a rule of thumb estimate about the size of neural network that be trainable on common consumer grade gpus   for example   the emergence of locomotion  reinforcement  paper train a network use tanh activation of the neuron  PRON have a 3 layer nn with 300200100 unit for the planar walker  but PRON do not report the hardware and time   but could a rule of thumb be develop  also just base on current empirical result  so for example   x units use sigmoid activation can run y learning iteration per h on a 1060   or use activation function a instead of b cause a n time decrease in performance   if a student  researcher  curious mind be go to buy a gpu for play around with these network  how do PRON decide what PRON get  a 1060 be apparently the entry level budget option  but how can PRON evaluate if PRON be not smart to just get a crappy netbook instead of build a high power desktop and spend the save  on on  demand cloud infrastructure   motivation for the question  PRON just purchase a 1060 and  clever  to ask the question afterwards huh  wonder if PRON should have just keep the  and make a google cloud account  and if PRON can run PRON master thesis simulation on the gpu   PRON depend on what PRON need  PRON can train any size of network on any resource  the problem be the time of training  if PRON want to train inception on an average cpu PRON will take month to converge  so  PRON all depend on how long PRON can wait to see PRON result base on PRON network  as in neural net PRON do not have only one operation but many  like concatenate  max pooling  padding etc    PRON be impossible to make an estimation as PRON be search for  just start train some infamous network and measure the time  then  PRON can interpolate how long PRON will take to train network that PRON be search for   as a caveat  PRON would suggest that unless PRON be push up against fundamental technological limit  computation speed and resource should be secondary to design rationale when develop a neural network architecture   that say  earlier this year PRON finish PRON ms thesis that involve bioinformatic analytic pipeline with whole genome sequencing datum  that project take over 100000 hour of compute time to develop accord to PRON cluster job manager  when PRON on a deadline  resource can be a real constraint and speed can be critical   so  to answer PRON question as PRON understand PRON   would PRON have be good off to use the money to buy time in the cloud   probably  the few hundred dollar PRON spend on the 1060 would take PRON far train PRON models  in the cloud  further  as far as PRON can tell  PRON do not require the gpu to be crank 100  of the time  PRON would if PRON be  say  mining crypto currency   finally  with cloud instance PRON could scale  train multiple model at once  which can speed up the exploration and validation of any architecture PRON settle on   be there a way to gauge the compute time of a neural network on a give gpu  well  big o be one estimator  but PRON sound like PRON want a more precise method  PRON be sure PRON exist  but PRON would counter that PRON can make PRON estimation with simple back of the envelope calculation that account for thread  memory  code iteration  etc  do PRON really want to dig into the gpu processing pipeline on the 1060  PRON may be able to come up with a very good estimate by understand everything happen between PRON code and the metal  but ultimately PRON ’ probably not worth the time and effort  PRON will likely confirm that big o notation  the simple model  if PRON will  capture most of the variation in compute time  one thing PRON can do if PRON notice bottleneck be performance profiling   usually the problem be to fit the model into video ram   if PRON do not  PRON can not train PRON model at all without big effort  like train part of the model separately    if PRON do  time be PRON only problem  but the difference in training time between consumer gpu like the nvidia 1080 and much more expensive gpu accelerator like the nvidia k80 be not very large  actually the good consumer card be fast than gpu accelerator  but lack other property like vram  random comparison and benchmark  httptimdettmerscom20170409whichgpufordeeplearning  httpsmediumcomalexbaldoacomparisonbetweennvidiasgeforcegtx1080andteslap100fordeeplearning81a918d5b2c7  to calculate if PRON model fit into vram  PRON just approximate how much datum and which hyperparameter PRON have  input  output  weight  layer  batch size  which datatype and so on   
__label__matlab __label__ode __label__boundary-conditions PRON be try to solve a couple system of ode s use matlab s bvp4c function  PRON want to impose the condition that    int0pi  y1t  y1t  dt  1  where  y1 be the first state variable   another post suggest add a additional equation to the system of ode s  namely     it   int0t  y1  y1s  ds     then the additional boundary condition  ipi10  would enforce the integral condition PRON want  however  PRON be not sure how to code this in matlab  can anyone help PRON with this   as per nicoguaro s comment  the integral equation can be write as    fracddtit   y1t  y1t      update the code accordingly  PRON have  this be the whole code PRON be use with bvp4c  as kirill say  PRON would be useful to include the whole code    s  100   solinit  bvpinitlinspace0pi1001mat4init  s    sol  bvp4codeproblemeightbc  solinit    x  linspace0pi1001    y  devalsol  x    trapzy1y1      ybar  y1normy12    plotx  y1      fprintfthe intial guess for s be  d the value solve for be  dns  solparameter   function dydt  odeproblemt  y  s   b  pi   dydt1   y2    dydt2   y3    dydt3   y4    dydt4   y5    dydt5   y6    dydt6   y7    dydt7   y8    dydt8    b4y1   2b2y3    b4  2y5   2s2b2y7s2   dydt9   y1y1    end  function r  eightbcya  yb  s   re   ya1   yb1   ya3   yb3   ya5   yb5   ya7   yb7   ya9   yb91    end  function yinit  mat4initx   yinit    cos2x   2sin2x   4cos2x   8sin2x     16cos2x    32sin2x   64cos2x   128sin2x   1    end  here be a plot of the solution   all other boundary condition appear to be meet  PRON do not include the plot show that   but the integral condition be still not   the comment detail a perfectly valid way to solve this by define the integral as another part of the ode and add a boundary condition on that  PRON will discuss another way to approach this   bvp4c be a two  point boundary  value problem solver  instead of a two  point boundary value problem solver  PRON need the ability to specify a condition that use multiple point or  even better  use a continuous extension of the current solution  PRON can do something like that use differentialequationsjl because PRON let PRON define residual with continuous extension of the solution   function bc3residual  sol   residual1   quadgktgtsoltidxs120pi   residual28   0  there be no other boundary condition mention   end  this work because solt  be the solution at time t use an order  match interpolation   okay  so PRON be possible to handle this directly  but what be the difference  this be the part that be more interesting  two different case  when use a shoot  type boundary value problem solver  continuous extension come naturally and one be do rootfind of the initial condition  treat the bvp as an ivp with an unknown initial condition  thus there be no downside to use this kind of continuous extension condition  provide the bvp solver allow PRON   but shoot method of course have a downside that PRON can not be use for problem which be sensitive to the initial condition  or start with a singularity   that be not true for collocation or mirk  implicit rk  type method  in these case  one grid space  xi and relax the solution at every point in space  yxi via a rootfinder  in this case  the  jacobian  be the dependency of each point on every other point  since for runge  kutta or collocation method PRON only use value from just before to compute the new  this typically have a banded structure  however  that be assume that the boundary only depend on  yx1 and  yxend if PRON impose a boundary condition which depend on all point in space  like this quadrature form for the integral  then the jacobian of the full system have a full column which destroy the banded structure   since the jacobian be of size mns where m be the number of spatial point  n be the number of ode in the system  and s be the number of stage in the ode  this can be very large even for simple problem  but as note before  the two  point bvp problem make the dependency such that this be an s  band matrix  make PRON easy to solve  bvp4c be one of many such solver which thus assume the two  point bvp form because PRON allow solve this system to be greatly simplify   if PRON be not a two  point bvp  this  jacobian be a sparse matrix  either PRON be store actually as a sparse matrix or if the problem be small enough one use a dense matrix for PRON  but the inversion algorithm can not use a band form and thus resort to a much slow  therefore a solver which generalize like this should only be use when necessary   let PRON conclude by summarize this   1  if PRON can do a transformation to not have a more complicated boundary condition  that be probably PRON good bet if PRON be not difficult to do  integral can be add as extra part in the system of ode since an ode solver be actually just a quadrature method   2  if the problem be more difficult and such a transformation be not possible  then a shoot  type method be a good bet and do not have an overhead for have these kind of condition  PRON just need to find a bvp solver which let PRON do PRON    3  shooting method require that the problem be not sensitive to the initial condition or have an initial singularity  if that be not the case  PRON need to use collocation or mirk  type method  however  most of these method  like bvp4c  do not allow this form because PRON can greatly optimize by assume the problem have a two  point condition  the few solver which can handle this case should only be use as a last resort   hopefully that explanation help clear PRON mind that the workaround from the comment be not just a hack to get PRON do  but probably the good way to do PRON  
__label__fluid-dynamics __label__navier-stokes __label__advection PRON have a chamber full with a fluid flow horizontally in laminar regime from one side to the other  PRON carry a suspension with concentration  c this suspension also fall to the bottom of the chamber with settle velocity  mathbfvs  when PRON reach a critical concentration  cmax at the bottom  PRON start build up as a deposition  this block the influx of particle  hence the function  phic  this nonlinear term be similar to the one in the burgers equation for the traffic problem  now the region with concentration  cmax can grow upwards   this deposition be big enough that PRON can disturb the flow  to account for this  PRON add the term  psicmathbfu in the navier  stokes equation below  this term stop the flow in region where the concentration be  cmax the equation be    fracpartial cpartial t   nabla  phic  cmaxmathbfvs     mathbfv   frac  partial mathbf  u    partial tmathbf  u  cdot nabla  mathbf  u  nu nabla 2mathbfu   nabla p  psic  cmaxmathbfu mathbff   nabla cdot mathbfu   0     psic  cmax     left   beginalign   0  ampamp  c  lt  cinit    10000  c  cinit3  ampamp  else  endalign   right      cinit be the initial concentration in the entire chamber  PRON only increase at the bottom  when PRON start build up  therefore  psigt0  only at the bottom   PRON be not 100  confident with this model  and hence  PRON question  PRON believe that the penalization term  psi can restrict PRON step size   be there a good approach to model a flow with a grow deposition that can disturb PRON   be PRON admissible to decouple the equation by treat  mathbfu in the first equation and  c in the ns equation explicitly   PRON be solve both equation on the same mesh  so far PRON have not do a proper error estimation  but PRON believe that PRON would need a high refinement near the deposition layer  can PRON model this phenomena with level set method and save cost in refinement   PRON have not be able to find literature on similar physics  PRON would appreciate any reference   PRON will try to answer as best as PRON can PRON three question   1  PRON approach be quite classical in that PRON be consider the particle to be an active scalar  what most people would usually do be to consider that the particle concentration affect the viscosity  via a model such as the kreiger  dougherty model for particle suspension  instead of penalize the velocity via a darcy term like PRON be do  generally this give a more appropriate result for the velocity gradient and model the viscous dissipation that occur at the particle scale  furthermore  PRON model do not consider the particle  particle interaction and the diffusive flux that occur due to the particle  particle interaction  PRON may want to look at model like the phillips model  see httpaipscitationorgdoiabs1010631858498  which consider a diffusive flux due to particle  particle and particle  fluid interaction  the author of that paper also implement a viscosity model instead of a darcy term   2  PRON be admissible to decouple the equation  but PRON may want to solve PRON iteratively inside a single time step  for instance  solve for velocity  then solve concentration  recalculate viscosity and solve again for velocity  etc  until convergence  PRON be good not to put the concentration and the  velocity  pressure  in the same system from PRON experience  be careful about the equation for the particle concentration  as PRON will most likely need to do some form of upwinding   3  PRON can model this phenomenon use concentration value at the node  or cell center  depend on the numerical model PRON be use  there be no need for multiple mesh or for a level set approach  
__label__neo4j __label__csv PRON have an excel file contain lot of column  PRON want to create a graph database in neo4j with all these column as a separate node so that PRON can establish relationship between PRON and play around with the cypher query to get to know PRON datum quite well  PRON do write a cypher query to make each column as a node  but when PRON run the relationship query PRON show PRON the name of one node and give a random number for all the related node despite change the caption in the property section   sample create statement   load csv with header from  file location  as row create   nodename  dateoccured  row  date     query to visualize the relationship between node   match  arel namegtb  return a  b limit 25   this give PRON the value the node  a  and random number for all the node  b   PRON do not know where PRON be go wrong  PRON think PRON have something to do with the way the datum have be set up in PRON file  so PRON create a file for each column of PRON main file and write the create statement to get the node  PRON still do not get the actual value of the related node   
__label__data-mining __label__bigdata __label__data PRON have a database of hundred of thousand of loan  PRON want to find some trend in user behavior  not just some information regard all loan together but PRON want to find something similar for particular usersor group   example of what PRON want to find can be formulate like  30  of user take PRON loan in 10 day after PRON pay the previous one or 40  of user take loan in the same time of the month   1 day   PRON background in software development and usually i get report by simply make a sql query  also PRON sometime use ssas  but in this case PRON do not know how to simply ask  one way be to make a hypothesis and then try to prove PRON  but PRON need so much resource to program appropriate report in that case and also PRON be limit to PRON imagination that generate hypothesis   so what approach would PRON recommend   PRON seem like PRON be try to see what be the different type of user  in that case PRON would suggest cluster  clustering be an unsupervised learning algorithm that find pattern and split PRON datum into the observed pattern  there be several clustering technique and which to use depend on PRON datum   scikit learn resource for cluster  if PRON be unsure which  just go with k mean first  PRON suggest PRON read the documentation and example in the scikit learn page   to piggy  back off of impul3h  PRON recommend check out orange data mining tool   in case PRON be unfamiliar with python and think that PRON would experience a steep learning curve with scikit learn  then the drag and drop interface from orange would be a good tool for PRON   outside of clustering  PRON would think that a naive bayes classifier may be useful for PRON  if PRON data be in categorical form   this would be a supervised learn classification model  and be often one of the first and more easy to implement model on datum in this format  
__label__machine-learning __label__python __label__unsupervised-learning __label__anomaly-detection __label__outlier PRON be work on project where PRON task be to find unauthorized access use any machine learning technique   let PRON clear PRON problem definition   usera access website use chrome browser from window pc   userb access website use internet explorer from window pc   usera never use safari to access website but all of a sudden PRON use safari to access website from mac system   point 1 and 2 be normal login but point 3 may be use by attacker because PRON be not match with usera behavior   thus i be be try to implement an unsupervised model in python to first learn normal behavior of all user for website access with other feature such as   username  country  servicekey  unique identifier   system  apptype  client  server  enterprise etc    below PRON be share sample datum  set of mine                                                              username  country  servicekey  system   apptype                                                              usera   india   e08fe2d   window  2     usera   india   e08fe2d   android  2     usreb   china   bb15d36   window  3     userb   india   bb15d36   window  3     userb   russia   bb15d36   mac   3     usera   usa   e08fxxx   mac   3     userb   china   bb1xxxx   ios   1                                                              above 5 record be manually analyze and mark as normal log and collect from different user system  but the last 2 record be abnormal and should be detect as outlier because PRON be no match in feature  here to be note be that login can have different country and system but PRON can not have different username  servicekey and apptype   can anyone suggest PRON to model this or can share blog or sample in python   this be a scenario where PRON need a meta  dataset  the sample datum that PRON post be not something that be ready for model  PRON would work towards develop a dataset that would establish the same pattern that PRON list in the rest of PRON post  how many country have PRON log in from  how many time be PRON during the day  at night  etc  to be clear  this would be a dataset where PRON be one user id per row  and each id only have one record  if PRON can develop that  then PRON will be in a much good position to model this out   PRON guess PRON need anomaly detection algorithm  PRON be like fraud detection for find abnormal behavior   in datum mining  anomaly detection  also outlier detection  be the identification of item  event or observation which do not conform to an expect pattern or other item in a dataset  typically the anomalous item will translate to some kind of problem such as bank fraud  a structural defect  medical problem or error in a text  anomaly be also refer to as outlier  novelty  noise  deviation and exception   PRON highly recommend PRON take a look at here   this question be quite broad  PRON will try to set PRON on the right path  more so than provide a truly complete answer   theoretical background  as other have mention  the task PRON be try to do be usually know as anomaly detection  also know as novelty detection   there be many possible way to approach this kind of task  depend on the assumption PRON be willing to make about PRON datum   before PRON start explore alternative  PRON be important to mention here that all PRON variable  be categorical  country  servicekey  system  apptype   this be important  since usually algorithm usually handle either continuous variable or discrete variable  and categorical one fall into the latter type   there be some trick PRON can do  however  to handle PRON interchangeably  one  hot encoding be simple and popular  and enable PRON to transform a categorical variable into n binary variable  for some class of algorithm  PRON can then use these binary variable as if PRON be continuous  PRON can even mix binary and truly continuous variable   now  there be usually two family of method to approach this sort of problem  parametric vs non  parametric   parametric method require that PRON do assumption on the underlying probability distribution of PRON datum  PRON can then estimate these parameter use PRON datum   non  parametric method  on the other hand  be simple to use  as PRON do not require assume a underlie distribution  PRON may also be less powerful  exact  especially if PRON do not have a lot of datum and PRON datum very closely follow a know probability distribution   practical advise  if PRON be willing to do some statistical work  look into bayesian inference   for a easy non  parametric method  look into into one  class svm  here be a code example use python and scikit  learn and more practical information on anomaly detection  but  honestly  PRON be give PRON a important clue here   here to be note be that login can have different country and system  but PRON can not have different username  servicekey and apptype   if PRON already know all this  PRON may want to completely skip the complicated part  just compare those value directly  summarize PRON into a distance measure  PRON can give different weight to servicekey and apptype  for example  and see if that be good enough 
__label__fluid-dynamics __label__reference-request __label__navier-stokes the rhie  chow  1  interpolation seem to be a standard tool in the finite  volume discretization of incompressible flow   PRON be commonly define on the discrete level  2    in the lecture note  3   chapter 6  page 56  2   PRON say that  PRON find that the rhie  chow interpolation be the same as  add a pressure term  which be proportional to a third  spatial  derivative     in the continuity equation    the term be proportional to a fourth  order derivative term     be there any work on how this third or fourth order term actually look like  or be there an approximation of the nse on the pde level such that a  say  1st order fvm discretization would directly lead to the rhie  chow interpolate scheme   
__label__algorithms an element  a in mathbbzn be a quadratic residue in  mathbbzn if PRON be congruent to some perfect square modulo  n  be there an efficient algorithm to find all quadratic residue in  mathbbzn    n be composite and PRON know all PRON be factor if that help   update   PRON have one more restriction   n   p1 p2 dot pk  where  pi be distinct odd prime and  pi equiv 3 pmod 4 can PRON get something in this case   PRON use the follow approach at the moment   iterate over  leftlfloorfracn2rightrfloor  1  perfect square start from  0  and store PRON as PRON go  the problem be that PRON become slow quickly as  n grow  here be the code example    include  ltstdiohgt   int main     int n  7  11   int qr  0   int step  1   for  int i  0i  lt n  2i    printfqr   in   qr     perform some operation on qr here   eg store PRON somewhere to access later  qr   qr  step   n   step   2     return 0     PRON need to solve    fracp1  12   fracp2  12  cdot fracpk12  congruence system of the form  x  qmod pi where  q be  1  2  2  2  3  2  cdot  leftfracpi12right2  use chinese remainder theorem   this will give PRON all the distinct    p1  12  p2  12 cdot  pk12  square solution  
__label__classification __label__feature-selection while compare two different algorithm to feature selection PRON stumble upon the follw question   for a give dataset with a discrete class variable PRON want to train a naive baye classifier  PRON decide to conduct feature selection during preprocess use naive baye in a wrapper approach   do this method of feature selection consider the size of the use feature subset   when consider how nb classify a give instance  the size of the feature subset be use for classification only influence the number of part that the product of the conditional dependency have but that do not make a difference  or do PRON   PRON would be great if someone could offer a solid explanation since for PRON PRON be more of a gut feeling at the moment   short version  the difference be in the trade  off between complexity of model and how hard PRON will be to train   the more state PRON variable can take  the much more datum PRON will need to train PRON   PRON sound like PRON be reduce the feature space through some front  end processing   example  context   say PRON be use nbc to model if PRON go outside give the high temperature that day   let PRON say temperature be give to PRON as an int in celsius which just happen to range between 0 and 40 degree   over one year of run the experiment  record the temperature and if PRON go outside  PRON will have 365 datum point   internal representation   the internal structure the nbc use will be a matrix with 82 value  41 value for PRON one input variable  2 because PRON have two state in PRON output class    that mean each bin will have an average of 36582  89ish sample   in practice  PRON will probably see a few bin with lot more sample than this and a many bin with 0 sample   a pitfall  say PRON see 8 case at a temperature of 5 c  all of which PRON stay inside  and nothing at a temp of 3 or 4 c  if  after the model be build  PRON  ask PRON  what class a temp of 4 c should be in   PRON will not know   intuitively  PRON would say  stay inside   but the model will not   one way to fix this be to bin the class 012   into large group of temperature  ie  class 0 for temp 0  3  class 1 for temp 4  7  etc    the extreme case of this would be two temperature state  high  and  low    the actual cut  off should depend more on the datum observe  but one such scheme be convert temperature within 0  20 to  low  and 21  40 be  high    discussion  this front end processing seem to be what PRON be talk about as the  wrapper  around the nbc   the result of PRON  high  and  low  bin scheme will result in a much small model  2 input state and 2 output class give PRON a 2  2  4 number of class   this will be way easy to process and will take way less datum to confidently train at the expense of complexity   one example of a drawback of such a scheme be   say the actual pattern be that PRON love spring and fall weather and only go outside when PRON be not too hot or not too cold   so PRON outside adventure be evenly split across the upper part of the  low  class and the low part of the  high  class give PRON a really useless model   if this be the case  a 3class bin scheme of  high    medium   and  low  would be good  
__label__classification __label__ensemble-modeling consider two binary classifier a and b suppose that both a and b be predict the same target  but that a be train on a subset of the datum for which a different set of feature be available than for b suppose further that a and b output probability esitmate and that for a set of mixed input type  some with feature that a can accept  other for b  PRON need to rank PRON by probability of be positive   perhaps  and this be completely fabricate  PRON be try to predict early mortality of lion and tiger in a zoo  different feature be available for early lion mortality than for early tiger mortality  however  PRON ultimately will need to order all of the big cat   among both lion and tiger  by likelihood of early mortality   PRON idea so far   train a meta  estimator that accept the input type  belong to a or b  and the score from a and b PRON be think xgboost or another tree  base regressor   attempt to directly and monotonically transform the probability estimate from a and b so as to  bring PRON into alignment  in such a way as to maximize the cross  validate auc  this seem to be similar to the first option   other information   in PRON actual application there be class imbalance  and the class imbalance vary between a type datapoint and b type datapoint  precision tprtprfpr  be most important in a business context   
__label__machine-learning __label__labels PRON be work in emotion analysis of tweet  PRON collect close to 6 million tweet  PRON do not want to label PRON manually  instead i cod a bunch of complex rule to arrive at PRON label   before PRON use these algo generate label for PRON ml solution  PRON want to check  analyze how good be this labeling  be there any statistical  otherwise test that PRON can run to understand the quality of the labeling PRON get from PRON code    be there any statistical  otherwise test that PRON can run to understand the quality of the labeling PRON get from PRON code   yes  PRON can treat PRON automate labelling as a model in PRON own right  PRON essentially be an  expert  model  that take in additional feature   collect ground truth datum with know accurate label  and use a metric such as accuracy  auroc  f1 score to determine how well PRON expert model work  usually PRON should pick one metric that make sense to PRON base on the problem PRON be try to solve  most statistic library will allow PRON to assess these metric on arbitrary datum  so PRON should be possible to find the relevant part of the api and feed PRON PRON synthetic label to compare with ground truth datum PRON have collect   but wait  that be exactly what PRON would need to do in order to train and assess an ml model  about the good PRON can hope be that PRON will need less ground truth datum to assess PRON expert than PRON would to train the ml version from scratch  for complex ml model such as deep neural network  this could well be true   note that PRON may hit a limitation from this approach  say that PRON assess from ground truth datum that PRON expert be 85  accurate  PRON will not be able to use PRON to train an ml solution that be good than 85  accurate  because the ml will learn to copy the expert s prediction as best PRON can   one thing to watch out for  if PRON analyse why the expert get some value wrong and then adjust PRON to give correct answer for those label  PRON will need to remove the label PRON use to do this from PRON test datum  and ideally collect more  or take more from some unused pool  to replace PRON  otherwise PRON will fit PRON expert to the test datum and get incorrect reporting of how well the expert generalis to unseen datum  same reasoning mean PRON should not use tweet and label that PRON analyse when develop PRON expert model in order to test PRON  
__label__r __label__text-mining __label__text __label__nltk PRON be analyze the result of a survey where a survey taker be ask a   double  barrel question   PRON be apply text analytic in order to answer the question   which part of the question be survey taker most associate with PRON response    PRON approach involve identify frequently use word  group into topic  and then apply correlation analysis to binary flag associate with these topic in order to identify how similar  different these topic resemble the associated outcome of the scale question   PRON recognize the risk of   correlation do not mean causation   PRON plan for answer the question be simply to identify how correlate theme relate to subject 1 differ andor overlap with subject 2   PRON question  be the method that PRON have describe above rational  or be there good way of break apart double  barrel question   
__label__finite-element __label__operator-splitting PRON have a problem with this finite element formulation   after apply a splitting operator  qhatq   tildeq PRON do not know how to procede   PRON need to obtain the solution of the follow finite element formulation    give  ahn1 and  hatqhn1  find  tildeqhn  in vh0  such that    bigg  dfrac1ahn1tildeqhn1phih bigg   dfracmrho  bigg  dfractildeqhn1partial z   dfracpartial phihpartial z  bigg   dfracmrho  bigg  dfrachatqhn1partial z   dfracpartial phihpartial z  bigg   qquad forall phih in vh0    PRON do not know how to deal in this situation because PRON have two unknown term   thank   
__label__deep-network __label__neural-networks __label__image-recognition __label__convolutional-neural-networks can a convolutional neural network be use for pattern recognition in a problem domain where there be no pre  exist image  say by represent abstract datum graphically  would that always be less efficient   this developer say current development could go further but not if there be a limit outside image recognition   convolutional nets  cnn  rely on mathematical convolution  eg 2d or 3d convolution   which be commonly use for signal processing  image be a type of signal  and convolution can equally be use on sound  vibration  etc  so  in principle  cnn can find application to any signal  and probably more   in practice  there exist already work on nlp  as mention by matthew graves   where some people process text with cnn rather than recursive network  some other work apply to sound processing  no reference here  but PRON have yet unpublished work ongoing    original content  in answer to the original title question  which have change now  perhaps need to delete this one   research on adversarial network  and related  show that even deep network can easily be fool  lead PRON to see a dog  or whatev object  in what appear to be random noise when a human look at PRON  the article have clear example    another issue be the generalization power of a neural network  convolutional net have amaze the world with PRON capability to generalize way good than other technique  but if the network be only feed image of cat  PRON will recognize only cat  and probably see cat everywhere  as by adversarial network result   in other word  even cn have a hard time generalize too far beyond what PRON learn from   the recognition limit be hard to define precisely  PRON would simply say that the diversity of the learning datum push the limit  PRON assume further detail should lead to more appropriate venue for discussion    the simple answer be  no  PRON be not limit to image   cnn be also be use for natural language processing   see here for an introduction    PRON have not see PRON apply to graphical datum yet  but PRON have not look  there be some obvious thing to try and so PRON be optimistic that PRON would work   convolutional neural network can be apply not only for image recognition  but also for video analysis and recognition  natural language processing  natural language processing  in game  eg go  or even for drug discovery by predict the interaction between molecule and biological proteinswiki   therefore PRON can be use for variety of problem by use convolutional and subsampl layer connect to more fully connect layer  PRON be easy to train  because have few parameter than fully connect network with the same number of hide unit  ufldl  convolutional neural network can be use whenever pattern be locally correlate and translatable  as in shiftable   this be the case because cnn contain filter that look for a certain local pattern everywhere in the input   PRON will find local and translatable pattern in picture  text  time series  etc   PRON do not make as much sense to use cnn if PRON data be more like a bag of feature with an irrelevant order  in that case PRON may have trouble detect pattern that contain feature which happen to be farther apart in PRON input vector  PRON will not find local and translatable pattern in PRON datum if PRON can reorder the datum point of the input vector without lose information  
__label__computational-geometry PRON be do work in computational geometry where the robustness of the algorithm be important  on two separate occasion now have PRON come across a scenario where PRON compare the numerical size of two expression where the final inequality end up look something like this     sqrta   sqrtb   lt  c   where  a   b and  c be integer  PRON only care about the value of the predicate  instinctively PRON try square both side but of course that do not rid of the square root term   PRON question  be there a way to determine the truth value of predicate like this use only integer  computer  arithmetic   assume  a  b  cgt0   otherwise PRON be easy   check that  csqrtbgt0   then square  sqrtaltcsqrtb once to get    c2b  agt2csqrtb    so check that  c2b  agt0   if not then PRON be false because rhs be positive  so lhs must be as well   then check that     c2b  a2gt4bc2     but there be an important issue with this  with integer arithmetic PRON must guard against overflow  for example  when compute  c4  with  32bit integer  this will overflow for  cgeq 256  by the way  PRON can arrive at the last inequality with just one line of mathematica   groebnerbasisc  u  v  u2  a  v2  b    a  b  c    u  v    which give     lefta2  2 a bb2  2 a c2  2 b c2c4right     and here be the same in sage   rlta  b  c  u  vgt   polynomialringqq   a b c u v    PRON  ridealc  u  v  u2a  v2b    ieliminationidealu  v    output  ideal  c4  2ac2  2bc2  a2  2ab  b2  of   multivariate polynomial ring in a  b  c  u  v over rational field 
__label__neural-networks __label__convolutional-neural-networks __label__image-recognition PRON have a set of image that PRON already train a cnn to classify successfully  PRON wonder if PRON would be possible to encode the image  use xor in combination with a key of the same length as the image  and train a new net on PRON   think logically  the feature still exist in the same relation to each other  just in a different form  encode   consider that neural network be incredible at pattern recognition  PRON assume that PRON would still be doable   for people  who can not imagine how a xor  encode image would look like   for a human  PRON may look like rubbish  but the information be definitely there   would love to read PRON opinion   first PRON should give PRON a try  because anyone s guess could be off  as there be not really a complete high level analytic model of how neural network behave on real datum  most result with neural network be inform by theory  but involve a whole lot of experimental testing   PRON suspect PRON network may at least learn something from the new image  but would struggle to get anything like the same accuracy as without the noise  because the cnn filter rely on be able to detect similar feature at different position  in PRON scramble image  there will not be any meaningful and consistent edge  corner etc that a single learn feature detector could learn to match  and therefore present to the next layer    a fully  connect network would not have this limitation  and would learn just as well on a set of binary feature that have be xord identically in each position for each example  as PRON do on the original copy  ie only if the picture be 1 bit depth   PRON would learn less well if each feature be a scale 8bit pixel value that be xord with the same 8 bit random number in each pixel position  because that would introduce many more non  linear mapping between input and output  of course a fully  connect network will generally not learn image task as well as cnn in the first place    but if PRON could learn anything useful at all for PRON image problem  then PRON will probably out  perform cnn after the scramble effect   as a cnn usually have a few fully  connect layer  then PRON may be possible to get something from PRON scramble image   think logically  the feature still exist in the same relation to each other  just in a different form  encode   in term of be recognisable in the way that a cnn filter extract PRON  then the feature do not exist  that be a problem   the answer to PRON question depend on the nature of the noise that PRON have xor  ed the image with  if PRON be the case that the noise be random  or pseudorandom in the formal sense   then PRON be provably the case that the original pattern will not be learnable in the statistical learning theory sense  this scenario be equivalent to the application of a one  time pad   to quote the relevant wikipedia article   one  time pad be  information  theoretically secure  in that the encrypted message  ie  the ciphertext  provide no information about the original message to a cryptanalyst  except the maximum possible length16  of the message   this be a very strong notion of security first develop during wwii by claude shannon and prove  mathematically  to be true for the one  time pad by shannon about the same time   PRON gut feel base on the paper PRON will mention below be that yes  if PRON apply the same xor operation on the train and test datum  PRON will be able to train a very  accurate  classifier   to elaborate on PRON  gut  feel  please allow PRON to introduce to PRON what PRON personally think be one of the most important paper that come out this yearin fact this paper win the good paper award at iclr 2017    understand deep learning require rethink generalization   in this paper  the author show that deep learning model will generalize to  any  dataset  to give an example of the sort of experiment PRON conduct on this paper   PRON randomly shuffle the training and test set s label around in such a manner that for example some image of cat be label as dog whil some dog be name cat whilst some cat and dog image remain correctly label  now PRON be well understand that deep learning modelsincluding cnn  be quite resistant to a few noisy label but in the experiment conduct in the paper mention above this be a significant amount of noisy which beg the question why neural network still perform well on what end up be a garbage dataset   the moral of the story be that contrary to what most researcher believe in the past namely that deep learning model magically discover low level feature  middle  level feature  and high  level feature hide within the dataset more like the v1 system of the mammalian brain by learn to compress datum  PRON seem to just memorize anything PRON give PRON  include random datum   in short the paper mention above show that deep learning model generalize well to completely random noisein PRON case  think image generate from random pixel   deep learning model will generalize well to anything  anything  and if PRON can generalize to random datum which have no structure  then image that undergo a fix  predefin transformation like xor have nothing to a deep learning model   PRON must say  this be very worrying finding  to PRON at least  
__label__data PRON have a automation system which manage file send by multiple client inside s3  PRON so happen client send file on a regular basis  let PRON not consider the content of the file for time be as PRON can be something of new datum or append on old datum   typically PRON file be arrange on a pattern   i  clienta  datatype120180610filename1somerandomtextcsv  ii  clienta  datatype120180611filename1somerandomtextcsv  iii  clienta  datatype120180610filename2somerandomtextcsv  iv  clienta  datatype120180611filename2somerandomtextcsv  v  clientb  datatype220180610filename3somerandomtextcsv  vi  clientb  datatype220180611filename3somerandomtextcsv  vii  clientb  datatype120180610filename4somerandomtextcsv  viii  clientb  datatype120180611filename4somerandomtextcsv  this be a sample of the file management  now PRON want to be able to be able to identify unique file  for example case i and ii be same file receive on different date  but may be separate by somerandomtext  usually date or something    the primary assumption here be   file name length vary as per client and datatype  random text maybe at the first or end of file name for different file but  for not same file  ie filename1 will always have random text after the name  file name extension will vary on different file   so PRON want to build a system which take realtime input of the file receive and do an analysis of the name and path to determine if new version of PRON same file be receive and place PRON accordingly  same in the sense datum may be add to the file    PRON have very limited understanding of machine learn algorithm as of whole and not sure which set of algorithm this require  PRON be think of look at cluster algorithm  PRON just want to sure i start in the right direction  suggestion will be highly appreciate   PRON be look for the python package glob and re  PRON suggest PRON model the  the logic by hand  of the hip PRON doest sound like something PRON want to have search by a machine learn algorithm   this do not sound like a machine learning problem  PRON seem to PRON that PRON could come up with a list of rule that determine whether two file belong together   as s van balen say  PRON should take a look at the glob and re package in python   if PRON be on linux  a bash script would probably be much more elegant than a solution in python    the follow link may be helpful   walk through a directory and PRON subdirectory to get a list of all file  httpsstackoverflowcoma9545224743630   group string in a list base on partial match  for instance  turn   filename1xyz    filename2xyz    filename1abc    filename2def   into    filename1xyz    filename1abc      filename2xyz    filename2def     httpsstackoverflowcomquestions40167651pythoniteratethroughalistofstringsandgrouppartialmatchingstring   PRON mention that file may have different file type  be careful when convert between type  make sure for instance that separator and quote character be treat correctly  
__label__machine-learning __label__data-mining __label__methodology in experimental science there be the  scientific method   in mathematic there be the  proof   what be the standard for knowledge production in data science   PRON have gather that there a few different philosophy one can take  some of PRON be similar to the scientific method  introduce a falsifiable model and see if the data fit   other seem more hodge  podge  introduce some kind of learn machine and hope that the regularization prevent over fitting  and that cross validation catch PRON if not     there be some loose connection between those two  like how falsifiability fit into vc entropy and learn rate  this be explain in vapnik s book on the foundations of statistical learning    so  suppose that PRON have some datum and PRON want to extract knowledge from PRON  what be the community standard for do so  where can PRON learn about the epistemological philosophy behind these standard  and PRON mathematical expression   interesting question   the way PRON see PRON  the justification come from both mathematic and experimental science   if PRON suggest a new algorithm  PRON should state PRON assumption and provide a proof  just like in math  however  PRON should also demonstrate PRON on real datum  show that PRON assumption be reasonable and that PRON algorithm can deliver value   if PRON suggest a model  PRON should justify PRON base on PRON performance on the observation  just like in experimental science  this be true no matter how do PRON derive the model  however  examine the model logic PRON and the way PRON be derive will help to gain confidence in PRON and trace where PRON need modification  
__label__machine-learning __label__deep-learning __label__keras in keras  the area outside of the input be normally pad with zero  could PRON put other value instead of that  because the characteristic of zero to border be something PRON do not care about   
__label__machine-learning __label__data-mining __label__nlp __label__text-mining __label__topic-model PRON have categorize 800000 document into 500 category use the mahout topic modelling   instead of represent the topic use the top 510 word for each topic  PRON want to infer a generic name for the group use any exist algorithm   for the time be  PRON have use the following algorithm to arrive at the name for the topic   for each topic  take all the document belong to the topic  use the document  topic distribution output   run python nltk to get the noun phrase  create the tf file from the output  name for the topic be the phrase  limit towards max 5 word   please suggest a approach to arrive at more relevant name for the topic   if PRON do not want to dig into much nlp in that task  PRON suggest PRON to generate a set of most frequent ngrams  of length 2  5  from PRON document and find the most distinct ngram for each category use tfidf metric as sense importance of a particular ngram  normalizing measure by word count  and select those ngrams that be use in a particular category and be not  or rarely  use in other   PRON can suggest several paper on this topic   automatic labelling of topic models  automatic labeling hierarchical topics  represent topics labels for exploring digital libraries  PRON can find more by look at PRON citation  
__label__linear-algebra __label__eigensystem __label__matrices __label__sparse as a part of other work PRON need to solve relatively large  1e5x1e5  and sparse  100 non  zero element in each raw in few block  hermitian eigensystem  usually only few eigenvaluesvector be need  but with high precision  currently PRON be use arpack  arnoldi with shift  inverse when precision be preferable or spectrum folding when size be important   as an option plan to use trlan  thick restart lanczos  and try chebyshev filter instead of spectrum folding   probably  new method exist for this purpose   slepc be the package PRON be most familiar with for the scalable solution of sparse eigensystem   the developer also maintain a survey of software for solve sparse eigensystem  last update in 2009   
__label__computational-geometry __label__discretization __label__mesh in work PRON be currently work on PRON need to mesh some structure with equilateral triangle to study PRON use a kind of discrete element method know as spring network or lattice model   to mesh the structure  first PRON generate the equilateral triangular grid then PRON do the intersection between the structure and the grid   PRON problem lie in the second step  the intersection between the structure to mesh and the grid  PRON find difficulty to solve PRON   any help or suggestion will be highly appreciate   
__label__apache-hadoop __label__tools apache hadoop be once consider one of the tool PRON should have as a data scientist  around 2012 to 2014  PRON gain the popularity steeply among data scientists  and seem to be consider one of the toolset on the same boat as python  r  and sql   but these day PRON have not hear PRON story  in fact  the google trend also agree with PRON  that the popularity be peak at may  2015   why have hadoop fail to become one of the necessary tool as a data scientist  like python or r   PRON would not say hadoop fail to become popular rather PRON would say  PRON be still the base of any production big data system   python or r be handy at the beginning when PRON just need to try out thing but when PRON come to put thing in production  hadoop be the way to go  PRON do not directly provide any data scientist tool what PRON do provide be the base where datum would be store  process and can be use to apply machine learn algorithm use spark   so to summarize  PRON see hadoop be use in the follow context  for data storage in production systems  hadoop yarn as a cluster manager for tool like spark  flink  if someone still want to code in r  python  sas  hadoop be use as backend   hope this help   PRON be compare apple with orange  hadoop be one of the backend of big datum platform and python  r be programming language which be use build prediction model and data pipeline  hadoop still can be use as datum storage however more robust and faster distribute datum storage framework be gain popularity such as apache spark hence hadoop lose PRON charm  
__label__time-series __label__pandas __label__feature-extraction __label__feature-construction __label__dataframe how to transform raw datum to fix  frequency time series   for example PRON have the follow raw datum in dataframe  a  b  2017  01  01 000101  0  100  2017  01  01 000110  1  200  2017  01  01 000116  2  300  2017  01  01 000235  3  100  2017  01  01 000240  4  100  PRON would like to transform PRON into a time series   1 minute frequency  column a should have sum of value in time interval  column b should have mean of value in time interval  possibly other function over other column  note  raw data be not periodic   transform datum should be   a  b  2017  01  01 000100  3  200  2017  01  01 000200  7  100  this sort of effect can be achieve with  panda  dataframeresample   combine with resampleraggregate   like   code   dfresample1minagga   sum   b   npmean    test code   df  pdreadfwfstringiou     a  b  2017  01  01t000101  0  100  2017  01  01t000110  1  200  2017  01  01t000116  2  300  2017  01  01t000235  3  100  2017  01  01t000240  4  100       header1  parsedates0   indexcol0   printdf   printdfresample1minagga   sum   b   npmean     result   a  b  2017  01  01 000101  0  100  2017  01  01 000110  1  200  2017  01  01 000116  2  300  2017  01  01 000235  3  100  2017  01  01 000240  4  100  a  b  2017  01  01 000100  3  200  2017  01  01 000200  7  100 
__label__machine-learning __label__python __label__scikit-learn __label__data-science-model PRON be use mlpclassifer example from  scikit  learn  the code for training   from sklearnneuralnetwork import mlpclassifier  x    0   0     1   1     y   0  1   clf  mlpclassifiersolverlbfgs   alpha1e5   hiddenlayersizes5  2   randomstate1   clffitx  y   at the predict step  PRON use test datum  2   2     1   2   in  clfpredict2   2     1   2      the output of this function be  array1  0    as PRON observe  the test datum  22   be not in the train dataset PRON pass  still  PRON get the close match as label 1   what i be try to find be if the test datum i supply be not in the train dataset  i should print a message to user that data be not valid instead of tell PRON the wrong label as 1   for instance  in knn classification  i have kneighbour function which tell the distance of PRON close neighbour to the test datum i supply in a 0 to 1 scale  so  i could easily eliminate the test datum sample which be highly distant from PRON train data sample by keep threshold at 06 or 07   be there any criterion  threshold like this i could do with mlpclassifier or with any one of incremental classifiers mention here which can restrict PRON test sample if not present in train dataset   question migrate from so  sgdclassifier have desicionfunction which tell the distance to the hyperplane  where the value be compare to   this value could imply too big and too low value  
__label__numerical-analysis __label__algorithms __label__computational-geometry __label__performance suppose PRON have a straight line in cartesian space such that    xk  x0  k delta x  quad quad yk  y0  k delta y  quad quad zk  z0  k delta z    where  k can take any real value  if PRON project that line into cylindrical polar coord then    rk  sqrt  xk2   yk2       now suppose there exist some rectangle in the   r  z plane have a centre at   rc  zc and side of length  lr and  lz  the rectangle be align with the  r and  z ax such that a point on the curve be inside the rectangle if  rc  rk  lt  lr  2  and  zc  zk  lt  lz  2  can anyone suggest an efficient way to compute whether the projection of the line onto the   r  z plane intersect the rectangle   give a line    mathcal l   left  beginbmatrix  x0 y0 z0endbmatrix   t beginbmatrix  vx vy vzendbmatrix   t in mathbb r right  and a cylindrical shell of thickness  r2  r1  and height  z2  z1     mathcal s   left  beginbmatrix  x y zendbmatrix    r1  2 leq x2  y2 leq r2  2  land  z1 leq z leq z2  right  PRON would like to determine whether  mathcal l cap mathcal s neq emptyset  intersecting line  mathcal l with the cylindrical shell  mathcal s  PRON obtain the follow system of  4  inequality in  t    beginarrayrl   x0  vx t2   y0  vy t2  ampgeq r1  2  x0  vx t2   y0  vy t2  ampleq r2  2 z0  vz t  ampgeq z1 z0  vz t  ampleq z2endarray  PRON can decide the feasibility of this system of linear and quadratic inequality use quantifier elimination  mathematica have the function resolve  there be also qepcad and redlog  of course  PRON can also do quantifier elimination by hand  but PRON be rather cumbersome  
__label__linear-algebra __label__numerical-analysis __label__iterative-method PRON can not imagine PRON be the first to think about the follow problem  so PRON will be satisfied with a reference  but a complete  detailed answer be always appreciate    say PRON have a symmetric positive definite  sigma in mathbbrn time n  n be think of as very large  so hold  sigma in memory be impossible  PRON can  however  evaluate  sigma x  for any  x in mathbbrn give some  x in mathbbrn  PRON would like to find  xtsigma1x  the first solution that come to mind be to find  sigma1x use  say  conjugate gradient  however  this seem somewhat wasteful  PRON seek a scalar and in the process PRON find a gigantic vector in  mathbbrn PRON seem to make more sense to come up with a method to calculate the scalar directly  ie without pass through  sigma1x   PRON be look for this kind of method   PRON do not think PRON have hear of any method that do what PRON want without actually solve  ysigma1x  the only alternative PRON can offer be if PRON know something about the eigenvector and value of  sigma say PRON know that PRON be  lambdai  vi  then PRON can represent  sigma  vt l v where the column of  v be the  vi  and  l be a diagonal matrix with the eigenvalue on the diagonal  consequently  PRON have that  sigma1vt l1  v and PRON get that     xt sigma1  x  xt vt l1  v x  sumi lambdai1   vit x2      this would of course require PRON to store all eigenvalue  ie  a full matrix  v but  if PRON happen to know that only a few of the eigenvalue of  sigma be small  say the first  m  and the rest be so large that PRON can neglect all term with  lambda1i for  igtm  then PRON can approximate     xt sigma1  x  sumi1n lambdai1   vit x2 approx sumi1m lambdai1   vit x2      this then only require PRON to store  m vector  instead of all  n eigenvector   of course  in practice PRON be often equally or more difficult to compute the eigenvalue and eigenvector  compare to simply solve  ysigma1x multiple time  
__label__linear-algebra __label__ode __label__linear-solver __label__stability the equation and PRON meaning   consider two set   al0  mabl0  mb of hermitian matrix and a set of positive semidefinite matrix   cl0  mc each matrix have the dimension  n time n these form the ode      yprimet   sumi0ma   aiyyaidagger   sumi0mbbiy  ybidagger   sumi0mcciycdaggeri  qquad        note that y0  be a hermitian  ntime n matrix too   eqs  like      describe the norm  preserve evolution of a linear operator with respect to the anti  commutator  1st   commutator  2nd  and  basis  transform   3rd   eqs  like this occur often in quantum mechanic  see  master equation  for more information   the problem with vectorization of the matrix matrix form   the most common ode solver operate on vector  so say xt  be a vector that contain all column of yt  then the ode become with the kronecker product  otimes      xprimet   leftsumi0ma   1 otime ai  aiotimes 1   sumi0mb1 otime bi  biotime 1   sumi0mcciotime cirightx equiv xxt       but x be a full  n2 time n2  matrix  note that the jacobi  matrix coincide with x so each call to  xxt be asymptotically bad than the matrix  matrix multiplication above  thus  PRON be look for an algorithm to solve the ode in the matrix form    for  yti0  mt  a few additional property  the eigenvalue of the matrix may vary over several magnitude   the dimension of  n be never big than  10  3   usually PRON be between  10  1 10  2    the matrix  ai  bi  ci may be sparse as single one  but the sum  ie x  be not sparse   the eigenvalue can be negative   stiffness and time  dependent scaling of the matrix  ie  aj to alphatj aj  can occur   precision should be at least  105  but never more than   108  what PRON have try  PRON know odepack  but PRON rely on the jacobian  which be unnecessary big here  rk4 fail  except for very  very small step size  so  be PRON solution which be easy to adapt for this problem  preferable in fortran   related   algorithms for linear system of ode  how to choose a method for solve linear equation  there be a few way PRON can do this  even though rk4 fail unless stepsiz be really small  one thing that can happen a lot be that the model can start out more stiff than PRON really be in the full timespan  thus PRON would still try something adaptive like matlab s ode45 or julia s tsit5   before rule out non  stiff solver  if that fail  then PRON would try something make for large equation like sundial  PRON can directly call sundial from something like differentialequationsjl and use the option for linear solver choice and see if anything do well enough  if nothing do well enough  then PRON need to get fancy   then there be three option  the problem PRON see here be PRON want to avoid vectorization  build the matrix and instead want to do something that be matrix  free  the most standard option be then option 1  use a matrix  free representation of PRON operator in a krylov subspace method  so take any stiff solver like bdf  rosenbrock  sdirk  etc  what PRON want to do be re  define the linear solver that PRON use to not use a factorization method  but instead use an iterative solver like gmres and do so without construct the matrix  in julia  PRON can do this by define a a  linearmaptf  from linearmapsjl where f define the matrix  vector product  PRON equation     now PRON can use a in iterativesolversjl  or any library which allow abstract type  to define how to solve ax  b use PRON  without have to do the slow path of form x great  PRON be almost there  now PRON have to get PRON work like this in the ode solver  the trick be that the linearmap type know how to compose linear map  so if PRON do i  gammaa  PRON be a type for the identity matrix in julia  and gamma here be a scalar  PRON will automatically generate another linearmap which can also be use in an iterative solver  this be the fact PRON want to exploit   PRON be sad to say that right now PRON do not have full control over the internal jacobian typing so there be a quick modification to the algorithm that would be require  but let PRON say PRON want to use the 2nd order adaptive rosenbrock method  then PRON would go to the code which define the implicit system and make PRON so that way the rosenbrock w be simply  w  i  γa   PRON will need to do scop to define a in there  etc  etc  but that be standard stuff   just see shampine s matlab ode suite paper which describe this algorithm and PRON be pretty clear exactly why this be the need change  then use the linear solver specification to tell PRON to use that matrix  free gmres to solve the implicit equation  and there PRON go  that be an adaptive stiff solver PRON can use PRON equation  to solve all of the implicit equation   in a few month PRON plan on release an update to differentialequationsjl s function definition which will allow PRON to explicitly define w so that way option 1 can be do without modify the code  but for now that be what PRON get   option 2 would be to implement the full method PRON and make all of the linear solve matrix  free  the second order rosenbrock method really be not difficult and shampine explain PRON well so an implementation where PRON replace the linear solver would be pretty quick to make   option 3 would be to use some form of an exponential integrator  there be specialize  strang splitting  method which can handle this type of equation well  but PRON do not know of any implementation  PRON be build one into differentialequationsjl right now  so PRON can see some of PRON discussion and use those plus hairer s geometric integration method book to piece together how this be do and implement one PRON before PRON  PRON will be a lot easy since PRON can make PRON specifically for PRON set of operator instead of make PRON generic  but the hard part be PRON will want to code a fast  path for compute expmv  which be w  expadtv use a matrix  free a   
__label__data-formats PRON have a data stream that PRON would like to share with some datum scientist   PRON be a regularly capture time series with some field that be simple scalar  boolean  each sample have a utc time and fractional second since start of capture   also capture be a 3d spline that vary in size  these spline be also regularly sample   additionally  there be some other multi dimensional field as well   x  y  z  pitch  yaw  roll     for simple data set PRON would normally use csv  however due to the nature of the more complex datum PRON need a more appropriate format   what be PRON option for format that would allow easy loading into matlab or other common datum science tool   base on this article about data formats in data science  the short answer to PRON question be  use json   this may seem natural next step after csv and no more  but PRON be actually quite interesting   json be widely support in many programming language  PRON be a valid mime type in internet standard  application  json  data stream mention in PRON question  and PRON have several standard and non standard extension  geojson  binary json   if there be not a standard structure  json make PRON easy to invent PRON own  because PRON be easily modifiable   accord to the article json work as value in hadoop  for hadoop PRON be suitable  because basically json be plain text when PRON think PRON as a data item   also article mention json more efficient than csv  
__label__deep-learning __label__amazon-ml PRON have discover that amazon have a dedicated deep learning ami with tensorflow  keras etc  preinstall  not to mention other prebuilt custom ami   PRON try out this with a typical job on several gpu  base instance to see the performance  there be five such in the ireland region  maybe in other region exist even more  PRON do not know  this variance be a bit confusing    g22xlarge  g28xlarge  p2xlarge  p28xlarge  p216xlarge  PRON first question be  what be the difference between the two group  g  something and p  something   both group mention  gpu along with cpu   but no further clue for deep learning usability   PRON second problem be that PRON have be run PRON job on g22 and g28 as well  and while the task processing toke quite long time to run  the workload of the gpu be relatively low  20  40    why do not the framework increase the workload if there be spare processor capacity  be be necessary  possible to paramter  set anything to optimize the work    PRON think the difference and use case be well point here  as far the workload  there be feature which help PRON optimise PRON  accord to the official documentation  PRON can try   for persistency   sudo nvidia  smi pm 1  disable the autoboost feature  sudo nvidia  smi auto  boost  default0  set all gpu clock speed to PRON maximum frequency   sudo nvidia  smi ac 2505875 
__label__reinforcement-learning PRON be try to implement a value  iteration algorithm to solve a grid  world problem  PRON be new to the field   the usual formula that PRON encounter about the value function vs  be     vs   rs   maxa in a  sums  in s  ts  a  s   vs  where  s be the set of state   a the set of action   t the transition model    ts  a  s    pst1   s   st  s  at  a  and  r the reward function   since PRON be work on a model  base problem   t and  r should be know  the problem be that PRON do not know how to define  or compute know the detail of the problem   r if PRON be in a state  s and take an action  a that would make the agent hit a boundary of the grid world  the new state  s will be  s  and the reward  as define by the problem  be 5   on the other hand  if PRON have arrive in state  s through another state  the reward will be 1  so basically   rs depend on the previous state and the previous action  how do PRON represent that in the  vs formula   the formula PRON have quote be a bit unwieldy  precisely because the r function as define need to  look ahead  to all possible outcome and PRON probability  but how to do that be not include explicitly   there be actually a few variant of the bellman equation that express more  or less detail  a good place to start for a truly generic version be sutton  amp  barto  2nd edition      vs   textmaxa in mathcalasumr  spr  ss  ar  vs  where  sumr  s be over all possible reward and next state pair   the above equation change PRON transition function that only handle next state  to a similar function that handle successor state and reward     pr  ss  a   pr  rt1   r  st1   s   st   s  at   a   usually this do not increase the number of item to sum over  or add much complexity  because reward will most often associate with the transition   the benefit be that this approach remove the need for a reward function that work with expected reward  and just work with specific reward  other variant be possible too  such as an expect reward function base on   s  a or   s  a  s  the difference be just a little bit of juggle with the expression so that PRON remain effectively the same give the subtle difference in definition for  r 
__label__reference-request __label__approximation-algorithms suppose PRON have some function  f and PRON want to find  x such that  fxapprox 0 PRON may use the newton  raphson method  but this require that PRON know the derivative function  fx an analytic expression for  f may be unavailable   for example   f may be define by a complicated piece of computer code that consult a database of experimental value   but even if  f be complicated  PRON can approximate  fa for any particular  a by choose a small number  epsilon and calcult  fa  approx  faepsilon   faoverepsilon  PRON have hear that there be distinct disadvantage to this approach  but PRON do not know what PRON be  wikipedia hint that  use this approximation would result in something like the secant method whose convergence be slow than that of newton s method    can someone please elaborate on this  and provide a reference that particularly discuss the problem with this technique   for the sake of notation  let PRON suppose that  f  mathbbrn  rightarrow mathbbrn  ie  PRON be a vector  value function that take a vector as input and output a vector of the same size   there be two concern  computational cost and numerical accuracy   calculate the derivative  mathrmdfx  the jacobian matrix   jx  or   nabla fxt  or whatev PRON prefer  use finite difference be go to require  n function evaluation  if PRON could calculate the derivative use float point arithmetic directly from the definition  PRON would have to calculate the difference quotient  beginalign   mathrmdfxei   limvarepsilon rightarrow 0  fracfx  varepsilon ei    fxvarepsilon   endalign   for each  i  1  ldot  n  assume PRON do not do any sort of  smart finite differencing   like curtis  powell  reid  because PRON know  or can detect  the sparsity pattern of  mathrmdf if  n be large  that could be a lot of function evaluation  if PRON have an analytical expression for  mathrmdf  then calculate PRON could be cheap  automatic  also know as algorithmic  differentiation method can also be use in some case to calculate  mathrmdf at roughly 3 to 5 time the cost of a function evaluation   there be also numerical concern  obviously  on a computer  PRON can not take the limit of a scalar as PRON go to zero  so when PRON approximate  mathrmdf  PRON be really pick  varepsilon to be  small  and calculate  beginalign   mathrmdfxei  approx fracfx  varepsilon ei    fxvarepsilon    endalign   where  approx mean PRON be an approximation  and PRON hope PRON be a really good approximation  calculate this approximation in float point arithmetic be tough because if PRON pick  varepsilon too large  PRON approximation could be bad  but if PRON pick  varepsilon too small  there could be significant rounding error  these effect be cover in the wikipedia article on numerical differentiation in superficial detail  more detailed reference can be find within the article   if the error in the jacobian matrix  mathrmdf be not too large  newton  raphson iteration will converge  for a detailed theoretical analysis  see chapter 25 of accuracy and stability of numerical algorithms by nick higham  or the paper by françoise tisseur on which PRON be base   library generally take care of these algorithmic detail for PRON  and usually  library implementation of the newton  raphson algorithm  or variant thereof  will converge quite nicely  but every so often  there will be a problem that cause some trouble due to the drawback above  in the scalar case   n  1  PRON would use brent s method  owe to PRON robustness and good convergence rate in practice  
__label__python __label__numerical-analysis __label__numpy PRON need help plot the heaviside function   real analysis often involve construct bizarre function which be intuitively correct  but ultimately wrong   see the great book counterexamples in analysis   PRON write a numpy script to plot the function  displaystyle sumn1infty frac1n2  hleftx  frac1nright to illustrate some concept in real analysis   this be in a response to another question on mathstackexchange   demonstrating continuity properties of  fx   sumn1infty  an hx  xn  here be be example with accompany plot   b    1n  for n in range150    n  1000  y  npzerosn   count  1  for b in b   x  npzerosn   x  nparangeb010nb1   nastypeintcount2  y   x  count   1  pltplotnparange0110ny   pltaxisequal      how can PRON remove the vertical line and really show the discontinuity     PRON can  hold  the plot and plot each component separately  loop over the different function and plot   as long as PRON know the exact position of the discontinuity  PRON just have to set the jump position to nan in x  y or both  PRON can set this manually in the desire position or use some criterion  for example  PRON can use the npdiff function to calculate the difference between contiguous position in an array  for the simple case of the heaviside function   def heavisidex       see httpstackoverflowcoma15122658554319     y  05   npsignx   1   ynpdiffy   gt 05   npnan  return y  x  nplinspace1  1   pltplotx  heavisidex    pltylim1  2   use this in PRON specific example   def ux  n    return sumheavisidex  1  i   i   2 for i in range1  n     n  1000  x  nplinspace0  1  n   pltplotx  ux  50    get rid of the vertical line in this plot would involve find all the discontinuity and then plot each segment in a separate call to pltplot   another  less elegant  solution be to plot point instead of line  since the point be very close together  high value of n   PRON will look like PRON form a line  to do this  replace pltplotnparange0110n   y  with pltplotnparange0110n   y   k    the tells plot to use dot  k be for black   this be what PRON will get  
__label__machine-learning __label__clustering PRON be try to build a model  gather specific hotel booking datum  try to find the pattern how the hotel be book  stay  what type of people live  how many booking average per day  the booking may vary from weekday to weekend  also from winter to summer  normal day and vacation period  all these factor be accountable   then  as the time pass by  PRON want to know if the booking become abnormal  eg normally  young couple book the hotel quite a lot  all the sudden  a bunch of business man check in for a couple of day   since in this case  PRON do not have sample  label for normality and abnormality  PRON start think use unsupervised learning  like cluster for a start  say  PRON construct a samplebook feature for every week  go back all the way to the beginning of the year  and then  PRON try to cluster PRON  then  every week  PRON calculate the current week  and see if PRON belong to any cluster  or PRON be an abnormal point stand out require attention   be this a reasonable approach or there be some good way   look at the distribution of the feature PRON want to consider for anomaly  eg  user attribute  condition on the date  so PRON do not trigger a warning for normal seasonality  an anomaly then be when the current conditional distribution be significantly different from the historical average  for more information look into contextual anomaly detection  welcome to the site and good luck  
__label__machine-learning __label__python __label__data-mining __label__clustering __label__decision-trees  PRON be kind of new to the datum mining subject but i need help to choose a learn algorithm for PRON application   the problem  identify that a certain curve or data set belong to a certain fault in a component   PRON training datum should be like this   motor current value    0506  04    fault in komponent 1   0203  04    fault in komponent 3   107  04    fault in komponent 2         0807  03    fault in komponent 3  and i be wonder if i should use a cluster analysis  k  mean and save centroid center then compute the distance of each new entry then give a fuzzy estimation to where which cluster PRON datum may belong to    decision tree algorithmentropy of the value and stuff   or should i calculate a distance between the nominal datum  healthyno fault  curve  and the faulty datum and play on a more simple basic decision tree with threshold   proceed with a peak analysis and count the number of peak within PRON datum and add PRON to PRON learning algorithm   and when should i do a datum pre  processing like normalization   this be an example of PRON entry parameter   speedmm  s   motor current valuesampere   distancetonominalstateampere    here be what PRON datum look like   any suggestion   PRON seem PRON have a data set for one component where the component suffer a fix number of failure mode  PRON want to find out which datum  let PRON assume continuous in time  so  what time  correspond to what failure mode  in other word  PRON be do  pattern recognition  in PRON failure datum   have PRON think of use self  organizing maps  som   PRON be a sub  branch of artificial neural network and have great capability in such problem   PRON should also consider that not all failure mode appear in shape of a  peak  value  so  only look at peak be not a very smart way  PRON most probably will cover most of the failure  though  there will be moment that PRON miss  som could take care of this too   data pre  processing be do before the analysis  be careful in normalization  PRON could miss the peak or valley point easily if PRON do not pay enough attention in normalization  do not just use any code or normalization method PRON find online  test and check PRON with PRON datum  for instance  some normalization could make all negative value positive which a negative value may have an important meaning in PRON work   PRON assume PRON have another variable call  failure   another approach PRON suggest be build a neural network  nn  model  which be very common  PRON have three input datum that PRON mention  consider the  failure  variable as PRON target variable  build the neural network and apply PRON to PRON datum again  if the number of failure be little  the nn will be able to rebuild the normal behavior of PRON datum  nn here be call a normal behavior model   when PRON apply PRON to PRON input datum  the nn model will detect any deviation which be not expect   matlab have a very good support for both of these approach  
__label__linear-algebra __label__matrices __label__computational-geometry __label__high-dimensional suppose an euclidean distance  dinmathbbrntime n matrix between a set of  n object be give  to obtain inner  product  which will be further be use to recover coordinate   entry of  d be square  and the matrix be double  center and scale  ie    kfrac12jd2j  where matrix  j  ifrac1n11t define the origin wrt which the inner  product be form  so  the aim be to reconstruct coordinate  x that give rise to inner product  k     kfrac12jd2j  jxxtj this be do by eigendecomposition of  k  give  the above equation  PRON wonder if one could use a shortcut  frac12d2xxt  ie  obtain coordinate  x without double  center  frac12d2  matrix  j be remove from the left and from the right    PRON proposal will give meaningless result  this can be see already with simple example consist of two point only  indeed  the matrix that PRON want to decompose be not even positive semidefinite as PRON be nonzero but have zero diagonal entry   note that PRON can not cancel  j since PRON be singular  in general   ju  jv imply  u  v if and only if  j have a trivial kernel  ie  iff the rank of  j equal the number of column of  j 
__label__linear-regression __label__pca PRON have a matrix with 11 variable and 1258 observation for each variable  and another matrix the same size with the corresponding measurement error for the variable  in two variable i have a nan value in some of the vector element  and of course also in the correspond error vector   PRON have 2 question  PRON want to activate pca algorithm on the datum  how to treat the nan value  PRON that of replace with average or median  be that true   same question about the error  if PRON want to calculate the weighted error   frac1sqrtsumfrac1sigmai2 what should i insert instead of the nan   PRON be do linear regression with the datum and after i get the coefficient PRON want to do error propagation    delta yleftxi  xnrightsqrtsumlimits   inleftfracpartial ypartial xiright2delta xi2 but the problem be that for each variable i have vector of measurement error so how i need to apply the method in this case   
__label__finite-element __label__finite-volume __label__discretization __label__numerical-modelling __label__conservation PRON know that in fvm  PRON be possible to show that a discretisation scheme be conservative by add the discrete term over a few control volume and show that all term cancel apart from those relate to the flux in and out of the entire domain   PRON be not sure what happen in fem  PRON thought be  for instance for a 1d case where there be two neighbour element share a node  and hence 4 equation exist for a 3element grid  one add the relevant term from the 4 equation to see which term cancel out and which remain in the end  be PRON right   thank PRON   the way this be usually prove in the finite element context be different  but many finite element scheme satisfy conservation property  for example  if PRON think of the stokes equation  as long as the pressure space contain the piecewise constant function  then mass be conserve  similar property can often be show for the mixed laplace equation typically use for porous medium flow   PRON be typically more complicated to show such property for first order conservation equation  but even there PRON be sometimes possible if finite element space be appropriately choose   however  the way PRON would show this result from the weak form of the equation  with particularly choose test function  PRON do not quite work as PRON suggest in PRON question  
__label__backpropagation __label__lstm when try to overfit the network  what be the practical maximum depth of an lstm neural network before PRON will start to fall apart  if possible  what be the state  of  the  art depth in lstms produce by company like google   PRON know that in real world number of layer be implementation  dependent  amp  number of neuron can be estimate   1    2  but curious if PRON result be typical   PRON have implement a system in c  use irprop  here be the paper  section 31   however  PRON be struggle to get the error  propagation under control when go above 10 layer   PRON be just try to overfit the network  PRON successfully and very quickly converge with 4  6 layer  100 irprop iteration and the error be down to 000001  where PRON start at around 400000   the network try to predict the next character in the alphabet  make from 26 character   so each layer have an lstm that work with 27dimensional vector  the error  propagation happen after 25 timestep  if PRON crank  up to 10  14 layer  the error be really hesitant to even begin climb down  and seem to simply oscillate around the 36 value  in incredibly rare case if a weight initialization be lucky  with 10  14 layer the error will decrease  but usually PRON will just oscillate  be that usual  PRON be use float datatype  however  test the double datatype and the oscilation still happen  so PRON doubt PRON be anything to do with precision  adjust the  n value  acceleration  do not seem to affect PRON either  be look at example  3   but so far get impression people use a maximum of 2  4 layer   
__label__dimensionality-reduction __label__pca __label__tsne let PRON say that PRON have sparse feature vector and PRON would like to use dimensionality reduction in order to visualize PRON more easily   what if PRON have some prior knowledge on the colinearity between PRON feature  as in  PRON would be able to create an approximate distance matrix between PRON feature  and therefore between PRON data point   PRON be aware that if feature be actually colinear  method like pca will find a way to reduce PRON  however  PRON be afraid that PRON do not have enough data point to infer that colinearity strictly from the datum  let PRON say that PRON data look like something like this    x0  1nan1nan   x1  1nan  nan1  in PRON case  PRON know that the 3rd and 4th feature be colinear and that the distance between  x0  and  x1  be close to zero  therefore  PRON should be map to a very similar datum point in a lower dimensional space   can pca still do PRON with so few datum point  be there way to  force  the know colinearity or distance measure  t  sne perhaps   
__label__fluid-dynamics __label__computational-physics __label__navier-stokes __label__vorticity eg 1 2  be PRON just vorticity  what be actually happen   similar  steam engine  volcano  cloud    example be grid  base  use  vorticity confinement  in phoenix fd   edit some technique   divergence control  be use to increase the smoke concentration  and fuel particle in the air may combust  further add smoke at that point  ie not at the initial source of the smoke    a reference  feldman 2003  animate suspended particle explosions  acm trans  graph   proc  siggraph  22  2003   708  715  
__label__dimensionality-reduction __label__pca what be the case when PRON should not use pca for dimensionality reduction and what to use in such case   PRON should not use pca if PRON only have categorical variable  and thus the distance function in pca be invalid   correspondence analysis be a common alternative  
__label__search when PRON visit this site  PRON find the word  search  appear quite often   but why be PRON important  what kind of search algorithm be use in artificial intelligence   and how do PRON improve the result of an ai   as tag go  search be relatively uncommon  if PRON be not for this question  PRON would not be on the first page of tag  that say  search be important for at least two reason   first  search be one of the early and major consumer of advanced machine learning  as find the correct result for a search query boil down to predict the click  through rate for query  result combination  more relevant result mean more click  more traffic  and more revenue   second  many planning and optimization problem can be recast as search problem  an ai decide on a plan to route package through a network be search the space of possible plan for a good one   in regard to the question PRON mention  in the comment of the op   these search be relate to optimization  PRON be not sure of PRON background  so let PRON describe PRON from scratch  briefly   remember the derivative  the base idea be to talk about how the function change in regard to change in input  so now  PRON be out of high school and PRON be build neural net  PRON have do the basic coding  and want to look at how PRON model be work  back from PRON statistic class  PRON remember PRON use a certain measure of error  eg least square  to determine the efficacy of the model from that class  so PRON decide to use that here  PRON get this error  and PRON be a bit too big for PRON liking  so PRON decide to fiddle with PRON model and adjust the weight to get that error down  but how   this be where the  search  come into play  PRON be really a search for the good weight to put on the edge of PRON net to optimize PRON  PRON use the derivative  in some fancy way  use the  stochasitc   think random sampling  and other way the question mention  to search for which way be  down  in the high dimensional space of PRON weight  in other word  what PRON be search for be minima or maxima to optimize PRON neural net  and PRON  search  for PRON by do a derivative which tell PRON which way to go  move a bit in that direction  then do that again and again iteratively to find  hopefully  the good weight   this video here go into all the detail PRON would want  and PRON recommend the entire series as a robust but understandable intro to neural net  demystify neural networks  go and look up  gradient descent  to get any related material   note  the gradient here be equivalent to multidimensional derivative direction to go in  and descent be just search for the minima   search have always be a crucial element of ai in multiple way   first  what many people refer to as  search  be a reflection of how what PRON call  intelligence  frequently involve search something   a physical realm  a  state space  of possible solution  a  knowledge space  where idea  fact  concept  etc  be relate as a graph structure  etc   look up some old paper on computer chess  and PRON will see that a lot of that involve search a  state space    as such  search algorithm that be efficient  in term of time complexity andor space complexity  have always be important to make advance there   and while computer chess be just one example  the principle generalize to many other kind of problem solve and goal seeking activity   here be a reference that explain more about some of these idea   note too that  search  be closely related to the idea of  heuristic  in an important way   many search problem in the real world be far too complex to solve by exhaustive brute  force search  so human  and ai s  resort to heuristic to narrow the state space be search   use heuristic can yield search algorithm that allow for reasonable solution in a realistic time  frame  where no simple  deterministic algorithm exist to do likewise   for some more background PRON may want to read up on a  search  which be a widely use algorithm with many application  and not just in ai   the other major regard in which something PRON could call  search  apply in ai be through the use of algorithm which be also often refer to as  optimisation  technique   this would be thing like hill climbing  gradient descent  simulate annealing and perhaps even genetic algorithms   these be use to maximize or minimize the value of some function  and one of the canonical us in ai be for train neural network use back  propagation  where PRON be try to minimize the delta between the  correct  answer  from the training datum  and the generate answer  so PRON can learn the correct weight within the network    state space search  be a general and ubiquitous ai activity that include numerical optimization  eg via gradient descent in a real  value search space  as a special case   state space search be an abstraction which can be customize for a particular problem via three ingredient   some representation for candidate solution to the problem  eg  permutation of city to represent a travelling salesman problem   tsp  tour  vector of real value for numeric problem    a  solution quality measure  ie some mean of decide which of two  solution be the good  this be typically achieve  for  single  objective problem  by have via some integer or real  value  function of a solution  eg total distance travel for a tsp  tour    some mean of move around in the space of possible solution  in a heuristically  inform manner  derivative can be use if  available  or else  eg for black  box problem or discrete solution  representation  the kind of mutation or crossover method favour  by genetic algorithm  evolutionary computation can be employ   the first couple of chapter of the freely available  essentials of metaheuristics  give an excellent overview and  michalewicz and fogel s  how to solve PRON  modern heuristics  explain in more detail how numerical optimization can be consider in term of state  space   in response to a request in the comment to explain how  clarify how  the  search through possible plan  may occur   the idea be to choose all three of the above for the planning problem and then apply some metaheuristic such as simulated annealing  tabu search  genetic algorithms etc  clearly  for nontrivial problem  only a small fraction of the space of  all possible plan  be actually explore   caveat  actually plan  in contrast to the vast majority of other problem amenable to state  space search such as scheduling  pack  rout etc  be a bit of a special case  in that PRON be sometime possible to solve planning problem simply by use a  search  rather than search with a stochastic metaheuristic   the aim of an ai be to fulfill one or the other task  say solve the task adequately  but there be result that be no solution at all and there be result which be satisfy the task and thus be accept as solution  since there be generally more result that be no solution  the set of all possible solution be just a subset of all result  but this mean that the task involve the search for a suitable set of solution   every problem can be reduce to search   every problem have an input within some range  the domain  and an output in some other range  codomain    that is  every problem can be formulate as a kind of map from one space to another  where the source be the given of the problem  and the destination be the solution to the problem    brute force  be the algorithm which solve every problem by inspect every point in the codomain and asking   be this the solution    every other algorithm be an attempt to improve on brute force by not search the entire codomain of possible solution   typical software engineering problem can be solve by algorithm which arrive at the correct solution very quickly  sort  arithmetic  partition  etc     ai problem be generally those for which a strong polynomial  algorithm be not know  and thus  PRON must settle for approximation   basically every common problem that the human brain must solve fall into this category   consider the problem of move a multi  jointed robotic arm to pick up an object   reverse kinematic do not have unique solution  there be more than one way to move PRON hand from a start position to a target position   this be due to the excessive degree of freedom in PRON joint   if PRON want to minimize energy usage  then there be a unique solution  due to the asymmetry of joint and muscle    but what if there be an obstacle in the pathway of the minimum  energy solution   there be many pathway which avoid the obstacle  but again  many of PRON will have a similar cost   even if there be a unique minimum  energy solution  PRON may not be the most practical to compute   the brain be the most metabolically expensive organ in the body  so PRON be not always good to find an optimal solution   thus  heuristic come into play   but in all case  the problem be not   move PRON hand  or  move the robot arm    the problem be   search the space of joint rotation sequence which best achieve the goal    and even though there be a closed  form solution for the simple minimum  energy case with no obstacle  PRON be too expensive to compute precisely when a set of cheap heuristic will get PRON very close with a small fraction of the computational effort   if computation be free  then ai would be mere mathematic  and PRON would always compute the good answer to every question use logic  calculus  physics  at bad  numerical method when PRON do not have close  form solution   in reality  time be money  and the time and effort to get an answer be as much a part of the cost as the quality of the solution   so PRON be an engineering tradeoff to decide how much effort should be expend in what way to obtain the good answer give the value of the response   or  in other word  ai problem be all about search the space of solution as quickly as possible to get an answer that be  good enough    PRON may seem curious that such far  flung problem as natural language recognition and theorem proving would be search problem   but language parser strive to determine the meaning of statement via part  of  speech tagging   a give phrase can be parse in many different way  yield many different interpretation  and the space of parse tree be yet another search problem in decide which parse tree be the most likely intend meaning by the speaker   a theorem proof be graph start with axiom  proceed through lemma  apply the rule of procedure until the theorem be derive or refute  by prove PRON negation    there be many way to represent this sequence  but at the end of the day  PRON be talk about a process of explore the intermediate proof space and find the derivation which reach PRON goal   everything be search  in the end   consciousness be an attention selection mechanism that search over salient input  the robotic saccade of PRON eyeball show PRON first hand the algorithmic nature of PRON brain s conscious attention mechanism  while PRON search among salient input   a smart search algorithm can help with dimensionality reduction  
__label__pde __label__numerical-analysis __label__python PRON be currently solve the heat equation as a part of the pde sequence in class   PRON have be give the formulati  n1   ti  nalpha left  fracti1n2 ti  nti1ndelta x2  right  quad delta t  PRON have get boundary condition handle  PRON be not the problem   the problem here be  if one notice   alpha   delta x2   and  delta t be all permanent constant throughout the number of iteration  thus  PRON can be pre  evaluate  there be just that one problem where  if the value of  frac  alpha delta tdelta x2 become large than 05  PRON start run into problem with the equation PRON  one can see that when that term  05     beginmatrix   100  amp  0  amp  0  amp  0  amp  0  100  amp  50  amp  0  amp  0  amp  0  100  amp  50  amp  25  amp  0  amp  0  100  amp  675  amp  25  amp  125  amp  0  endmatrix  here  every row represent a new time instance  and every column be a new discrete x  position element along the  thin rod  that PRON be consider   the moment the  frac  alpha delta tdelta x2 go below 05  however  PRON be  good   mean that this problem cease to occur  but the propagation of temperature through the  thin rod  be still really slow  even with a really high temperature at one end  which be hold constant   be the equation provide wrong  or be PRON misunderstand something  be there some error in sign somewhere   edit  this be indeed a class assignment  but give the nature of the situation  PRON be incline to think that PRON be either PRON understanding that be fundamentally flawed  beyond the code  just the concept  or PRON have simply be give the wrong equation   edit 2  error in evaluate the matrix  correct that   PRON have forget a step   the equation be     ti  n1   ti  nalpha left  fracti1n2 ti  nti1ndelta x2  right  quad delta t  and PRON be evaluate     ti  n1   alpha left  fracti1n2 ti  nti1ndelta x2  right  quad delta t  PRON have stumble upon a common problem with explicit scheme  this be a common issue with numerical analysis and PRON be call the courant – friedrichs – lewy  cfl  condition  httpsenwikipediaorgwikicourante28093friedrichse28093lewycondition    in short  PRON have to adjust PRON time step size and the spatial discretization in order to meet this condition  which very much depend on the physical situation   usually  the time step be adjust  since adaptive meshing  or remesh and map PRON solution  be somewhat costly   conceptually  this condition occur because the  communication  between node become much slow than the  motion  of the solution   this mean that the temperature  let PRON think the diffusion of phonon for a more physical conceptualization  move over multiple cell per time step   what PRON want be a condition such that the phonon travel through one cell over many time step   the cfl condition be a require condition only for explicit numerical scheme and be a necessary condition in both convection and diffusion problem  
__label__clustering __label__clusters __label__weka PRON have recently be learn about the various clustering method  and PRON decide to apply furthest point cluster in weka with vary seed value   the seed value dictate the initial choice of point in the data set  PRON can not be totally random  since repeatedly do the clustering with  say  seed  3 give the same result   how do PRON select data point base on the seed   pseudo  random number generator be initialize with a seed   there be little real randomness available  most just look random to PRON  because the pattern be too complex  read up some example on pseudo random number generation such ad mersenne twisters  
__label__machine-learning __label__neural-network __label__dimensionality-reduction __label__pca  the dataset which be extract from the database consist of more than 50  column  PRON call these column dimension  can PRON call PRON dimension   obviously  PRON have to do dimension reduction on PRON  but since pca like algorithm often do axis rotate to generate some new axis  PRON do not think PRON will pca algorithm in dimension reduction  so PRON calculate the correlation between these columnsparameter   and filter these who have a high value and some other rule  so can PRON still call PRON dimension reduction  since PRON only do some parameter filter  the reason PRON do not use pca like algorithm be because PRON want to implement neural network classification  and PRON need the real parameter   please comment on these  anything even criticize be welcome   the approach PRON use to do dimension reduction be agnostic to the method PRON use for classification  PRON can use pca to preprocess PRON datum before to train any type of classifier  include artificial neural network if that be what PRON want to use   if PRON datum consist of observation in row and each column be a variable for that observation  then PRON can call these column dimension  these can also be call feature of the observation   dropping column  feature from PRON dataset be essentially dimensionality reduction  PRON go from a high dimensional space to a lower dimensional one   less column    the fact that PRON be not combine feature  rotate space do not mean PRON be not a valid form of dimensionality reduction   why be PRON against use pca for neural networks in particular  pca do not care about what kind of classification PRON be run after PRON  in fact there be a convenience function in r s caret package implement pca before neural net to ease computation of the nn  httpsartaxkarlinmffcuniczrhelplibrarycarethtmlpcannethtml  the dataset which be extract from the database consist of more than 50 column  PRON can call PRON feature  feature set be something that PRON operate on   obviously  PRON have to do dimension reduction on PRON  but since pca like algorithm often do axis rotate to generate some new axis  PRON do not think PRON will pca algorithm in dimension reduction   PRON be not sure if PRON understand pca completely  pca do not generate new random axis  but an axis  that represent the most variance of PRON datum  so  pca will surely perform dimension reduction give that datum have some value which do not add info to PRON model   PRON can call this feature extraction or feature filtering  the reason PRON do not use pca like algorithm be because PRON want to implement neural network classification  and PRON need the real parameter   what make PRON think  post pca PRON do not have  real  parameter  PRON be just a different view of the same dataimagine these ax as eye  and PRON datum as the object  PRON just have a different view of the same object    the only problem PRON see be the ax on which the component be project be orthogonal  which if PRON want to avoid  PRON can go ahead with icaindependent component analysis  
__label__computational-geometry __label__mesh-generation __label__mesh PRON be write the pre processing program for a porosity base cfd project PRON just start  basically PRON have a 3 dimensional mesh make of cube and PRON need to import a stl file over PRON  and calculate how much space of each cell be be use by the object  the stl file be a cad format that use triangle to represent a surface  but PRON will assume that PRON represent a close surface  a solid    PRON make a drawing of the 2d problem   on the stl object be over the mesh  each cell of the mesh receive a value range from 0 to 1 base on how much of the object be over PRON  cell 12345 have a value of 0 while cell 6  have a value of 15 and  cell 7 have a value of 1   so PRON question be  where do PRON start  be there any  surface cast  algorithm  how do PRON deal with the triangle  cube conversion   
__label__reference-request __label__neural-network hopefully PRON be on the right website   PRON be in search of trustworthy source for paper or book about neural network to a certain topic  PRON read in many website how PRON can be divide in general  but PRON be not sure if PRON right  into feedforward  backpropagation  recurrent or be there something more eg convolutional  PRON need a reliable source for divide PRON generally not into different type at all of neural network   
__label__fluid-dynamics __label__boundary-conditions __label__advection PRON be solve the 1d advection problem give by     fracpartial upartial tcfracpartial upartial x  where c be the wave speed  and u be the unknown field variable  and x and t be time and space  PRON be use central differenc to discretise in space  PRON initial  u profile which be be convect be a step profile give by   uleftx  trightleft  beginarrayc   1   for0ltxlt1  0   for1ltxlt2  endarrayright  PRON question be about the boundary condition  a the moment at the boundary  if c0 PRON have set  ul  u1t  where u1  be the value of u at the first node   but PRON end up with a lot of oscillation at the bc   PRON have also try set  ul1  and  ur0   r indicate right and l leave  and that give PRON oscillation too but more sensible one  PRON know that with central differenc spurious oscillation be expect but PRON be not sure about the correct boundary condition and PRON would appreciate some help  PRON search online for a clear answer but be unsuccessful   PRON have also solve the problem with upwinding and by set  ul  u1 if c0 which seem to have work   thank PRON   let PRON briefly explain one helpful and simple approach how to better understand the boundary condition for PRON wave equation with constant speed  the idea be that PRON can consider PRON problem on the infinite interval and think about an equivalent definition of boundary condition to such situation   so for instance if PRON extend PRON initial condition  ux0 to be valid for all real number then PRON can get the value at PRON boundary node from the exact solution  ux  tux  c t0  ie  u0tuc t  0 and  u2tu2c t0 PRON see that for the constant speed  c PRON should prescribe the value of solution only for one boundary node  depend on the sign of  c   because the other boundary value be in fact define by the initial condition  for some time   if PRON want to have the constant value at boundary  PRON mean PRON prescribe  u0tul if  cgt0  or  u2tur if  clt0 in a theory PRON can prescribe the both interdependently of  c  but then in one node PRON be not  compatible  with PRON wave equation in the sense that PRON prescribe some different  process  than PRON pde  at such node  eg a boundary layer phenomenon    concern the oscillation in nuemrical solution  as PRON mention PRON and PRON get also correct and good advice in comment  PRON be due to inappropriate numerical differencing  
__label__machine-learning __label__regression __label__loss-function there be several  classical  way to quantify the quality of  any   regression model such as the rmse  mse  explain variance  r2  etc   these metric however do not take  cost  into account  for example  for PRON PRON be bad to under  predict a value  real  05  predict  04  than to over  predict PRON  real  05  predict  06    how can PRON model such cost into an evaluation function  PRON just need a first idea to start with and will welcome any suggestion   a loss function and cost function be the same thing   as PRON intuit  classical regression treat loss  cost as symmetric  which be not always what PRON want  in classification task  PRON can make an asymmetric loss matrix   PRON can do a similar thing with regression if PRON solve PRON with gradient descent  but the ordinary least square have symmetric loss bake in   so PRON would consider either  1  use a numeric optimization library like sklearn or tensorflow to explicitly define the regression parameter PRON want to estimate  write PRON own custom loss function  and then do parameter estimation via gradient descent  or  2  find a software package that allow for asymmetric loss  for example see this discussion  
__label__machine-learning __label__beginner __label__c be there any machine learning library for c  specifically interested in unsupervised learning   if PRON want to use svm  libsvm be write in c  here be a detailed table about different machine learning library on different language   httpsgithubcomjosephmisitiawesomemachinelearn  checkout c version here and c version here   personally speak  try opencv  opencv provide multiple machine learn implementation include kmeans  knn  svm  etc  
__label__python PRON usually debug a python script by put the following line into the source code   import ipdb  ipdbsettrace    then when PRON run the script  ipdb start  very often PRON need to plot some array in an interactive graph  PRON use the follow command to make PRON possible inside the ipdb   import matplotlibpyplot as plt  pltion    PRON question be whether PRON be possible to run these two command automatically when ipdb start   p s PRON put this question here  in computational science  as PRON believe that ipdb be mostly use by people who do scientific computing   
__label__neural-networks __label__recurrent-neural-networks let say PRON have a neural network with 5 layer  include input and output layer  each layer have 5 node  assume the layers be fully connect  but the 3rd node in the 2nd layer be connect to the 5th node in the 4th layer  all these number be choose at random for the example   PRON question be when be the 5th node in the 4th layer feed forward  let go through PRON step by step  the first layer be normally feed forward to the second  the second layer be normally feed forward to the third  but the 3rd node be also feed forward to the 5th node of the 4th layer  so the problem here be  be the 5th node in the 4th layer now feed forward or be PRON feed forward when the 3rd layer be do be feed forward  the 1st method would mean that the node would get feed forward 2 time and PRON concern be  if the output be still valid  further more PRON would also come to 2 asynchronous output and how would these be interpret   because in the brain  PRON hear  the neuron be fire when an impulse arrive so this would equal the 1st method   PRON be unclear what kind of network PRON be refer to  there be not a single neural  network model so conceivable both case could exist and serve some purpose  yet if PRON be look for one that emulate nature and real neuron  then PRON be miss at least 2 ingredient  time and the mechanism of rest potential and refractory period   which in turn introduce new computation to the neural network   this be PRON network graph if PRON get PRON right   the calculation in a neural network without refractory period and rest potential without time  would instantaneously modify the weight of PRON node4 layer 5  n4l5  if there be input in column 3   additional input on other column would just add up unless PRON have some other explicit computation on any layer   if PRON want to emulate a neuron  each node would need to have a rest potential  that is  a level above which PRON will fire  in the above example zero   and a refractory period   a time before PRON would fire again  as well as a clock to keep PRON in sync  this would be a crude realtime fascimile   httpcodepeniok3nopenwrqbyv  a common alternative be to use sequential phase or step   reference  source  artificial intelligence  a modern approach  by s russell and p norvig  deal with a general step approach to ai  gateway to memory  by mark a gluck and catherine e myers present a great and readable introduction to model neural network   PRON publish the little neural network model in a medium article  memory and the machine  relevant source be there  
__label__support-vector-machines PRON be currently learn the maximal margin classifier with kernel  and PRON be wonder  why do PRON work  in which case do PRON work best   PRON be especially interested in the rbf and polynomial kernel  but not only   intuitively for the gaussian kernel  the decision function of the max  margin task for the two class  11 be the follow   fwx   mboxsignlangle x  wrangle  b  that be when PRON want to classify a new observation  x PRON calculate the inner product of  x and the parameter vector  w and decide for the class accord to the sign of the inner product  give a training set  xi  yimid i1dot  l with  yiin11  upon which  w be learn  PRON know from optimisation theory that the parameter vector  w can be express  as   w  sumialphai yi xi  for some  alphaigeq 0 the index  i run over the element of the training set  substitute this into the above PRON get the decision function   fx   mboxsignsumialphai yilangle x  xirangle  b  now if PRON use the gaussian kernel  kgaussx  xiexpfracx  xi2sigma2 instead of the linear kernel  klinearlangle x  xirangle  PRON get   fx   mboxsignsumialphai yiexpfracx  xi2sigma2b  therein PRON can see that all training element for which  alphai  gt  0   the support vector  have influence on the decision  since the element for which  alphai0  do not contribute to the sum  this hold for all kernel   what can also be see be that the support vector which be close to  x give high contribution to the sum than the one which be far away because the term  expfracx  xi2sigma2 decrease exponentially with the distance of the point  x and  xi especially for small  sigma2  only the very nearby point to  x have significant influence on the sum which in the extreme case render the max  margin classifier into a kind of near neighbour classifier  therefore for a small enough  sigma2  PRON can classify any given training set  as long as PRON be consistent  correct  but PRON may encounter difficulty later because of over  fitting onto the training set  another thing be that PRON end up with more support vector the more  powerful  PRON kernel be  hence decrease  sigma2  give more support vector  up to PRON knowledge there be no  elegant  way on how to determine the good value for  sigma2 PRON thing this be mostly do by cross validation  
__label__word2vec __label__lstm __label__word-embeddings __label__word currently PRON be work on a sentiment analysis research project use lstm network   as the input PRON convert sentence into set of vector use word2vec   and there be some well  pretrain word vector like google word2vec   PRON problem be  be there be any advantage of use custom train word2vecstrain use a dataset which relate to PRON domain  such as user review of electronic item   over pretrain one   what s the good option  use a pretrain word2vec  train PRON own word2vec use a dataset related to the domain  can any one help PRON on this  thank  
__label__reinforcement-learning __label__game-ai __label__game-theory __label__combinatorial-games __label__negamax PRON invent a chess  like boardgame and PRON build an engine so that PRON can play autonomously  the engine be basically a decision tree  PRON be compose by   a search function that at each node find all possible legal move  an evaluation function that assign a numerical value to the board position  positive mean first player be gain the upper hand  negative mean the second player be win instead   an alphabeta prune negamax algorithm  the main problem about this engine be that the optmization of the evaluation function be really tricky  PRON do not know which factor to consider and which weight to put  the only way PRON see to improve the engine be to iterate game try each time different combination of factor and weight  however PRON computationally seem a very tough feat  can PRON backpropagate without use deeplearn     PRON would like to use reinforcement learning to make the engine improve by play against PRON  PRON have be read about the topic but PRON be still quite confused   what other reward be there in a game a part the win  or  lose output  1 or 0    if PRON use other reward  like the output from the evaluation function at each turn  how can PRON implement PRON  how do PRON modify the evaluation function to give good reward iteration after iteration   PRON be new in this field  so let PRON know if PRON be misunderstand any part of the process   all input be appreciate    PRON would like to use reinforcement learning to make the engine improve by play against PRON  PRON have be read about the topic but PRON be still quite confused   be warn  reinforcement learning be a large complex subject  although PRON may take PRON on a detour from game  play bot  PRON may want to study rl basic  a good place to start be sutton  amp  barto reinforcement learning  an introduction  what other reward be there in a game a part the win  or  lose output  1 or 0    depend on PRON game  that be usually PRON  actually for a win  draw  lose game like chess then the reward from each action be 0 except for win   1  or losing  1  at the end  in a zero  sum game then this align nicely to minimax  alphabeta prune etc   reinforcement learning be intend to address environment with delay reward  add  helper  reward for interim non  goal be usually counter  productive   if PRON use other reward  like the output from the evaluation function at each turn  how can PRON implement PRON   typically PRON do not  what apply self  play rl will do be learn a return  sometimes call utility  function which predict the expectation of PRON total  101 reward by the end of the game  PRON would use this in place of PRON current heuristic for minimax search  or  potentially PRON would adjust PRON current heuristic function to output in the same range and use rl to optimise PRON weight to make the good approximation to the true optimal play return function  which be probably too complex to calculate exactly    how do PRON modify the evaluation function to give good reward iteration after iteration   that be what the different rl approach all attempt to do  there be a variety of different solver  there be no short way to explain PRON  PRON could start with a simple method such as q  learning  q  learning learn estimate of qs  a   call the action value  which be the expect return when in state s and take action a  and thereafter follow an optimal policy  PRON make an arbitrary guess to start with and refine PRON closer to the true value with each step make in the learning environment  simple tabular q  learner do this refinement simply by store a big table of all state and action with the good estimate so far of the true value  and average in each new estimate as PRON be experience   PRON be also possible to combine an rl method for heuristic with look  ahead minimax search  that be what the original alphago do  and what alphago zero do during training  this be a powerful approach because minimax search will work to double  check the rl  generate heuristic  although for simple enough game  rl can learn perfect heuristic and PRON would only need local search  what the next move should be    unless PRON game be very simple  all possible state would fit in memory   PRON will need some kind of function approximator inside the rl algorithm  neural network be a standard choice  have something for that part be unavoidable  although another good choice be to define a bunch of proxy feature  that PRON may use to construct a heuristic by hand  and use a linear approximator  just a weighted sum of all the feature  this can work well enough  and have be use for example in checker  draught  player train use rl   in fact  provide PRON own heuristic function be not too unusual  PRON can probably treat PRON just like a linear approximator and use rl to learn the good weight for PRON  
__label__graphs __label__social-network-analysis PRON compute the high k  core  coreness  each vertex in PRON network belong to   however  PRON turn out that only one vertex have an extremly high coreness  ca  5time as high as the vertex with the second high coreness   PRON think that this should not be possible   PRON use the graphcoreness   function of the igraph package in r  be PRON possible to obtain such result or do something go wrong   
__label__python __label__pandas PRON want to count number of code by month   this be PRON example dataframe   PRON would  month  code  0  sally  0  sa  1  sally  0  sb  2  sally  0  sc  3  sally  0  sd  4  sally  0  se  5  sally  0  sa  6  sally  0  sa  7  sally  0  sb  8  sally  0  sc  9  sally  0  sa  PRON transform to this series use count     dfgroupbyid    code    monthmcount    PRON would  code  month  count  sally  sa  0  12  1  10  2  3  7  15  but  PRON want to include zero occurrence  like this   PRON would  code  month  count  sally  sa  0  12  1  10  2  3  3  0  4  0  5  0  6  0  7  15  8  0  9  0  10  0  11  0  base on the short example dataframe PRON provide  this block of code will include all of the month  PRON be base on use the seriesreindex method and create a new multiindex with the additional value for the month   import panda as pd   load example datum into dataframe  df  pdreadtablecategoricaldatatxt   delimwhitespace  true    transform to a count  count  dfgroupbyid    code    monthmonthcount     re  create a new array of level  now include all 12 month  level   countindexlevels0value  countindexlevels1value  listrange012     newindex  pd  multiindexfromproductlevel  name  countindexname    reindex the count and fill empty value with zero  nan by default   count  countreindexnewindex  fillvalue0   printcount   print the result  PRON get something like this  only show the first entry for sally  sa    PRON would  code  month  sally  sa  0  4  1  0  2  0  3  0  4  0  5  0  6  0  7  0  8  0  9  0  10  0  11  0 
__label__advection-diffusion __label__diffusion PRON want to solve the diffusion equation  ie     dotf   f   0     with a boundary condition  f0   f1   0  and with an initial condition that  f be a boxcar function concentrate over some small region of size  d ll 1 the unit be scale so that the diffusion constant be equal to unity  then PRON want to use the result solution to compute an integral of the form     int dt int dx ffx   fx        where  f be some function  PRON be actually quadratic in  f with some  xdependent coefficient if this make any difference    PRON have try two different way to do this  first be to solve the diffusion equation analytically in fourier space and then write PRON integral also in fourier space and calculate PRON numerically  the second be to solve the diffusion equation numerically  PRON have try both explicit and implicit method and the well  know crank  nicolson method    in all approach  PRON end up with the same problem  the initial condition for  f be very concentrated which mean PRON need a ridiculously small space and time discretization  correspond to take into account very large fourier mode in f  space solution   on the other hand  since PRON need to solve the equation also for large time  to calculate the integral   PRON need to take into account also solution in large time  fourier  space method be not faster because in that PRON actually need to compute three integral numerically  two fourier sum with a kernel that be calculate from the  xintegral   this make PRON calculation very slow   be there any good method to do what PRON just describe  since diffusion equation be very simple  PRON feel pretty stupid for not be able to do this faster  PRON have be think about implement some kind of adaptive grid that would be concentrate to small time and close to the initial condition  but this seem like a complicated thing to do tho solve such a simple problem   PRON should never use explicit method for the diffusion equation  implicit be unconditionally stable and just as easy to implement  also if PRON use an implicit method  like backward euler or crank  nicolson  PRON will not matter how small d be  in fact PRON could use a dirac delta function if PRON want   as far as speed go  do a implicit method with a tridiagonal solver  on  complexity  this should be very fast especially for 1d PRON can send an easy matlab script if necessary  
__label__python __label__c __label__libraries __label__molecular-mechanics PRON would like some classical molecular mechanic forcefield library or super  simple program which do not have any external dependence  so that PRON can plug PRON into PRON software without worry of  dependency hell   also PRON would like to modify PRON  so PRON would prefer something with simple and compact source code  PRON should do just one thing  in summary PRON should follow unix philosophy  for example there be rdkit but PRON seem to be too ambitius  lot of functionality PRON do not need  lot of source code and dependency to care about   also many molecular editor like avogadro and openbable have some molecular mechanic implement  but PRON would be much more convenient to have clean simple library than try to extract this functionality out of big software package   PRON do not have much requirement on precision of force  field  but PRON would prefer something which be as general as possible  eg uff  for all element and exotic bonding topology although organic chemistry be main target  not something specialize to eg protein like gromos  amber  charm etc   
__label__machine-learning __label__classification __label__clustering kmode be for categorical data  but sometimes PRON read PRON somewhere that PRON be applicable on mixed data ie  numeric and categorical   so what s right   as far as PRON remember kmodes be use for categorical datum  even in the documentation PRON could not find anything relate to mixed data type  if PRON have some reference do share   PRON have use kproto for mixed data type as PRON have mention and the package be clustmixtype  PRON combination of kmeans and kmodes   do have a look and let PRON know if have any question  
__label__random PRON be look for an algorithm to generate discrete  time pseudorandom sequence base on other sequence PRON already have that would be use to train the algorithm   more specifically  PRON have some time  base discrete  time measurement of some real  live event  for example  sunspot number  stock market  sea height at a beach    PRON have long sequence of those measurement and PRON would like to generate other sequence have the same long  term and short  term property that the sequence PRON already have   for example  in the case of the sunspot number  PRON show some random short  term noise and a clear long  term periodicity  PRON would like to produce more like those sequence   PRON have be look at markov chain  but PRON would be valid just for the short  term  and just for a discrete set of value   PRON do not look like a new problem so PRON suppose PRON have be solve elsewhere and have a name  but PRON can not find PRON   PRON can model PRON time series with gaussian process  once PRON have decide on the correct covaraince function and estimate PRON hyperparameter PRON can draw new series from this process  but PRON agree with kirill  this be only one of many possible way to do this  
__label__regression __label__logistic-regression __label__octave so give that the sigmoid function be define as hθx   gθtx   how can PRON implement this funcion in octave give that g  zerossizez     this will compute the sigmoid of a scalar  vector or matrix   function g  sigmoidz     sigmoid compute sigmoid function    g  sigmoidz  compute the sigmoid of z   compute the sigmoid of each value of z  z can be a matrix    vector or scalar    sigmoid  z  11  expz     g  sigmoidz    end 
__label__human-inspired while think about ai  this question come into PRON mind  could curiosity help in develop a true ai  accord to this website  for test creativity    curiosity refer to persistent desire to learn and discover new thing  and idea  always look for new and original way of thinking   like to learn   search for alternative solution even when traditional solution be present and available   enjoy read book and watch documentary   want to know how thing work inside out  let PRON take clarifai  a image  video classification startup which can classify image and video with the good accuracy  accord to PRON   if PRON understand correctly  PRON train PRON deep learning system use million of image with supervised learning  in the same algorithm  what would happen if PRON somehow add a  curiosity factor  when the ai have difficulty in classify a image or PRON object  PRON would ask a human for help  just like a curious child   curiosity make a human being learn new thing and also help to generate new original idea  could the addition of curiosity change clarifai into a true ai    when the ai have difficulty in classify a image or PRON object PRON should ask a human for help just like a curious child  PRON be call active learning  PRON be already use quite often    do this addition of curosity change clarifai into a true ai   as per PRON answer to this question  PRON do not know what the ingredient for a  true ai  be  via the turing test and PRON variant  the good PRON can do be  know one when PRON see one    curiosity certainly appear necessary for intelligence  though PRON do not seem sufficient  a lemming  like creature curious to see what be at the bottom of a steep cliff may not survive long enough to learn caution  even if PRON have the learning mechanism to do so   here be some work by schmidhuber on artificial curiousity   pierre  yves oudeyer have also do quite a lot of work on this and active learning  intrinsic motivation   PRON be a well know concept that be already use  what PRON call  curiosity  in human and animal be in effect the choose level of the  exploit vs explore  tradeoff for any active system  for example  the field of reinforcement learning be one approach that study implementation of what essentially be the equivalent of curiosity  and PRON have research on how much curiosity be good eg multi  armed bandit concept   so  use curiosity  be something that PRON already do as much as PRON can  should  be able to  but PRON would usually be call in some other  more specific term to specify the exact meaning instead of the vague word of  curiosity   
__label__python __label__data-mining __label__sequential-pattern-mining PRON want to do sequence learn for that PRON want to find frequent sequential rule  this rule consider the order of occurrence  probably PRON could try conditional random fields  httpsgithubcomscrapinghubpythoncrfsuite  or a long short term memory rnn  kerastensorflow backend  
__label__finite-element __label__stiffness __label__explicit-methods PRON be work on a simple explicit  integration lump  mass elastic fem code which implement cstdkt triangle  plateshell  and constant  strain tetrahedra  httpwoodemeudoctheorymembraneelementhtml  httpwoodemeudoctheorytet4elementhtml   the code focus on contact dynamic  so fem be there only to model flexible boundary  PRON would like to add some kind of viscous damping to the model  and PRON be look for some resource which be not overly complicated   PRON independently think PRON could use the elastic stiffness matrix  mathbfk  as in  fmathbfku   scale by some viscosity factor  eta  that would be compute from material s  e and  eta   to compute viscous resist force as  fvetamathbfkdot u PRON this formulation something know in literature  or be PRON plain wrong   thank for pointer   
__label__linear-algebra __label__linear-solver __label__computer-vision __label__least-squares in debevec and malik  mention similarly in forsyth and ponce s computer vision  a modern approach  PRON highlight a method of solve the camera response function use linear least  square   PRON collect image intensity datum for a number of point with     ipk   fepdelta tk       where  ipk be the image intensity for pixel  p for the  kth exposure time   tk be the  kth exposure time   ep be the intensity of the surface project onto the image pixel   f be the camera response function   ipk and  tks be know and PRON be use least  square to solve for the unknown  ep s and  f this problem be turn linear by take  g  ln f1 and take the log PRON can formulate the above as     gipk    lnep    lndelta tk      then minimize this as the follow objective function by choice of  g     sumgipk    lnep    lndelta tk2  sumz  gz2  where  z be the discrete domain of all pixel intensity  and  gz   gz  1   2gz   gz  1 create a smoothness penalty on  g  from debevec and malik  because PRON be quadratic in the  ei ’s and  gi ’s  minimize  the above  be  a straightforward linear least square problem  PRON do not understand how PRON have reach the follow conclusion  as mention by the paper  PRON should be able to solve the following by create the correct matrix and perform singular  value decomposition  what be the construction of this matrix   be PRON say that  g  ln f1 be a linear function   this be a penalized least squares problem  with penalty on the second derivative of  g  so PRON be not say that the function be linear    in practice PRON have one unknown parameter for each pixel intensity  ipk  that represent the value of the function at that point  gipk  PRON just have to express PRON penalty in a matrix form  git m gi  where  m be a tridiagonal matrix with 2s on the main diagonal  and 1 above and below the main diagonal  look right  but not completely sure about PRON   then either just give PRON to an optimizer to estimate the value  gi and  e at the same time or PRON estimate PRON one at the time  for example for fix  e  the close form solution for  gi should be  hatgiiti  m1itlndelta t  lne then for  e PRON can just use an optimizer  with fix  hatgi  and alternate until convergence  
__label__python __label__scikit-learn __label__dimensionality-reduction PRON have a really large datum set with mixed variable  PRON have convert categorical variable to numerical use onehotencoding and PRON have result in more than a couple of thousand different feature  combine that be   be PRON possible to apply dimensionality reduction algorithm on onehotencoded datum which look like   1  0  1  0  0  0  0  0    or should PRON be do by merge with the original datum set   follow PRON example  PRON have different point in a 4dimensional space  so  yes  PRON can use any dimensionality reduction technique  from pca to umap   in general  if PRON data be in a numeric format  and one  hot actually be   all the element have the same dimensionality  and PRON do not have undefined value  nan  inf   PRON can always use dimensionality reduction  
__label__pde __label__finite-difference __label__numerical-analysis __label__nonlinear-equations __label__stability PRON be read a paper  1  where PRON solve the follow non  linear equation  beginequation   ut  ux  uux  uxxt   0  endequation   use finite difference method  PRON also analyse the stability of the scheme use the von neumann s stability analysis  however  as the author realize  this be only applicable to linear pde s  so the author work around this by  freeze  the non  linear term  ie PRON replace the  uux term with  uux  where  u be  consider to represent locally constant value of  u   so PRON question be two  fold   1  how to interpret this method and why do PRON  not  work   2  could PRON also replace the  uux term with the  uux term  where  ux be  consider to represent locally constant value of  ux    reference  eilbeck  j c  and g r mcguire   numerical study of the regularize long  wave equation PRON  numerical method   journal of computational physics 191  1975   43  57   what PRON be say be refer to as linearization  PRON be a common technique use in the analysis of non  linear pde s  what be do be to cast equation in the format    utau0   here a be a matrix result from the linearization of the equation   now to PRON question   as PRON be think  PRON work to some extent  but do not to some other extent  the utility be that stability can be prove for linear system but not readily for non  linear system  so the linear result be extend to the non  linear system  often  different method be adopt for particular case  for example    uux  frac12u2x  which be the conservation form  so    utfrac12u2x  0   when represent in a finite volume sense give limit on the evolution of u  what be the utility of do the replacement  PRON will remove the equation from a wave equation form  which would mean that the solution would not behave as a wave equation  so in the stability analysis  the test solution would have to be completely different and un  physical as well   to elaborate on the linearization argument  in uux PRON want to assume u be locally constant  not ux  for two reason  a  u vary more slowly than PRON derivative  and b  in this particular case  if PRON assume ux be locally constant  by definition PRON also assume u be locally linear  which mean high space derivative be zero  and this not only introduce additional approximation error  but PRON may imply PRON may be throw out the baby with the bathwater  depend on PRON equation  
__label__finite-element __label__data-visualization __label__visualization on PRON triangular mesh  PRON have the   x  y  z coordinate of each vertex of each triangle  for high order element  PRON refine each element a few time so PRON have more point to work with  if PRON just need to visualize the solution for PRON  PRON use fill3 but PRON be not entirely sure how to insert PRON own colorbar  ideally  PRON want the small z value to correspond to blue  and the large to correspond to red  like the typical colorbar in the surf plot  here be an example of what PRON be try to achieve   fill3 work fine  PRON be just not really sure how to set the custom colormap to PRON need  and the documentation have not help much   PRON be open to other suggestion as well  PRON be just that matlab be what PRON be familiar with for visualization   also  just curious if anyone know what software this plot be make in as PRON have see plot like PRON in a number of paper   PRON be simple to use the function pdemesh or pdesurf since there be do exactly for that purpose  PRON can then change the colorscale with  colormapjet    this change the color map to what PRON need  there be other preset colormap and PRON can also define PRON  see the colormap function help   if PRON have not the pde package in matlab  PRON can maybe use octave which have the same functionality   PRON could output PRON datum in one of the widely use file format  for example vtk  and then use either visit or paraview to visualize  these program be use a lot for visualize pde solution and be make for this purpose    for high order element  PRON refine each element a few time so PRON have more point to work with  if PRON just need to visualize the solution for PRON   let PRON use quadratic lagrange element as example  PRON need the mesh datum  point p and triangle t  also the numerical solution  uh for visualization purpose  PRON merely need the nodal value from the numerical solution use quadratic lagrange element  quadratic lagrange have edge dofs and node dofs   same for the high order element  PRON only need the nodal dof s value to visualize the solution   for example  PRON solution column vector be u which be go to be the  z value of the surf plot  for quadratic element  the most natural datum structure be to assign the first nnodes  sizep1  row the value of nodal dofs  then nedges  sizee1  row the value of edge dofs  because PRON use triangular mesh  PRON can use matlab s trisurf to do this   h  trisurft  p1   p2   u1sizep1       the colormap s range can be manually assign  see matlab s document here  there be some build  in scheme like jet  PRON can use an  mtime 3  rgb matrix as well  PRON can retrieve the color matrix by type c  colormap  then check what c be like    here be another very helpful matlab doc on how to manually set the colormap for a patch object  surf and trisurf be both use patch to draw thing   color mesh and surface plot  here be what the numerical solution use quadratic element be like for  delta u  1  with homogeneous dirichlet boundary datum on an l  shape domain  there be no discontinuous gap between the node like PRON plot yield  unless PRON use  hpdiscontinuous galerkin  PRON plot be wrong for usually continuous finite element solution for any elliptic equation    
__label__molecular-dynamics when PRON simulate a bulk molecular system use molecular dynamic in  say  nve ensemble  microcanonical   PRON apply some periodic boundary condition  pbc  to avoid surface effect  what effect do this have on quantity like pair distribution function  since PRON so happen that part of a molecule be near one edge of the box and the other be in a different edge of the box  what effect do this have on pdf  msd and other such thermodynamic quantity   ps  PRON be currently simulate spc  e water molecule use cubic box  in some frame PRON find water molecule which be split apart  but connect by pbc   
__label__machine-learning __label__classification __label__sentiment-analysis PRON be work with datumbox ml framework for sentiment analysis  here the git link  PRON find a sentiment ml code in code example of frame work here be the code example link  PRON want to use same code for twitter  sentiment but in the code example  only two training dataset be use pos  neg but for twitter sentiment analysis  PRON want to include neutral sentiment also  PRON do have PRON own training dataset of pos  amp  neg tweet but PRON do not any neutral dataset   PRON doubt be   1  do PRON need to feed neutral training dataset for give the neutral sentiment   or  2  do PRON need to predict neutral sentiment base on positive  amp  negative training setsif PRON need go for predict then PRON need an idea on how to do that    the machine can not return neutral sentiment if PRON do not feed PRON  if PRON just have positive or negative  the machine will not return neutral   alternatively  maybe PRON can try other approach  PRON can check the confidence with which PRON predict positive or negative and if PRON be low  maybe PRON can assign PRON to neutral  
__label__visualization __label__tools __label__tableau PRON often get the problem when this or that alia name be already use somewhere  and PRON can not easily find that variable or aggregation to release the name   be there some place in tableau where PRON can view  edit  reset full list of alias   finally solve the issue   PRON should go to   data –gt  yourdatasource –gt  edit aliases –gt  measure names  and see full mapping between variable  aggregation and alias   another way of edit alia for a member have multiple alias   right click the  measure names  in dimension panel   select  alias    a small edit box appear with all the member have alias   edit  update alia for each member  as prefer   press ok to save any change make  
__label__python __label__visualization __label__scikit-learn __label__svm __label__parameter PRON be follow introduction to machine learning with python  a guide for data scientists by andreas c müller and sarah guido  and in chapter 2 a demonstration of apply linearsvc   be give   the result of classify three blob be show in this screenshot   the three blob be obviously correctly classify  as depict by the colored output   PRON question be how be PRON suppose to know how to interpret the model fit output in order to draw the three line   the output parameter be give by  printlinearsvcfitx  ycoef      017492286  023139933    047621448 006937432    018914355 020399596    printlinearsvcfitx  yintercept     107745571  013140557 008604799   and the author walk PRON through how to draw the line   from sklearnsvm import linearsvc  linearsvm  linearsvcfitx  y     line  nplinspace15  15   for coef  intercept in ziplinearsvmcoef   linearsvmintercept     pltplotline  line  coef0   intercept   coef1     how do we know  pltylim10  15   pltxlim10  8  pltshow    the line of code with the comment be the one that convert PRON coefficient into a slope  intercept pair for the line   y  coef0  coef1  x  intercept  coef1  where the term in front of x be the slope and intercept  coef1 be the intercept   in the documentation on linearsvc  the coef  and intercept  be just call  attribute  but do not point to any indicator that coef0 be the slope and coef1 be the negative of some overall scaling   how can PRON look up the interpretation of the output coefficient of this model and other similar to PRON in scikit  learn without rely on example in book and stackoverflow   here be one  admittedly hard  way   if PRON really want to understand the low  level detail  PRON can always work through the source code  for example  PRON can see that the linearsvc fit method call  fitliblinear  that call trainwrap in liblinear  which get everything ready to call into the c function train   so train in linearcpp be where the heavy lifting begin  note that the w member of the model struct in the train function get map back to coef  in python   once PRON understand exactly what the underlie train function do  PRON should be clear exactly what coef  mean and why PRON draw the line that way   while this can be a little laborious  once PRON get use to do thing this way  PRON will really understand how everything work from top to bottom  
__label__neural-networks __label__machine-learning __label__deep-learning __label__convolutional-neural-networks __label__implementation as far as PRON can tell  neural network have a fix number of neuron in the input layer   if neural network be use in a context like for example nlp  sentence or block of text of vary size be feed to a network  how be the vary input size reconcile with the fix size of the input layer of the network  in other word  how be such a network make flexible enough to deal with an input that may be anywhere from one word to multiple page of text   if PRON assumption of a fix number of input neuron be wrong and new input neuron be add to  remove from the network to match the input size PRON do not see how these can ever be train   PRON give the example of nlp  but lot of problem have an inherently unpredictable input size  PRON be interested in the general approach for deal with this   edit  for image  PRON be clear PRON can up  downsample to a fix size  but for text this seem to be an impossible approach since add  remove text change the meaning of the original input   three possibility come to mind   the easy be zero padding  basically PRON take a rather big input size and just add zero if PRON concrete input be too small  of course this be pretty limited and certainly not useful if PRON input range from a few word to full text   rnn be a very natural nn to choose if PRON have text of vary size as input  PRON input word as word vector just one after another and the internal state of the rnn be suppose to encode the meaning of the full string of word  this be one of the early paper   another possibility be use recursive nn  this be basically a form of preprocess in which a text be recursively reduce to a small number of word vector until only one be leave  PRON input  which be suppose to encode the whole text  this make a lot of sense from a linguistic point of view if PRON input consist of sentence  which can vary a lot in size   because sentence be structure recursively  for example the word vector for  the man   should be similar to the word vector for  the man who mistake PRON wife for a hat   because noun phrase act like noun etc    often PRON can use linguistic information to guide PRON recursion on the sentence  if PRON want to go way beyond the wiki article  this be probably a good start   other already mention   zero padding  rnn  recursive nn  so PRON will add another possibility  use convolution different number of time depend on the size of input  here be an excellent book which back up this approach   consider a collection of image  where each image have a different  width and height  PRON be unclear how to model such input with a weight  matrix of fix size  convolution be straightforward to apply  the  kernel be simply apply a different number of time depend on the  size of the input  and the output of the convolution operation scale  accordingly   take from page 360  PRON can read PRON further to see some other approach  
__label__python __label__gpu __label__precision have anyone here use double precision scientific computing with new generation  eg k20  gpus through python   PRON know that this technology be rapidly evolve  but what be the good way to do this currently   gpu be out of scope for the popular scientific python library numpy and scipy  and PRON have want to use theano but PRON seem to use only float32 precision for gpu   PRON be aware that google can provide search result for python gpu  but PRON be hop for more insight than a list of project which may or may not be on PRON way to meet PRON maker   PRON do not know why PRON put this answer in a comment   if PRON need this PRON would probably use pyopencl  general purpose gpu coding be still quite low level  try opencl c interface  PRON be tough go   yet pyopencl seem to abstract as much as possible and appear to have considerable momentum behind PRON  
__label__time-series __label__forecast PRON have time series  r  which show  how something change at the regional level   PRON have several time series  ui  which show  how something change at a special unit  i level   there be many unit in the region   r have no miss datum  different  ui have PRON own missing period   PRON want to forecast  u after a missing period use information of  r and information of  u  when PRON be availible   PRON thoght till now   suppose  r be know on interval   0  365 suppose  ui be know on interval   0300 let PRON take  r and  ui both on interval   0300  take difference between PRON and try to predict that difference with linear regression  so for interval   301365  PRON will have difference and to restore  ui  PRON will just have to take out PRON difference from  r  PRON do not like PRON solution  because   PRON need a model for each  ui  because  sometimes data be more sparse and PRON do not even have a   0300 know interval  so not able to train regression properly   just a thought   why would not PRON do the training on the difference datum for 1  250 and test use 251  300 to see if the underlying pattern actually be accurate  if that be the case  PRON can generalize from 1  300 to 301  365   not a fully complete answer  but some input   PRON time series be correlate   PRON assume that the measure PRON want to forecast for a region be an aggregation of unit forecast   to address the first point  PRON usually use vector autoregressive model  var  that forecast all time  series at once  each one be express as a regression use the other   the second point involve the concept of hierarchical forecasting and reconciliation  PRON can exploit the fact that the regional forecast should  must equal the unit forecast  there can be a process to adjust forecast to take that into account   there be both package for var and hierarchical reconciliation in r but as far as PRON know no direct code to handle both at the same time   PRON may find this paper provide some detail on the propose approach   httpsmpraubunimuenchende765561mprapaper76556pdf  look like casual impact library could help PRON with that  httpgooglegithubiocausalimpactcausalimpacthtml  or a at least principle from PRON 
__label__mesh-generation __label__mesh __label__geometry say PRON have a triangular mesh on a flat plane   this have be draw to eventually solve some problem in mechanic  for example   a mesh of equilateral triangle be the good inasmuch as the distance between the vertex and between the centroid be the same all over  this make interpolation and the calculation of gradient an easy and accurate task  however  because of constraint and circumstance  PRON be not always possible to work on a mesh of all equilateral triangle   so  the question regard a mesh of triangular element of arbitrary shape   concern individual mesh element  which metric be commonly use to quantify the dissimilarity of one generic triangle from some underlying ideal equilateral shape   concern the whole mesh  which metric be in use to quantity the irregularity of a mesh of arbitrary triangle on the whole  these metric should indicate how scramble the mesh be   thank for think along   note  all contribution from the finite  element community have be greatly appreciate   for this question  please note that the interest be to quantify difference purely in the geometry  arbitrary vs equilateral triangle    the subsequent effect on the interpolation and conditioning error be outside scope  grant these can be insightful and relevant  PRON complicate the mathematical handling   as nicoguaro and paul have say in the comment to the question post  there be a great many way to do this kind of thing  and PRON be not sure if there be a single  good  approach   from a review study of jonathan richard shewchuck at berkley  an answer be   please refer to the original document  version 31122002  for symbology  terminology  special feature and possibly more  eg tetrahedra    chapter 6 be about quality measure   the document link to be the extended version  and in jrs s webpage there be also an abridge one   personally  PRON be a fan of the  volume  length  metric  PRON be a good robust scalar indicator of  isotropic  simplex quality and be cheap to compute  in two  dimension    a  frac4sqrt33fracamathbfemathrmrms2  where  a be the sign area of the triangle and  mathbfemathrmrms be the root  mean  square edge length  ideal element achieve  a1   which decrease toward zero with increase distortion  inverted element with reversed orientation have  a  lt  0  to ass the quality of an unstructured triangulation PRON be typical to look at histogram of such element quality metric  there be many implementation of such thing out there  but one straight  forward matlab code  base of mine be here   in addition to volume  length score  histogram of element angle and vertex degree be also compute by default   PRON do not think that there exist an answer to this question in general  because PRON all depend on the intend use for the mesh  for instance  if PRON be do computational fluid dynamic  PRON may want to have a mesh that be extremely anisotropic near the boundary layer  now if PRON be do computational electromagnetic  the good mesh will be probably completely different   there be in the literature many different definition for a  mesh quality  criterion  most of PRON will favor mesh with triangle that be as equilateral as possible  one can also mention the idea of maximize the small angle  which be realize by delaunay triangulation for a fix set of point   PRON be justify by jonathan shewchuk s analysis mention in one of the comment  that relate this angle with the condition number of the stiffness matrix for the laplace equation discretiz with p1 element  but again  depend on the intend use  somebody s good mesh can be somebody else s poor mesh   PRON do not think that PRON make sense to  quantify difference purely in the geometry  arbitrary vs equilateral triangle    before measure whether the triangle be equilateral and decide which  deviation wrt  equilaterality  be the good one  PRON be necessary to figure out whether  equilateral triangle  be what PRON want  and PRON be not always the case  PRON all come from the  interpolation and conditioning  that PRON mention  yes  as PRON say  PRON complicate the mathematical handling  but without PRON  PRON be not possible to make the difference between objective criterion for a give application and criterion that do not make sense at all  
__label__optimization __label__algorithms __label__c++ __label__matrix PRON want to generate lu decomposition of large size dense matrix   ngt10  7    the lu decomposition PRON be currently use be base on adaptive cross approximation and be take very long time to execute for large  n can anybody suggest a few lu decomposition technique that can be very well parallelis  use openmp  and take a short period of time   note   PRON write the code in c and make use of xeon processor128 thread   and eigen library   the entry in the matrix be fill through a kernel function of  form  ex1x22  storage of matrix be not a problem  PRON be work on xeon processor and have enough memory and moreover  PRON be not store the full matrix and whenever  PRON need to find an entry in matrix  PRON use the kernel function and generate type  double number for that cell   PRON can not  unless PRON have some special knowledge  PRON  l and  u factor be go to be dense  and will have  n2   cal o1014 entry  more than PRON can store on any reasonable machine  about a million gb   furthermore  PRON take   cal on3cal o1021 operation to compute these factor  ie   3cdot 10  4  cpu year  long than PRON want to wait   the problem PRON have can simply not be solve via lu decomposition  PRON need to come up with other strategy for do what PRON want to do  eg  solve linear system with krylov space method  compute explicit green s function  or use knowledge about sparsity   PRON be not exactly sure what type of framework for lu be PRON use  as one can apply aca to various different setup  and to make PRON explicit  the approach PRON be work on right now  and PRON be go to propose do not give PRON an lu  decomposition  PRON be an approximation of an lu  decomposition in some form   since PRON be use a non  singular kernel  PRON guess PRON can try to compress the whole matrix  usually  those method be apply to singular kernel and then PRON have a hierarchical pattern  when PRON subdivide PRON matrix into block in a multilevel fashion and most of PRON be compressible  those block would be compute use some fast technique  like aca  PRON be worth mention  that aca be less than ideal  have trouble with the control accuracy and etc  then  PRON can factorize the matrix  the approach PRON describe be pretty much hierarchical matrix   mathcal hmatrix  approach  develop by dr w hackbusch httpwwwhmatrixorg  to cut along story short  that approach allow for  mathcal oktextmax3nlog2n lu decomposition and  mathcal oktextmax3nlog n for back substitution and pretty well parallelizable via openmp  notice  here  ktextmax be the maximum rank  that for aca would mean the number of skeleton  of course  that assume that PRON have a nice balanced partitioning  detail and limitation be available in numerous paper on  mathcal hmatrix paper   take a look at this approach  and PRON available  mathcal hlib library  that be available to the public  other suggestion may include several other framework  like hss  that PRON know about  but much less familiar   and more from computational electromagnetic   j shaeffer   million plus unknown mom lu factorization on a pc   2015 international conference on electromagnetics in advanced applications  iceaa   turin  2015  pp  62  65  doi  101109iceaa20157297075  s kapur and d e long   n  body problem  ies3  efficient electrostatic and electromagnetic simulation   in ieee computational science and engineering  vol  5  no  4  pp  60  67  oct  dec  1998  doi  101109mcse19987102081 
__label__machine-learning __label__dimensionality-reduction __label__data-augmentation let PRON consider a dataset of label image ready for a classification task  PRON would like to augment PRON data set  by apply shifting and rotation for example  the problem be that the dataset alone fill up PRON 20go ram  let PRON say PRON can sacrifice 5  of variance in order to reduce dimensionality and save 20  of space  so PRON would like to augment PRON dataset by 20  and use all the space available   PRON question be do PRON make sense to perform rotation  flip  shift on PRON reduced datum set  if so  how   
__label__python __label__random-forest __label__pandas PRON get valueerror when predict test datum use a randomforest model   clf  randomforestclassifiernestimators10  maxdepth6  njobs1  verbose2   clffitxfit  yfit   dftestfillnadftestmean     xt  dftestvalue  ypr  clfpredictxt   error   valueerror  input contain nan  infinity or a value too large for dtypefloat32     how do PRON find those bad value in the testing dataset  also  PRON do not want to drop those record  can PRON just replace the bad value with certain mean or median  thank   assume xt be a pandas dataframe  PRON can use dataframefillna to replace the nan value with the mean   xtestfillnaxtestmean     with npisnanx  PRON get a boolean mask back with true for position contain nans   with npwherenpisnanx   PRON get back a tuple with i  j coordinate of nans   finally  with npnantonumx  PRON  replace nan with zero and inf with finite number    alternatively  PRON can use   sklearnpreprocessingimputer for mean  median imputation of miss value  or  panda  pd  dataframexfillna    if PRON need something other than fill PRON with zero   for anybody happen across this  to actually modify the original   xtestfillnaxtestmean    inplace  true   to overwrite the original   xtest  xtestfillnaxtestmean     to check if PRON be in a copy vs a view   xtestisview  PRON face similar problem and see that numpy handle nan and inf differently   incase if PRON datum have inf  try this   npwherexvalue  gt npfinfonpfloat64max   where x be PRON panda dataframe  this will be give a tuple of location of place where na value be present   incase if PRON datum have nan  try this   npisnanxvaluesany    
__label__r __label__data-wrangling PRON be currently work on a dataset which contain a name attribute  which stand for a person s first name  after read the csv file with readcsv  the variable be a factor by default  stringsasfactor  true  with 10k level  since name do not reflect any group membership  PRON be uncertain to leave PRON as factor   be PRON necessary to convert name to character  be there some advantage in do  or not do  this  do PRON even matter   factor be store as number and a table of level  if PRON have categorical datum  store PRON as a factor may save lot of memory   for example  if PRON have a vector of length 1000 store as character and the string be all 100 character long  PRON will take about 100000 byte  if PRON store PRON as a factor  PRON will take about 8000 byte plus the sum of the length of the different factor   comparison with factor should be quicker too because equality be test by compare the number  not the character value   the advantage of keep PRON as character come when PRON want to add new item  since PRON be now change the level   store PRON as whatev make the most sense for what the datum represent  if name be not categorical  and PRON sound like PRON be not  then use character  
__label__boundary-conditions for a second order pde  for example heat conduction equation   fracpartial tpartial t   fracalphacp  nabla2 t  be PRON possible to determine the steady  state  or even transient  solution with two dirichlet condition  PRON have two different question regard this  from PRON understanding  the solution be non unique for all equal value ratio of  alpha and  cp so two dirichlet condition say nothing about how fast the disturbance propagate with a temporal change of one boundary condition  so only the knowledge of  t and  nabla t together can fix the solution curve for specific value of  alpha and  cp instead of the ratio   integrate the 1d second order  steady  state  equation give   t  c1xc2  where  c1  fracpartial tpartial x so  two dirichlet condition be two value of  c2  and therefore still do not give PRON the value of  c1  which be require to fix the solution curve  so in this case  how be PRON possible to assume PRON know the solution with two just dirichlet condition   before PRON begin answer PRON question  PRON just need to clarify one key point   on the notion of  uniqueness   the way PRON use the word  unique  in PRON question be not correct   uniqueness  have a very precise meaning in a mathematical context and be very different from the way PRON be use PRON   when  alpha and  cp be of equal ratio  PRON produce the same solution curve   however  this do not imply that the solution of the pde be not unique   uniqueness refer to whether two completely different function solve the same pde with the same datum  ie datum  coefficient and initial  boundary condition    PRON be more appropriate to say that the problem with the same ratio  fracalphacp be equivalent problem and must have the same  solution  if one exist   please keep this in mind as PRON answer PRON question more directly below   both the steady state and transient pde s be well pose  under sufficient assumption on the coefficient  amp  initial  boundary condition  for both pure dirichlet and mixed boundary condition   thus  a solution exist  be unique  and depend continuously on the datum   therefore  PRON can uniquely determine the steady state solution use only dirichlet boundary condition   the transient solution can also be uniquely determine from a combination of the dirichlet boundary condition and the initial condition   PRON can determine the coefficient of the analytical solution  1d case  by set up a system of equation   suppose the boundary condition of the pde at  x1  and  x2  be give as  gx1g1  and  gx2g2   respectively   then  the coefficient  c1  and  c2  be uniquely determine by the system of equation  beginalignc1x1  c2  g1 c1x2c2g2endalign  
__label__feature-selection sklearn have a featureimportance  attribute  but this be highly model  specific and PRON be not sure how to interpret PRON as remove the most important feature do not necessarily decrease the model quality most   be there a model  agnostic way to tell which feature be important for a prediction problem   the only way PRON could see be the following   use an ensemble of different model  either start with a big set of feature and remove one at a time  to find the feature  uplift   compare the ensemble quality with the full feature set against the ensemble quality against the remove feature set    what this can not do be to find connected feature  some feature may be not exactly the same  but have a common underlying because which be important for the prediction  hence remove either of PRON do not change much  but remove both may change a lot  PRON ask another question for that    way to  determine feature importance  be normally call feature selection algorithm   there be 3 type of feature selection algorithm   filter approach  PRON choose variable without use a model at all  just look at the feature value  one example be scikit  learn s variance threshold selector   wrapper approach  PRON use whatev prediction algorithm to score different subset of feature and choose the good subset base on that  these use a model but be model agnostic  as PRON do not care about which model PRON use  one example be recursive feature elimination   embed approach  in these approach  the variable selection be part of a model  hence the feature selection and the model be couple together  this be the case of the featureimportance  in random forest algorithm   from the question  PRON understand that both filter and wrapper approach be suitable for the op need  a classic article that cover both very well be this one by kovavi and john   here PRON can see an overview of scikit  learn feature selection capability  which include example of the three aforementioned type of variable selection algorithm  
__label__nlp __label__dataset __label__training context  PRON be use natural language processing engine such as ibm s watson conversation and microsoft s luis  which take a sentence and classify PRON intent  for example   i want to buy food     buyfood    problem  PRON be try to come up with the good set of utterance to train those engine to understand natural language for a give application  PRON goal be to improve the accuracy of the nlp engine predict the correct intent   suppose PRON have a few thousand utterance  each with a know intent  PRON need to come up with a set of just a few hundred good utterance that will be use to create a model in the nlp engine   what PRON try  at first PRON try to take a few hundred sentence randomly  create an nlp engine model  and test PRON accuracy use a few hundred other sentence from PRON set  then PRON would replace one sentence in the nlp engine model  and re  run the test again  at first  microsoft s luis would guess the intent correctly around 90  95  of the time  and after a few dozen iteration the accuracy go up to 91  96   very slightly   PRON be also think about use part  of  speech tag to not just replace one sentence randomly  but to strive for syntactical diversity of those sentence   reason behind do this  the reason why PRON be do all of this be that PRON seem that dump more utterance example into the nlp engine do not seem to make PRON good  in fact  PRON seem to make PRON bad   this be why PRON be try to think of a way to create a limited set of utterance that will be use to train the nlp model and improve the rate of how often the engine guess the correct intent   
__label__text-mining __label__word-embeddings hello PRON be build a python program for text spin  PRON have read about glove  word2vec  doc2vec or text2vec  PRON understand that what PRON do be to represent each word as a semantic vector   so PRON guess that if PRON train a glove PRON could use PRON just for finding synonym  or something more   edit  PRON have come up with one strategy   first PRON train a doc2vec model  the goal would be to build semantic vector of phrase  then PRON use PRON original paragraph input  word by word PRON change PRON randomly with a synonym  provide by nltk or other synonym db   before finally spin one word PRON compute the doc2vec for the change paragraph and compute the distance with the original paragraph vectorized  PRON only change the synonym if the distance be small   how do that sound to PRON   
__label__finite-element __label__boundary-conditions PRON have the follow problem in finite element method    alpha u     beta u   gamma u  f  with  omega   0  1   u0   0  and  u1   3   to be able to write the weak formulation of the problem do PRON need a lifting function   ie  do PRON need to define something like    tilde u  u  r  PRON be stuck after integrate by part  PRON get something like      big   alpha u  v   0  1  int0  1 alpha u  v  big   beta int0  1 u  v  gamma int0  1 u v  int0  1 f v      forall v in v  hgammad1  01  to get rid of the first term    alpha u  v   0  1   and have the bilinear form  au  v  when   v0   0 the low limit dissapear  but for the upper limit PRON get    alpha u   1  v1 and PRON do not disappear  how to proceed   for the second term  alpha u1v1  PRON can insert the neumann boundary condition  u1   3  to obtain  3alpha v1 since this term can be evaluate  for a give test function  v   PRON become part of the right hand side  the weak formulation be to find  uin h1gammad01 such that     int0  1 uxvx  dx  beta int0  1 uxvx  dx  int0  1 uxvx  dx  int0  1 fx  vx  dx  3alpha v1      for all  vin h1gammad01  note that in two and more dimension  the term arise from integration by part be     alphaxnabla uxcdot nux  vx   qquad xingamman      where  alpha can be a matrix and  nux be the unit outward normal at  x to handle that term  PRON need to specify     alphanabla ucdot nu  g     on  gamman  and not just the normal derivative  nabla ucdot nu 
__label__machine-learning __label__classification __label__data-mining __label__data-cleaning __label__supervised-learning PRON have training datum that be classify into 2 category  x and not x  each piece of training or experimental datum have a variable number of boolean feature  each piece of datum may have 100 feature  but the total number of possible boolean feature may be in the ten or hundred of thousand   PRON want to train a classifier so that give an observation  which have a set of boolean feature   PRON will output the probability that PRON be x  could PRON recommend which ml algorithm could be use for a large number of sparse boolean feature   follow  up question   PRON also have a large amount of training datum that would be best describe as maybe x  PRON have not verify be x  but there be a good chance PRON be x   PRON could further segment those maybe x be into likely x  maybe x  probably not x be there a way PRON can train with maybe x or PRON be subclassification  in the end  what PRON still only care for be the probability that an observation be x  map PRON input into a sparse matrix and use logistic regression with an  ell1  penalty should work well  in r this be very straight forward with the glmnet and matrix library   additionally  if PRON have boolean feature with a low frequency  PRON can filter some of PRON by take a subset with at least some number of observation   here be a quick simulation in r of a lasso logistic regression with sparse feature   libraryglmnet   librarymatrix   n  lt 1e5  setseed420    these be all convert to boolean variable since PRON be arbitrary category  ls  lt dataframecat1sampleletter  n  replace  true    cat2sampleletter  n  replace  true    cat3sampleletter  n  replace  true    lscat1cat2  lt factorpastelscat1  lscat2  sep     xs  lt sparsemodelmatrix1datals   betaslt rnormdimxs2    error  lt runifn   ytrue  lt xs    beta  ylogit  lt 11  exp  ytrue  error    ylabel  lt ifelseylogitgt0510   trainfilter  lt runifnlt03  sparsemodel  lt cvglmnetxxstrainfilter     y  ylabeltrainfilter    alpha1   nfolds3   familybinomial    pred  lt predictsparsemodel  xs  slambdamin   typeclass    printtableylabel  pred    the classifier do not matter as much in this case as the underlying algorithm s ability to handle sparse matrix   PRON be also worth note that PRON can handle sparse boolean feature by map PRON to arbitrary numeric index and run PRON through a tree base algorithm  eg  random forest or gradient boosting   in practice  this seem to work well  
__label__time-series __label__rnn __label__prediction PRON have some set of datum in the form of year  week  sequence in a week  duration  min   title  start time slot  0335   take from google calendar api   eg   2017  2  0  60  party at mathews place  268  2017  4  0  60  itpm lecture for 4th year  20  2017  4  1  60  workout 20  2017  4  2  60  watch flash 21  2017  4  3  60  take PRON pet for a walk  23  2017  4  4  330 hangout with colleague 25  2017  4  5  60  re  arrange PRON wardrobe  40  2017  4  6  60  ai lecture for 4th year 40  2017  4  7  180 wash PRON car 70  2017  5  0  60  se lecture for 3rd year 169  2017  5  1  60  hangout with tiffany  263  2017  5  2  60  workout 320  can i use this type of datum to predict the good future time slot for a select event   like the next good time to workout   what be the additional condition or factor require to achieve this   PRON do some research and find that use rnn with lstm be the most suit for time series prediction  any suggestion of tip to achieve this    
__label__data-cleaning __label__apache-hadoop __label__apache-pig PRON have a xml file have this structure  not exactly a tree though    ltposthistorygt    ltrow id1  posthistorytypeid2  postid1   revisionguid689cb04a8d2a4fcb  b125bce8b7012b88   creationdate2015  01  27t200932720  userid4  texti just get a  pound of microroast  local coffee and be curious what the optimal  way to store PRON be  what temperature  humidity  etc   gt   PRON be use apache pig to extract just the  text  part use this code  gruntgt  a  load  hdfsparsingdemo  posthistoryxml  use  orgapachepigpiggybankstoragexmlloaderposthistory   asx  chararray    gruntgt  result  foreach a generate xpathx   posthistory  text     this return      null   upon examine the xml file  PRON learn that PRON xml file should be in this format    ltrootgt    ltchildgt    ltsubchildgt  ltsubchildgt    ltchildgt    ltrootgt   but PRON xml datum file  stackoverflow datum dump actually  be not in this format  be there a way the tree structure can be impose  what be wrong with PRON pig query   use regular expression  follow be a generic format  foreach a generate flattenregexextractallxltchildgtsltsubchild1gtltsubchild1gtsltsubchild2gtltsubchild2gtsltchildgt       this xpath will look for a tag call  lttextgt  inside a tag call  ltposthistorygt    xpathx   posthistory  text     PRON want to find the text attribute of the row tag in posthistory tag   an xpath something like this will do that  posthistory  rowtext  see example here  httpwwwxpathtestercomxpathbac9874ec344f9d8ebcfb250633aaf65 and click  test  to see result set   learn up on xpath notation for more  
__label__predictive-modeling __label__regression PRON have three dataset  let PRON call PRON x and y1 and y2  a scatterplot be produce out of PRON  with y1 and y2 share PRON same x dataset  or support    PRON question  if the two regression line be different in both slope and intercept  be there a way to evaluate if the x dataset have more influence on y1 or y2   base on the image below  this be to say  which y dataset be more influence by the x dataset   blue slope  y1   112  red slope  y2   90  edit  PRON be visible that an increase in x produce a decrease in both y1 and y2  PRON question could be interpret as follow  which y dataset decrease the most  give the increase in x  be the slope everything PRON need   image   what PRON be look for be the analysis of covariance  ancova  analysis  which be use to compare two or more regression line by test the effect of a categorical factor on a dependent variable  y  var  while control for the effect of a continuous co  variable  x  var    here be an example for carry out the ancova analysis use r 
__label__pde __label__finite-difference __label__eigensystem the computation of eigenmode of a semi  circular membrane reduce to the follow eigenvalue problem    nabla2u  k2u  where the region of interest be a semi  circle define by  rin01 and  varphiin0pi  PRON be appropriate to work in cylindrical coordinate  where the laplacian be write as    nabla2ufracpartial2upartial r2frac1rfracpartial upartial rfrac1r2fracpartial2 upartialvarphi2  boundary condition fix the value of  u at the boundary of the semi  circle  where  u0  first  PRON make a discretization of  u with  uijurivarphij  where  riifrac12hr and  varphijjfrac12hvarphi   i  j0dot n1  and  hr1n   hrpi  n this be a center mesh   PRON then use a finite difference approximation for the laplacian and obtain    begineqnarray  nabla2uampapproxamp   fracui1j2ui  jui1jhr2frac1ihrfracui1jui1j2hr    ampampfrac1ihr2fracui  j12ui  jui  j1hvarphi2   ampampk2uij  endeqnarray  or    begineqnarray    ampampui1jleft1frac12irightui1jleft1frac12iright   ampampfrac1i2hvarphi2leftui  j1ui  j1rightui  jleft2frac2i2hvarphi2k2hr2right0  endeqnarray  because PRON mesh be center PRON have to make the follow replacement in the above equation   ito ifrac12 this replacement also help PRON get rid of the coordinate singularity for  i0  boundary condition at  varphi0pi and  r01  can be all handle with the same trick  where PRON set at the boundary    ui  j1ui  j    ui  j1ui  j    ui1jui  j    ui1jui  j  from  uij PRON then form a vector  vec v and get a classical eigenvalue problem for a matrix  rm a  which be carefully form from above equation     rm avec vk2hr2vec v  the matrix be an unsymmetric real matrix and eigenvalue and eigenvector can be obtain with a routine dgeev from lapack   analytical solution can easily be obtain by the method of separation of variable    urvarphirrphivarphi  PRON be    urvarphinmsinnvarphijnleftfracxinmrrright  where  jn be a  cylindrical  bessel function of the first kind of order  n and  xinm be the  mth zero of  jn  eigenvalues and frequency be  beginequation   omeganmsqrtk2fracxinmr  endequation   PRON problem be that the numerical solution obtain by the describe procedure do not match the analytical one  the difference be around  r0   so this boundary probably be not correctly account for  PRON result can be see in the follow plot of both analytical and numerical solution   here be the plot of the analytical solution for the first eigenfunction   the following plot show the comparison of numerical result for three different discretization  as far as PRON computational resource allow PRON to go   the next plot show the  n2  dependence of difference between numerical and analytical solution  normalize as  l2vecuvecuanalyticaln2  in logarithmic scale  the slope of the linear regression be  05   which mean that absolute error be linearly decrease with  n  this linear accuracy be not surprising  since the boundary condition be satisfied to first order only  though other difference approximation be second order   in the interest of have at least some answer  PRON seem like the analysis PRON have above be correct  first  order boundary condition do affect the accuracy of a second  order discretization  if PRON be remember leveque s book on finite difference approximation correctly  davidketcheson can help PRON out with that   so the solution of the discrete problem appear to be converge to the solution of the analytical problem at the appropriate rate   PRON do not understand the boundary condition   why the minus   these four condition hold for  r0  and  r1  and  varphi0  and  varphipi  why   could PRON give more detail  please   have skim over most of the analysis and look at the pretty picture  PRON do tell a convincing story  the analytical solution do not have any circularity to PRON  but PRON numeric solution do  the central region be curve   so PRON would focus squarely on PRON finite difference scheme  PRON can not see what be wrong yet  but perhaps some part of the angular dependence of the coordinate be not quite completely take care of   PRON doubt PRON be the boundary condition  both solution seem to have zero at all the boundary  PRON could conceivably be the nature of the condition though  be a derivative rather than value condition  
